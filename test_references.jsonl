{"id": "task1356-bb5ff013dc5d49d7a962e85ed1de526b", "references": ["Edinburgh International Film Festival admissions up by 9%"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-546ece6b8dad4af9b757b96fece79e4a", "references": ["Helicopter rescue at flooded farm as rain causes disruption"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-a42613fc7f374a6dadb19ba4b2955f90", "references": ["Schools 'cut staff due to funding shortages'"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-677fb0b71a0c4c4f81a5057fa9214c78", "references": ["Intel unveils Project Alloy 'merged reality' headset"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-b886242785cf40bfb1f3e750acc84c85", "references": ["Croydon police officer shot dead named as Sgt Matt Ratana"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-15bae70b3df64c94b44150343994b4d1", "references": ["Trouble flares at flag protests in Belfast"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-bc8fbe651fa84a26b48c9dbf88501ad0", "references": ["China Uighur scholar Ilham Tohti on separatism charges"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-fd9f8f473e5044ed91076b7854d39f0e", "references": ["Hull MPs plead for more support for caravan industry"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-ae4b6e5f8b124df29cfb33c4250a08cd", "references": ["Security at Anglesey's RAF Valley base had 'broken down'"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-da230e8318024468ab9c5593d112b948", "references": ["Norfolk County Council leader Derrick Murphy standards case 'should be heard by outside authority'"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-1dfdac83155245c790fa7a964546caab", "references": ["Is the 10-a-day diet only for the wealthy?"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-0c21b82910aa44a9b4794923883542cf", "references": ["Swindon's Health Hydro gets new Grade II* listed status"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-e669ff934e3e40c9b1d000579f62f23a", "references": ["Writer and artist Alasdair Gray seriously injured in fall"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-9f37688798f04835ab758384c1fa9eee", "references": ["Ronan Hughes: Funeral held for Coalisland schoolboy"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-c32f2d3826804b78abf7909354dc6bc8", "references": ["Christmas TV: Downton Abbey pitted against EastEnders"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-07f24829765d462596fc91a4dc3997c4", "references": ["Ervin Staub: A Holocaust survivor\u2019s mission to train \u2018heroic bystanders\u2019"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-f352b932e1bb43e289e0a4990a847627", "references": ["Prosecutors given guidance on teen and elderly abuse"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-694b9a67d0c8402fa16e14d58d23fce4", "references": ["US warns Westerners may be targeted in Uganda's capital"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-f94f6dfe0e7f4188884b549a39c9987d", "references": ["Prostate cancer 'may be a sexually transmitted disease'"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-15b40da230bf47d0bf02525a217d49b9", "references": ["Hillsborough trial: Police chief said 'gate had been forced'"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-b4381cf723b1408da488361bd478aa1f", "references": ["Skills shortages 'threatening growth' for business"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-018e7314264c4d2fb6b980845687ffd6", "references": ["Peterloo: The man who ordered a massacre"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-0630e2139e6e45058f6362dc51dcc8e9", "references": ["Dog stuck on roof in Llanelli 'well' after rescue"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-8b211470ff5b402c9129bb12a8715b8f", "references": ["This 20-year-old crashed his car at 100mph while on Facebook Live"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-5b628018ee4b440b98fc28dede7ac551", "references": ["Father of British computing Sir Maurice Wilkes dies"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-77858837a2714d639325b48a9e3df800", "references": ["Tokyo Olympics: Why doesn't Japan cancel the Games?"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-d17c538ebf5d420db739b42c148fdd36", "references": ["Syrian town breaks Ramadan fast with Iftar among rubble"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-11ce898fa6154e748db86a5fb8441399", "references": ["First pantomimes cancelled ahead of make-or-break Christmas for theatres"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-dfede570174a4799b0f28331ee630e9c", "references": ["Pakistan woman stoned by family outside Lahore court"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-bc38622a002b4a4f801dbb9f2c9e6849", "references": ["Homeless Montreal man dies in cold just yards from shelter"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-c54e0d5ca1d949cea0ec939deba1bbba", "references": ["Man stabbed to death in Tower Hamlets"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-31d69eba5f4345dbbe913cdfea91c58b", "references": ["Two manslaughter arrests after woman's Tesco death"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-a489e41e47f9415d88ad1dc9c74784ae", "references": ["Harding starts job as BBC News director"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-edf2cdd2fd9f4200aadaaf7429eff736", "references": ["Bristol mayor George Ferguson defends swearing at member of public"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-ca98de79dc0147958d036c53212a2ce6", "references": ["Dr George Daniels: Blue plaque for 'watchmaking great'"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-272adba8c59442dbb25af7cd7c4032d6", "references": ["University Hospital of Wales heart surgery improving say surgeons"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-6b59a6e0bff24ce080954e2734d822d2", "references": ["VION buyout safeguards 3,000 Welsh meat processing jobs"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-1656249f09564c86b75727acbfaf20b7", "references": ["Bodelwyddan Hotpoint site: Reflex & Allen deal collapses"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-a20bcfd301df41e48601749a5abdb8be", "references": ["Catholics now outnumber Protestants in Belfast"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-1abae1a3f5cc40658fecf074de7c4016", "references": ["Public Health Wales calls for public places e-cigs ban"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-281b48d1bfb1467899f2e9ee1e39ac52", "references": ["Barclays pays extra $150m penalty for forex misconduct"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-e1c0ef2724c745d4839148c5f666400f", "references": ["Oscars 2016: Best actor nominees"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-9925d29b52eb41a68373562d31acaa0d", "references": ["Frances Andrade inquest: Lawyers 'did not know' of overdoses"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-e244ea58d7c64228916fa6f84477107c", "references": ["FBI investigating 'suicide' of woman found dead in a Texas jail"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-2a512705b83744378a34bcf260335682", "references": ["Andy Burnham's profile shows success of new mayors, says George Osborne"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-9bcd21ecc6634a8b9034157847b9b70f", "references": ["Chorley Council seeks \u00a311.5m compensation over Ikea retail park plan"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-ec550f89401643578cd5f54594410525", "references": ["Bill Gates condom challenge 'to be met' by graphene scientists"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-88898a0619ed44708cec7d832a11bf26", "references": ["Do the technology giants finally face a backlash?"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-24acb1b16e1b4655aa038c7e0d401e8a", "references": ["Carney and the \"no deal\" Brexit threat"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-5fd6a8780b7e4708a3a6e97396baed29", "references": ["Major rail works to affect Derbyshire train services"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-cbfbab5557504cd98b3e5696d4402667", "references": ["Schools: 'Too many' children and staff without symptoms getting tested"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-8b7ae26e1de8414dbfb342481191fa34", "references": ["Car cloning arrests in Leeds, Bradford and Bournemouth"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-7e236dd38e0c4f7b848c13b805c73450", "references": ["Romsey solar farm proposal approved at Hive Energy HQ"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-793ff45c99704b8e8cf2a52792b489f3", "references": ["Chief constable Neil Rhodes' suspension 'unlawful'"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-9264c671b6b04fce9c7280b96997671f", "references": ["Wales lockdown: How Wales reached a Covid firebreak - in six charts"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-4a52f9b10f724dc9bd2dc9e934290564", "references": ["Springtown Camp: Plans for Derry naval base monument"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-850ee5fc330d41f7babb1b75f4e2e93f", "references": ["Mystery Irish woman gives \u00a350k fiver to charity"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-52431819c1584545859b7b9c8269937b", "references": ["The health budget \u2013 under strain again"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-d3c169cef3ac4133b67e7b7a4d715476", "references": ["West Midlands gun converter gang jailed"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-a70cbc81230f47a3a997ce234f0e109f", "references": ["Strictly Come Dancing new professional dancers revealed"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-6c22ac3299b64bda8184fdb2139c99a5", "references": ["X Factor winner told: The hard work starts now"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-687db68dd42a48c48d59d2e89247a4fb", "references": ["London 2012: Will the Olympics bring more prostitutes?"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-c5de7c6512104b0ea59792fea5eca9dd", "references": ["Pakistan wins UN Security Council seat alongside India"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-864b69eeb71d456c8b7a7d3022811d93", "references": ["Petition over Peak District Stanage Edge 'sale' plan"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-2d775001aa004372b3b2815491a03be4", "references": ["Is it time for a time-shift?"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-ee5a64ed4b37457aa28c1e02115bb729", "references": ["iPhone X: 10 key moments leading up to the new handset"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-213fe939ac0f474787ab31f3a190dea3", "references": ["Biden orders 100-day review amid supply chain strains"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-32d78c7e0f874c54b9e2ffe8c7a03e81", "references": ["Jeremy Corbyn discusses Nato comments with Nia Griffith"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-3944bd9d72aa4ad4abfca4660216d4a0", "references": ["Virgin Media's rural broadband plans anger campaigners"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-b8431d7f0e294de4b01bc4578d00d01e", "references": ["'My first collection failed, it was heartbreaking'"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-22ce0515cd7a4075a5e634b9d80fc0dc", "references": ["Horsham Victorian photos fail to sell at auction"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-ebb7f461687943c7a5da19e95c5b757a", "references": ["Haiti unrest: Protesters forced back from president's home"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-8ca1c2783b7c46a38a18915ad06a34fb", "references": ["Coronavirus: Drug dealers 'stand out' during lockdown"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-e358632059b5465f9f7219cc6638e525", "references": ["Solihull murders: Man charged over mother and daughter deaths"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-1c99f8516dae4c898b424b5e0503bba5", "references": ["Frankfurt is winning the battle for Brexit spoils"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-645df7998cc14ed6923f2bd40626841f", "references": ["Storm Eleanor photos could improve flood warnings"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-5dd717816a7043c49036d4a2e79d1c7a", "references": ["Driving test changes bring in 'independent driving'"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-4a9b54f8f81b41d3a94301cc9ffb14fb", "references": ["Slovakia journalist murder: Italians named in story detained"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-9855924b49ec438896fc5d64a8049393", "references": ["Dorset Gypsy sites: No new pitches until 2022"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-b99f541f1f1240eda503553f00a49c29", "references": ["Sandbag wall protects 18th Century dock"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-895ea78cb3ef42529e2a8e4e111176d0", "references": ["Billionaire Koch brothers take on Trump over tariffs"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-6f7e4af196db4cf79d5cb1a9a3166355", "references": ["Leicester explosion: Trio 'wanted \u00a3300k insurance payout'"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-5ea6af2dc50e4fa7aafa19496d885001", "references": ["Sheffield plans new retail scheme after Sevenstone scrapped"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-db0460a564e741979f1c395f8b0835c5", "references": ["Covid-19: Palestinians lag behind in vaccine efforts as infections rise"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-a500543d32b544f9adc0fd512f4779b6", "references": ["LTTE recruited more child soldiers than released -UN"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-9372479dc8d54b94a419f6a0691ec189", "references": ["Didcot A Power Station switched off after 43 years"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-b48d8a2d35c142c1bf3e36e8a5c8658a", "references": ["Everton fan 'Speedo Mick' sets off for Wembley in trunks"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-f495a0a3369447d6997e3e4a7f030334", "references": ["Colin Davidson's painting of Angela Merkel makes cover of Time magazine"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-756d39fb9d2347fbac5761bdb66e88f1", "references": ["Covid delays Hull A63 footbridge opening again"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-04a36c0c04884b0b9bf38791720416ba", "references": ["Is Westminster culture really changing?"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-36939f2792464fbeacc706f6b563dc0c", "references": ["#UhuruChallenge: Kenyans mock president for launching projects"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-4003d3b4bbdb47b694b7113a97a0f579", "references": ["Yellen warns US financial conditions have worsened"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-7f9bc1a22cd84ec09d8a5727b8dd448c", "references": ["Shell profits jump 77% on higher oil prices"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-acbb7e21e1ea48d2a8b877b6f9a40947", "references": ["Severn barrage: Pre-election consent not likely - Barker"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-f68b3551753448c38025dd73dcecd5fe", "references": ["Love Island: 10 best moments from 2018"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-69692e43f8bf4a2f8eec324561d5556e", "references": ["Jersey care home abuse inquiry documents handed over"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-627d40fb51b24aa08189f86e59681eaf", "references": ["Quadruple amputee given double hand transplant"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-0a5600e757b04267ae30088c76b66a92", "references": ["The female bikers heading to the top of the world"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-96d5584624554c62accd0dbe2e36c611", "references": ["Welsh election 2021: How to organise a national vote in a pandemic"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1356-66cc4595bc5f4fd3a543b6082b2726d0", "references": ["Man v machine theme at New York's Met Gala - all the pictures and tweets"], "task_id": "task1356_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task893-90465ee36bf64b2ab90d06e7f4f558bf", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-6189a4b41cc8402d845778b1ebc2b901", "references": ["him"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-b3571f0998e34ba7aab3b2a1e2496dd2", "references": ["Her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-37d488309f3946278ec1f9702f8fc5c1", "references": ["She"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-711cca8536de47038dbecaff47e600f4", "references": ["he"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-ba9efb0501214ec9a172cd9c77828160", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-cf44c13ccac34adba88eec405736851d", "references": ["His"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-3b21a383565e4a4d9b9709d4248cbb05", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-ec246322814d475185ab4ea190f8a953", "references": ["she"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-7d8d4f3d6b41404596b2ed67bf622555", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-fe581bec14114f0faecfbc2922397226", "references": ["him"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-eabd93aeae5049f992ba4da6037d59ea", "references": ["Her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-f540dea37b29417ca65982a886fede8f", "references": ["She"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-0b987caa57854c088e4f223f912f5092", "references": ["he"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-6034de52db274de4912bef5edea486f9", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-f2ded5ec5d614513afd6954376119e49", "references": ["His"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-d3773fa3ed27487ba7307512555507f0", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-5ab24ae8b83a4ec68e0e71b78a387b40", "references": ["she"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-9bd5ae97988d4da3bf3fffb1696be100", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-17a0ba63ac994aec8e48d9075079f46b", "references": ["him"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-e17b280cb4804f09aaeaf28a666e4dd8", "references": ["Her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-d5eab067d1e64157af0fb79f6b95c76e", "references": ["She"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-e8c310d5ff0e40188614bed187b7a0d6", "references": ["he"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-24cede189a1b4d3f83b27c9ac74e51c7", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-4a9f2c9fb3be449e87b0144ba146a24f", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-4426592a16014753bbd5483bf2acbdba", "references": ["she"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-9df882fca9ab4879b1dd1c8d70043623", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-ea115d0e2ce0464fb282c4ebc445867f", "references": ["him"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-b01a8d4d14d24144a89e0f37fa8f458d", "references": ["She"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-ff21990d35d442e79e730b1b7dc58a00", "references": ["he"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-b3d90309f8484986be37a67a9d891a5b", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-8fb4c4e9869e47d6b781bbe7f887ff17", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-97a97ab31acb4c419d0a948c63300cf2", "references": ["she"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-d7cd92b60d0740ac9357f777cb2f0a37", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-7e2ddc6dffac42c49cbc764ec8ef0700", "references": ["him"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-9fb47b80c2c04ce695eff54505dd5235", "references": ["She"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-6ee58ebd335f4bb2969b9e6008166c76", "references": ["he"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-d012cce9ad004cc3ac6e27a318e243d7", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-de97073c037f44ae824e1c5e26d87acc", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-f7e15eeda8564b94ad6b79a49ce01c0b", "references": ["she"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-892a64fc8133468fb844a409e1b76148", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-3ba2b654041d44edba63d7a5b11893a7", "references": ["She"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-f907557baf2f4d899dbf205f3621692e", "references": ["he"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-ffad8a5c67eb4f46ada4214575b784c2", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-87534b7f8415442daa69c7bab2f75140", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-15674678300a46a8834643c0b8aa1fae", "references": ["she"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-685b2344bf9f4442925de8cfb8b5114b", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-5e4c0490ee7943ddb52d5b7390a16dfd", "references": ["he"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-8def2e3cb58d461aae0d2a50f9b56946", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-f255b0fddbe641878ee382e3b0dd3924", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-3721cbd619c34689adee80bb96e61963", "references": ["she"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-5ff7be3ad25e4a49b867390ae91f0edb", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-9ec8e1ab68424f4d99e579246df6c277", "references": ["he"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-65a2961a4c30401b8ea06a5757098c80", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-7e8e504c73fa484091444d65dfabdbde", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-efb59a96da074f2ebdf6c4fc76b99c7c", "references": ["she"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-6b601f2dd3be4ffa8d743683c12216dc", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-120196e94411498ebc88ce918dc35355", "references": ["he"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-9a8aad7edbf4495eb64eed462d616e43", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-747bb3946b3b4590bf1552a343e0995c", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-9ba2164850be4f6db6cb55494d12034c", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-88eaae6f0e324d7bbed038ace734ee55", "references": ["he"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-bde2d9a48d7e4530b26d7c81f83f7afc", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-1859c087df4e4aa281e401c19feee765", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-ac82bc8ca70547b4ae2fcb9d179a6e54", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-298b6b41bf9a4d9a816cf195ca017892", "references": ["he"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-84fc3a21c21547ffaef1e71aec6741f4", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-decd77e667c64099b361e7f2d38b54e0", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-6d546733d31543fe805f72e0f90cd2e9", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-62d3ee99feff45fdabb8789cb5a18dbc", "references": ["he"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-9bbcb6e40a564dceb00463b82698f03f", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-d94047064b1b47debd901e347d8072ce", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-676501352e74450d9be7f58006929e36", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-7769a12b1f3e489596c7daf7679dbce8", "references": ["he"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-2069de77809348e883d2db6d2748878c", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-32bb54258f544277903ea0a5826c69b3", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-8a86c22d7fdb421e8f387442b4854ff8", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-87edf6a1ae11415985ecf1fb570d51ee", "references": ["he"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-d869f7acd08d442c9560fd76a70a052c", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-778b2ae70deb40b080cdaa76cf0175d4", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-1d2d52f6e83c45f4b719b04e0d31c055", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-ec4c5e6e5e724c54883056c897ab31e1", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-aaba1179ae3446c79f30fb91c356b26b", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-29cee4b72f2847c7812b8357e2f1cfd8", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-c1975ee0b4b6460caf4ccee3f7e9ed35", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-4546fffc41bc4265a27ab818df943011", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-bfba7ebb176d4088bb6f69429435bf6c", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-eaad458281b54884b9dec542d9cd2c58", "references": ["He"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-00e48e2f780b430fa95e3dd8a69dc40a", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-2900573ae27b4e88961c81cef4c3d0e9", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-446fc4f52e5c4707ba7730af63e2e4de", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-e11a0f8ff302460fae023c162a140a44", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-866a44cee4ca467c8d67dc3897fa344f", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-163f4f7aed56472d8fdd21efd0ccf868", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-25b2881eaf69479ea28f26b59fac54fa", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-f665533a5e3e49558351d59785a1de66", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-a0faa028cbf04de98e97de0343be6508", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-f35f83b6bc1248e691f166b5ad20d21b", "references": ["her"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-2ac447e2fb2f4b928a62a2085d57331d", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task893-11d472ef2c3f4fbcb435ed66e7057239", "references": ["his"], "task_id": "task893_gap_fill_the_blank_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task641-4f05eccc05d242438134f373c8948029", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-8c1796e375164884b31585e58b4586db", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-cabe19c2410247c3b0bb6fcaff99ada9", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-5be253a87ddf4adcb5381b22c83db4ad", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-8eb0dd23ca204f90be82197af798d963", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-0c74fbdd94ba4751b3dac0f350baa201", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-08e4d5655d8d473fbd3dd51d4b531500", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-701f3622333440b8aa4ddaa987ab2d0e", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-6d2a280482d34cbfb0469f3d077cbe28", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-cca37e9c293c4aa1baccfca63e1e0bb4", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-0a4f23cad3c24dbba987522248c74428", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-c391ff72dfe343a688b4e698d238adb6", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-567ab787fece49daa3b8d48bd6b48d42", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-1f08be41011b41d489df2df6b9015ed9", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-61cba81fa7c544179d9cba50def8721b", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-86e9e98b059342ca8ad3a79de7c35c5a", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-67a775223bab42a5affc29fc19d97197", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-fb29e860221b4c819a2837843eab89d4", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-ef6199615a60499bbf5a3d5a0af72c5e", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-d78ad7e3d20948ec84651c6489dcbf30", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-3bcc7869735c4990bbdb1393da6bcb2f", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-2cdd0d0a4ad945a29486bd164b1a63a6", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-c8874490cf004a899391de85f2360cbf", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-11e8aefde4784f2ea459798f3d42d63e", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-2b3220def9a846599aaabfb0d456d0d2", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-03bbb569b67845ba912da955da369e8f", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-f48958238ccd4a6bb3cd1fdccc9ee8d8", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-a5608f7eb25a4e22a6e6445c1c4f70b6", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-1e07376aa17d48f18e7cbdaf4020dcb6", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-e989a04175654cb7806bc45b264b784c", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-d0337cb7c79e43ec99f364a5bd46a92a", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-07b7a7a5f2514b39930ebabbda481200", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-63735aebd92d4131a59cd804a3fdfd7d", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-9d74e9cd15b34d96b7cefd6f798fe99a", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-f96184c1f427421ea4fca7e949f33f50", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-fbce08ead14e4696873691bf2f4035d8", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-46cb2be263ed4396a8b40230f4961247", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-8d23c3f45e0d4cf19d6b2eca63df98d9", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-7d556f951a6a4469812c1815ed60a701", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-1aba41a5b2ed43e2ac0d228555f77062", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-8f69f5dfbd0d4835b57c8700328d8f12", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-8cdc7f42172b4eabb930a749913fc04d", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-095a3407937447a1aa4de199971f28e9", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-ab80314a490c4b078a6d29fd759312bd", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-304100e4e1e84aa9af5de78bc83f0668", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-b8f941e98ef940a6a3eb3807e3899f40", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-2f64c06ba3c142828a017148a8a47689", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-cc918800a2d3461784772be6fa410ee0", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-96c55e6d92434deb899efa119f5c0584", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-4ef68946e86f4f3c87314e365606af0d", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-9900ec268b7c48c59d367fff24299ff9", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-37a2378da461489589839db4e87f263b", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-f5a612cafb8c486daf9e1c95996cb8b5", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-385ac3b418cd479ea6818ef997107f7c", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-d8b3d4d36a4d4cd4b9553b7a49a8df2d", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-dd59accb08d74aa5bc744492f4ae2574", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-74144241a5474fa19aa9edbf132d4c42", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-ecbb3e6a9642489a98d6a64011c89442", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-61637da542b34e0788f1521115d972c8", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-4e482ce0d9e34937b7066ec42d90fa6d", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-260eeecb77fa4069b397b82dd9a55611", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-661511708ccb43618cc2651a4fe1e56a", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-9bc461e54d6e417fbd4a2e62aaf4b6df", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-509f8af39b79438889e67d0348a73a0a", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-8639e3d12c334d169bcd79aa7af095e7", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-6abb805ae0224736ace96be9d95972f4", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-c670b4c4e97d4fb5a9123d016ebc19e8", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-bdf479d70bac46cb9c0a9f6e63a88892", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-5ec53b273d484caca0345f1a9833490d", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-8fd9836a09754c0ebe835539a9e2f98a", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-7fe7d18a8b4a4ea7baaac658e0d4aba2", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-163b91797e6b4fa9aedbc6ca39d508f4", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-b6fa43a90f7a44e581d3d9d981487331", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-4a11e5af393e47478c4cb31cc9d2fde9", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-924aacd8515e48a6ae91a81521ecf577", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-7738cc0de37848c49dc7cd9f27e1d1b6", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-192aed3d269f4dcf906e7469240e10f3", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-b05443ed2434472d86f6e61317ded865", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-16d7378f479d47ac9c0a95d0d03da5b8", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-4b15fba7c72c46cf9e3e69a20efb1588", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-8d1b8de83f974c35a5ca421601f6679b", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-8cbda9f387824236948e2febf40afd8f", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-3f6396b5b0214288bc087ff7d9e58d16", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-c59309d379a9469a9622cd7e17976297", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-a7b8f5133ec342628f1dab06f99c9aff", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-3777f8a3888b4365b2505da440d0be54", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-c2f1a5a4a24f4ae6b0e3d8f872065c2c", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-bf5affd78d334f9f812826a997f3b46e", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-e7a9d86433b04396a1c78dbe18bc61ca", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-e66f71e1778e4339abde20827ff08717", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-b8e94fe659d54c98ada2c6d764b3b308", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-3081825c718e4a69afdec7a2ef7894f1", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-ff1604f2e16043488539f822950f3889", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-278925f99db14b34a0e9d191e36e31ce", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-bf745036fe4c430f85a3353cd379bd22", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-968448a994ec45939dd76ac89546e429", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-50ab6af58e2041a3ac9228998962ddb9", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-e38978931f1847f286401c0332f2f224", "references": ["E"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-975c08dfa32c4b9bb07ead379cd5db0b", "references": ["N"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task641-dc694e2abe904d59a948afc48ea43e6b", "references": ["C"], "task_id": "task641_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-58da941adc644889b71714e587c02459", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-cc16aebc2e454860afd5899d33c2b4d9", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-8ef1ce4753c4410fafb2690f0e4ee482", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-22ac93689dff4a08b9a04f55a6650d24", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-d831ead1fece48c5928381341f037ddd", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-629c52fd633a427da1323c5f7bf8760d", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-d1898bc6812f4ac298d52ac8e1a8c89c", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-30785659d48c4e92b503489acc9c2e90", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-61d8bf3c6d454ace9427fd3197ae8888", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-7bd13dd3af6a45e08e19c0bb9c08f28e", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-5ef1035cc4c84370b78a3ee002cab76a", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-c53fc902c6354fa9819559d8012f0d7f", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-3ef6bc0056654f1aa09e2ec75c988c83", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-54d976fedf88471ab29d8263747b28ef", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-a69acacadc204236a0d88525c4c7ce0f", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-6f61660204c14c1b92adaff44c118d7b", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-ba01486b13af44e48590ceb7e20073b7", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-b5dfb97dba644d32a6fde42b8d9b02d5", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-f2f5b3ecfe4a4d4ca5947f07ee6e4292", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-b98f8b73560a4559a252543be126f83e", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-cfcca2c5815548ef88744f5743f1f066", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-d060fdb144cb4e22a97fc82963166937", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-b8db4b3ca0df4647a7f915fb2a8f62e4", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-a8be59cb76d34ce0a8b248f2329470f9", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-2d63b7fdd21d4154b77f0cb710873986", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-28925203152d45a3a1272bf3917a327b", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-9b0dac250b1e419aaa1ca37645c2ca2a", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-758d2e5a6e014a4f8c45e149a9048571", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-8cb898ea56d44680801ca4da688afd2e", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-71a444963c2645aaa36932a5fc154e1f", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-00438e99af3a4fc79a6dcc07697f6ee8", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-88023ffff8b6467a86e9b191e5cfaee6", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-053632b1fe1445da9314d6bf7ecc80fd", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-54174dda81a14f778667c573a9633485", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-cd6d722571244d62b9c8513a0757fdd3", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-12246d0192884cd093232122839ca26c", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-301997a212414e94b7a4f71be7e0bd59", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-19708d064e0f4b9daa21087ea99df0e4", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-a5eb01b8511a48f58940a9aecd706f5a", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-2e73d3a0c57749ba9afdb53eaef0f166", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-39eb22fe817a4745806482613d7ad6dc", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-078bee99f5cd44ef8821d1e73ba0fae6", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-ad98cc6640954b1fa579e6dd101d2de6", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-ab0906a940aa4e4893eefa526a247f1a", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-c13d95df224947f9b688e7430975e5f8", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-6701123cfbc64b609313330e2eebb7f1", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-5dbb5c10c90048feadf4ddbace2e4823", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-a488e3e9329d4674b94203ec9d2df17e", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-c10b8415a7334dcd8c809860dffb6feb", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-e31dff8af3e9490aaa96ab1ec49b06c1", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-1b487ac7129b4b97a0a6960ccb150cf1", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-21960046fa6741c493d130dc02f0c61c", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-76332d01400048b286e3078d18c73978", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-81a5b58f6f9948adad3d36c7dbc2fe14", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-08a5413024404539a66bed9c54c74701", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-c8fdbc3e8fb3457080d4360b8ceaaea9", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-b263601770984473bf14a7cbc1285041", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-6df4ee15287d4632874460bba5903e06", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-8b38dbe1b19e471e97287504fb0d54a8", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-bd37b7561456457f9c426ca6a9a51966", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-08473b4e36514235b8823604e61ffb1b", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-75879b434a9b4c449509a44b5d62d582", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-36facbf633bc4838b580e22cdac7660d", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-46027363aef04200bd839ca2fbae3a33", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-2568469ce6e8416c91ed5eb3cdad6176", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-99425e8631f34bd899542daef306be49", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-e84f2923c1314cddb1fdb7e0d9c7bd98", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-f5a79d312ce545b2bc9330045af95c36", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-61e85bd0ea7146ada6e1fb19def499da", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-5c2e2a4847df47ca8715428def18528c", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-8469972e0d914c92beb2690fe99beefd", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-3facac42db2044729e4615ec3ece2f5e", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-9ef68d5234744fddb20b8109ce8bef1a", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-aecda4d3354b430a8882cc8169264b37", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-79337747083f4522b5d5f721ec20859f", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-66028795f5ac42c79422beaaa4672536", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-654d5645caf94307a79ec7e4db856e99", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-ec2624ceaf31492a999a9396b45fb9f4", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-ccb43f44935a4ddbadc4a749b68e3af9", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-e7227c91e5e843919c2388ffd5acefe6", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-dcd5e169f95e41c7ac7c9b579b7c7de8", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-94dec610c0f14f3981e6f305a2f8b685", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-a6afa5b5243446c4a6cd9440ef9be170", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-d7b9f3637b304aed95126a4c8e750ae9", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-a1d292574ac54b469ffea0cb763b17e4", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-9f95b09d133b45ef9818c41108244300", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-f51840beda384c45a4da2c735edb7292", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-0e63082fc28944ac815bca61c54c19bf", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-986021b2ff4f4ae8955f44868eb53a4b", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-76848e73eb3c49dbbc7d6c8dd9b00e98", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-6a9865ca2b274301bacfec98ad5d73bc", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-1b86de26aca241ac84668cc33301eaa7", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-8c568aeaf53545f9bdad9ff75dd87e44", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-91dafda357f8493ebd724423b2d64685", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-ffe62f42262245d6a95711076314e2cc", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-f5eec9ee73bc43bca94d61e484a453bf", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-a73756dea83c45be83febca14fe6324f", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-baa174ce2a7348748632c4bfaeb11a62", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-eb01fa9e817b4e92be149a9a48e375c6", "references": ["neutral"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1529-7242603c618e41acb3d8979a94bd41a2", "references": ["entails"], "task_id": "task1529_scitail1.1_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-f6e0681180324f59a9ba2b4b7d03a895", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-d4143da0995a4344a34d0676fc29b040", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-c77ee7df885b4c0883ab1be0b6e91475", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-3fd1c9ad36714167978f2dd84e135485", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-850b360fb2f1491b830b0b8216ede2b8", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-4c915817eadf4dbf86f7d71478d63648", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-2b0a5b8402e64acab036fc831f59dd75", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-22a562a248354fc59526527d3414ba3a", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-f7955c9933e84b58a685d649a665e937", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-1daaa0a509d14089be3cca453725e8e5", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-7d0a46f541844c7dbfb63c9d686aa922", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-0f3edc17fc9b4a4eaf4d439aed628387", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-326a9f90d15a4a378201f4ce10c5d5e1", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-021bf6cbf7fb4de7bafca533f6ad4c95", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-77f6868a8e2242ad948715a46b25350f", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-138bc2afd5644f59b609f6b75c981986", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-1861bf0624f24fba8fd491db2f3c793b", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-f09f8ef6e07b4c0aa1f38d09616a6b30", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-1c1139337bfc4e5c83189f501add33da", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-67417f5814134c3d9270fa01c7a090f9", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-c0c32cc65a1f4240bc0646c1536d0ae7", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-59e69db7c4464aa4ade00666adb850a2", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-b2ca43a7342448e8b533d6ee12b635a8", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-62921ebb122640369e408eab06ca8369", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-5e72c84442d4480093e66d916f8196f8", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-e33787657c47400695eb9a22aeb30927", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-69c04637861f4553855a230bbcb9d13b", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-aa12486d5d9b469a9ccdc551970eb74c", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-bf30dc767ce04e4f9746b70d88afe840", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-b698ff3adc6242df8afe22f78bf2e4eb", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-3ab1bbac399540b78e1781b273b62ae4", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-2496e3bcca534c93b0ed326a1d92491a", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-11184e8bf944447297646c5ceb46f05a", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-7dbee882cf2845378d30bd55ff4c3430", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-7432a696210b4e62a70782edce673e5c", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-b6de5c67429649cba2a9394fb57f9a5c", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-9a68b13d61fa4eff916b666f52769a1b", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-b4a3c19036a748fca06f93a4df246360", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-df230bac24404c49b198f0933bb675e1", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-b6d4341d2fa74774ae577a3e2bf5809e", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-c7313d8679f844488a03bd25e5e99911", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-18f9ff80d9dd41c5aa5f3283f72b200a", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-c5609eb0b7ed4a66b77929b56f8056e4", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-4ec58f4a37ad42d9b1c28aa2a551dd4c", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-f2773a8cb01d4b1abf7adaf1134ba770", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-15937b7dce704046aacbb0b0d39772c5", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-c328caaabc764d07afe550829fe458d6", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-f78a34f731574b31a14519b66347beda", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-641277f9597b4e2ab6026a7bf9361054", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-f6b1fd5dce1f40c28897a7c0ecc58330", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-d790abb252cf4837a3dc9663814cc91a", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-8e5ec923517442cb9e73c7ab2b64d237", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-6d258dd74a7241c6975a27cada8f7928", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-fdf9392f6b23487289c88cf9b4607cca", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-35b5a86646e24c1fadadd7c54b44f656", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-c61d7aa252de4c2f8459ca527cf3aefc", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-fb2a23f985ea444ea15ad66b49a522a8", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-32202e337a03420cafa0accdac09fb5d", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-5474a1d36ec14250a257edc754443153", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-6ff263a514f14daa870e3c33926b5376", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-04c3cbe19ef24505a8baa7bf6a455ca4", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-9b44a2133c3149d690a12650f5e5f684", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-002a8e6499b5451e913b117c6bdffe96", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-d8331c9f632e4bf89dfb7ed4e93d9c4c", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-8813fb99ab3f4eceb2dfb800655f3a1d", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-5c7c0bbeb6074495aceaf8009d922478", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-0349da7934774d1c9bc79b0c6095b1a3", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-47fef44682b6443cb7161f89c0fa8f96", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-1fb8eca84dcb42feae7ec30f217feb57", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-a0760db2bfa745aba2c86c75ff7c5d6e", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-e4c567bb208a4eff9b15f3b3c41b4ce9", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-c3867388c7ad4d90a74257000a1e4273", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-8f63443c068b4b63a528fca6be05b447", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-3dcc380dc28a4b579f6067985e9db0a0", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-20d562914fc2433cb314fcd2a322fddd", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-d897c4edf9ae40b196f32f22f7cf7838", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-33522755e5fe44959d25975a21c85352", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-d3777d49489646b895f0b7b423d94ae0", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-82ef54d4f941401fa0ecf3c88c1dc0ce", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-9f24a64c62d14269ab003b6a7ea171f9", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-f1938e764e3e47f5907a4ab6fcee2b9d", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-8d1c965d3d2544bfad564b51058e332a", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-16231b335ff441baa15aafb36b799132", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-b8bc11a2ed0a4af398621ad78503ae8d", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-9ffa89249d3e41ce8df288e7dc60d5e8", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-54cd52790c8449039ccb2b301a13985b", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-fbe2e123612843f8a07d88aaf6775c3d", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-35940f540673458480a7e4edbb4e55ed", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-124be085f19949bdacd86be2a5166cb0", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-019b984dc1814be5b98ed0e60c6619c1", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-1969c8a7b5fe427dbb8e7e5e7a1b0088", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-edca54d2357b4981875d0b18e0723654", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-ac8a7c75e9f842888651121c0302fdc9", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-5f984716cdce423480ccae087c92d1ac", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-9def690299ac4dcb91c3c27a1353a8de", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-1175fc1d007a405f866a68c8f351b1f3", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-94ac5c560bc44eaf834d12e11ff101ee", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-11d192efd55141d9bf6e31bd36158e51", "references": ["2"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-aac3c0c8e5d64ddbb163c9f2df4a8887", "references": ["1"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task202-95ed183bd0f146cba3bbc56f9a03943e", "references": ["3"], "task_id": "task202_mnli_contradiction_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task670-58e9036e269e4233a66b4e783db6e3ed", "references": ["When did the Simpsons first air on television as an animated short on the Tracey Ullman Show?", "When did the Simpsons first air as a half-hour prime time show?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-62b49367714c4d05a48681d23202f57f", "references": ["What is the legal age of marriage, without parental consent or other authorization, in all but two states in the usa?", "What is the legal age of marriage, without parental consent or other authorization, in Nebraska?", "What is the legal age of marriage, without parental consent or other authorization, in Mississippi?", "What is youngest legal age of marriage possible in some US states when circumstances permit?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-db8a92b48654463a98082411f557c0a6", "references": ["Who starred in barefoot in the park on broadway as Corie Bratter?", "Who starred in barefoot in the park on broadway as Victor Velasco?", "Who starred in barefoot in the park on broadway as Mrs. Banks?", "Who starred in barefoot in the park on broadway as Paul Bratter?", "Who starred in barefoot in the park on broadway as Telephone Man?", "Who starred in barefoot in the park on broadway as Delivery Man?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-2f1d9ad07aae4c78ac234e3d975a97fe", "references": ["Based on the initial thoughts of the project, when did the Manhattan project begin and end?", "Based on when the project was being worked on, when did the Manhattan project begin and end?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-6e57170729414c02b678b1d55667295e", "references": ["When was the last time UGA won a national football championship?", "When was the last time UGA won a national gymnastics championship?", "When was the last time UGA won a national baseball championship?", "When was the last time UGA won a national golf championship?", "When was the last time UGA won a national women's swimming and diving championship?", "When was the last time UGA won a national track and field championship?", "When was the last time UGA won a national tennis championship?", "When was the last time UGA won a national equestrian championship?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-28b77fd4c2e0452f8e472cb0b2225df3", "references": ["Who sing play that funky music white boy in 1976?", "Who sing play that funky music white boy in 1989?", "Who sing play that funky music white boy in 1988?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-63c668596d924f85843d0909f9f3fa08", "references": ["Consubstantial with the father in the creed means what in humanity?", "What do translations  into English often say Consubstantial with the father in the creed means what?", "What do some English-speaking translators and authors still prefer  Consubstantial with the father in the creed means what?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-3b383615295a4cf8965e333ff37cf7e8", "references": ["Who was the voice of Kaa the snake in the 2016 film The Jungle Book?", "Who was the voice of Kaa the snake in the 1967 film The Jungle Book?", "Who was the voice of Kaa in the Jungle Book TV series?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-8dcc224dd9e44755a2f6ec466a66418b", "references": ["When did the han solo movie first premiered in Los Angeles?", "When did the han solo movie came out in United States?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-2b45d4dcafc145c6b2c955a546e050d2", "references": ["What is the IATA airport code for Abu Dhabi International Airport?", "What is the IACO  airport code for Abu Dhabi International Airport?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-4646fd4da069433e89f0e69987391593", "references": ["Who sings the original recording of \"You Don't Mess Around with Jim\"?", "What character sings \"You Don't Mess Around with Jim\" on Stranger Things?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-036da92fd1fa43f6a85221c8731889e3", "references": ["How many ligue 1 titles does psg have as of 2017?", "How many ligue 1 titles does psg have as of 2016?", "How many ligue 1 titles does psg have as of 2015?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-8bb246670a78410cacff90312b8bbfa3", "references": ["As of 2015, when is the next time Easter falls on April fools day?", "As of 2016, when is the next time Easter falls on April fools day?", "As of 2017, when is the next time Easter falls on April Fools Day?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-76ba3fbf2c7c4c60844e5d0bf4d8623e", "references": ["How many seasons are there of Star Wars: The Clone Wars (2008)?", "How many seasons are there of Star Wars: Clone Wars (2003)?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-3709551efb4d45c2b0e6cf0748abcd6b", "references": ["When did construction start on the national World War II memorial?", "When was the National World War II memorial officially established?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-592826ec6115425cbfca078c01898ddd", "references": ["With what group are the german die br\u00fccke artists associated?", "With what art type are the german die br\u00fccke artists associated?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-be807929087a40a7b1f57499121fa903", "references": ["What is the scientific name for all red foxes?", "What is the scientific name for a european red fox?", "What is the scientific name for the red foxes in Alaska and western Canada?", "What is the scientific name for the red foxes in the rocky mountains, the cascade range, and sierra nevada?", "What is the scientific name for the red foxes in Sacramento Valley?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-518c5415b002448090a525639b181e84", "references": ["Who is the \"father of accounting\"?", "When did the \"father of accounting\" live?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-d225d247ad12453a8deefce297408c24", "references": ["How many jury members deliberate in a criminal trial for serious felonies in most of US?", "How many minimum jury members in a criminal trial in Florida?", "How many jury members in a criminal trial in Scotland?", "How many jury members in a criminal trial in republic of ireland?", "How many jury members in a criminal trial during WWII in England and Wales (except murder and treason)?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-e6d31ddf22254ef19c5e789f3e93fa03", "references": ["Which team has conceded the most goals in the premiership?", "Which goalkeeper has conceded the most goals in the premiership?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-372ff079e74f402dbfd3eca6f86aae7c", "references": ["Which country did the 60's scoop take place?", "Which providence in which the 60's scoop took place had an Indigenous transracial adoption program?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-d7123323920448c080e152cfc540c705", "references": ["What Englishman invented the process to remove impurities by blasts of cold air blown through heated iron?", "What American independently discovered  the process to remove impurities by blasts of cold air blown through heated iron?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-68e59f12c7c64313998db8a4b338a311", "references": ["Who played lionel in all in the family in two unaired pilots?", "Who played lionel in all in the family from 1971-1975?", "Who played lionel in all in the family from 1975-1978?", "Who played lionel in all in the family in 2019?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-ade908af67a84501a2dd1f3698d10659", "references": ["What setting did the Just do it saying come from?", "Who coined the saying Just Do It?", "What was the inspiration for the saying Just Do It?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-fa7d4881a7a14981833a9ed3423e6951", "references": ["When did Brazil bid for its first World Cup?", "When did Brazil organize its first FIFA World Cup?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-034a423b42ab4a198a512974ee70d4de", "references": ["What ideas were used to justify u.s. foreign policy during the cold war era?", "What doctrine was used to justify u.s. foreign policy during the cold war era?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-1b1ff94999964bf997cc3089d11b0db9", "references": ["When did The Sims Mobile release in Brazil?", "When did The Sims Mobile release worldwide?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-9c968bfb043b4ff8b11ce0f136c26598", "references": ["Who is playing in the peach bowl in 2016?", "Who is playing in the peach bowl in 2015?", "Who is playing in the peach bowl in 2014?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-1f9daf3f4e32400dad5216d589384dcb", "references": ["When did ireland rugby first win the grand slam?", "When did ireland rugby last win its second grand slam?", "When did ireland rugby last win its third grand slam?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-37a7982a3ae346728f7b798af9d9da81", "references": ["When was the second time eagles were in the superbowl?", "When was the first time eagles were in the superbowl?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-d21fc8d71bc645a4a907ae08abc7ba0a", "references": ["Who was the first captain of the Indian men's cricket team?", "Who was the first captain of the Indian women's cricket team?", "Who was the first captain of the Indian Under-19 cricket team?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-d9f17a98b3814d13b5610987520120e2", "references": ["In what book are the ten commandments first mentioned in the Bible?", "In what book are the ten commandments mentioned second in the Bible?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-d9a3441303db400fbf47667ed912d595", "references": ["Who became the minister of agriculture in south africa in 2014?", "Who became the minister of agriculture in south africa in 2009?", "Who became the minister of agriculture in south africa in 2006?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-4816376e14ac4cdfb7be49a5e96dc355", "references": ["What day is the new star wars movie releasing in 2017?", "What day is the new star wars movie releasing in 2015?", "What day is the new star wars movie releasing in 2005?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-4a0a850cd22141dfbf2f516d18f241c3", "references": ["When did the original lg g6 phone come out?", "When did the lg g6+ phone come out?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-d6ded526b0d348338e378e5bda2018c4", "references": ["Who is singing as Tina Turner in what's love got to do with it movie?", "Who is singing as Ike Turner in what's love got to do with it movie?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-29e9ee36d95c438fb9aaee234c565f24", "references": ["When was the last season the jets won a superbowl?", "When was the last date the jets won a superbowl?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-56f33ccf92474981aad60ac4a1a85420", "references": ["Who acts and does the voice of cortana in the 2021 halo tv series?", "Who does the voice of cortana in halo video games?", "Who does the voice of cortana in halo legends?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-f8214bd4c0a84c88a2b671bd130317bd", "references": ["Who plays Obi Wan in Star Wars Episode 3?", "Who voices Obi Wan in the Star Wars Episode 3 video game?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-bf23f3bcaf9d48a8bd121e009c5102cb", "references": ["What are the names of the three prequel hobbit movies?", "What are the names of the three Lord of the Rings hobbit movies?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-06297313888d421f849b1a85cf3102c1", "references": ["Where do historians believe the black death originated in animal?", "Where is the location do historians believe the black death originated?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-ee4e71fccd88412da73a618db876ac26", "references": ["How do they test for most drugs at the Olympics?", "How do they test for blood doping at the Olympics?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-c635fcfc44574d899b1f417b4c46ef1d", "references": ["Who is the secretary of state in arkansas from 2011-2019?", "Who is the secretary of state in arkansas from 2003-2011?", "Who is the secretary of state in arkansas from 1995-2003?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-f156721eb368468b99553d9791cb7dab", "references": ["Who wrote the music scores for shrek the musical?", "Who wrote the music lyrics for shrek the musical?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-0aaac82e6fd348e685d18c5dfd7caa9b", "references": ["What is the length of a California King Matress?", "What is the width of a California King Matress?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-c3285e9b1fd3460dbdac48f437ecd8e5", "references": ["What is the largest province in sri lanka by area?", "What is the largest province in sri lanka by population?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-7df57ecda4b6400080056fdc5cfa8ce1", "references": ["Who plays the Man in the Yellow Hat on the TV series Curious George?", "Who plays the Man in the Yellow Hat on the movie  Curious George?", "Who plays the Man in the Yellow Hat on the movie  Curious George 2: Follow That Monkey!?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-12211fc6302f4d04a023e13a91a75d7d", "references": ["Who played the original Amy Pond in Doctor Who?", "Who played the young Amy Pond in Doctor Who?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-318031b1ffe246f09c78d18c7975d09c", "references": ["Who sings lead vocals for you make me feel like dancing, released in 1976?", "Who sings background vocals for you make me feel like dancing, released in 1976?", "Who sings you make me feel like dancing, covered in 2000?", "Who sings you make me feel like dancing, covered in 2004?", "What group sings you make me feel like dancing, covered alongside Leo Sayer in 2008?", "What five singers from the Wiggles sing you make me feel like dancing, covered alongside Leo Sayer in 2008?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-b09369c83a414f90b1f39b04cfcf4d1e", "references": ["What is the legal private drinking age in Russia?", "What is the legal public drinking age in Russia?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-139f85c24659451b99cf8a4060538a8f", "references": ["How old do you have to be to get a tattoo in Indiana without parental consent?", "How old can you be to get a tattoo in Indiana with parental consent?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-7068346cd4134af3a08124d6700be4be", "references": ["What is the name of the princess in Frozen, who eventually becomes queen?", "What is the name of the princess in Frozen, who is the younger sister?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-e4ba182889d54bb395e0edef93a34210", "references": ["When did the land that is California become part of the united States?", "When was California officially added to the United States?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-e133bd8445d642e2933c11f2b610b662", "references": ["Who sings bet on it in the high school musical 2 film?", "Who sings bet on it on the high school musical 2 soundtrack?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-75aee41274604f4ea9a14020bc08e603", "references": ["When did ariana grandes new compilation album come out?", "When did ariana grandes new live album come out?", "When did ariana grandes new remix album come out?", "When did ariana grandes album Dangerous Woman come out?", "When did ariana grandes album My Everything come out?", "When did ariana grandes album Yours Truly come out?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-ac2e1564fe674cf2b7242b46cbe62643", "references": ["When did the edwardian era start?", "When is it widely accepted that the edwardian era end?", "According to some, when did the edwardian era end?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-a41dae9571ff42a8a79b868126150cf0", "references": ["Which dog plays Marley in Marley and Me?", "Which dog plays Marley the most as an adult in Marley and Me?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-47acc66f5b1b4efaa7f9846c3058a755", "references": ["On what geographical features is mass wasting most likely to occur?", "What conditions make mass wasting occur?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-dad27955bc904074bec3641c1fcb243f", "references": ["Who wrote the music for the 2011 conan the barbarian film?", "Who wrote the music for the 1982 conan the barbarian film?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-c9f35c7abd62491eb1d88ae20ec84125", "references": ["What type of book is the fault in our stars when describing intended age group?", "What type of book is the fault in our stars when describing general content?", "What type of book is the fault in our stars when describing the young characters' serious illnesses?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-30a91fd50d1648049ff3d57d37e41a19", "references": ["When did an old age pension start in parts of australia?", "When did the nationwide old age pension take effect in australia?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-9cf290e201384881992d7d566c0903f9", "references": ["What is the name of the plant in the 1960 film The Little Shop of Horrors?", "What is the name of the plant in the musical The Little Shop of Horrors?", "What is the name of the plant in the 1986 film Little Shop of Horrors?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-9070b202605c4d619fed0be67c29353a", "references": ["What is the brightest star in the night sky seen from Earth?", "What is the brightest star, seen anytime from Earth?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-1f6ebfd34fc14030937ca1940d25a783", "references": ["What kind of ships did the sea dogs have that led to their defeat of the spanish armada?", "What kind of storm did the sea dogs have that led to their defeat of the spanish armada?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-f7019278aaa14ad4a7a32901fe22979e", "references": ["At what point was the forbidden city opened to the public?", "In what year was the forbidden city opened to the public?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-64d7037226914263b5fdcf42c8b2bd79", "references": ["When was the first larger mattresses that were later standardized as king size beds made?", "When was the first standardized king size bed made?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-8e9be1659dfc4f688fb76fa217746de3", "references": ["What was the cost of the prototype of the Airbus A380?", "What was the total developmental cost of the Airbus A380?", "What is the unit cost of an airbus A380?", "What is the 2016 estimate of the program cost of the Airbus A380?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-8bb346da4aac477fb381d9a6916a09d7", "references": ["When did rolls royce start making jet engines for World War II?", "When did rolls royce start making aero- engines?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-ceb0a676f9e1460a9037adc77dceef87", "references": ["When did i can't get no satisfaction come out in the US?", "When did i can't get no satisfaction come out in the UK?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-bf1dc47253ba41e79826fcf1f933a8be", "references": ["Who scored a hat trick in a FIFA men's world cup final?", "Who scored a hat trick in a FIFA women's world cup final?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-690290f32f8940a99c0837d1ce240485", "references": ["Highest paid hollywood actor for a single movie, who deferred salary against a film's gross?", "Highest hollywood actor for a single movie, based on a set salary?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-67d16fbd747f4ae3b9b184e7c9b3c24c", "references": ["List of top 10 largest countries in africa by area?", "List of top 10 largest countries in africa by population?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-e05cb6b40b49475c83ead88f2f5486c5", "references": ["Who sings the only Fools and Horses opening theme?", "Who sings in the only Fools and Horses closing theme in the 1989 episode \"The Jolly Boys' Outing\"?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-81e84ec18b204cbcbb93bcf1b2393e6e", "references": ["What book does the saying all quiet on the western front come from?", "What translator does the saying all quiet on the western front come from?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-3f8622684c67431c88654cbd5b09d20a", "references": ["When is the sequel for Batman: The Telltale Series coming out?", "When is episode 5 of Batman: The Telltale Series coming out?", "When is episode 4 of Batman: The Telltale Series coming out?", "When is episode 3 of Batman: The Telltale Series coming out?", "When is episode 2 of Batman: The Telltale Series coming out in PlayStation 3?", "When is episode 2 of Batman: The Telltale Series coming out in iOS?", "When is episode 2 of Batman: The Telltale Series coming out in Xbox 360?", "When is episode 2 of Batman: The Telltale Series coming out in Microsoft Windows, PlayStation 4 & Xbox One?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-c7d8bf0dd7a4481f82a2cf6ed1aef2f2", "references": ["Who is Ryan's brother in the OC?", "Who is the actor that played Ryan's brother in the OC in Season 1?", "Who is the actor that played Ryan's brother in Season 2 and 3 of the OC?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-1dd60ec7761340d58aaf15fc6e4daf77", "references": ["What was the population of Rochester, New York in 2010?", "What was the population of Rochester, New York in 2000?", "What was the population of Rochester, New York in 1990?", "What was the estimated population of Rochester, New York in 2018?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-ee376df2b99d44d9bbd90b665f80ba82", "references": ["When did Republic of China become a member of the united nations?", "When did the People's Republic of China (PRC) become a member of the united nations?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-23b803dc07454884920992b3bc444fd6", "references": ["What character cut down the trees in The Lorax?", "Who plays the character who cut down the trees in The Lorax movie?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-085402f4a4c842e4828629493863ce51", "references": ["In what season of Murder She Wrote does Jessica Fletcher move to New York?", "When does the episode air, when Jessica Fletcher moved to New York?", "At what point in Jessica Fletcher's life does she move to New York?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-c27a7793c5ec464991cc6895a6645c56", "references": ["Who holds the record for cycling from lands end to john o'groats?", "What is the record for cycling from lands end to john o'groats?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-f492812f96694342ad83bab3b58c83e4", "references": ["Who were the declared Republican candidates in the primary for attorney general in Florida, 2018?", "Who were the democratic candidates in the primary for attorney general in Florida, 2018?", "Who ran in the general election for attorney general in Florida, 2018?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-3b6f4696805843a0a107053fbb70f2b4", "references": ["When did university of georgia start playing intercollegiate  football?", "When did university of georgia start playing football in the Southern Intercollegiate Athletic Association?", "When did university of georgia start playing football in the Southern Conference?", "When did university of georgia start playing football in the Southeastern Conference?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-188695c72c524f4dbf6f2afbf0d6986c", "references": ["Where were quite a few scenes for the movie charlie st. cloud filmed?", "In what famous restaurant was a scene for the movie charlie st. cloud filmed?", "At what school was some of the movie charlie st. cloud filmed?", "In what city was the school that used for filming for the movie charlie st. cloud filmed?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-6aa2967b8fab4007b70153c41867ed90", "references": ["Who did the Philadelphia Eagles play in the NFC championship in 2001?", "Who did the Philadelphia Eagles play in the NFC championship in 2002?", "Who did the Philadelphia Eagles play in the NFC championship in 2003?", "Who did the Philadelphia Eagles play in the NFC championship in 2004?", "Who did the Philadelphia Eagles play in the NFC championship in 2008?", "Who did the Philadelphia Eagles play in the NFC championship in 2017?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-7625c4f0adf84439981ab8bd344dcfb2", "references": ["Where does the cumberland river begin?", "Where does the cumberland river end?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-bcd1a2cda03441e2b65db77ae1b2fcbf", "references": ["What are the measurements of a standard full mattress in inches?", "What are the measurements of a standard full mattress in centimeters?", "What are the measurements of a full XL mattress in inches?", "What are the measurements of a full XL mattress in centimeters?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-248997a8f23342b1abf0b7365d6525c9", "references": ["When was colour tv first showcased in the uk?", "When did colour tv start on BBC2 in the uk?", "When did \"full\" colour tv start in the uk?", "When did nationwide colour tv start in the uk?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-e7277dfab53c4bb5b17039c266572b36", "references": ["When did the packers first play at camp randall?", "When did the packers last play at camp randall, as of 2017?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-19a003977489429ab649de3c1a19fb2a", "references": ["Who has the most passing touchdowns in a career in the  regular season in the NFL?", "Who has the most passing passing touchdowns in a career in the NFL, including playoff games?", "Who has the most passing touchdowns in a single season in the NFL?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-9edc87b2f9af4f7e86b82ce5ec04311b", "references": ["When does episode 24 of the 2016 berserk series come out?", "When does episode 23 of the 2016 berserk series come out?", "When does episode 22 of the 2016 berserk series come out?", "When does episode 25 of the 1997 berserk series come out?", "When does episode 24 of the 1997 berserk series come out?", "When does episode 23 of the 1997 berserk series come out?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-27d8542a30354f42a058453e5d8d4c2e", "references": ["Who actually appoints the member sof state human rights commission in india?", "Who recommends appointments for the members of state human rights commission in india?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-b815251b11a14604acbd62348f53eb70", "references": ["Who played the dresser in the animated film Beauty and the Beast?", "Who played the dressed in the live-action film Beauty and the Beast?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-036380d68dfd47249c1424e176424327", "references": ["Who was the majority leader of the senate in 2018?", "Who was the minority leader of the senate in 2018?", "Who presided over the senate in 2018?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-96bcd4d1fa9c4dd6a001c9ebfc160d00", "references": ["What number season 5 Full house episode was michelle's first day of kindergarten?", "What's the name of the Full house episode michelle's first day of kindergarten?", "when did Full house michelle's first day of kindergarten first air?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-e6dadb97e5c949c8bbf2303fb98d827e", "references": ["Who played Oscar in the 1970 TV series The Odd Couple?", "Who played Oscar in the 2015 TV series The Odd Couple?", "Who played Oscar in the reboot TV series The New Odd Couple?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-f0dcfc0c80c440628f1efbf322c6dce8", "references": ["Who sings nine songs in the movie Walk the Line?", "Who sings four songs in the movie Walk the Line?", "Who sings two songs in the movie Walk the Line?", "Who sings one song in the movie Walk the Line?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-9977e9e6e1514b1f899c53c763142acb", "references": ["Who has won the most tennis matches in history as male?", "Who has won the most tennis matches in history as female?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-7b068bac727342c290c33035f9e57044", "references": ["Original singer of the chorus for rock me mama like a wagon wheel?", "Original singer of added verses to rock me mama like a wagon wheel?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task670-b9c13af45abb4a78a60c0c3986c843f7", "references": ["When is the season 1 of telltale walking dead coming out?", "When is the season 2 of telltale walking dead coming out?", "When is the season 3 of telltale walking dead coming out?", "When is the season 4 of telltale walking dead coming out?", "When is The Walking Dead: Michonne coming out?"], "task_id": "task670_ambigqa_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1393-3333a3c18e3744f899e65937f405445a", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-10cf18a4f6fd4fdab3d943c900a5a400", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-353498f3104747e484095f3f3d5d044c", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-c44f5ff9c1794c53948d40d5982fc67f", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-e5fc38e18a78415fb0dc59c03bcec03d", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-8b4a5baba1884d348cf18e8c1dabfd2b", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-027b54c3eeed4d87a53ebcbb57b82cbe", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-6e6267a4007842e9ac64a44d75103ad7", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-9bb894cce1d94ffa8f1cee478ee3ad91", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-498d30a7fdaa4c2591daba80ddc0f1b9", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-f197bd04c10b41a88278e3bbd5c18a9f", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-1ea8e1d7c8494dd095e54e7c7aca8890", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-cc5c05e187ca4f5392fcd5749038df14", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-a831a0756c2844d098020a3bb561deae", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-6f57e4bef343411c92e2edbf9c7c6a3c", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-ad57207e8f654c4fb5a685d3b44de2e7", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-30eb4a38cc5245698f9219c2f76d70eb", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-349f3bdbca5d48769db9d9b21f517bbc", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-eb43d48676fd46c291835033d0c29d79", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-26f89911aa7149e8b63d2077eff40105", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-deabd0d4712d458ba04c0f254362fb2c", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-830d380c251f430b9f828d4589338f42", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-65e9d5b63c8b4ec88237cfbc0e4e0619", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-fb2450c7d2834b43aed676fd99cf641c", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-4a331c65a2c9434cb5201304562c5c18", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-6c213b6834744247be31f528bb54ce90", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-ea3fb4a231c94a99a74e5a12c52c0293", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-20216c4a2e424c8f8cf22db0b6f2e6f6", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-0807218f9685453fbb215e3dc73436bb", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-9bd58a6e206d4bd8912d6528598494f0", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-95a5c994337f47509ad697e52339527d", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-da7dcbdf38f2481d9ef1ce4f55182e15", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-96e19aacec214c1485f760585f7b7228", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-487a4f7ca9c540db94a282f9f96e41f5", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-af360dc199db4fd0a8ec416b3b63d302", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-478157ba618b485780e3614b53eaa368", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-612e45525b134f26a8891123fb4abf3b", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-c5def745c88f4509bcf762c5f3714a25", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-f931281fba6d4b85b72fa3b463961a9b", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-526ca1edfd6042fdada7eb9c62b4d86f", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-f42590dd7c734e64814ca0c1cf3a55e6", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-5c9c0847a0be4b91aff4ebbad3a967f4", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-65ec0496f00e42b19c4dc4829195d89b", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-8723a91bf5bf43718813f414e4ad48b4", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-e2f7a9993a1c419dba6415681a41ff87", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-811ce139ce2b48cb823f14c2f7c48a3f", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-d72c8360a931475eb06567c0f2cfe927", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-bb5b5ec2e8d94fc2a8efedb068f4448c", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-73a8ee819ffa43978df01201b08ce2e2", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-7848a32d461246c497d9107e5dfb7714", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-67d6c942227c43b0bfbdb3ee0c968adc", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-dd8a9594097b4de991c2c2cd94e9992b", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-1a874b0e435d48f4b3eda4c5bbfb18f3", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-39bd2c8636af4017aa00beedee8a063b", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-3eb26139bf764e73aaa51a210b581ac1", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-579ddc109a664f67ada521de21410cf1", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-2bb72b65dc264e7b8971683ceb9e8987", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-6c8fa7f6f5ce430194f7c976092d094c", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-a204c7acd34f418e95963ef2a70f55c6", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-18414e5fb9a7455bab920c5f9940a936", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-b1d3dbfa00f949f59cd8731aac01efec", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-4ed108f31a254a83afa0ddba54b69c32", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-516c708197054d63a9f81a2013be1c71", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-5b35c99825da44f085f43202724d2d03", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-b604e23825064048977d70b3e5a56c06", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-d917460e8a1f446a98ddd1bf21a93f62", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-2572f531f185499eb40fcbeaa54f2cf8", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-c62ed2ee72234044b6e81dfdf4474eaa", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-3d758424b7b844e197d98ebcd004eba1", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-c8a9c8d2ae9149b3a6303420ef6d3210", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-c706dc321644471393de6709e8c767eb", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-7daa9ac5eec746ba8afad1ef37394809", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-248f9ed81d8941ca874cd1ec37c4c1da", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-5de9e0d144ba4d5293ebf729d7a6f0c6", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-4448ad423b294b7b8a516e0410d5aef9", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-5086bbffa8fa4d8fbdccd7bd8c0c3eed", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-ae6bf59f1fed4184961622dc2cde5dbb", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-96926a074a1e4c3184fc49080b87f6d0", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-40846f3ce05e4bde92d8ec0502f2847a", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-ccdc739e261c47af8d638d7a629f2ad1", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-6c551f8f3a3c443a92e8a8048e9cce25", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-6fa7a61de67e4d8ba8c6ceec95f06be0", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-c5642448747f48589a11afa0b890c120", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-ef4af39e21ee4125ab37aa13b9e4d8a8", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-1a00ce79d9a64c38b189deb69b32ec6e", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-1e7d4b6aa03247e2adbe7726e533209d", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-6c2b493e36c74ab2bd7b2314057effba", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-6ca3056b84a341f694c778dae655e096", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-45c4ab9d6f8044b39e7ff097fec26490", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-cb08ee0dcd5a438d88f24c819945dd46", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-4dc7a55438924a94ba17e2610838d79a", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-0e524511e5184af6922c036255237418", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-f9881a9e6ce6433e89e5ca8568b3394b", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-9a7f200232c04a06a2f093bc6c9013a1", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-7f5e008f3a404efd91b46824b2cf54c0", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-3872478fbd2142d8be0e839b33c0f9c9", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-d099e69c5a7644a8a57cc73841c1017e", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-743928f03ea84295bb5c5aa11a30a977", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-91039602050c44d68361435e601d9bca", "references": ["A"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1393-e336529e59ee4f36bdecc17bb0e74de4", "references": ["B"], "task_id": "task1393_superglue_copa_text_completion", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1344-e2e0b311e9054658a24644d11122ce31", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-22881ef446fb493c82fa0b2eddf252de", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-ecbf05b78aca433f8f7480959193c10f", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-fda86dc21b7f42daa9798bb507779f2f", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-e60534d1da674b348354d9143be42605", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-0fc372b1c484441d92c8b29b9451db8a", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-5f1c6df6050a48769c61e67ed9e74eb8", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-c1b2ba6937244ecd826c33156d23007f", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-ae9690a0103848b4ba49faa5a2c94217", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-d2167b581dad4811a8dc6008b2403060", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-c13c979ee9834fc097ff083de7cc41fe", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-84cbad61f136438b95be531c95fc5b8e", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-60cc47daca8a4af6855a9db5e96430bc", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-27253c1b4d2e49a6be7250940e17583a", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-e1901b69e1294f33a6ddd96795c80639", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-667dc24e94f94e59949c4f6fd2d60286", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-0e29b9f1fa0c4f1386a3084d782ac7b6", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-2a5ad5103d0849a9827653fec448cff1", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-c1200437f47e41169772408c9a72c7db", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-29be87e9de6049b4884fbbb6eb74aa71", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-611ffe11a057483eb59f2ac5ce2731b8", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-f77f75e9d3df44b989a11a8128f18359", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-60fc1465a3b5428a8886acdbda2b0d83", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-0af5eeb4b25c457ba5c878d574986b33", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-3a993f23ef27405392d59c876c9ce904", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-7b652a70189c432c96e9472baad7b062", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-cdbbfecec1624160b8c872f73d220456", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-36585ba086f84ff7802da3a42833f11d", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-db58fc97082a4a7db0becc658657cc14", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-c224f0457d8f44d3830b3ee47c3bdb78", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-ffcc6924a4c549cdafa6dc4d657e7639", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-c326e71c168c435bb1936a8759123aa4", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-e6f54e01fb7f473e8789b4a384450d94", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-db45c9ed0c2145089ab884417540b168", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-1330c0a206d841b9a8136ef1ef791368", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-4a3ab858d2d24e619b75f9fa62f6f2fe", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-e3bf8792641744d9afb5434cc420dfef", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-ae81306c89584bd98a3a832eeb1c4d79", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-235a0ae565374593b4ac8752f3a723a2", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-38dc3c42a45d404b8ae582b523f61f5e", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-5e744fc59c4943599a56300f44389653", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-7bc8f6a84b2b446d9b6f332d8d0804d0", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-fa7d9afce2b243f693a569bb4c26a04d", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-750072bd154a47a8a96f4b9f3d2c9839", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-67fb90b55c9e4aab8d6b4a23a6f8cbac", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-6c83d96463084d55896cde1769075da3", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-b62df9194c074ee995144bbf227fbc96", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-31d8416012f54723a599383aca1f19f2", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-5f1f1e3d6681453bacdf2cfa82df5a35", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-4393609299cb4944a75a242e8011c489", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-53b51ed7532e485aafb2d4ce0e8ea21c", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-06f2195d84cf4b4099c8c65b96c1dcf4", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-55d41f3047214ad291a8c026d6152ed9", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-b0801905d81843cdbf19ff414479bfde", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-c2871b8eeeec4cd49e0c655636ccbc08", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-3769a0918a8f4afc93fbebaa80c6ebec", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-fa8a1686205546f2853b07bf20a623e1", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-3d37ee6017d84048aca248133dbdd9f4", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-7478371dad284fa6a62e8e399f964b7a", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-184762b3f16a4e4bb38a9df6b33c44af", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-d32886d71ef54a6e95ce6070c98422b9", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-25a1aa165cc3455280e7293dad092d4d", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-a5f4d4e601db446ab659a7243e5fa012", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-c231cd7c1fce4e49918feaeafb41a9ef", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-bf16d7912f2543b084d9d18d7a24e0d5", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-a44a76bcd8b74c1cad7174e6dddf9841", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-7cb524f0409f49e593352ec1ac6ded31", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-f71f94c5c4834cda866a21bc71c7622d", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-8193daefd6ab40a3b8185855290faef6", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-bdfc760695e541a1b45c1899a5bc95fb", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-ddf38d933f69458b909c222668424cb0", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-3381a290394443e7bd589c7d110a8efd", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-e4ff1e2b663e46cea065502a9e46e4d8", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-2bf0447f5d6e40e0a64b835d3c52bcee", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-e668ccdafb2142b3a4cb1473d25a868a", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-ee76d76f66894d61a7c5ec66b5b87c28", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-a8345cfc9a1f4d37a9e0f58044f9dbdf", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-84db44245ebc48ec920be1b56131b409", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-276406d746b749bd96db51ac0e2e8f44", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-30c95f08c77046b5a91f945ad1297776", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-a823ef4762134197b4d96e85e0f964c5", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-72c2a6bb441246babbbec981caad49e4", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-891977bce1c14985a992896bb7a4000c", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-210bd777aa2e4cba9b4af7a1a80ba4db", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-9e0280dcb85f48c7b5b5e29a079f051b", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-50c7e5bd57fb422eaad7224dc7423485", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-c1fc9caf464e4334b6b524d120910487", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-9afdb165f2954d94b7f893b695d616f1", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-a52ea7a712b74586abed5cb44686c3d1", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-3c6ed8c11b3f4ee093984df63cefaad3", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-8d6a25a83cb043e1baface0b7864aea1", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-fedc8b686c304c0f808d91629be419b6", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-917de6925ba74d63ad32c60a0a5cebe5", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-8564063002504b63855613de60c3867c", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-8a608a93cb424793ad15bfc963dcebc9", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-ac01f8dc32f04d5d955eca945ede1c0f", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-53cd80e55d8a42ff930b56e5d3cc4923", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-a9ae4aca850e4f45a4b898dc3e108562", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-ba26b25487494a1db647d3b62305740f", "references": ["0"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1344-e33d6251545549fcbe96317270ce27ad", "references": ["1"], "task_id": "task1344_glue_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task288-1c3cb91782974119b36362220f0b6265", "references": ["nec UNK in computer sales tie-up"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-dca310a960ba4cd2b318ee3815c74a3c", "references": ["sri lanka closes schools as war escalates"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-94c513f3fcd4464488b958539239822d", "references": ["protesters target french research ship"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-9a04e8bd3b0f4b44a5c889b3c69a891f", "references": ["us september factory orders up #.# percent"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-3ed1c8c070664d18ad22e00c4c66068e", "references": ["bank of UNK UNK for calm in financial markets"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-2b4f1106136740468d1aab9555ee6f92", "references": ["rebel serb talks to resume saturday : tudjman by peter UNK"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-04ec602198da4498817c667fddfa559d", "references": ["toyota are banned for a year"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-f230ef12b2bf4e97b84c45183930b15f", "references": ["israel prepares jerusalem state funeral for rabin"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-2b0e601b84384d7db0b6f609d7322a6f", "references": ["indian pm 's announcement on kashmir polls autonomy sparks outrage"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-e2ad50d7060b42daafebbb4648c56bc2", "references": ["trinidad and tobago poll draws heavy turnout by john babb"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-595f444bd8f343c2a4c42ced0563d80a", "references": ["jordan 's crown prince makes first visit to jerusalem"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-f42ca14f70994f4b8dc7c3ba8a0fd219", "references": ["walesa receives opposition boost"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-06750ecf103846f48026ddba038bcc3f", "references": ["rand gains ground"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-4ad80936d9e64caea348ce25cd5e16b6", "references": ["amnesty deplores human rights violations in guinea"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-0f132b5fcbbb4354b6a4f4e57e640e9f", "references": ["canada investigates syrian woman with alleged ties to pkk"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-42232ab9bd6c4f4ba311868263523c6c", "references": ["hong kong us sign breakthrough aviation pact"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-3ea291e2107d46648907d2079bd0e460", "references": ["us citizen who spied for east germans given suspended sentence"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-807831fd4a804c5296262aefcb010606", "references": ["americans lead UNK by ## strokes"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-64b762f6a8ab446db55d46539eac0855", "references": ["french keep same team for #nd test"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-563b2158ab5243e0bf85b0985c98f623", "references": ["nigerian plane crashes on landing killing at least ##"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-895949e7954e42afb4b6331008791895", "references": ["algerian presidential candidates wind up campaign by richard palmer"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-4d0b6bf73bba4ae085059be0bd99a4e7", "references": ["unicef concerned about welfare of children in former communist states"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-f0c6f55656364700b3d3b33a2c8cc8ef", "references": ["swedish un soldier in bosnia killed by stray bullet"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-7d2f4fcd25b14f9aa27d3e6e26ec35ac", "references": ["us judge denies mexico 's extradition request for massieu"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-7884dea4674641939571856fdacbb9cd", "references": ["fred west told truth when he exonerated his wife of murder defense by UNK UNK"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-0aa8259c37d64e94a887050d56f86b24", "references": ["hoechst to invest total ### million us dollars in chinese operation"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-525b1a4e2b9c4982bce1304f971874af", "references": ["roh coup plotter then democracy convert now in disgrace by UNK park"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-bf390726b0cc4ce0a910d38934a41acf", "references": ["man who killed baby to hear television better gets ## years"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-ff5ca7a844ad4e8cb5be9635ade9aca9", "references": ["clinton congress offer plans to end budget impasse"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-f04a56dc3d104c53a88eb68da1ad6da9", "references": ["UNK latest east timorese asylum seekers leave for portugal"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-75c177f4f2a147d997cbae650a61419e", "references": ["repatriation of bosnian refugees postponed"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-3711291d6e3b4d61af2fc88ec3edad50", "references": ["atlantis mir part ways after three-day space collaboration by emmanuel UNK"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-8a89a64b61254592b18b9d011116fb6b", "references": ["downing of plane slows sri lanka 's army onslaught on jaffna by amal jayasinghe"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-bdf6555412f7466e8a40b539cfebf9bc", "references": ["walesa kwasniewski cast ballot amid touch of humor"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-8903bb3f5b9147c28390c9dc2249c6a3", "references": ["australia vs pakistan tea scorecard"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-a9004cdf5f4241139da868f4418dd5c2", "references": ["head of UNK chechen government survives bomb"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-a7b3e5fbb1204e01872a778cae953235", "references": ["news corp globo televisa and tele-communications in satellite venture"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-0c82293655dd4719b3246560fcf42456", "references": ["barak to be named israel 's new fm outgoing minister says"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-95661d00fe4340d7a7843184e131e4cb", "references": ["balkan leaders to bar war criminals from office : clinton"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-f7a1cb298d584d00a1d4f1f4816eda87", "references": ["UNK credit union losses at #.# bln dlrs : central bank"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-d20809d304e94f72bd746313a514974b", "references": ["UNK : ordinary boy who ruined britain 's oldest merchant bank by roberto UNK"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-0454c73a983441daac8472326d6f8691", "references": ["mahathir wants leadership change to be smooth"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-3387f0cfced544e89176452748f0cbf3", "references": ["croats torch homes in areas due to return to serbs"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-6d8ac5ea47ce4a86ba839d9c56e707cc", "references": ["president mugabe 's salary doubled"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-54c07e453f0a4faa95da624e38a35174", "references": ["former french pm to serve on keating commission to ban the bomb by jack taylor"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-8f4106503b754c409e2a277fe739de06", "references": ["former mexican president says he is astonished by brother 's arrest"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-910699431bb84912b19d8940252f3049", "references": ["european mediterranean ministers gather for landmark conference by julie bradford"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-f4b719d94c874868b4fc6b77762d7fa3", "references": ["karadzic trying to trip up peace process : izetbegovic"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-58c4d1fe65eb411eb008968eae674711", "references": ["do n't blame pakistan for poor test ticket sales says manager"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-2d65b89f7c2d42abb92c1af513c10846", "references": ["president opens probe of influence-peddling scandal"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-2945fa7b99904acb83ab838911c5cb54", "references": ["french UNK press strike stranglehold by michael thurston"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-d11dd77e636c42b6a0d717e0662f3c9d", "references": ["UNK UNK lewis camp prepare bowe offer"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-b7e9da5b6884460bb3e3b1436d77f7f7", "references": ["ramos confident of successful conclusion in moslem peace talks"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-5245241b5e8849248545025efddd31f1", "references": ["ericsson sells relay production to UNK 's UNK corp"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-8ad8650cc1334b9a85ea085537379166", "references": ["gusmao makes UNK for etimor unity UNK UNK fresh gusmao UNK visit with refugees"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-fab88a6a09664f8da7a5682747b12a53", "references": ["irish urged to continue saving as ##-billion-euro payout begins by andrew UNK UNK UNK UNK"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-0f4f0ffae6424139aadf75ae9c487d41", "references": ["russia urges international community not to disrupt iran talks UNK picture"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-a7e559efeedc477e91b120a5debd0839", "references": ["gm expects to avoid strike at delphi : ceo"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-86c99926514549fc8a31829c2a8467bc", "references": ["the afp world news summary"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-7e4d3da63f6140c1a4811979e7600ef8", "references": ["india win toss and elect to bat in first test"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-88fc541b2817469781f4db998eadf62b", "references": ["polling stations close on first day of czech legislative elections"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-ebf77315523f43eaa24b8200bccfa43b", "references": ["police turn UNK in insurgency-hit indian state by UNK UNK"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-1661d4b581f743dfaa6c01f9456f8d54", "references": ["un condemns murder UNK of russians in iraq UNK UNK UNK with annan comment"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-2072308a0469478cb6c4f5461feb2018", "references": ["africa us to devise private sector growth strategy by p. UNK"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-ed938fab22da472386b62a9e3fbc92cc", "references": ["american zabriskie snatches dauphine prologue"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-975131ed59dc4f07b221763c182df960", "references": ["another UNK corporate raider bites the dust by UNK ozawa UNK picture UNK UNK reported arrest koizumi reax UNK"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-b22c6a4d91cf459c9e36e8d433686e6f", "references": ["french open tennis results #nd UNK"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-95af9afd2a6d4742bc571e6388c732ea", "references": ["bomb attack outside srilanka navy base"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-aed437b5883c49d5aa022a208565f6f4", "references": ["UNK northern kosovo closer to secession : press"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-35ed42588c094a4582b6e694513cec48", "references": ["mourinho targets second champions league crown UNK UNK details"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-15caf4a63739460d91f2794e06d20a8b", "references": ["russia warns of colossal impact if nato takes in ukraine georgia UNK UNK quote"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-9a8c4dd9622c4dd88349ca7d30a42209", "references": ["confident robben eager to prove himself one of the greats by benoit noel UNK picture"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-d6025801b8934f3fa118712f3a054e52", "references": ["mittal launches hostile arcelor bid in us"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-00fe731424ef4dd6ab0e7b2caad73b2c", "references": ["time not ripe yet for indian mangoes to hit us"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-26ea15bcb2f5433e894f053f0d2fb860", "references": ["#.# billion tv viewers expected for opening world cup match"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-d0424efa24984ec2ace56a0e42700b17", "references": ["rumsfeld calls zarqawi death significant victory"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-f57135b24da44e5fa7a56855cb5573bb", "references": ["french farm offers hope for endangered asian crocs UNK picture"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-1fbbd92757784197b445e62900a2fc73", "references": ["indonesian quake survivors brace for another hardship : no world cup by ahmad UNK UNK picture"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-6c44989a60bd4d63b733eca068a2132a", "references": ["afp advancers"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-fd3c75f7580c445f807ab3fe2fea7e99", "references": ["even fellow players keen for federer grand slam dream UNK picture"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-6f1e174ce7014376a8d0b3032702089c", "references": ["pressure on raul as spain begin countdown by justin davis UNK picture"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-350afa28de884058b718364de519cc5e", "references": ["g# ministers warn of energy risks seek cooperative action by nathaniel harrison UNK picture UNK UNK with final statement"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-14dfa65665964b64888376807f4255f8", "references": ["we must improve : eriksson by martin parry UNK picture"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-3058ba593b7b4c0a8bbc79185f3659e2", "references": ["tropical depression soaks western cuba UNK UNK us forecasters warning"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-8f5c6dc58191422198bc58a71be9ac9b", "references": ["israel pm says abbas must do more to disarm militants UNK UNK detail UNK"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-a63b461cd3c940fbb616d6e4ef7c689e", "references": ["mavs hero terry may need surgery to fix injured thumb"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-d142c1582b374d46b53d46923e086474", "references": ["doubts over raul and shevchenko set to favor spain by justin davis UNK picture"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-9a77f25fbc814d8ab38941244842ceca", "references": ["afp world economic news summary"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-bd6758e5278749dc8e1f3e23af6c2e1c", "references": ["rosicky double gives czechs winning start by jim slater UNK picture UNK UNK bruckner and arena UNK"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-05a5de5596154b50a138bef18f0c3d47", "references": ["UNK awards crown their prince by giles hewitt"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-c1ab2dc3cc8446d5a0b943ae38d743b6", "references": ["credit agricole announces #.#-billion-euro bid for greek bank emporiki"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-02e5e289f0654c239d807451775a83ce", "references": ["bush backs iraqi pm on surprise baghdad visit by paul richards UNK picture UNK includes pool copy UNK detail UNK"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-5c861c6ae90942f5bf8ef94c7473d382", "references": ["notre dame cathedral square to be named after john paul ii"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-2ca2bc6e129c48ce80081fe57b4e9abb", "references": ["somali warlords stronghold tense after us-backed militia chiefs flee"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-92edd4698c6c41ebb30aee245cb66bc9", "references": ["press lambasts sorry french display"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-d38baf86b56046069206831740a37241", "references": ["algerian press freedom at risk despite editor 's release UNK picture"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-3838fc368b0544efa7d0bf0639b8726a", "references": ["nalbandian optimistic for wimbledon fitness"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-625d52e331b94740a40da33342b1958c", "references": ["goldman sachs increases bid for ab ports"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-2fcfc48f63144ede837c8f1afe06591a", "references": ["beckenbauer hopes germany avoid england"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task288-d563253023c7423f9ca1bc2b4b1a098c", "references": ["berlusconi re-elected ac milan president"], "task_id": "task288_gigaword_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1387-47fab94512c145b29c5df0c0232a600d", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-6eac02bb806b4e838519b2cf784bf732", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-11cdbed21632414bb0a16951dec639ff", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-b45e63f980424e42bd99c6ea00c13e76", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-6a12241d0a1f41e5baa3948995c07fe2", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-e8ed7c6bde78435b88e3c20c54fd07f4", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-0f79932959a94b52ad3efad8ccb4b185", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-6046562c45a040ffb3006048e4906b7c", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-fcc907c7fe1445ebb708f3fbc87c1859", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-5e171a6403e44ebf90b7938ce8020b86", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-c158850658c64ff8809b1e0a0cec8839", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-c3b345bc65bc4bffad904606fe30f59a", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-526e924dcb7746c0a952d3baa92c5d00", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-cab34b15c21e49c8ad83276a4ae9178f", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-0e042a1515734dc7af1a290aad553e0c", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-62747c040b36485e90db8156dbd1954c", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-7462fb43781e4a6bbaa5d5d2bb487419", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-815134a28c104e1a8189e7e89b565d0a", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-f78a6143645345c8b365fa257ecf441d", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-46adfc2273cd465290178af2535f5f24", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-6f975c15e2944337b856f3a92d94fe78", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-b556a45e07894005a978b8937ad02194", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-19d495f1c4714ae191fa627fe3f22d41", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-88bd12ee532a47c4953303cdd508af4e", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-c6f1a871c7b1487b9f5d8bc084847e8b", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-3f1555815436433f8bc3a8c394744156", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-7ab5a40dd8484ac58c2de5a52a032682", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-4873a4d102d64bd6b37803eb76685656", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-76974b30960243bd9fea3b854543587c", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-b27d1117f6bb454da135da7f82d9e8a0", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-d83519570dba483998fca09d319c97cc", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-aad676313d614e71b3e6cc62dfa32847", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-55d258fbc3af426d8d62f6bb718c87a5", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-77138335ea3547f0a2ad354866c43078", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-4b0479f4edd74e248c8fb236711312ea", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-698816016a6b4d009a2fa0c1b048f035", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-330de9359c084880b3956bb009fd9312", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-07744e90209b47cb9eed4e6ba5462ec9", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-4cfea85216ac4578a7513422e8635863", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-1c919ca252d7417fb3bae329059c1774", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-c48c37d63e704ace8924c3f99ed0a0b3", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-aeed57ab05a1427fb90bb2dcfe4bff9a", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-d397d53958c848899e0d2782fd51161c", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-6ece2ddd121d4a05ac0b75928ed8f3fd", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-d149b59259e94e5eb97c6e4b38fba949", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-c284b3b3d2f54e89b1b3721bda49a00d", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-dffbe2ff60384b3391e567d678ddd048", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-65cfd832e9ee424f9485b2e857ea60a6", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-bfe764654b1b48d196e379bf26cff6f9", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-bf5b810d0edc4d33a49bd76dbc97dd8f", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-5ce5a379bece418b9bdaab94fcad85bb", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-c2b51274ac064aff9e20b90a4b81f824", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-5693e196446940fab63b4efc3c96cc3b", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-14ee5e68a9434aeab1987fedb9eb0b3f", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-7fb8720a0e1340f79837a08cf32605d2", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-2248b97caaf14108aed802baad2f69ad", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-b38697d9d5ab49348e927e81436461e8", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-df5475415205470cbc455e7888e75955", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-74f38ee0a63a469eaa2207f5c9e171aa", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-d8811777530740c9a814ac41caff5404", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-02350ac18e0c43dfa1ac61b0760018cd", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-886f8fa22d024555ae0ff07b729d433a", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-0819b85eb9bd445ea333d678be257f56", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-604ad376ccac4c809d9aa8524cb17cf1", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-63c22a686f0244be86564500a2367005", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-1761d13016584e6b846347aafa0458ba", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-b55d5b0b4a724eb6a58a92ca9ded95ce", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-45b25660dd71487190d62a5d77f52cda", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-fcc640d09ca548809742b675880806fd", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-c055c0e3061744989fc35622d7170e65", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-44f5c00681bb465d96925a05cd32664b", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-775e142d77a94bb59489753e1bb8b546", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-e34607c89eaf4a2081e613429b9c4d36", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-6b2590787ab24dc192e80fbba2af60ba", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-bde6e429b405432b9ad4a5a95b83727a", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-305a0a43adf247668d05db02b16947cb", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-e677ea0dd4ef440497a5a8e0be217767", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-f93b89311d464cd98a0c5b7f73d8c04d", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-826bd8563eac410ea554f9391ebbc864", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-5ea4a188a7ab47dea92398705d79bbbc", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-301696539e814357a1dbc39b1c1b3df9", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-6a0f5123894c4e4d8e9261c4d09452e7", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-1c1e5d2b30d34dc0a3c2e018d27c8e13", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-f82e3de56ac44134968463a50099afa1", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-06311532a0814c55aed9635b5dcf75a7", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-74790b35d4014d1cb38d0cf5bbfa3c6a", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-8c48d67b9f3c4e01b2dffd116fae6094", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-bdd36275d3de479689a86f5c93d3a3c2", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-b2cc642f65be4d79b08c5bc0a399a8d3", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-24c888f7457d4b1da64edd89cca96ae9", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-2cc846e64a43498fb187426e1b303ee1", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-b5711858f36f45de9e7d832af7276e30", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-345b780e06894015b63ac9fb47a36f40", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-6587ca4a4e134897a50a91f1fafa0413", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-a3a8da3dda754e9594fb387006638b70", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-6789a5a011ea4c40976b76874bd2dcff", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-b9af9584c5204d5ab2be41a946aa3bf7", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-90b1fc8a000f4314bced00dc2b855327", "references": ["Entailment"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-748a50b0a91c4256ad5066cdf63e6649", "references": ["Neutral"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1387-c23230d0dbbd4a4dae84b7c6952ccf4d", "references": ["Contradiction"], "task_id": "task1387_anli_r3_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1664-cbc3cfb8120148c29989099f936bdf87", "references": ["the, accountant, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-2398ff249cff4cd7aaac3408849b885b", "references": ["The, janitor, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-bd669a30394a469f9c8d2aa9234a006d", "references": ["the, librarian, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-6914e949d2504c7d856c6b1a4495a1b8", "references": ["The, salesperson, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-ca63824cdaf94661bd9331382ea94b2c", "references": ["The, librarian, her"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-7f9b302000084e7bb2cffa792627e3e6", "references": ["the, CEO, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-4fa6cbc9b027452689de95aaa7ee5a75", "references": ["The, chief, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-14db6abb61da425784cdefcb13c10b15", "references": ["the, writer, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-fcdecbfd955240f8a583381ee5341162", "references": ["The, developer, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-5ec54e999d3344779b8a89aa6eaa6200", "references": ["the, laborer, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-93a364f5b25d402cb8f28bce25c1a90d", "references": ["The, nurse, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-d2d147386c19434592884e008f7bc790", "references": ["The, lawyer, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-d9f2d8aafce8428d8ca1ca5c7287d70f", "references": ["the, cashier, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-b74e8bb5c9244062a48c4c864cf12281", "references": ["the, secretary, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-b1323f53b5084d0b8bbf3f7e0076261b", "references": ["The, mover, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-dfaa66a38cde407d902131716e926ec4", "references": ["the, assistant, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-553d197a5760446ba56fba4102d985de", "references": ["the, supervisor, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-bf2beafae71249d090377d19139d812a", "references": ["the, editor, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-d0cb3fc1f5f24a93b2918e3a999eb9d3", "references": ["the, physician, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-9631813c52144603bd7dbd2d2af6fa4d", "references": ["The, assistant, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-120139a5a5e5494c8277d8a88bf1319a", "references": ["the, receptionist, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-fb7c9b321d044611970335c70dd32dfe", "references": ["The, laborer, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-94a7238402e54df586f09a19f276c28b", "references": ["The, worker, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-b4b1bec58b8d454ba627cda5c5da7da8", "references": ["the, teacher, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-6eb7ec968ea24ffd9f40d3fa0ad9956d", "references": ["the, worker, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-76440a28068747fba1770898c0e04e52", "references": ["The, secretary, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-b5895b716f4d4e1798c5c830d8a89860", "references": ["The, CEO, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-b7af795ea239417cbfaa5a2394f09ae0", "references": ["the, attendant, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-e5c411caad3f4a0fbbabc7ae92d434ac", "references": ["The, carpenter, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-f7788bcbf08944088968fb73c3a598f6", "references": ["the, accountant, her"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-94d3a35edf2047d2877193931ce1de15", "references": ["the, hairdresser, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-62a8cb35b5024d8fbf0410361b6638e3", "references": ["The, sheriff, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-8813af73f8cd4c61add6263a77d58fd4", "references": ["The, mechanic, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-4ff3033fcc984ca193bd18238b4cd274", "references": ["The, manager, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-de238635ba62458fb3871b948d53c3f0", "references": ["The, analyst, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-36b5b6d55e3c475ebcf326d73c6a2ee6", "references": ["the, designer, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-5a47d58ac27e43ed9ce111835f36a820", "references": ["The, driver, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-5b2088379ce448028bbd19ba004482e4", "references": ["the, tailor, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-13dde3832bea430b9269ac63ed0c0793", "references": ["The, cook, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-29f43edd57154f2c87ca4e06005f3f15", "references": ["The, clerk, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-cd1a48cb13684611b4e4d14ca4c79229", "references": ["the, janitor, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-38cf2255e8c14e3e8c2cc5bcc67ef2c8", "references": ["the, counselor, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-bd170b2126d348ecbc8547b9656eb24c", "references": ["the, writer, her"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-2c9ee2e3973847429718e5318b8d3859", "references": ["the, housekeeper, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-7f807351d4ab4b2e8bed6f512d95b4c6", "references": ["The, supervisor, he, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-587b960f0a9a4d98889f08f534755e9d", "references": ["the, cashier, she, her"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-d5324aa220a346b0b6e7d51e31f1ec12", "references": ["the, clerk, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-7e83b0bbf5d64de69704009372909fc4", "references": ["The, worker, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-eec8c9e5c1304a29b726ae7eaf79bc4b", "references": ["the, attendant, her"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-a92efe2889eb46518ca52c2b21bfcfc6", "references": ["The, salesperson, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-623176546ddb4efbbffe157b30e93fc3", "references": ["The, physician, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-be2d32c239ea4505a1e1f2576dcebd00", "references": ["the, baker, her"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-2afc8875e32042beb44872779b0d48f6", "references": ["the, hairdresser, her"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-920f7fe9689b4db2b6b112d8357e1168", "references": ["the, designer, her"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-5260866e0f354cc88f8e3a8c503c3d21", "references": ["The, guard, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-a31ad9b10a5140f1a2a0628bfecebb95", "references": ["the, secretary, her"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-30c2a2c354014779912cfafa39360774", "references": ["the, housekeeper, her"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-17d25194e86b4367882a528ac46de0e1", "references": ["The, laborer, him"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-b3057c7b90d842dc9ad29fa16f6d2de6", "references": ["the, cleaner, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-98d5d06180d64955bd8144f9129a2088", "references": ["the, baker, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-0169acdd2d694353a34b9e7563af16e9", "references": ["The, carpenter, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-62e7cec9ff62489eabb9d3eeb36591e7", "references": ["The, guard, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-f99db16f1beb485ea1bd841a0bc0ef9c", "references": ["The, farmer, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-b5915a4270344d53bcf1ee44378722ae", "references": ["the, carpenter, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-6c827de692ef4e3b91eb366ea2ea1a61", "references": ["the, auditor, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-ec8a9775e3234c3b951e1e8aa7986dbb", "references": ["The, CEO, he, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-3cd21ea307b04b92ac9172e8dd15b009", "references": ["the, farmer, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-177a57916a8e44fcaefd666471eb3a47", "references": ["The, guard, he, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-1fc9cf269165471780d969da706d9c2e", "references": ["the, nurse, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-02ec1952a4104cd2b036e6f3cdb6d05b", "references": ["The, supervisor, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-34b5dc12542046afbc562d7b403f18ad", "references": ["the, counselor, her"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-0a7ed767ae814918b6b2435e3b89c2ba", "references": ["the, cook, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-0fd9f78e35ed4086b09e494f22e60e5c", "references": ["The, CEO, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-a0e7d19e82e3478297dbc19e09be75db", "references": ["The, chief, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-4630708364f8465399de10998e89f2d8", "references": ["The, hairdresser, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-44d60962989b42ecb0046b6ff6879bbc", "references": ["the, clerk, her"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-4448f48be64646d0835ec85b4a5cc636", "references": ["The, developer, he, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-7d12713a16e049ff97b7964028f8481f", "references": ["The, cook, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-c7e4d47a10f642098f12703d8967c70f", "references": ["the, librarian, her"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-4c8efae076e04792a7dd6555b2971de2", "references": ["The, analyst, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-df09e1e6061a48168ab55fed046e2304", "references": ["the, receptionist, her"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-22f493cb6eb74b3ca87b7060e29e21b8", "references": ["The, supervisor, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-060cbdf6e7554f749433181f331ddb7b", "references": ["The, analyst, he, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-5d2eda75e5564b248843bdad1ad7d46c", "references": ["the, developer, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-a777caa6964e460e89d431661b1b213d", "references": ["The, mover, he, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-4c883bfe5fc04288a13fe61054b57f25", "references": ["The, auditor, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-14b1dd5a296c4acda74374f62571b789", "references": ["The, janitor, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-40c63b4ce4f1492789583a0f04c1f403", "references": ["the, assistant, her"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-56a01ff0c55c4dd998b00aba07fff559", "references": ["The, mechanic, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-c57fdf3149914a4381fbd23848479ea8", "references": ["the, salesperson, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-1aa26d19742047578a73e0d0e5bdf680", "references": ["the, guard, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-41e86c02098e4bef87527b7277624130", "references": ["the, mover, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-c518b0c2e0fe4a17be4ed71f1c397791", "references": ["the, driver, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-dd8d84f2b9b5403d97da46f35da3b1c6", "references": ["The, receptionist, her"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-d8d1724dfe03420eb51315be302440fd", "references": ["the, lawyer, his"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-df776c7e6b5b45828ff7dce768a4e260", "references": ["the, analyst, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-e0cb2b75f57a4d26ab04954ded1e068a", "references": ["The, cleaner, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-0da49e2f58fe4303bcd717399e6b5969", "references": ["The, writer, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-077eb61b23134ac789ca407ff1ce4c6d", "references": ["the, manager, he"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1664-b0acb3b11da64a5493f2fda4a89e4a57", "references": ["The, editor, she"], "task_id": "task1664_winobias_text_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1161-2762aed49d33491f9fd274f97af05ff4", "references": ["Heterozygous Mutations in OAS1 Cause Infantile-Onset Pulmonary Alveolar Proteinosis with Hypogammaglobulinemia"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-2eba408a8efa4ce0a622acb61bc53a04", "references": ["The LXR ligand GW3965 inhibits Newcastle disease virus infection by affecting cholesterol homeostasis"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-44a4172f3cd949458cdc3df8ad07a4ef", "references": ["Fall-related attendance and associated hospitalisation of children and adolescents in Hong Kong: a 12-year retrospective study"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-d4e5c30547ad4190a137353db81584e6", "references": ["High Prevalence of Anelloviruses in Vitreous Fluid of Children With Seasonal Hyperacute Panuveitis"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-2e4d7f09512c4d74a46f7dbd24f82c4f", "references": ["Palmitoylation of the cysteine-rich endodomain of the SARS-coronavirus spike glycoprotein is important for spike-mediated cell fusion"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-c6012971402548dba6c54248aeba886f", "references": ["Characterizing Influenza surveillance systems performance: application of a Bayesian hierarchical statistical model to Hong Kong surveillance data"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-d7f114a1209b4786a5b6b5392467caf9", "references": ["Article Dimeric Structure of Pseudokinase RNase L Bound to 2-5A Reveals a Basis for Interferon-Induced Antiviral Activity"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-953763afb40b4031bf91af1584f3cb2c", "references": ["Multiplex MassTag-PCR for Respiratory Pathogens in Pediatric Nasopharyngeal Washes Negative by Conventional Diagnostic Testing Shows a High Prevalence of Viruses Belonging to a Newly Recognized Picornavirus Clade"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-989b925665b94afda4f95f29bc03d1cb", "references": ["viruses Virus Metagenomics in Farm Animals: A Systematic Review"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-33cf7637a04e4b83bb595e020d707fd2", "references": ["In-flight Transmission Cluster of COVID-19: A Retrospective Case Series Running title: In-flight Transmission Cluster of COVID-19"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-328bc38c02514befb67396f4a635115d", "references": ["Type I IFN family members: Similarity, differences and interaction"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-38dbd482245b4ce18c2769075d174a8f", "references": ["Emergence of a Large-Plaque Variant in Mice Infected with Coxsackievirus B3"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-0df5c5ef610542e19219c7516863313b", "references": ["Electrochemical nucleic acid detection based on parallel structural dsDNA/recombinant azurin hybrid"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-1858f1bd4f9b4dda8219dcf025ccd360", "references": ["Nuclear TRIM25 Specifically Targets Influenza Virus Ribonucleoproteins to Block the Onset of RNA Chain Elongation Article Nuclear TRIM25 Specifically Targets Influenza Virus Ribonucleoproteins to Block the Onset of RNA Chain Elongation"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-da65b7a43e324debbf3ccd4c3500eed3", "references": ["Comparative in vivo analysis of the nsp15 endoribonuclease of murine, porcine and severe acute respiratory syndrome coronaviruses"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-51347e1323334170886845a18b7760f8", "references": ["Comparison of FTD\u00ae respiratory pathogens 33 and a singleplex CDC assay for the detection of respiratory viruses: A study from Cameroon"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-7fe87d5c00574201bc671100cd36e55a", "references": ["Oscillation, cooperativity, and intermediates in the self-repressing gene"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-cfb5f8ee878a4ccb81f8a77083299692", "references": ["A recombinant Fab neutralizes dengue virus in vitro"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-98e32c56b938411b9e6d09767dce43ab", "references": ["European Journal of Case Reports in Internal Medicine Severe Immune Thrombocytopenia Complicated by Intracerebral Haemorrhage Associated with Coronavirus Infection: A Case Report and Literature Review"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-54758c5b31d64dd88dea397107ad5b42", "references": ["Specific Single or Double Proline Substitutions in the \"Spring-loaded\" Coiled-Coil Region of the Influenza Hemagglutinin Impair or Abolish Membrane Fusion Activity"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-4dc665fe9750493f91590517f8fe7127", "references": ["Downregulation of angiotensin-converting enzyme 2 by the neuraminidase protein of influenza A (H1N1) virus"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-052a1b62fb3e425db9d9fc8b71953341", "references": ["Evaluation of a coccidia vaccine using spray and gel applications"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-64fef9601b4e469dbf330cbee217e3dc", "references": ["Cross-sectional investigation and risk factor analysis of community-acquired and hospital-associated canine viral infectious respiratory disease complex"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-d127cc121355434e9f8c32fd8d172d9d", "references": ["Coronavirus disease-2019: is fever an adequate screening for the returning travelers?"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-1112210f573d4f78a5aa4700d0e0bae1", "references": ["marine drugs A Chemoinformatics Approach to the Discovery of Lead-Like Molecules from Marine and Microbial Sources En Route to Antitumor and Antibiotic Drugs"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-1841a6e36ea74b96b3d43fe27667d15b", "references": ["Oligonucleotide Functionalised Microbeads: Indispensable Tools for High-Throughput Aptamer Selection"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-d59ba8e07eb74f10b9c0a91b86875007", "references": ["Persistent Foot-and-Mouth Disease Virus Infection in the Nasopharynx of Cattle; Tissue-Specific Distribution and Local Cytokine Expression"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-24e6b1b49da945789a4738e3aaac7717", "references": ["Discovery of novel low-molecular-weight HIV-1 inhibitors interacting with cyclophilin A using in silico screening and biological evaluations"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-2e31335612f4477ba6e2a2cb99ff9e83", "references": ["AGING NEUROSCIENCE REVIEW ARTICLE Herpes simplex virus type 1 and Alzheimer's disease: increasing evidence for a major role of the virus"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-b409a7661a194b7c92611ba619987d6e", "references": ["Demyelination and remyelination in the dorsal funiculus of the rat spinal cord after heat injury"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-7f439f25cb034a8492d9abf48ffde818", "references": ["Oral Mutian\u00aeX stopped faecal feline coronavirus shedding by naturally infected cats"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-cda6b06f274f447fb0ee3f702dea0d28", "references": ["Pentoxifylline inhibits replication of Japanese encephalitis virus: a comparative study with ribavirin"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-47b1b2479d414d3d814ddb1ea97948ba", "references": ["Microstructure of atmospheric particles revealed by TXM and a new mode of influenza virus transmission"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-93764c04ce034159ade8909d5d7beff0", "references": ["Infiltration of immune T cells in the brain of mice with herpes simplex virus-induced encephalitis"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-1e6e04ee93ec4063b784804e8b6ee3b4", "references": ["Antibody response of definitive hosts against antigens of two life stages of the neuropathogenic schistosome Trichobilharzia regenti"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-f8c154978b1e481aafa8a8f6db138d55", "references": ["Contribution of porcine aminopeptidase N to porcine deltacoronavirus infection"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-6defd5294e73466f936ef154ed8a3309", "references": ["Vaccines and Therapeutics Against Hantaviruses"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-e69bb19bf77f47a091a2fb550ec65d1f", "references": ["PI3K-Akt-mTOR axis sustains rotavirus infection via the 4E-BP1 mediated autophagy pathway and represents an antiviral target"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-5037230db22d4aec9f847b288b7cc8cd", "references": ["Post-translational Processing of the Glycoproteins of Lymphocytic Choriomeningitis Virus"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-b3a56ab4c19c4ccdae67bea8c3c5cc06", "references": ["Immune Heterogeneity in Neuroinflammation: Dendritic Cells in the Brain"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-11bca56ff934461b9c468a1ac97e786e", "references": ["Identifying factors and target preventive therapies for Middle East Respiratory Syndrome sucsibtable patients"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-471c8650053946e58b3dbe14d7c0be61", "references": ["Bioinformatics and evolutionary insight on the spike glycoprotein gene of QX-like and Massachusetts strains of infectious bronchitis virus"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-ef9fff5a1cdf44eb8be37119da06e274", "references": ["Effectively Communicating the Uncertainties Surrounding Ebola Virus Transmission"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-65a10f8a12b34977b4af3f847dfe8aa1", "references": ["Influenza A (H1N1) pneumonia: HRCT findings* Pneumonia por v\u00edrus influenza A (H1N1): aspectos na TCAR"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-307f273287064d41ab2dc7ddb6e217e1", "references": ["Legal Aspects of Biosecurity"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-628962ef6d014b57b55664d18d8eb2d1", "references": ["Mortality estimates among adult patients with severe acute respiratory infections from two sentinel hospitals in southern Arizona, United States, 2010-2014"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-c0217b06a6d743dc9edb4981af6568f9", "references": ["The Importance of Understanding the Human-Animal Interface From Early Hominins to Global Citizens"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-62c2d6997f7f4e06b28e32ec5977c2d8", "references": ["Epitope Addition and Ablation via Manipulation of a Dengue Virus Serotype 1 Infectious Clone"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-781f78b9078a4e849b8149426da17a2b", "references": ["Cross-talking between autophagy and viral infection in mammalian cells"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-32410047bce04c1299ccc8cd1aeb1b4f", "references": ["Clinical Infectious Diseases Clinical Infectious Diseases \u00ae 2017;65(2):183-90 Procalcitonin as a Marker of Etiology in Adults Hospitalized With Community-Acquired Pneumonia"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-f43e034433b046e485e47f5582856c70", "references": ["Braess's Paradox in Epidemic Game: Better Condition Results in Less Payoff"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-a702b941de42411ea4450a57e5dc7961", "references": ["Are identity badges and lanyards in pediatrics potentially contaminated with viral pathogens?"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-a254a81f5da64e1fbce969ecd67a2ced", "references": ["Comparison of SARS and NL63 Papain-Like Protease Binding Sites and Binding Site Dynamics: Inhibitor Design Implications"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-57709bb0872540cc8644f50e156df15c", "references": ["Advancements in DNA vaccine vectors, non-mechanical delivery methods, and molecular adjuvants to increase immunogenicity"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-b7db9c93f5dd45b1a2d879dec91547df", "references": ["Discoveries in Molecular Genetics with the Adenovirus 12 System: Integration of Viral DNA and Epigenetic Consequences of hamster cells with Ad12 \u2022 Adenovirus type 12 (Ad12) \u2022 Ad12-induced hamster tumors \u2022 Ad12 viral oncogenesis \u2022 A model"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-f61f330e311b48f4a61639ad623f2d31", "references": ["Experimental infection of hamsters with avian paramyxovirus serotypes 1 to 9"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-6a83c357d3254f089e22c3c23fb98b0d", "references": ["Cloning and expression of the membrane protein gene of TGEV HB06 strain"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-10e94df7d15147ad989b2a95f7412708", "references": ["A20 (Tnfaip3) Deficiency in Myeloid Cells Protects against Influenza A Virus Infection"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-4872c9a5153340278a3a07ae549f1625", "references": ["Alternative conformations of a major antigenic site on RSV F"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-9f8098133c5847d2a5e6f3cf23bb7500", "references": ["The European race of Gremmeniella abietina hosts a single species of Gammapartitivirus showing a global distribution and possible recombinant events in its history"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-054bdc9c10d9483794b9f4e2f08d3de0", "references": ["Recent advances in thread-based microfluidics for diagnostic applications"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-9679fb8f338847a89da8a70f22770c0a", "references": ["Epidemiology of Enterovirus D68 in Ontario"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-1f2d288918cc4fe8ae1c338d7e40a391", "references": ["Acute Myocardial Injury of Patients with Coronavirus Disease 2019"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-a92150839bff4f1db3f902dbed154919", "references": ["Clustered Cases of Pneumonia among Healthcare Workers over a 1-year Period in Three Italian Hospitals: Applying the WHO SARS Alert"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-da3fb0a46ebc43b394ab8b7b7583c2b2", "references": ["Chapter 1 G-Quadruplex DNA and RNA"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-99a01dd112ea42d5970869ed841a77ab", "references": ["Threshold parameters for a model of epidemic spread among households and workplaces"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-1bf2a8c047b44df498349e8e13f9c10e", "references": ["VIEWPOINTS PaPErS journal of health global Improving health aid for a better planet: The planning, monitoring and evaluation tool (PLaNET)"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-6b7826cedded41d89e1abf7b9ca2c72c", "references": ["Host Gene Expression Profiling of Dengue Virus Infection in Cell Lines and Patients"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-7aaa303fd1574594b99402ffe1dc44b0", "references": ["Enhancement of the vaccinia virus/phage T7 RNA polymerase expression system using encephalomyocarditis virus 5'-untranslated region sequences"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-1de1027a571c43e7b80b122b4bb33726", "references": ["The potential SARS-CoV-2 entry inhibitor"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-70c84e817c3144abaf4a369c68380d54", "references": ["Influenza-associated Deaths in Tropical Singapore Influenza-associated Deaths in Tropical Singapore"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-ce42bb19fbbb4342969b99a24e26324a", "references": ["Mechanisms of Reovirus Bloodstream Dissemination"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-632784c8f61d4e0496af9484e6da22cb", "references": ["Core components for effective infection prevention and control programmes: new WHO evidence-based recommendations"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-321a969d804c49e7923536067beecdc8", "references": ["The Journal of Infectious Diseases High Environmental Stability of Hepatitis B Virus and Inactivation Requirements for Chemical Biocides"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-89f97dea454b4273a1fe0c894552a4eb", "references": ["And Why So Great a \"No?\" The Donor and Academic Communities' Failure to Confront Global Chronic Disease"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-c592dec928e643498240f70d41d8d7ff", "references": ["Bovine coronavirus in naturally and experimentally exposed calves; viral shedding and the potential for transmission"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-c0b8f9d757e740e79dbda36932db80ba", "references": ["Transmission of Panton-Valentine Leukocidin-Producing Staphylococcus aureus to a Physician during Resuscitation of a Child"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-2c875ec34fa2461ebca1444a7174b7bb", "references": ["Insights From Deep Sequencing of the HBV Genome-Unique, Tiny, and Misunderstood Abbreviations used in the"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-8d6d66e732824f4bbf5d8f8365f07cc1", "references": ["a section of the journal Frontiers in Microbiology IFN-\u03b2-inducing, unusual viral RNA species produced by paramyxovirus infection accumulated into distinct cytoplasmic structures in an RNA-type-dependent manner"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-08fe79ebe7ad4f98ac85b4990cfa05cc", "references": ["Activation of JNK1/2 and p38 MAPK signaling pathways promotes enterovirus 71 infection in immature dendritic cells"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-0ee437d3a1654ff8a011c43125a9562e", "references": ["Immunity-Related Protein Expression and Pathological Lung Damage in Mice Poststimulation with Ambient Particulate Matter from Live Bird Markets"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-d7834d7c635745d5bd000268f7ed1e0a", "references": ["Can long-term historical data from electronic medical records improve surveillance for epidemics of acute respiratory infections? A systematic evaluation"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-78d68714e8174bb4bdf1e327d92120ea", "references": ["Inhibitory Effects of Epigallocatechin Gallate and Its Glucoside on the Human Intestinal Maltase Inhibition"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-4307eef2ce7b4117b3baa0ec8bed5420", "references": ["Structural proteins of human respiratory coronavirus 0C43"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-bf2ea873d8364d12a5d540b596b583d9", "references": ["A human microsatellite DNA-mimicking oligodeoxynucleotide with CCT repeats negatively regulates TLR7/9-mediated innate immune responses via selected TLR pathways"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-5c5bf179be804aabaad9b5f0454d46b6", "references": ["The Journal of Infectious Diseases Fully Human Immunoglobulin G From Transchromosomic Bovines Treats Nonhuman Primates Infected With Ebola Virus Makona Isolate"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-9722fdbb70324274969c40ffc566e40d", "references": ["Physica A A dynamic mathematical test of international property securities bubbles and crashes"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-5d624749c81143908beb963de62b5139", "references": ["Inflammatory monocytes and the pathogenesis of viral encephalitis"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-bcd4d0e3ae2840089b19414c040ba3d4", "references": ["Molecular Sciences Review B Cells and Antibodies in Kawasaki Disease"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-c793e57d0b124702a0ded21ce9c4dbbc", "references": ["An Animal Model of MERS Produced by Infection of Rhesus Macaques With MERS Coronavirus"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-d3159fb93eee4eb98fa85f2d5f40e377", "references": ["Virus interactions with bacteria: Partners in the infectious dance"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-0a3baa7c0b2c468393f41c2c4d0c36ad", "references": ["Transmission Parameters of the 2001 Foot and Mouth Epidemic in Great Britain"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-e43e1f73cd9e47b79f479c708bb7257f", "references": ["Sequences, Annotation and Single Nucleotide Polymorphism of the Major Histocompatibility Complex in the Domestic Cat"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-31e568fb844343348b2c274b78a207ca", "references": ["Further Characterization of the Coronavirus Infectious Bronchitis Virus 3C-like Proteinase and Determination of a New Cleavage Site"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-1f44228daf1a40fea84217dbf50fa961", "references": ["Activation of Egr-1 expression in astrocytes by HIV-1 Tat: new insights into astrocyte-mediated Tat neurotoxicity"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-4880d1ff6f2f48559346c4aca88d72b6", "references": ["Low prevalence of equine coronavirus in foals in the largest thoroughbred horse breeding region of Japan, 2012-2014"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-5d44d4358a24483495fea09d030aabc2", "references": ["Clinical significance of dynamic monitoring of blood lactic acid, oxygenation index and C-reactive protein levels in patients with severe pneumonia"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-cb963e70376143639a5f39793d52fb8f", "references": ["Influenza and the Vocal Performer: Update on Prevention and Treatment"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-aaa6c952c9c743a6ba6b14322de07725", "references": ["Integration of Membrane Proteins into the Endoplasmic Reticulum Requires GTP"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1161-e062b67b8148493ba39b72d834f61c1e", "references": ["Autophagy induced by bovine viral diarrhea virus infection counteracts apoptosis and innate immune activation"], "task_id": "task1161_coda19_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task880-9e33376ae5514689996ae3eb47e9f987", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-4db6b6a0f4a84878ad6c6a12ec4509aa", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-36d9bd02962c4a67bae013c7622f24d7", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-990d6774af4e40058068d96f4f2a1b31", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-bb6f58a488184a959036fe5bf6c3bc38", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-43bf675621624c9daa3ee30060cd8e85", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-7a19a0b69a5a481689f344fd5541da4f", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-76130a19f974469aacb74b1e0fb742ed", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-05319a8a9b1f48a19f60795bee1d3631", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-d2252cbcf0d040d29960bbc1695256c4", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-8f99eb08bff3403285156c82e00ebd18", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-a993b9b0356340cd9fbd3c37487067f1", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-b7e35a58cf1e4214b738527b62ebbbcc", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-f4a4b9638ac34213a132bb63625f6f94", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-3fed020e0d684b159099d1459f36bd43", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-01654d0d3dd042b1aaf195263ffb8c64", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-8d0054dc46e7483ca8fda5247c4b1861", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-e0cf1d35ec1b407a924d8d622c5656d4", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-7181de154f544b12aa723d4d381a2bb6", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-0e2625449d04490592b7b41d718b0d54", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-e28cf0edc0bd4b9ba2268f9fb4cf8f77", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-0634735279fc4b16acc05459c87d8684", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-adb2161fcfc648d48d2bd4d8d41a6975", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-116d12ff3b2a475ba251b70507bb510d", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-bbc3e30150bb43429960448c255dc6dd", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-6fd96f6016a14a5186af1fd89c71a26f", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-23353d35cef3429fa1e4cfb3d1322d54", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-451a64d843244f7aa99d44048a1bbc36", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-ac61a105eaa949acb423bf88233407f3", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-76f892d3191f44dd904259c0d80a3433", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-8d05d47c91c94e45af8649ae41ea2ea1", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-ab70f42757784c91be9693314d111c5b", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-39093ecce7e849cf9cf58ab5a70df882", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-e01d5b65c5f343c09e82795172ee44ad", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-fa0d17849ea54a4a988c89466eccc5de", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-77bae35a2089468f8c302b02d5fe2535", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-9f4bea51a00c44959d78d1a320c3509f", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-34a00fd10b9d4d6692a01369567ebc8b", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-45eb2adb947148868aa5982eff8a00a0", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-a9d6b42cabbb4912a6f4a0c21c8b8481", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-e91ac3c6937d4591a7cf3104645b8542", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-adf9a5e233be448588715045597c0173", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-4c71513e1037438686768b4a53f9f6c4", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-98314531ff684e29af77c4f953c5c5d0", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-cfd28247d0ed40079c2b6fc92ca54eb2", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-4b28f1f9d88e43089ca1a4424d580f58", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-2dc4ff125e154985af3776dd0e8ee1d2", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-da602d835e564255ba828d051c4bc36a", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-044699899a21493e830e1b23e1f011cd", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-5cf1cf7646de44148d808c5bad0521fe", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-a1c1676a5d27458ea8fc93306a253184", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-07a618708a4849d8845281bf3e2ac3ed", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-15cc0153cd0f41548d28bd8f3aa9195b", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-5ab1034b232246ffbce22b180bd3630e", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-70ffdf8622064ea1b0b363f362706f8e", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-c28983a3484c4c0eba01fdf14f7aadce", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-8ceb8d5db4be44738f4b5a75062f3f56", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-fb6a56fe216d48a1a0c47c91d6531936", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-2e42d8511a4344ffa8b5437e00ee677a", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-d317b80fc1bf47d2b2bdd840fbeadfc8", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-ef74168bbd74404ca05ba168ac9b749a", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-f36f2677d8b942fcb2bef00998139d88", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-c38db869eb46424a86f6c6e3cd0fee96", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-24bf34d0616c40fea743b4bf1f99b82a", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-4c2082a074b0441fae201b508d63793a", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-71bc6ee06cd9453bad9fb2918cbf5a86", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-6bd0df24554f4c34bcf57b356976f14a", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-8d5ff877288f4832aa990b5a738aa474", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-28cce8e0d9dd4e3591e70cff4b5aa2b9", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-38357b6992414df993467b4e5116060e", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-dd60eee4ee7a4cdf9cb9d607a21b7164", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-de9497118da946f2ad0396eb3cfc72f3", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-39c45e0211e345d2b4b72b55164be7c6", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-0533bb380f0e424f94dfe365f860c193", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-3d17a5d8cd774843ab8a37a2a2bc1f83", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-9bc4d404a7a949ebaa35dc3ee0164894", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-623eab87236f4b4aafa85928491369bd", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-b0adc38cdcb24b43a80fb9caa39fe0e4", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-c9f7420d1fb24b688b129966f3943895", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-b5870db75d144c0bb9600804bc8994da", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-4783879c8ecd468897489dac76abb98d", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-37afe9fc3eaa4a849260ec21e7bbd7dd", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-8b33c19de2ea4a42b1c233a1746270bd", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-7b0bedfe67324e4084921bbc02242678", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-2e543a4c43cf46a08f761fb66734ff0e", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-e1f003cc2aa14deea69eb381016fee41", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-a6428eba5ac04dc189b7ce49842cce41", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-24dfd8b1ae3e470ea896a97490984774", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-9e1764fa1c324170b8f0781fb3f30e5c", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-aab925a3249f43d2a607f1e12ea92446", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-a4247b14acd1466d9e81c781bd530b5f", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-1ea0d0912901406797df454c934d3d86", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-4c8c1cb8b52f40d9bd2bed19babe593e", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-02407f80470c467294020f0e7c9faf5d", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-17338af7a3df4169bdba646ab8e226bc", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-f7823d8819a5453e95b71e0d6ac827fa", "references": ["REQUEST_ALTS"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-ef1faf9dfbb84c5b9f9187cb5f8524b3", "references": ["INFORM_INTENT"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-cfa8add9bdab457aada81a918966dc41", "references": ["OFFER"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-21e08c0131ae41c581e0732a7250241d", "references": ["REQUEST"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task880-322f32edf9e641a3a92c2edd49815ee2", "references": ["INFORM"], "task_id": "task880_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task738-c9c33d62d9a44563a6cafd4e6d9ec67b", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-7bde66d008f6449f90b430dac8a78257", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-9c72c59698de42b28e2eb660e71704d9", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-64c076c130a84ce7974ba89ee7b042e8", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-a13b5d667ada4ad9b3c2c4f3098903e6", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-4ef765ba5e2c4d68b037f6b3d4696e75", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-dc7eab43a64d4c079729b7609c50fe53", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-554a6f1d9fa3484fa004a2dcd610930f", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-dd5fd91596ed44b38181b13d4ad5b1c3", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-df72a31689eb42219dccc3174763efc3", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-7cfd93569c6a425782e0d6ab69d51e97", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-4c4ceb9d79f24e95914a5c069da81a46", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-610711bb5669485e9cf916439b3bcb01", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-80e4c0292b65466da5549342f73bc20d", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-f19ffef4b3bf436986d3ed3ca6af9269", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-d1cdd6aa9f8d4936b12ccc9eb16f9fb0", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-bdb32a02e39441acb7642d399f79e3f1", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-9fa6c37885064aa8a0aa5242365cd140", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-c65f1555232d49bd9772e501d0068972", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-bf920893ba3b4d0fbd6226951697a40e", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-49099fa9b2f84e2fa17fddaca4a9a1e3", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-9973794d3d034a9daaf82b163af3cb07", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-f7820de660d7440ebe760224c2c8cbd6", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-7345e5224ed749cba292946c8c645b2f", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-c311369b659640fd848640a4ff4cc22f", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-33873d41b1b0464ca4f999248aa9d363", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-12625021551e49d9ae5195be8b6124f9", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-93a5139742fd4d2789e78a90284c2083", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-91202058cd8c47bfb117ef2dc8ac0305", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-c4038473c1ea4d46ac3d468ce0d927bb", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-1fc16332fca24f209ef77c3bf4e62177", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-586ab9c6acf94199b747b6eb4cee79ac", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-9dc7bf57897e4d5b8c6ac05fa26761df", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-8b6967f37cee425d9c4aca8a059f691e", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-8458867be1e74f268e092de458d9a6b8", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-3c56b3af2c3d426ebbfc65808c12785e", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-20eaebedb5654f71aa910ddc7a0803e8", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-bef8becebe1143c3baa5a0f351768b96", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-d77f2f2a96654f798be20d3350971adf", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-affaf75d1262445cba93972723746cb2", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-320ed56728344d58b3f599f3f0bbd2fd", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-cd0b2b13c73349fdb64b517ed86a8bb8", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-138ca2d835994c6693d0011aa4ed0e37", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-f186796e9e2942ea8a3a71c520c98fc5", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-defbbb3239944dd3a851e36533befbde", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-2f885841fcfa43d0ba0abff7ee8de695", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-c3b3d12e905c4b9aa0dee27e246b0f53", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-65070959bd7b4a2c8a3a3fc4e770b222", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-8e6a0689088145778de1a021671d1112", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-610044dedbea4185b1474d3b920f6271", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-512d68c894d14aa9908553bf00c1a5db", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-bd9cca55cd7443279fe833d39af80615", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-167ee0b08f2e4425bf340371370e839a", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-93b97a2473ae4f9a9582d5df6d2ded18", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-fc3f0d2bfedf4f689329cba0cd31b71c", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-d8f87852811642d481e01d7aaed7ec42", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-39bcc9d6059f4f7a8982e87a4d8dcfe0", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-d5db9920fd364287a5958241c2ce00b3", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-06e6e098ae844c36ae9fd2190a7ae30b", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-b0738990de004d2d88cd4c5487749247", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-a66a3cbb55f2489a842f916f4e119def", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-89ad1c0e78c2440cb0c6033d630d998b", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-8cc31542ab944a3994fe0ce7d17b637c", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-7025a09304d44f71a97131bc4c09cb82", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-21ce4e9f80ec404cadef5df04c0a5f6a", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-21d3e6a68cdd40ada21d9e5ea50ac592", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-a4b42f69b2d34fb79f518ac2e0956a12", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-e563887e1c6f4ba4a482bea229bc8b97", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-56055cc093144503b2e60eaffdeb07ec", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-db022e2e115a4ecebf642f55707d8981", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-02499eac013747419f1bb07b156b1fc1", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-7fd1e579419e4177bc04c42de4a20267", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-9a0258ff3c5e4957b4d0d3effb847f7f", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-167c82ff7dc743a7945dc7ebb992af3c", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-10d23a7952af4c7fa1d2fe0029f935c7", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-784cdd0532b3482e94ce8112fef2242a", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-8327d6a5a2634f8cb9f56596252d962f", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-739a60f4bb834150a9583b144b13e46a", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-620ee84c8f394eb49d695bcc72385c9d", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-da431bde321d499bb30d56603da75bf4", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-cc96b45839494270b61fdeaccea6751f", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-c93897ec59544a82b50a860355220fc9", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-3b6fef95407a4932b075b73b66065c0c", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-9faba77bb7284636b22b57d5341de194", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-bc3936552f8d489baf6b2f18ceb6ab5c", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-e83bc9af41254d1daf2c78a9b4ff086f", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-40d7fb2e47804a8e9259b9633ab8b11f", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-edc361fe22054ee5bcbcfc7f74cac7ba", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-1bb7ed2e71004de2a6833474231f8bc1", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-8ef4e6637c45402b801d359b84d00010", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-f368a544b70247448392779901f55b01", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-79647babaff448b981c48541ef97997e", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-7d8a122c587e45d195eed1e6ac88bc7f", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-18d8930c48584a49807e7e698e7f654b", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-467ee4d383754746a868362ef37721e4", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-37f8c978742e48abb7f8b4e92a6eff5b", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-c2d3521d4941414a99831c41c11a60a7", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-1065a762399042698dbe5a037f8d99b2", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-8f3c7c6841c34fe18d5b10110e8bd8ce", "references": ["support"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task738-22db4694ffa04244acac91bb710d309a", "references": ["undermine"], "task_id": "task738_perspectrum_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1439-bd67557da6a24e50b86a619fc678960d", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-8869a0bbcc474773be7cb3fa3c269f87", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-dc344cb498824de986c9eedb55b97c3c", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-ac6d1d860b0744b8b0f4ca5b18fa04a5", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-eacdae4a0d6d4b11860bd13988c9c16f", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-abcf7e5c58dc48f9a03dba2b873e2986", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-5307bac4f4bd4c3a8db3b96ffa8e02a2", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-4ff6ba841a1e429fb5372aead81dc7b7", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-64552f91fa8d42dbb03df0f0af1b6499", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-98d3c89c7838438f8cacb454ffd9d553", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-fba9ad8a119e48329e633911c58fa1ba", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-123e25c9045647a6bc1a38ebf332f395", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-bd4f4cedc50542df975d25de658768c8", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-174d4d97f07c412293604cc018149ac4", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-4668dbd09be4433ab534cc878e9efc9e", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-a5f2b93fcaa14348b7808278916c3fd9", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-120793f6867a4f2bbafc6a498ae71407", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-75bfe279a3a3427c8eda181fed8ff2c0", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-d18b982a23ef4a17aa1e4b86e32d58a4", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-7f6796dc4f7f4bbc93a45e714630e1e1", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-d3cc4735c4f64d5d97253f83a5f10dd2", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-7d88f45c6eff4aea9353875c972cfd7c", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-a7475f0c3d61408e933225243e0471ad", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-7f4d54292a6b45b287add2b6800fbcb7", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-bd7de16c964a4b04a66f5be230d40449", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-08ca4ad1e9e9479e9e7bc3cbfb3c3793", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-3d7ca060f8fb450b9a4a33adf156b5a3", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-50636aa43fa64679b70cde92485dcbd5", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-e4c6715211c441f7a120f7fd1b31c3c4", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-f6d32f5e6be941e9bcae1c05f896435e", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-cd13f6ca28974fbda199e65aa5b25750", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-f9aab6d68d044457a62a2dcb29edf7af", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-820807bbbc564942a9673ff4b7100b67", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-0234ff2e0b9441c18a57ad3073a3a9da", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-6106e6ac2fdc44b486849eae66b87513", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-f08d967beab74bad8c711830d756e899", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-65d4e254808a4300a2f9f49dab569284", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-27d95c4c6c5443d4a6d1f5bee7666da1", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-c830a405c0c34bd295e3520d4d8d49ab", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-cd97162ac74049719bb2f40980e7f7c1", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-3a2029183b5b45b0b4fc1cf2c6904eca", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-6ebfce9d40a04028b3765c8407a9e715", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-17c2a2b83ac54cb590ce04a05879253f", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-e4d8c81eb5824dab9cd19f9a17bb8e45", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-8222056362794b7ba3263c0f0da2b4f9", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-0b4ac9fd1b754f4b84ed60a3d5bd0aad", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-f22ccb3343ec4f8aa4c456b6c58a6fd2", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-2b251a521ca74c6b9bbae695b0352863", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-edbd2ce766ee4379ab95cf88afdd99a8", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-d307cd230e9d45e0aa657d65a073e0c6", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-cd5dbea0197d4b8b96c4c99e96c9ecec", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-3e747651bc4f47a1a193a79f08246971", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-7a3022c1435b4f278d2ae06f6a8371e9", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-98ad7526effe4ebcb9da8516120c6083", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-cba9baa5b7b64601a2f2cfb26f4b1924", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-91326159b93041f0b9051b665ecc3d4c", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-9df6e94dfacb4f43b445a3bf0e81f2b0", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-f3cc118d9f80478cb585ac59d3083443", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-9ce7cf6de598405aa34ee38aa5ceff3c", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-1a2ae44cca2c4fc09e5d6982db88b88c", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-27b5b8f40b0e40aa8f7e6b791bb114ec", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-9389c68c5e834d4dadabdf28b1a7ed1d", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-ffe22fd633a149aa85854c10a8a94700", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-fadcd2265bf14e319acd4463bae11f18", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-dfbd727fcd1b4115b7cc6fe2e7e3b83c", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-ce30fa5855364dde9c60d333cd9130e5", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-09c1a8dc157f4c568c0a13f791fcb874", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-bc8a0a9a027b49d9ab92a526086c03e2", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-13f2b06aa1a64e808a59061f0c48c7ad", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-ed3356aab75d42ae80eae7ba50cae729", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-c5e77b7e8e794d908913ba08a4b2ed92", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-cf22b3f7a798422780976e6b79261117", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-5c31eb36a19340f3a3769be0b16595b6", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-5d55a2b6c2b14f10a2b3692c3ef32de6", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-543265d1b20444ac8aa939501d3b5110", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-13437d78785745a3abe89edabff5dac2", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-119902d5cd1b40319ecff8a0571adf0c", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-9b1e770884264294adffe057cc25ea81", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-28d81b71a5724bc4966d32ebbd988ea4", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-4f0ff9d92b804a26bf04fbde5cce9204", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-3c3956f256124279b5e869f2a08bf183", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-65ffa69a12af4d84b715881082309493", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-696dc72091fb480fa3be0b6cf568ba8a", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-08a1c3325bf741788c25f1f9f0732691", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-180aaaab97184f21840ae58cf3075df7", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-a3da0ffe105b46f2b35d8ee6a28110ec", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-52444b30c9d7404c8ce039de3cf3b904", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-ebcdf0c8927b42039175ca1efbea37e7", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-6c44be95503e48e19510ede0eef11af9", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-77b7013218fc4493b5a121d6fd713c95", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-b64630978f334b38949cfcc92adcb295", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-86086cf524f145f1904f312ced43a6e7", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-f51a31617d2f410db0bd5a49c81746a2", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-13cc74fc88814d32bb8c186a64d8fa22", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-bb1bd9f3ab8945b2a02fbbc45356f885", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-aa0b8650fc6149b1a101c1582eba88bc", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-eebc395b02ba45a2a1b70535a3ddddda", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-6b09656955ee4ae78d778145ebe021fe", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-743575815feb4795b20a5ca235035a23", "references": ["Yes"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1439-a77d5657fddf45748ea72f0680dab12a", "references": ["No"], "task_id": "task1439_doqa_cooking_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task645-d601858b099345b99eb1d9696f5e43e0", "references": ["lata mondal"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-a0409aacd1d14398869b657a39f387b1", "references": ["tuulikki ukkola"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-efc08d239f24449c878c41fad290c83c", "references": ["kot sarang"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-a18cf3f5b98d48e69873b7578caee8b5", "references": ["elisabeth charlotte of the palatinate"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-c5a0a3acfb5e49a2976ebeb09b4eb324", "references": ["not without my daughter"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-fbdfb70d018b46a8b63c2b645a026f05", "references": ["nbc symphony orchestra"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-a9c40ca0d9e84da284d6548d5b989858", "references": ["priyanka chopra"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-907d7b73b4ea4ea384235807773ddf21", "references": ["michigan international speedway"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-7360ec8be9d14a90aa1a5b866f53bfcb", "references": ["loneliness"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-9f5d7b8807fb441ba721a7086b11d6ae", "references": ["changdeokgung"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-c7329e252be249b9a289793982a3ccc9", "references": ["thiel-sur-acolin"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-7aa33b475ad444eb9a403e8e68ba4df1", "references": ["masayuki ochiai"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-c273c5285f5e4effb0be6e2c17e3f04e", "references": ["fernando del paso"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-9b11a2ae964e4e97993457e6b00672d6", "references": ["cape may bird observatory"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-a1a0fe15354e4792a09995d60bba8070", "references": ["william demarest"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-8432a39fe10641f8b8da05a88686062f", "references": ["charles cyphers"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-7f905d86685043679fdfbc5b4ebe536e", "references": ["illinois"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-eac11673ebe74631b7bea926eb03cfba", "references": ["the province"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-d2061f3bd96d4388af2a3d9b2edf3860", "references": ["mafic"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-6600ee49c58b4aaf9485607b5ec981a2", "references": ["north london line"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-66d54611f1db4ddd89ae05e1121e2a80", "references": ["dawson college shooting"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-6d0b46154c2241888d059d5017bb4fde", "references": ["old master print"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-67ee46b1c8e9495ba07727e2153cfa51", "references": ["charles kittel"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-eda91754c7154b89a4ce80e91d937d14", "references": ["bangsamoro republik"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-9dd86c6c60614e9482205a3a5e43db46", "references": ["melbourne storm"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-f70e2589100e4074a891ca773e763479", "references": ["yukitaka omi"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-a0bd45764dc34fe4acda61d2ffb0b415", "references": ["cottontail rabbit"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-3cfc3153bfa149eca4c2e1099cc669a3", "references": ["buffalo bills"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-a6a2a9c29cea4deca3cb73ae31340eb8", "references": ["pipo de clown"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-f9aff183402d4d8c97a012cee5ae9cc1", "references": ["vieille-chapelle"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-8cd42e49f564420e8963e503449b4ccb", "references": ["foxconn"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-d06afed6fbd24b269c94b603ff741076", "references": ["pope pius i"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-d601f18d22164d009309d421c9a63721", "references": ["gay byrne"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-5bb74179d1c04de191ca3e8b8192c4a7", "references": ["james webb space telescope"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-8324e0ef3d0e4f6891c79d971261b811", "references": ["galametz"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-be678c693e494679951665a4f4540187", "references": ["benjamin franklin"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-d2dfef5a85f4469a9b64b969f067cbac", "references": ["d-flat minor"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-078726c03b1a464f9ae141f120622604", "references": ["f-sharp minor"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-5c2717ca90e0472f8f226df1f434a285", "references": ["7 july 2005 london bombings"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-a968d31ad2d941b1b64b511b741275ec", "references": ["escherichia"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-91e99b87b3654d00b5f4dc7d9521181e", "references": ["creative commons licenses"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-62559f17e57240abb9aabbc92bf37d80", "references": ["bernhard rensch"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-becbab33ab264e76ba71d3f99b09b94e", "references": ["le bernard"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-59c09321769541bbbf4d4e155cb2d94b", "references": ["bob fitzgerald"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-8c080aca12c343c49162ec2815663f74", "references": ["british association for applied linguistics"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-cd5fe30dbe644480a704d121d83da9bc", "references": ["tadpole"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-807dd4b7ba2f41ae8e6f643dc742caa9", "references": ["island of the blue dolphins"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-d039a3f7ef114e1c93a0e9ee683e84ab", "references": ["altoids"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-66c0566f8203483283908691c97cb928", "references": ["vetlanda municipality"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-cb2167d9448f4ae8ad8e8c3f32ea73b3", "references": ["jared staal"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-1d2fe5fa93a6480eb4641fea0db6fb82", "references": ["taipei metro"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-30eb404109ff48d4a521f6ba4185b64d", "references": ["alyque padamsee"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-118b3d13cff54a52bafefabfd33a9c21", "references": ["sonja sutter"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-ad9865d7e7fb4c188a22589c0c90ea54", "references": ["sonic firestorm"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-42f9f895b8ee4898964b3bfd138449f1", "references": ["economy of azerbaijan"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-0333a0b27c404618b381ff0a3bf2e9a7", "references": ["royal ontario museum"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-2d67992f328e473ea0f5eedb3dcd573f", "references": ["polythene pam"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-b223fcf728004ba1ac814490aca62701", "references": ["anne shirley"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-ac9b4703cda94986ae7bf8e5a58b2661", "references": ["mixed government"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-6cda46db181c4b4ca2a08b8010a0f9e6", "references": ["parides childrenae"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-9ed709f11b11458c9263f493625e5c69", "references": ["adam ries"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-016d1a0609664dc487f4edb3e13728a8", "references": ["veronika marchenko"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-56480f26a8544103aabf2244ba8e7d7f", "references": ["murmansk"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-134836ceccf34af48683855d72ab9d00", "references": ["mario adorf"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-e12960cbf3394901896cb56f84ffe863", "references": ["mainz"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-ec92cab977ec4c64980e515f69f56d64", "references": ["snake river plain"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-d87dd4d66fb843288a3013e69ba298c3", "references": ["1957 formula one season"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-0d14487d84ab4f26aea49969f5f115a5", "references": ["derby museum and art gallery"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-f46be148337c47a78b1e237ffb9422d1", "references": ["enoshima"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-b916a35cf9d74a01aaa2b5c743f7350b", "references": ["samuel wesley"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-e9d3f2f7b11c4f63acd0325f6415ee65", "references": ["roger wolcott sperry"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-7ea2999e877a4665b04bcb99aaf66fb2", "references": ["green economy"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-5c25c0052fd041d1be42b4c624c9ec1e", "references": ["higher learning"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-c13891252dab44a7b2e0175fe7ec095d", "references": ["kimbra"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-1e740b0d7bd1455eb1b771b66b59c477", "references": ["hurricane audrey"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-6effcbe2b3fe4c9b81abab6cdb21e67d", "references": ["melleruds if"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-092a40838fd94fef8739c5b3bebc7ab9", "references": ["somali language"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-68ec85681e564d349a2161f4ffb8d168", "references": ["k'naan"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-f9c0370a355146148651a3e4dd3f8803", "references": ["onomatopoeia"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-10c9a7e468844b8f9ce6f37dfebdbd58", "references": ["kazuya maeda"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-bfe4a25895a84cdb94952a012ea00dd8", "references": ["walter rudolf hess"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-56e779e7dd2246939c0ef67ac824e184", "references": ["university of bern"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-6979a70176364532a62d27ae8ac17100", "references": ["killing joke"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-720e293c94ea44948b93efe776abf968", "references": ["arrondissement of loches"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-21a85a8d9bc7409b80aea63df482804c", "references": ["chinese postman problem"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-6f96f8410ef5415daf04d8b50878fea6", "references": ["robin van persie"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-f2406f2568bd4c2b8e53fdbc71ee9b18", "references": ["viriato clemente da cruz"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-6e8857e88a5e4452833327aab4be8a12", "references": ["landeronde"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-3b95cee919ed46d9b2f7b7656867d8ba", "references": ["sitting bull"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-5e3b88da4247468c8d65ff1c2ca34e99", "references": ["demographics of pakistan"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-2bc66ba0815f4b6e96f0b6ed5ae00c6f", "references": ["warfarin"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-6739cc4011ac41c29f0104a282dd205c", "references": ["maicon douglas sisenando"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-371e6e7d0bbe4837b14d4f674a1a83bd", "references": ["blake and the aliens"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-cfd8c5409b48473fa1a9bef49ce64931", "references": ["shinto shrine"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-2a38104a4a264b8db69ec8901b09a1f7", "references": ["legacy of kings"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-9fef3acef92f4fef9b3c297d5747d643", "references": ["chamonix"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-2fb67454a10e479e810c3658e4353095", "references": ["approximation theory"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-712e6efa587a4bec9d0624adb3223a95", "references": ["shooter"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-f30e4a3efd554443bfb602253229ec8a", "references": ["nashik"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task645-06a452bada6441c59f272040294237be", "references": ["norman rockwell"], "task_id": "task645_summarization", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task619-c7e24b1c285449d4914076d3a67657fe", "references": ["Tricyclic antidepressant overdose: emergency department findings as predictors of clinical course."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-e90eb909a5fa42edacafb40929e5ff29", "references": ["Serum glucose changes after administration of 50% dextrose solution: pre- and in-hospital calculations."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-31ff599d5f9b428da48e96da8175c7cb", "references": ["Nasogastric intubation: morbidity in an asymptomatic patient."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-e4b6c5036ed44c8ea8070c5de54de81c", "references": ["Massive transfusion without major complications after trauma."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-e3bfcc6142b749de8337f5e18c2bd96d", "references": ["Intraosseous infusion of phenytoin."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-ba6f04a94d674383bbbc7c9148331ef8", "references": ["Boerhaave's syndrome: an elusive diagnosis."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-91f0c4988a82440e971ba6979a75d4ca", "references": ["Intraosseous infusions: a usable technique."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-13aa1e1de68a40c794cacce90e5f69c9", "references": ["Intravenous hydrocarbon abuse."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-3060c333f6fb4b2d800435c2766f317f", "references": ["Thallium poisoning in cocaine abusers."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-d712f6b96a5f486ba2cbadc934983cd8", "references": ["Pulmonary edema induced by intravenous ethchlorvynol."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-dbbff69bb47249f6a1c0f45d9cf18ca4", "references": ["Atropine in the treatment of baclofen overdose."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-aaa1dfacd432414e8f2d9617cdb5dc32", "references": ["Ingestion of Compound W, an unusual caustic."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-5726fee19a2e401b948d7b570d643ebf", "references": ["Extractable ethylene oxide from cuprammonium cellulose plate dialyzers: importance of potting compound."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-e3ddbb4c663445a2b9db3da5b1eb7545", "references": ["Calcium carbonate as a phosphate binder in hemodialysis patients."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-c7d70f863cfa49d997fd460a210cf244", "references": ["Laser welding of large diameter arteries and veins."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-a0f103b2e2784a1e8d0c52c1c3741273", "references": ["In vivo evaluation of a permanently implantable thermal ventricular assist system."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-fba4804c17614a619f1378a78fdf8598", "references": ["Roller screw electric motor ventricular assist device."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-b511465b10d64ea8bd0fa3fbcc3a528a", "references": ["Removal of aluminum from chronic dialysis patients by administration of desferrioxamine and dialysis."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-a101618d0566468687bf11ea1d4ca86d", "references": ["Is air under the diaphragm a significant finding in CAPD patients?"], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-b57cdc443d2746a0a273b3a839436567", "references": ["Quantitation of platelet and fibrinogen-fibrin deposition on components of tissue valves (Ionescu-Shiley) in calves."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-3b95f0d6eb1f4d368e0e28eac4312fcf", "references": ["Maintenance of compliance in a small diameter arterial prosthesis."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-fc21ecf77daa4ec7a151afc32b1cca4e", "references": ["Bilirubin removal from a jaundiced premature infant by resin hemoperfusion."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-92d583638aa74b84a12e023a8ba1d0eb", "references": ["Development of a regenerated cellulose non-complement activating membrane for hemodialysis."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-6d3d2254ed2641578fa3d151facfb25c", "references": ["Pharmacokinetics of atracurium in anaesthetized infants and children."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-b916446d2dcd4c6abcbca8c3d3542687", "references": ["Use of atracurium during major abdominal surgery in infants with hepatic dysfunction from biliary atresia."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-47688d13f94d48c38e3cf0960107ea96", "references": ["Prospective study of liver function in children following multiple halothane anaesthetics at short intervals."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-adedf2a3b6cc4e18bf78acd552d6e44d", "references": ["Histamine release during the administration of atracurium or vecuronium in children."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-14bb8c76d58d41d88d8e30a15279c2c2", "references": ["Ventilation, ventilatory carbon dioxide and hormonal response during halothane anaesthesia and surgery in children after midazolam premedication."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-16fe885704c94ea9a75d1cc8646aa9c3", "references": ["Evoked electromyographic and mechanical responses of the adductor pollicis compared during the onset of neuromuscular blockade by atracurium or alcuronium, and during antagonism by neostigmine."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-ec0113e67bca4c8b8a86fba4a68e2f84", "references": ["Antagonism of profound neuromuscular blockade induced by vecuronium or atracurium. Comparison of neostigmine with edrophonium."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-0fa6082329c8495bb967d438d039541f", "references": ["Antagonism of atracurium-induced neuromuscular blockade by neostigmine or edrophonium."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-540c7295e0b446c980f11636234c07d6", "references": ["Reappearance of the train-of-four after neuromuscular blockade induced with tubocurarine, vecuronium or atracurium."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-70635fa402fe4e749120b3d981806d33", "references": ["Comparison of visual and measured train-of-four recovery after vecuronium-induced neuromuscular blockade using two anaesthetic techniques."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-ed534b1adfc649f19f1f4c55c3860ffc", "references": ["Pharmacokinetics of galanthamine (a long-acting anticholinesterase drug) in anaesthetized patients."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-4f85b4ebcaf94e9ebbd36c4a748ea80b", "references": ["A sheep preparation for studying interactions between blood flow and drug disposition. VI: Effects of general or subarachnoid anaesthesia on blood flow and chlormethiazole disposition."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-a0968e50df2b4a9291b15f7c23cc5cdd", "references": ["Placement of double-lumen endobronchial tubes. Correlation between clinical impressions and bronchoscopic findings."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-17cfcd6cf6a043c7b0c26bb01014f7a6", "references": ["An open study of vitamin D3 treatment in psoriasis vulgaris."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-f909e88f7c8b4036a926adb255c88ef5", "references": ["Successful treatment of psoriasis with topical application of active vitamin D3 analogue, 1 alpha,24-dihydroxycholecalciferol."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-f5aa636d839e40eca8d169a341d73734", "references": ["Circulating antibodies and antigenic cross-reactivity in Hendersonula toruloidea and Scytalidium hyalinum infections."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-fe1eb77c0fcc457bbd370ca04a26c601", "references": ["A quantitative study of the effect of topical indomethacin on cutaneous erythema induced by UVB and UVC radiation."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-d516ccc5196d43b9ab49b4368d444dcf", "references": ["Behavioural treatment of scratching in patients with atopic dermatitis."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-ced114b959f14212b1ca2e281f46104f", "references": ["Effect of percutaneous absorption of hydrocortisone on adrenocortical responsiveness in infants with severe skin disease."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-35cb57235ea0464eb538752da9b286c4", "references": ["Challenge reactions in atopic dermatitis after percutaneous entry of mite antigen."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-ad86ea4a8c314ecfa3b80c72f3ad85b2", "references": ["Secondary syphilis mimicking Sweet's syndrome."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-87b97d5c655f4c09b884c5dcd513d672", "references": ["Blisters over burn scars in a child."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-cb2828e93e684bc3a0bf25d752a49cd5", "references": ["The effects of inflammatory bowel disease on pregnancy: a case-controlled retrospective analysis."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-47dfd52c7627497ba369a9d417cd675e", "references": ["Alpha interferon in human pregnancy."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-7fa0868c4bdb4107a7395f8382de5f79", "references": ["Enhanced phagocytosis of mononuclear phagocytes in pregnancy."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-ee0dd938e0b34aafb2522d0e30674ebf", "references": ["Vergence amplitudes with random-dot stereograms."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-b156da432c7e499aac6863128703a2c2", "references": ["Posterior chamber intraocular lens implantation--a new forceps to simplify capsular bag fixation."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-837bb13ec5384b1699f9798835de295a", "references": ["Regression of Labrador keratopathy following cataract extraction."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-d214e5a51252462db4ebad9da8ed3ac0", "references": ["Sussex Eye Hospital sports injuries."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-ae47529846b54d2ab4280ba9e468aaf5", "references": ["Inferences from beading of a retinal vein draining a choroidal melanoma."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-159ece1f19b1475abe9a35798c9d341e", "references": ["Photographic recording of slit-lamp appearances of the ocular fundus."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-6112e4c0223a4efea345fe608730d4d1", "references": ["Sunglasses--an ocular hazard?"], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-8eb4be0b02564587b0ab0e55d4de7330", "references": ["Retinal cotton-wool spots: an early finding in diabetic retinopathy?"], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-2eb7b3a8502b4f0da0697e1c25b25d66", "references": ["The presumed neurotoxic effects of Catha edulis--an exotic plant now available in the United Kingdom."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-fc6a979af3a64d109cf185ea2b6634a4", "references": ["Histopathology of mitochondrial cytopathy and the Laurence-Moon-Biedl syndrome."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-a853dd4556bd4286a0bf9925726d591f", "references": ["Lysis of human fibroblast colony-forming cells and endothelial cells by monoclonal antibody (6-19) and complement."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-fe153bedb0474ab388cace4933ec112c", "references": ["Sl/Sld mouse bone marrow stroma in vitro contains an active radiation-sensitive inhibitor of normal hemopoiesis."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-35b877e8c55c4261b3cace6ed7f45c1f", "references": ["Expression of fibrinogen receptors during activation and subsequent desensitization of human platelets by epinephrine."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-337b3ff0879540e6b7ca9f29d816b9c4", "references": ["Morphological abnormalities in the lymphocytes of patients with the Wiskott-Aldrich syndrome."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-04e8fd31b5684bb1afc64467070b170e", "references": ["Quantitative relationship between Heinz body formation and red blood cell deformability."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-41eafc277d23447785314c220cd45962", "references": ["Beta zero-thalassemia in association with a gamma-globin gene quadruplication."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-71970f4cb89f4125a4caad6b34728e7e", "references": ["Hyposialylation of differentiation-inducer-resistant HL-60 cells."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-b6ee6632f692433ea1358d8adb26118a", "references": ["In vitro tumor cell cytolysis mediated by peptide defensins of human and rabbit granulocytes."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-8515b08bfed841189485c7221ecb86c7", "references": ["Peptic ulcer in rheumatoid arthritis--intrinsic or related to drug therapy?"], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-17881f1cff7c4355a24cb90de4d48e43", "references": ["Effect of corticosteroid therapy on blood monocyte superoxide generation in rheumatoid arthritis: studies in vitro and ex vivo."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-ada876bf67d941c8aedc06ed294e9e0b", "references": ["Comparison of methotrexate with azathioprine or 6-mercaptopurine in refractory rheumatoid arthritis: a life-table analysis."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-9797b41680f54cea9d2dd1300e450a20", "references": ["Thioguanine-resistant mutations induced by cytotoxic drugs in lymphocytes of patients with connective tissue diseases."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-49e1cee8992140d3bd6b6a48a0050983", "references": ["Intensive immunosuppression in intractable rheumatoid arthritis."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-598d93dcf40843099a393150bab97b8e", "references": ["Clinical tests for carpal tunnel syndrome: an evaluation."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-9ef3b21487464cdc9d17aea772ac30c6", "references": ["Synovial involvement in Hodgkin's disease."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-13615b2532ba4bb0a796526273e1bc58", "references": ["Episodic arthritis in cystic fibrosis: a case report."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-756dd485ea1b4cd89763f713d286957d", "references": ["Inhibitors of urinary stone formation in 40 recurrent stone formers."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-5f04674cab25490fb6ee72e91eddc1c7", "references": ["Fistula and sinus formation in xanthogranulomatous pyelonephritis. A clinicopathological review and report of four cases."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-4207ca3050494db0acae0095c5da99a9", "references": ["Renal metastases from carcinoma of the lung."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-d9ac876ebfe9488b9c773871a5785aad", "references": ["Rigid ureteroscopy for the treatment of ureteric calculi: experience in 120 cases."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-2c96ad6e412f4241bd907464527d8072", "references": ["Endoscopic correction of vesicoureteric reflux secondary to neuropathic bladder."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-d7df1f5aba2c403eb517c2d004ed1c59", "references": ["Influence of serotonin on lower urinary tract smooth muscle in vitro."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-69f181280f3d487f8b9a913084910f3e", "references": ["The pressure-volume plot and prediction of treatment outcome in female incontinence."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-3df6b64042aa4562986d30f5a81a9edf", "references": ["A 5-year follow-up of undiagnosed haematuria."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-3d873b9e38d74c9299d6174a853513ff", "references": ["Comparison of primary orchiectomy with oestrogen therapy in advanced prostatic cancer. A 2-year follow-up report of a national, prospective prostatic cancer study."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-4b47d59aea2f406ea28cb2f7e89cb265", "references": ["Fracture of the penis and long-term results of surgical treatment."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-3ac4519b99dc49f1b91947b53fdbacf8", "references": ["Movement deficits caused by hyperexcitable stretch reflexes in spastic humans."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-c2523483f4c94b79ade1e6fb1dcdbdb2", "references": ["The contributions of motor cortex, nigrostriatal dopamine and caudate-putamen to skilled forelimb use in the rat."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-1846f9eeb461438f923088c652e28fdb", "references": ["Frontal lobe dysfunction in Parkinson's disease. The cortical focus of neostriatal outflow."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-c2a92d9b89f446c3828335c02045a53d", "references": ["The effect of doxorubicin on slow and fast components of the axonal transport system in rats."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-33117e0f39254893af8f02f638573f00", "references": ["The clinical features of mitochondrial myopathy."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-aa882966f67643d190a31fab3a928428", "references": ["Focal loss of anterior horn cells in the cervical cord in motor neuron disease."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-0fc48830b7fa4b2c9b73361839dccc43", "references": ["Effects of early and late transection of the corpus callosum in children. A study of tactile and tactuomotor transfer and integration."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-ccd81181928f4084a915e08361563195", "references": ["Suppressor T cells in family members of patients with multiple sclerosis."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-1ae0b0d779b94fdda5dfaca0fab4719c", "references": ["Visuospatial function in Parkinson's disease."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-ae25d0626f2949c5bfc70b2e9dddd2d1", "references": ["Early detection of extravascular lung water in an inhalation injury animal model [published erratum appears in Burns Incl Therm Inj 1987 Feb;13(1):82]"], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-110152ea9df24e4e91a5bf59f489bc1a", "references": ["Functional changes in rat-liver mitochondria during the early phase of burn injury."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-8ba600897dad4e3eab482dd8fc00424c", "references": ["Use of scarred flaps and secondary flaps for reconstructive surgery of extensive burns."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-bb04c26a5d4847aa8d0fb545baa1e0f5", "references": ["Evaluation of E and EAC rosette-forming cells in patients with thermal injuries."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-604e5beed5fa4e75858f0781c64f24a3", "references": ["A multidisciplinary group approach to counselling the parents of burned children."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-bd48c79d85e54e97acc87cd94674c8ab", "references": ["Epidemiology of industrial burns in Brisbane."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task619-e1d4b63065864cf48de4e7ce6996618d", "references": ["Epidemiology of severe burn injuries."], "task_id": "task619_ohsumed_abstract_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1728-4a6d55166ffb463290807b3a605b94d4", "references": ["Asam pedas (from the Malay Peninsula region) comes from the countries of Indonesia and Malaysia. The main ingredients of Asam pedas are fish cooked in sour and hot sauce.", "Asam pedas comes from Indonesia, Malaysia and the Malay peninsula. It is a dish of fish cooked in a sour and hot sauce."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-015845dfb9d04dd586191bd6afedeb09", "references": ["The 1 Decembrie 1918 University is located in Romania. The country's leader is Klaus Johannis and the national anthem is De\u0219teapt\u0103-te, rom\u00e2ne!. It's ethnic group is German.", "The 1 Decembrie 1918 University is located in Romania. Romania's ethic group is the Germans of Romania; its leader is Klaus Iohannis and its anthem is Desteapta-te, romane!"], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-c8f928d9d6504fbe8cace8c7c583e34a", "references": ["Beef kway teow is found in the countries of Indonesia and Singapore.", "Beef kway teow is a dish commonly found in Singapore and Indonesia.", "Beef Kway Teow is made in Singapore and Indonesia."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-bd13eb6213f443e08fae57f168e214c0", "references": ["1634: The Bavarian Crisis is preceded by 1634: The Baltic War, both written by Virginia DeMarce and Eric Flint.", "'1634: The Bavarian Crisis', which was written by Virginia DeMarce and Eric Flint. was preceded by '1634: The Baltic War.'.", "Virginia DeMarce and Eric Flint are the authors of 1634: The Bavarian Crisis which was preceded by 1634: The Baltic War."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-d0bde83c4a814906a1a0249029cfd2d4", "references": ["Italy's leader is Pietro Grasso.", "Pietro Grasso is the leader in Italy.", "The leader of Italy is Pietro Grasso."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-443d049f701b4c2786774f05de8efe85", "references": ["The American Journal of Mathematics was first published in 1878 and is abbreviated to Am. J. Math. It has the ISSN number \"0002-9327\".", "The American Journal of Mathematics, or Am. J. Math., was published in 1878 with a ISSN number of 0002-9327."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-76b3747a3d244f0c8efec471c5e74f60", "references": ["AD Isidro Metapan has 10000 members.", "A.D. Isidro Metap\u00e1n has 10000 members.", "The number of members of Isidro Metap\u00e1n is 10000."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-2067ab284e0a49bb9e52694257dba22c", "references": ["AFC Ajax own and operate Sportpark De Toekomst. Tenants include AFC Ajax (amateurs), Ajax Youth Academy and Jong Ajax.", "AFC Ajax (amateurs)'s ground, which they operate, is Sportpark De Toekomst, the tenants of which are, Jong Ajax and the Ajax Youth Academy."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-c861431626de4006b908ee38377b7d54", "references": ["Coconut milk is an ingredient of Binignit along with sweet potatoes which belong to the solanales order of plants.", "Sweet potatoes, which are part of the order of Solanales, are one of the main ingredients of Binignit. A further ingredient is coconut milk.", "The main ingredients of binignit are sweet potatotes, that is of the order Solanales, and coconut milk."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-1832a1ca9c664d099a88175dc443b531", "references": ["Tomato is an ingredient in Arrabbiata sauce.", "Tomatoes are found in Arrabbiata sauce.", "Arrabbiata sauce includes tomatoes."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-848e6ad0ef434e418e075f8e23cc3624", "references": ["The capital of South Africa is Cape town and the leader is Cyril Ramaphosa. There are various ethnic groups within South Africa which include both Asian South Africans and White South Africans. Additionally, 11 Diagonal Street is located in South Africa.", "Cyril Ramaphosa is the leader of the South Africa where two of the ethnic groups are the White South Africans and the Asian South Africans. The capital city is Cape Town and the country is the location of 11 Diagonal Street.", "11 Diagonal Street is located in South Africa, the capital of which is Cape Town; Asian South Africans and white South Africans are ethnic groups, and the country's leader is called Cyril Ramaphosa."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-c5e37f95e8a84b5c942003fc79728b8e", "references": ["The Bacon Explosion originates in the United States. It contains bacon and is served as a main course.", "The Bacon Explosion is a main course dish from the United States."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-32c185ae31c14c148b804bcd06dfab2a", "references": ["Test pilot Elliot See was an American who was picked by NASA as part of the space mission in 1962. See was born in Dallas and died in St. Louis.", "Elliot See is originally from Dallas and joined NASA in 1962 where he flew as a test pilot. Elliot See died in St.Louis."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-0d1d7daa9f9e4da2ac253ea61c0f0979", "references": ["A.F.C.Fylde's ground is Kellamergh Park."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-18e30aae68384e2f942e6a0a971741a9", "references": ["The \"Associazione Sportiva Roma S.p.A.\" is the non-abbreviated name of A.S. Roma.", "A.S. Roma's fullname is Associazione Sportiva Roma S.p.A.", "The fullname of A.S. Roma is Associazione Sportiva Roma S.p.A."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-11f0255be4254806b589a3a9262c72e6", "references": ["Brandon Sanderson, born in Lincoln, Nebraska, authored Alcatraz Versus the Evil Librarians and Alcatraz Versus the Scrivener's Bones.", "Alcatraz Versus the Scrivener's Bone, written after Alcatraz Versus the Evil Librarians was written by Brandon Sanderson of Lincoln, Nebraska.", "Alcatraz versus the Evil LIbrarians and its sequel Alcatraz Versus the Scrivener's Bones were written by Brnadon Sanderson from Lincoln, Nebraska."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-fd7ebff2be7b466697a9b539165a2190", "references": ["Alan Bean was a crew member of NASA's Apollo 12 with commander David Scott. He was a test pilot.", "Test Pilot Alan Bean was a crew member of NASA's Apollo 12 mission under commander David Scott."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-22d6387e54e14674a1cbe1f7d7913e75", "references": ["The 11th Mississippi Infantry Monument is located at Seminary Ridge, Gettysburg, Adams County, Pennsylvania. The monument was competed in 2000 and is categorised as a contributing property.", "The 11th Mississippi Infantry Monument is located in the Seminary Ridge, Gettysburg, Adams County, Pennsylvania. It was established in 2000 and it is categorised as a contributing property.", "The 11th Mississippi Infantry monument is located at Seminary Ridge, the municipality of Gettysburg, Adams County, Pennsylvania. The monument was established in 2000 and is categorised as contributing property."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-0e923b819a714a87a9c6fed74c62b6f0", "references": ["The Federal Chancellor of Switzerland is called Johann Schneider-Ammann. The Accademia di Architettura di Mendrisio is located in Switzerland.", "Switzerland's leader is Federal Chancellor Johann Schneider-Ammann . The country is the location of the Accademia di Architettura di Mendrisio,.", "The Accademia di Architettura di Mendrisio is located in Switzerland. The country's leader Johann Schneider-Ammann is officially known as the Federal Chancellor."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-ca7720b57aff4f728eb208ffc865729d", "references": ["One of John Cowper Powys notable works is Oliver Glendower.", "The novel Owen Glendower is a notable work by the author John Cowper Powys.", "The novel Owen Glendower is a notable work by John Cowper Powys."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-f7431ae193ae4dfa92825aabba58c57e", "references": ["Guanciale is an ingredient in the traditional Italian Amatriciana sauce.", "Guanciale is an ingredient of Amatriciana sauce, from Italy.", "Guanciale is an ingredient used in the preparation of Amatriciana sauce which can be found in Italy."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-4c55725a7a1a464fbe911771bd5df40a", "references": ["Fried chicken is one of the ingredients in the dish 'Ayam Penyet' which comes from Malaysia.", "Fried chicken is an ingredient in the popular Malaysian dish of Ayam penyet."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-25309be7aa72405894f53161e019c1de", "references": ["A Fortress of Grey Ice is from the United States where there are many Asian Americans and the leader is Barack Obama.", "A Fortress of Grey Ice is from the United States where Barack Obama is the president and one of the ethnic groups is Asian Americans.", "A Fortress of Grey Ice is from the United States where Barack Obama is the leader and Asian Americans are one of the ethnic groups."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-4878bd179c574ea4b394d1faa9374233", "references": ["Banana leaf is an ingredient in Arem arem.", "Arem-arem uses the ingredient banana leaf.", "Arem-arem have banana leaf in it."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-b2804b56e4314e508e516cb31743bbf4", "references": ["The College of William and Mary is the owner of the Georgian styled Alan B. Miller Hall in Virginia.", "The College of William and Mary is the owner of the Alan B. Miller Hall in Williamsburg, Virginia, which was built in the Georgian style of architecture.", "The College of William and Mary is the owner of the Alan B. Miller Hall, it is located in Williamsburg, Virginia and has an Georgian architectural style."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-8726c2b5f22547c183bd352f2b334bbc", "references": ["Barny cakes, which were invented in 1999 can be served in 30g sizes and these contain 1.8g of protein, 18.0g of carbohydrates and 4.8g of fat."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-fed54b551693443fa186e3675d8c0d42", "references": ["Aarhus Airport is operated by Aarhus Lufthavn A/S.", "Aarhus Lufthavn A/S is the operation organisation of Aarhus Airport."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-a7db8130200a47c19356dd490d6abbc8", "references": ["Jens H\u00e4rtel is part of the SV Babelsberg 03 club. He has also been manager of 1 FC Magdeburg and represented FC Sachsen Leipzig.", "The manager of FC Magdeburg is Jens H\u00e4rtel who plays for SV Babelsberg 03 and has represented the club FC Sachsen Leipzig.", "Jens Hartel, who played for FC Sachsen Leipzig and now plays for SV Babelsberg 03, is the manager at FC Magdeburg."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-295bf739a347496f8d9ef0cccc1e0d66", "references": ["300 North LaSalle is located in Chicago, Cook County, Illinois where Susana Mendoza is a leader.", "Susana Mendoza is the leader of Chicago (Cook County, Illinois) where 300 North LaSalle is located.", "Susana Mendoza is the leader of Chicago (Cook County, Illinois), where 300 North LaSalle is located."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-a0face32c38843d594a53bc1696d07ad", "references": ["AZAL PFK Bakou is playing Azerbaijan PremierLeague, 2014\u201315.", "AZAL PFK were in the Azerbaijan Premier League in 2014-15."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-1bf904e068d240b6b5d1cef009602721", "references": ["Tim Brooke Taylor was born in Derbyshire.", "Tim Brooke-Taylor was born in Derbyshire."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-c84f9b275ba84285a22e01a8785abd0a", "references": ["The headquarter of Port Authority of New York and New Jersey in Four World Trade Center controls the Atlantic City International Airport at Egg Harbor Township in New Jersey, United States. Egg Harbor Township, New Jersey is part of Atlantic County, New Jersey. .", "The Four World Trade Center is the headquarters of the Port Authority of New York and New Jersey. who operate Atlantic City International Airport. The airport is located Egg Harbor Township, Atlantic County, New Jersey, United States.", "Atlantic City International Airport is in Egg Harbor Township, which is a part of Atlantic County, located in New Jersey, in the United States. The airport is operated by the Port Authority of New York and New Jersey, which has its headquarters at Four World Trade Center."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-02e5deccb28e4605a3edaf14b29003d2", "references": ["Andrews County Airport is located in Texas, United States, where the people are known as Texans. The language spoken in Texas is English and the capital city is Austin.", "The Andrews County Airport is located in Texas in the United States.The state's capital is Austin and its inhabitants are called Texans.English is one of the spoken languages of Texas."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-90cdd43e4f144809b8fcb8e2f596f1a8", "references": ["The Acharya Institute of Technology in Bangalore, India is affiliated with Visvesvaraya Technological University. The school has 700 post graduate students and its motto is Nurturing Excellence. Its full address is In Soldevanahalli, Acharya Dr. Sarvapalli Radhakrishnan Road, Hessarghatta Main Road, Bangalore \u2013 560090.", "The Acharya Institute of Technology is located in Soldevanahalli, Acharya Dr. Sarvapalli Radhakrishnan Road, Hessarghatta Main Road, Bangalore \u2013 560090, India. The institute's motto is \"Nurturing Excellence\" and it is affiliated with the Visvesvaraya Technological University.", "The Acharya Institute of Technology is located in the city of Bangalore in the country India. The institute has 700 postgraduate students and connections to the Visvesvaraya Technological University. The motto of the institute is \"Nurturing Excellence\" and the exact location is \"In Soldevanahalli, Acharya Dr. Sarvapalli Radhakrishnan Road, Hessarghatta Main Road, Bangalore - 560090."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-f773dae5401a45a99c971e0e703ea010", "references": ["Lancashire born Bill Oddie was the star of Bananaman, broadcasted by the BBC out of the broadcasting house in London."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-5c07c46cf7374c2bbbdc247c7e96a56f", "references": ["A.E Dimitra Efxeinoupolis is located in Greece where the language is Greek.", "The A.E Dimitra Efxeinoupolis club is located in Greece, the country where the language they speak is Greek.", "The A.E Dimitra Efxeinoupolis club is located in Greece, where Greek is spoken."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-b1c3a5bbcc644a7992bebcf81186474a", "references": ["At 84 metres above sea level, and with a runway length of 3684.0, Al Taqaddum Air Base is located in Habbaniyah and serves the city of Fallujah.", "Al-Tazaddum Air Base is located in Habbaniyah and serves the city of Fallujah. It is at an altitude of 84 metres above sea level and has a runway 3684 metres in length.", "Al-Taqaddum Air Base serves the city of Fallujah in Habbaniyah. It is 84 meters above sea level and its length is 3684 m."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-b8ab30dc3774422caf74c7fe6eda267b", "references": ["Andrews County Airport is located in Texas. Although Houston is the largest city, Austin, is the capital. Inhabitants of Texas have the demonym Tejano and Spanish is spoken.", "Andrews County Airport is located in Texas, where Spanish is a language spoken and where the largest city is Houston. Tejano is the name given to citizens of Texas, where the state capital is Austin."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-997f6cb38ec44633b61db93e9904356a", "references": ["Margrethe II and Lars Lokke Rasmussen lead the country of Denmark which is the location of Aarhus airport in Tirstrup, part of the Central Denmark region.", "Central Denmark is served by Aarhus Airport in Tirstrup. Denmark is led by Lars Lokke Ramussen and Margrethe II.", "In Tirstrup, Denmark, there is an airport call Aarhus Airport. It's in the central region of Denmark, which is led` by Margrethe II and Lars L\u00f8kke Rasmussen."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-1efbaef574b64f5aa34aea84f9ff04c9", "references": ["The Dead Man's Plack is located in England, of which the capital city is London. Cornish is a language spoken in England and one of the ethnic groups are The British Arabs.", "England's capital is London. The Cornish language is still spoken in some parts and one of the ethnic groups to be found are the British Arabs. A place of interest is Dead Man's Plack.", "The Dead Man's Plack is found in England which has the capital city of London. One of the languages spoken in England is Cornish and one of the ethnic groups is the British Arabs."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-0afb6854bf1a44eea6c65a9c25ed10d5", "references": ["Fountain County is in Indiana, in the United States.", "Fountain County is in Indiana in the U.S.", "Fountain County, Indiana is located within the United States."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-746db0b988054017986dfd2462cf5557", "references": ["The leader party at Alcobendas is the Peoples Party (Spain)."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-cb54868248cb41a5b410049cda2c53aa", "references": ["The Allama Iqbal International Airport's runway 18L/36R is 2900.0.long. It is located in Punjab, the capitalod Lahore city in Pakistan and managed by The Pakistan Civil Aviation Authority.", "The Pakistan Civil Aviation Authority operate Allama Iqbal International Airport in Punjab, Pakistan. It serves the city of Lahore and has the runway name 18L/36R which is 2900.0 in length."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-e344d73b4e664a019f9a7895103f1a27", "references": ["Baked Alaska comes from Hong Kong where Carrie Lam is a political leader.", "Carrie Lam is the politician who leads Hong Kong, where Baked Alaska originates."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-02bf14cb7ff24ce38d913ab2fa912d35", "references": ["Asherton, Texas is part of Dimmit County, Texas in the United States. Asher and Mary Isabelle Richardson House is in Asherton.", "Asherton, Texas is part of Dimmit County, Texas, United States. Asher and Mary Isabelle Richardson House is located in Asherton, Texas.", "Asher and Mary Isabelle Richardson House is located in Asherton, Dimmit County, Texas, United States."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-70dc448e46ff4bdcbf6aaa227060c99d", "references": ["Asilomar Conference Grounds , constructed in 1913 is located in Pacific Grove, California and has the reference number 87000823 in the the National Register of Historic Places.", "Asilomar Conference Grounds in Pacific Grove, California, reference number 87000823 in the National Register of Historic Places , was constructed in 1913."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-a4698d4dc61f4575a83ceb1b4d36c534", "references": ["Pork belly is an ingredient in the dish Bandeja paisa.", "Pork belly is an ingredient in Bandeja paisa.", "one of the ingredients of Bandeja paisa is Pork belly."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-8970923a648a4f8995026b79082381cc", "references": ["Chicago is part of Illinois.", "Chicago is part of the State of Illinois.", "the city of Chicago is part of the state of Illinois."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-e2d79599f6f04762b1b2abbf28514e89", "references": ["Jorge Humberto Rodr\u00edguez manages the A.D. Isidro Metapan and plays for the El Salvador national football team.", "Jorge Humberto Rodr\u00edguez, who is a member of the El Salvador National Football Team, also manages the team.", "Once manager of A D Isidro Metap\u00e1n, Jorge Humberto Rodriguez, plays for the El Salvador national football team."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-3eaf8498df2c4e57be3f3faf040be817", "references": ["Bill Oddie starred in Bananaman which was broadcast by the BBC and first aired on 3 October 1983.", "Bananaman a BBC TV series which stars Bill Oddie first aired on 3rd of October, 1983.", "Bill Oddie stars in Bananaman which first aired on 3 October 1983 and was broadcast by the BBC."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-1a47573bda6b4cd488568dd79c1b56cf", "references": ["83646315 is the LCCN number for Acta Mathematica Hungarica and the ISSN number is 1588-2632. Its bbreviation is \"Acta Math. Hungar.\" and its discipline is Math.", "Acta Mathematica Hungarica (Acta Math. Hungar.) which has a discipline of math has the LCCN number 83646315 and ISSN number 1588-2632.", "Acta Mathematica Hungarica has the abbreviation of Acta Math. Hungar and covers the academic discipline of Mathematics. This publication has the LCCN number 83646315 and the ISSN number 1588-2632."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-68ed8d29913d4a9a822bd335f9fd1312", "references": ["ACF Fiorentina play in Serie A."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-11f7268dfe644aa6b079cbe63a4f29a6", "references": ["The 388-page \"A Severed Wasp\" has an ISBN number of \"0-374-26131-8.", "A Severed Wasp has an ISBN number of \"0-374-26131-8\" and has 388 pages."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-49b2e7f219ed4a8fbb0ca47b1c25d416", "references": ["The book Alcatraz Versus the Evil Librarians comes from the U.S., where Joe Biden is a leader and Asian Americans are one of its ethnic groups.", "Alcatraz Versus the Evil Librarians is from The United States which has many Asian Americans. Joe Biden is a leader in the U.S.", "Alcatraz Versus the Evil Librarians is from The United States where Joe Biden is a leader and there are many Asian Americans."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-24ac9890a8bf4c4aae7aa5e6daf60624", "references": ["Adolfo Su\u00e1rez Madrid Barajas Airport is found in Alcobendas and has the runway name of 14L/32R with a length of 4100."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-38b09fe16e3b49969b9db3e999e14571", "references": ["Amdavad ni Gufa is located in Ahmedabad, and is in Gujarat. Gujarat once was lead by Anandiben Patel, of the Gujarat Legislative Assembly.", "Amdavad ni Gufa is located in Ahmedabad, Gujarat. Gujarat's leader is known as the Gujarat Legislative Assembly and his name is Anandiben Patel.", "The Gujarat Legislative Assembly is the leader of Gujarat where Amdavad ni Gufa is located in Ahmedabad."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-fe9c509dc39d42af9be80994e4549622", "references": ["The comic book character Blockbuster, aka Mark Desmond, was created by Gardner Fox and Tom Lyle.", "The comic character, Blockbuster, has the alternative name, Mark Desmond and was created by Tom Lyle and Gardner Fox.", "The comic character,Blockbuster(a.k.a. Mark Desmond) was created by Gardner Fox and Tom Lyle."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-c55d5670622c46a7b9fee3072ea017b7", "references": ["Alderney is served by Alderney Airport which has a runway length of 733.0 metres. Additionally, its 1st runway has an asphalt surface.", "Alderney Airport serves Laderney. Its runway has length of 722.0 and is made of asphalt.", "Alderney is served by Alderney airport which has a runway made of asphalt that is 733.0 in length."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-68250d90cad144a89d01cffd9b937c6c", "references": ["Monroe Township is in Madison County which is a part of Indiana.", "Monroe Township, Madison County is in Indiana."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-1c1e2891663a4b2daa5b9492ac4498d0", "references": ["Avocado is a fruit of the Laurales order and Lauraceae family. It is an ingredient found in Bandeja paisa. That dish comes from Antioquia Department region in Columbia.", "Bandeja paisa is found in the Antioquia Department of Colombia. It is made with avocado, a member of the Lauraceae family and Laurales plant order."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-ed8418b1302943dfa20d02f65d8eed3f", "references": ["Ahmet Davutoglu is a leader of Turkey. The Atat\u00fcrk Monument (\u0130zmir) is found in Turkey, made of Bronze and designed by Pietro Canonica.", "By the way, the leader of Turkey is Ahmet Davutoglu and the Ataturk Monument designed by Pietro Canonica is made of bronze and located in Izmir, Turkey.", "Ahmet Davutoglu is the leader of Turkey, the location of the bronze Atat\u00fcrk Monument in Izmir designed by Pietro Canonica."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-d40ba9aa15ac4797add14bfeacaade75", "references": ["The runway length at Ashgabat International Airport is 900.0.", "Ashgabat International Airport's runway is 900 meters long.", "The runway length of Ashgabat International Airport is 900.0."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-e1c4a857288d484a8a51d10ec9a6f47b", "references": ["Lemons (part of the rutaceae family.) are an ingredient in Bandeja paisa, a dish from the Antioquia Department in Columbia.", "Bandeja paisa originates from the Antioquia region of Colombia.One of its ingredients is lemon which belongs to the rutaceae family.", "Lemon (part of the rutaceae family), is an ingredient of Bandeja paisa, a typical Colombian dish from the Antioquia Department."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-34b06e29c6ee428ca16efd02cc8b4a90", "references": ["A varient of amatriciana sauce contains onion, garlic, black pepper and chilli.", "one Amatriciana sauce variation is onion, garlic, black pepper and chili- sometimes garlic is added.", "Variations of Amatriciana sauce includes onion, garlic, black pepper, and chili."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-adf9494210354f08a0022dbdb3b12e55", "references": ["Arlington is part of Tarrant County in Texas.", "Arlington, Tarrant County, is in Texas.", "Arlington is part of Tarrant County, Texas."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-6be1a6fd678940febd552ae6cce923f9", "references": ["Peter St\u00f6ger is affiliated with the SC Wiener Neustadt club.", "Peter St\u00f6ger played for SC Wiener Neustadt.", "Peter Stoger plays for SC Wiener Neustadt."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-f58d838c898842c89f2a73c7d14b6870", "references": ["The headquarters of Turkmenistan Airlines are in Ashgabat, Turkmenistan. Turkmenistan Airlines operates Ashgabat International Airport which has a runway that is 900 meters long.", "Ashgabat International Airport is operated by Turkmenistan Airlines and has a 900 meters long runway.The headquarters of Turkmenistan Airlines are located in Ashgabat,Turkmenistan and the hub airport of the airlines is the Turkmenbasi International Airport.", "Ashgabat International Airport, (runway length: 900 metres), is operated by Turkmenistan Airlines who have their headquarters in Turkmenistan. The hub Airport of Turkmenistan Airlines is Turkmenbashi International Airport and their HQ is at Ashgabat."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-c4e064f6fd914a09b4c3d5e0f9c2263d", "references": ["Fried chicken is an ingredient of the Javan dish Ayam penyet which is popular in Malaysia."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-34422372b98e41c1b4894c2193813f99", "references": ["The main airport in Texas, USA, is Andrews County. English is the primary language here.", "Andrews County Airport is located in English speaking Texas, in the United States.", "Andrews County Airport is located in Texas, United States where the language spoken is English."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-dd4c2f51e14c4fd197f26fc39737d0c4", "references": ["Operated by Aarhus Lufthavn A/S, the Aarhus Airport serves the city of Aarhus in Denmark and is 25 metres above the sea level. The airport has a runway named 10L.28R which is 2776.0 metres in length.", "Aarhus Airport, which is operated by Aarhus Lufthavn A/S, serves the city of Aarhus, Denmark. The airport is 25 metres above sea level and has a runway length of 2,776 and is named 10L/28R."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-0dd24978367e42fdae866e3b3f05c059", "references": ["Sour cream, chopped fruits, condensed milk. granola, raisins and shredded coconut are the main ingredients in Bionico.", "The main ingredients of Bionico are chopped fruits, sour cream, condensed milk, granola, shredded coconut and raisins.", "Chopped Fruits, sour cream, condensed milk, granola, shredded coconut and raisins are the main ingredients in bionico."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-97431b319489468dad7b9909757dce8f", "references": ["Ayam penet is popular in Malaysia and also found in Java.", "Ayam penyet is a food found in Java, and it comes from the region of Malaysia."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-9196763ea2b74c51917e89f73f006f38", "references": ["A.F.C. Blackpool, is in Blackpool, and play in the English Football League.", "AFC Blackpool ground is located in Blackpool and they play in the English Football League.", "English Football League club AFC Blackpool's ground is in Blackpool."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-423bcbdd099b41a99bae8b8e8eaad3b2", "references": ["Texas, in the United States, is the location of Andrews County Airport. The demonym for the inhabitants of Texas is Tejano, the largest city is Houston and the capital is Austin.", "The capitol of Texas (United States) is Austin but the largest city is Houston. The Andrews County Airport is also in Texas. The people who live there have the demonym of Tejano.", "Texas maintains the capital as Austin and is the home of Houston (the largest city in TX.) and the Andrews County Airport. Tejanos are people of Texas."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-a7535b72bb1a49a9995373b3b188b65b", "references": ["The AZAL Arena is located in Shuvulan.", "AZAL Arena is located in Shuvalan.", "The AZAL Arena is in Shuvalan."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-161b88e20a3845c39c0d3b8fcb3f850f", "references": ["Allama Iqbal International Airport is located in Punjab, Pakistan, which is led by the Provincial Assembly of the Punjab.", "Punjab, Pakistan, led by the Provincial Assembly, is the location of Allama Iqbal International Airport.", "Allama Iqbal International Airport is located in Punjab, Pakistan which is led by the Provincial Assembly of the Punjab."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-ef0273bbae6242dfbf0a87756f419e9d", "references": ["The Adare Manor is located in the Republic of Ireland where Irish is one of the official language's spoken and the currency used is the euro. Enda Kenny is the current leader for the republic.", "Adare Manor is located in the Republic of Ireland which has Irish as the official language, the euro as currency and is lead by Enda Kenny,."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-7a8685b1dda244dbbd872e02330ca688", "references": ["Frank de Boer played for the Netherlands national football team and manages the AFC Ajax team.", "The manager of AFC Ajax is Frank de Boer, who played for the Netherlands national football team.", "Frank de Boer, who played for the Netherlands national football team, is the manager of AFC Ajax."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-10818c708f3a404a9d519f4d34f2e82a", "references": ["San Sebastian de los Reyes is part of the Community of Madrid.", "San Sebasti\u00e1n de los Reyes is part of the Community of Madrid."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-409684c5e0c84a219702cd133d6abfce", "references": ["Alcatraz Versus the Evil Librarians is produced in print and has 320 pages.", "Alcatraz Versus the Evil Librarians is produced in print which is 320 pages long.", "The 320-page book, \"Alcatraz Versus the Evil Librarians\" is currently in print."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-67e34e13115d4f799ffef080950a87fb", "references": ["The School of Business and Social Sciences at the Aarhus University in Aarhus, Denmark has 16,000 students and 737 academic staff. It is affiliated to the European University Association, which has its HQ in Brussels.", "The city of Aarhus in Denmark is the location of the School of Business and Social Sciences at the Aarhus University. The School is affiliated with the European University Association which has its headquarters in Brussels and has 737 academic staff and 16000 students.", "School of Business and Social Sciences at the Aarhus University in Aarus, Denmark is affiliated with the European University Association headquartered in Brussels. The university has a staff compliment of 737 and 16000 students."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-f4c21df8a6a1467487678cd00d10a026", "references": ["Found in the Andalusia region, Chicharr\u00f3n is one of the ingredients in Bandeja paisa which is is part of Columbian cuisine.", "An ingredient found in Bandeja paisa is Chicharr\u00f3n that is found in the Andalusia region. The dish is typical of Colombian cuisine.", "Bandeja paisa is part of Columbian cuisine, one of its ingredients is Chicharr\u00f3n, that is found in the Andalusia region."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-b4bf683ea30f49f49ab3fff3a8435cf0", "references": ["Amatriciana sauce can be found in Italy where Rome is the capital.", "Rome is the capital of Italy where amatriciana sauce is a traditional accompaniment.", "Italy (capital: Rome) is the origin of Amatriciana sauce."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-73e7866e51f24da3a0cc9b20af7963f5", "references": ["The School of Business and Social Sciences at the Aarhus University is affiliated to the European University Association. It has 737 academic staff and 16,000 students. The European University Association has headquarters in Brussels.", "The School of Business and Social Sciences at the Aarhus University is affiliated to the European Universiity Association based in Brussels. The School has 16000 students and 737 academic staff."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-55f4e70667de4f53bb6bb72eba959861", "references": ["Tony Tan is a leader in Singapore where standard Chinese is spoken and where Ayam penet is from. This dish is also from Java, where one ethnic group is the Baduy.", "The Baduy is an ethnic group in Java. Ayam penyet is from the Singapore region and Java. The leader of Singapore is Tony Tan and the language there is Standard Chinese."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-f2d89dd2a8494ea3a4778ce72e529cc5", "references": ["All India Council for Technical Education is located in Mumbai.", "The All India Council for Technical Education is located in Mumbai."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-5002014b057e4f03b90ff5d584a46414", "references": ["Edwin E. Aldrin Jr, also known as Buzz Aldrin, was born on Jan 20, 1938 in Glen Ridge, New Jersey. He graduated from MIT with an Sc.D in 1963. He was a fighter pilot and a member of Apollo 11. He is now retired.", "Edwin E Aldrin Jr (more usually known as Buzz), has retired. He was born in Glen Ridge, New Jersey on 20 January 1930. He graduated in 1963 from MIT with a ScD and became a fighter pilot before becoming a crew member on Apollo 11."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-bcf53982e4c64153a9fc319f2de3263d", "references": ["The fullname of A.C. Chievo Verona is Associazione Calcio ChievoVerona S.r.l. and their ground in Verona holds 39371 fans. The club competed in the 2014 season.", "The fullname of A.C. Chievo Verona is Associazione Calcio ChievoVerona S.r.l. and their ground is in Verona. The club played in the 2014 season and has 39371 members.", "The full name of AC Chievo Verona, which has 39371 members and plays at its ground in Verona, is \"Associazione Calcio ChievoVerona S.r.l.\". The team played in the 2014 season."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-a7e000fedde74da3a46f1b59cb998c91", "references": ["Chicago (led by Susana Mendoza) is located in Cook County, Illinois, United States and is the location of 300 North LaSalle (which has 60 floors).", "In United States, Cook County, Illinois part Chicago where Susana Mendoza is the leader and also has place 300 North LaSalle which has 60 Floors.", "300 North LaSalle is in Chicago which is part of Cook County, Illinois in the U.S. The leader is Susana Mendoza."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-95a05b44cbc942f7b630ce23ad0577ad", "references": ["Baked Alaska comes from the country of France where there are leaders called Manuel Valls and Gerard Larcher and French is the language spoken. Baked Alaska can also be found in the the New York region.", "French speaking France, led by Gerard Larcher and Manual Valls, is the home of Baked Alaska. Baked Alaska is also known to come from New York.", "Baked Alaska is from New York region and the country of France, which leaders are Manual Valls and Gerard Larcher and the national language is French."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-d9928d17ecbd4cd4b99c956f1b3f4de5", "references": ["The OCLC number of A Loyal Character Dancer penned by Qiu Xiaolong is 49805501.", "A Loyal Character Dancer, written by Qiu Xiaolong, has the OCLC number 49805501.", "Qiu Xiaolong is the author of A Loyal Character Dancer, OCLC number 49805501."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-1721111253264960a46ca48e01b9f089", "references": ["The type of government in Aarhus is that of magistrate. The city of Aarhus is where the School of Business and Social Sciences at the Aarhus University is located.", "The School of Business and Social Sciences at Aarhus University in Aarhus has a Magistrate as its government.", "The School of Business and Social Sciences at the Aarhus University is found in the city of Aarhus which has Magistrate as its government type."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-c454e9a8f82143b98645fea9907be5c1", "references": ["John Madin was born in Birmingham (with Andrew Mitchell as a key leader) and became an architect, designing 103 Colmore Row.", "Andrew Mitchell is a leader in Birmingham where the architect John Madin who designed 103 Colmore Row was born."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-7b7e4300adae44d5a0686cdbdbf5cfc5", "references": ["The book The Secret Scripture followed the book \"A Long Long Way\", which comes from Ireland.", "The book A Long Long Way was written in Ireland and the second book in the series is The Secret Scripture.", "A Long Long Way was written in Ireland and is followed by the book The Secret Scripture."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-43609fa427d8448a85a9f060adbf7c9f", "references": ["The Amsterdamsche Football Club Ajax Amateurs is the complete name for the AFC Ajax (amateurs) who played in the Topklasse in the 2014-2015 season.", "The AFC Ajax amateurs (Amsterdamsche Football Club Ajax Amateurs) played the 2014-2015 season in the Topklasse.", "AFC Ajax (amateurs), with full name \"Amsterdamsche Football Club Ajax Amateurs\", played in the Topklasse in the 2014-2015 season."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-0c4a57d956484c8aa7337f2189e54140", "references": ["One of the languages spoken in England is the Cornish language.", "Cornish language is spoken in England.", "Cornish is a language spoken in the Cornwall region of England."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-c19b7e8a8cec449386d724e1229f49c3", "references": ["African Americans are one of the ethnic groups in the United States where the dish known as Baked Alaska can be found.", "Baked Alaska is from the United States (along with ethnic African Americans)."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-abc39fa29c134d27a11f904a86233fbb", "references": ["20 Fenchurch Street has a floor area of 62145.3 square metres."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-6a34add43a284be8b30ed8dd5a7179bc", "references": ["The avocado is a member of the Lauraceae family.", "Avocado is part of the Lauraceae family.", "The avocado plant belongs to the Lauraceae family."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1728-1645f5055aa446fcb8f20d8e0137161e", "references": ["Graeme Garden, one of the stars of Bananaman, was born in Aberdeen.", "Bananaman stars Graeme Garden who was born in Aberdeen.", "Graeme Garden, born in Aberdeen, stars in Bananaman."], "task_id": "task1728_web_nlg_data_to_text", "task_category": "Data to Text", "track": "default"}
{"id": "task1640-b65c87776e604522bc24f685a839977f", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-ffce8f8bac0f4da999ff4912194f9910", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-b02ef2dbd4a04dd7aa57ec8529882a95", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-1bde006804a34ea6a04aea5750d57ba9", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-6fe5d6e568c14d55ad893e825f956f32", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-2dc2d91c800c44d1b897daaa2044e3cf", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-97231de8dd654642b46792c7ea36a5f0", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-eea724d953c64bfc83eda4363ed37218", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-b22936adcdb94fe998aa7d29a6772b83", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-9d55dd08a8074b60ac75c0b2cbba3f6b", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-112d4edc0dc3467c91aa54b587a48ad4", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-26a4a2f4e5e14b4b9fda495cf3971546", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-c54b51c04f3d49dfb9eae4b2e229a374", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-f3cc2cb8a88e4dd9aa0e13aba9b4905d", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-249a2b76fece42ca8bbfb4ce9067ff67", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-ea577feb57af4b16803e56fe41e88cc9", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-d4e6f5b6b1e4495eaa8ec91dab0787be", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-eaf1c4d8e48d40398c09aba0adeb0347", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-b1d755e1f8354ef7b9277dd0b3a9d577", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-18ebc28a67ef4f8fb8dc0d09d2818e14", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-d604f946892f45fb83afb1f6d02cb25b", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-f9a1d473686046ef926a0ead201ee880", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-9702d5c5f0cf4cec8e80045b8e73c511", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-910f36834a69453f9e7b52d87c2c5db3", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-7a8369425adb4cf38e36eff62a4284df", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-74eac1242d054ade9b84c1c076c836b1", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-e7a35c58539b448ab048911f6946ee05", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-fe4f1a067cdc4326ae0d4e85e9c2444e", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-19d9453eb71f4652a249b0fbdf3a4e71", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-d1fe18978e1941f68a658bdf55f686b4", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-e54afa2141d148ed8e7dd0492f3ee87f", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-9446222abede4565b60d570dd39c8303", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-dab07168e3ee4d8c9ee4b824838ed604", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-89ae275937744585a8ff69e0249c24a4", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-7e926f7c715645c4b0245740b90375da", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-fce1de09cb2c43a29b67b6dcd55d77c3", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-4978225e9bb14d47a149a9accb452160", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-5b6439ec1fda4066a93e97b1bcf321a2", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-1262259a0452404fb5320330e2f1a41c", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-613c613a9f514f6eb0af49cb866f6332", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-704da400e81b41cc91d78e11e26e7d2b", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-e0a30431bb5d4840b6226e5c32f05fca", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-9feeec30be4f4d3bbcaaf9112189d4f8", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-dd45c716d2b94fd291b6bbf5d4b26cd3", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-a336a13097b64403b58e45765f3fb740", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-b7f58d90d8994321ab451fedc32cc6b5", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-28bbc60013dd487f94e4d6f3b7fa7f1d", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-1caee41acb0a449f89c292f199f71b61", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-a5e11ec4ede74728a63c174795f3fa45", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-f77e3db733ba4922b5547a770f5f5869", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-90ce20d17fcc4f119fe2428d727a3c34", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-af42825e70584c6abd0ac3d54067c3d8", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-ddd726e5cac6467083554ca2f0df0bb6", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-051301cba2bf454a9ef9249953f225c2", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-afd0fae87cb442f499eb1e86659b2ac5", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-437585c079414c5bb22fb881828872db", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-90e0fe65b46740e7ab045ef4a6f55570", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-e35328c269ff457780ad1786b5562a80", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-867f966a5c8946ef9a62b852e4bc58b1", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-795c610c7f114fd5a7dd51b37c258e46", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-94c09eb5c66a453e8f63c4e859c13c61", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-2d58843b86d74146af243be9e69686d8", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-27e35fe39dae4b8e8765ff549fdb96f5", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-0c69a72a84064805ac97a952d6780503", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-020c05d0d9624c0f891382fc4c88034d", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-26dc757c06f5443c8056d85ae61d440c", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-51d016c1d0614279aaa596c19fde9616", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-2fa3f08853d34102917ad0e141e9348e", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-fdd2e1d8904b49e7be27cefec4ec4dac", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-ff87611ec2574bf6a7e0f80850f863d7", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-d5d14beb916a47b4a352405212400ff3", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-a3cbe31939934e999662c2c30b4e5e1d", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-ddaa40b165c74444831a58b62a950c88", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-c463fd8660e9445ba9ba0cdd990a1005", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-cf48b6951a9548a79f594a6860533156", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-9bba753c3511443eba14b7fc4cba6f39", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-76942a15104f4fd89a2e5aa2357a2a59", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-df95549c9b9840349aa34297b89b6dfb", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-832047a9ef6a42869d7239f582538696", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-95efb514733e449b8433981be0004331", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-9c47a3805a3945b0928c35048ddf0236", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-4758e78ce40c4ad8a6179003383f1ff8", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-dc2e9437514f4c3bab827dd3964a8d2a", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-779e31603ae9403cb7f5141f65bcbbbc", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-0ec801a124fa48a08225168a63edea64", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-244a89abead3496f9adf416c82d1db15", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-a5b069ac574547498f5dbf9fca929531", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-df78a7ac896f4bdb9a3523f4dd602e2b", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-5cc197d0c3d44b8ab2150e3e940438c9", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-950de384bbc547ef884e322baa95dd9f", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-1ee9ba268c1e4111873296c46d174b45", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-0a23a607f2b94bb2a1e8ef352c544156", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-73890d9bdd5641b8a4c44d8b6b15da1a", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-3e45830f5b9e410eb647abd75ab5d059", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-ab6c35db04cf4868b0c547b43e8fb61c", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-60e3b32f66334764ad89ce9271d7248f", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-ad51ecfb885c4d97ae6b2ffe50dd1905", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-fad2af5716a44e48ac7d8b183f89e951", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-2b9ce18651914b1291da9596a914ca47", "references": ["True"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1640-ac1d418507774dc48898e0a6a9860e7d", "references": ["False"], "task_id": "task1640_aqa1.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task648-f3b6e5ecf0d54875b47af379a505cfd2", "references": ["the city councilmen"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-3a3beedf626d40f8ba15b2690667e041", "references": ["the demonstrators"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-d62da6927d404b45b1dbea6bc37bc5dd", "references": ["the trophy"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-f2bcfdb5cd5f4d95abb4f67edfd2bbd4", "references": ["the suitcase"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-80e511d69d8f4a52af96ba22b503ee7a", "references": ["joan"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-67cc16958a064aab9502131a390177a3", "references": ["susan"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-d024d1adf8504e8d9c0b0abce8836a21", "references": ["paul"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-177dec0e767c401e8cc6d213bba4a11e", "references": ["george"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-d5e2936f1ae64d50b34f7bde321cc140", "references": ["the lawyer"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-1d8931799c8548febf67558d8cdc0846", "references": ["the witness"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-abb3875e561e446594abe2ee0ab09919", "references": ["the delivery truck"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-5590abcdf01b4b14b3a6f0cecf1e0828", "references": ["the school bus"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-a38d077cca494d4a9b34880327b6023d", "references": ["frank"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-f5b7dbb27a2446a7a0abaa91fb8769c0", "references": ["bill"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-ecfc9b9b57cb4c0aa877a76a351e358e", "references": ["the man"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-920273302a9e4f4188167ecebad94262", "references": ["the son"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-d5ae72ef6cf9417c87bea3a8a846720b", "references": ["the large ball"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-189c1e1da0aa40e3a90e466d09e69f45", "references": ["the table"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-323b6bc7184f48d1840b709ae2cb083b", "references": ["john"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-5fa32274f80646b58789dbcb1f61ee66", "references": ["billy"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-6b490e3535474079b84b27fd7cb12b08", "references": ["tom"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-418857d1923146959efda110e97a1fb7", "references": ["ray"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-db12869431da4e23afdd2afcf78e9f3a", "references": ["sue"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-65f20a29c95647c681b9596926cb345d", "references": ["sally"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-df7c076156754e37bbfa4705e0847048", "references": ["the sculpture"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-58871ad95b304b8686e44dd11a2f0793", "references": ["the shelf"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-6da690f0b49d4c87a911243eb65c7f1c", "references": ["sam's drawing"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-a4990b5e893e4fe6b7a8073940467a01", "references": ["tina's drawing"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-5b251223d17f4ddeb39efd968182efb2", "references": ["anna"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-f341a133282a4aa9acfa61a91c7b814d", "references": ["lucy"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-65ff12df39984c468f310f95636df051", "references": ["the firemen"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-dd910110a00f4d069ff14857f4cbd131", "references": ["the police"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-091c32f1803d4747bcd106de75367e7c", "references": ["jim"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-cf7bd2ebbd904b048dd032f46071965f", "references": ["kevin"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-884b3205373d4861b71433b44f45d100", "references": ["the sack of potatoes"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-b8cf6bcec46741e497781eef2b0c9d39", "references": ["the bag of flour"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-7e67ee7f65b04c989231e3de6778c675", "references": ["pete"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-6fd3eb4390fb45da846cde1828153661", "references": ["martin"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-6bc5e63aae1142e2aa1cb27a7bc61ae2", "references": ["the older students"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-24de5529ebe1460289027b4b0f8d5234", "references": ["the younger students"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-3c5975e6fb494a6e8e6de255afc5c688", "references": ["the bottle"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-e0f6006d048b4d89af75bbf886882a69", "references": ["the cup"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-9c88a7b6f8f143959c6b8bce3cce02e5", "references": ["ann"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-5db97f281b374095a5ae9a84cca90851", "references": ["sid"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-927182792b494659aefb312a2257e51e", "references": ["mark"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-5a1aeed73b254d88b2080287df842c34", "references": ["joe"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-8abe17a4f82c41b8ae63e56b1e66918c", "references": ["joe's uncle"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-0bdc79a3213b49fd844522ab8b3e7752", "references": ["the painting"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-e9e86d41b49242e39fd83b31ba73bc83", "references": ["the oak tree"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-9398b350ae2e4d05a53549431cf77530", "references": ["the gap"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-e8fc81ec2636400b8848e0c94cc8c8f5", "references": ["the wall"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-fb7379d8c1d74f90b4b954fd68b4d342", "references": ["the drain"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-861979630a4c436ca1ef4c1e6e4f1c36", "references": ["the hair"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-92108d45262b497d9f85182e6731c354", "references": ["the meeting"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-8b8567b04bf64156986c11f8059a665e", "references": ["the train"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-ddeb36658c3a4277931fb73082fa4ca3", "references": ["the pillar"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-459f30eab41a435b9c150a9f3aa6e051", "references": ["the stage"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-afb2c9273c604546977fe30da1197164", "references": ["the announcement"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-0a584c1e2afc4a21a400891229830f33", "references": ["the subway"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-2cde7fe7339c4b9192f48339157f8513", "references": ["the concert"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-1b44f48de3b84b0eb4bee0c5684d703b", "references": ["the rain"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-1b20ad74536f4751a696c4040a235ebb", "references": ["the rag"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-54bd3d886d8e46bf8a4805be71d002be", "references": ["the knife"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-f29846550adf4e75b44507a9b97e12ca", "references": ["mary"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-3f6bd98da8bd44e2ab4dacbc9faff52a", "references": ["the water bottle"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-870c13f5fc7c495c93279d0d87455684", "references": ["the backpack"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-a42102e60e1d4145b471cd145578809c", "references": ["the pot"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-2f1138aff075443f830f203e9d935040", "references": ["the map"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-5213dc3eaae4483ab9f18d8dcb887c56", "references": ["the building"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-52e042b475674884bf5d7cb9a4e77bd8", "references": ["bob"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-34c85dc7c0bd444d9c8f4e255606eb64", "references": ["charlie"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-6ea9e59d07ab4d34a9cff1c10d05fc7e", "references": ["adam"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-9c69033c13f44bbc902509c0e2463d0f", "references": ["the con artist"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-a65f43b9a6bc47a59422d5ce389520be", "references": ["sam"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-bfe9489763f8499182654f53df0a5aab", "references": ["the dog"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-2b9ccb8725c74e47afe8b15b541f5594", "references": ["the spot under the tree"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-c9a5830066e14289968c28c6206f626b", "references": ["the cat"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-26a6bbc5ec714a5482e90c6d348db7f3", "references": ["the mouse"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-0ace79a128934d91934469bcadd00835", "references": ["anne"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-6aedc0ab27ac48f9925814aded92c24d", "references": ["anne's daughter"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-a24122cbb93a4a5e8373f5a321ab6a6b", "references": ["alice"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-edb74f35c7934a148cd69dacc6b5b6b8", "references": ["alice's daughter"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-99fc9790065c4307924c5e8706aa7080", "references": ["the guy in uniform"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-79fe793005b5497b85b69f0650aeafe0", "references": ["the fish"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-917528b4c29d41dcb11bc9f31a5f5a7b", "references": ["the worm"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-c4418b795fd14909a16fd343e75f5a81", "references": ["the key"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-86a1894a80ea403086c05a40224edb4a", "references": ["the chewing gum"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-3cfd47c2be344206b43d4cb93c68e475", "references": ["the tree"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-d812fa7fc07c43c7b85e55c4c35e3f46", "references": ["the roof"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-0e6cf47b7b574bd0ba5f5226cd7ec8c1", "references": ["the customer"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-076cf82f28e94e2eb7366527b72db927", "references": ["the teller"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-d54893470d974c2cb0269d65699a25bc", "references": ["the juggler"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-f4de9a2c74cd4bcf939478c563137760", "references": ["carl"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-52bce6e777454dd1bca47761032aafa3", "references": ["sam and amy"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-dcb58ac3a23e428ebee8df06645f0a8e", "references": ["amy's parents"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-95c97f60ae914fac956eb8d7cf9b0cbd", "references": ["the old house"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-5556158c8e8e434eb56e4f04198962e5", "references": ["the new house"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-3565ced3e7754ab9985baf16b79c2ea1", "references": ["people"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-7e70bc01ac6846028dfbf66d61d62e9a", "references": ["paul's books"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task648-22208fbd96cf4aaa80bd962e0b5a0a35", "references": ["the flute"], "task_id": "task648_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task242-f505483b0ac949da841d2e0b6c7aa046", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-ce2e8ade051349c5b67cbc99f7b95dcb", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-4c38733696964b158d75f294cd20062a", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-b752e74728c4443ab48d2017426e2eab", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-67f2d32179f948a192284de99dc89398", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-1d6c66d6da144a15a35ba103cc040eaf", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-de7c43c10436464da62fbfa735bd883b", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-8169c6cd19d9448f91e224affa0d5c9f", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-edd2077030894a84a6bc00fc96fac80b", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-d78379842af44e4e8426230a2e655685", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-24166395d37a4b8ea302b302b1306002", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-e24331a31cf241fd9b028b6106b653ad", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-a5e972625a5c47408fab22b7c06282fe", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-bf8834e1ca504f2ca57d736de2d68ed5", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-5fbd6f3d92414379bcd389a81b1f15f2", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-0d9c6cd5a0024292b24294d59c9127f2", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-29999f9d2dbf450b9fdea78dce465221", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-deb7a050be8040fbaa5247e13cbb465e", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-991848a50c35493bbc6b577dbacd7836", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-93ad3ccb10d644d9bba3152111bba56c", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-997b744de1a3447885d46fb47bb31a9e", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-ce0f91d0c00d4ddabb2c36515a8aec5d", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-c088af29d5e74b6ebc5b83559f9bca8c", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-037391a2f6a74c58b219f4197c526923", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-4b59ac4fc6a14ca4a1c3f0ea790d0f7a", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-1b16fda19f4c4fe6b5958242db32f87a", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-d84b873abf164aef82bb7080c4cf093c", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-bfc2326fd81a47108131e6c7eed3238d", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-64b7df932aa849bd988ae86523464cf8", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-26b32fad9d454904b942921ac4d7248e", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-d3dfba8e89a64d53a4f638a11e0bf1a4", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-564be20b991245808d68fce2b886838b", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-7e2e6890af86411c8c766ecc76b8c1b7", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-652a64f3f17647fc8ba4e1d0a066dff3", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-6e3fbd14beb44731afb05e5ad008ce99", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-774dc252907a40afa9bccb6c65d9df18", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-a55811cd22124ddeaab90c555cf1272e", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-e177302129f3495d9d8ab019d1865374", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-3957fab5805d45c5b988109d32e6984d", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-c0db32838d5044869d95866cccefed40", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-fce0f09ec73c43338b98b41d95589e46", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-01ef9c4b88e844ba85b14856582fdb0a", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-6b9f8ced2d7342ebb41749859e325031", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-4f7c970a88d14d1cae66c71d052e04db", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-da76bd04115b40e388e0abf8d8ce2c94", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-6682f401cce246b6bea488519414f32a", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-e55acd9368f647ccb5e0b873d887c863", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-1ce9a1c0e4ed4547aceb46e9ca2801e0", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-3d15d8c2b84448b8b7ce6c197e332b3b", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-46b523de15a24ecf9e4cf5ac54567c44", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-ad95fb1cd0a5440095ab28741a41d9a7", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-90035163360643cc8664e1607fae252d", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-75214d5d3d4748eb9e79c86d87fcf7d8", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-e604a687297d40ed82d0fff6e5c9ca04", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-ad38d81aa5cb4076a776325d03cd18b9", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-91fb11a1b9e247e69ce8875c449ff13e", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-0826ccfe1af047e39966752efca9d5fa", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-d69cab0b8eae4ba5b0358fcf41d0e551", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-280bf547181e49828e43a248d5aefc76", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-269812fab61844478087a534a07969af", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-1fd6058a2b264989b6c6b05e4690c78d", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-fe25eaf3680c41e3b9d97c357cf7cd86", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-9bc8ad603599449c877f463dc2d46582", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-a758b2579cee424eaa6815d39850ebef", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-a12a7074ad4249a5af3b946802e63394", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-7b08a9cf266241f7bc24da9c4b84287c", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-fdba74f67ef04fefbe887aac25d77ec6", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-4b14bccffbcd45af970305112def5df2", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-bc94db30b505402798a799c9d19593f0", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-1eff952128e844828c7bc9d130c3cca7", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-7b1a43d4d4554a8f941d4b1b5cc50f8c", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-00eb1c6538894ca3a4e8f989436bcb83", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-e2e63717c6f14c358f948c4c42ea3705", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-247cf18c833345079d5c193e78bab46f", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-9b146dc5f11a47d78e0446023e443254", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-1be20ce4cf0147ff90ef54be32eca3fa", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-66c1df3729004740b0cd3424b82a38b0", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-81efbf61014d40bc852f1944a33ebf81", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-d3db5fdc74ed4432b0afac12dc69a63d", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-ae65380d1d8b4331a063a3fe95c4100b", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-2418662355a04e21a8481f8ab8e58c1e", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-ca927e258d9e4557914090951d11835f", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-7e6162d4ca414daab621dd0c39f9fe34", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-c4af334ed449438184d3b93b18391f7c", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-8bab4a16f4e7439eba8b5d8cb041ef8f", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-807e697669bc49bbbab097ba1d3da015", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-572789fccbdb4382be1e0d1e5657c470", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-fa247f65e58e44c89d890cc1640cf6f5", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-0fb98e0a07b94e51b37608b823776a86", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-f82c9c873a894f6fb84698177d98f3e9", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-de4fe8f1b1664dbd93a1b4b80806bc63", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-50654b6612444cdda4c038c7d58097dd", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-32b82f89233b4bd1a24f404fca748131", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-98432ecfe3914aa5a72633c5db87c1c2", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-b7e6e5aa0bc24a8c9077047631c7c33c", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-e6aa3670897445158014d1b97fc6fe1f", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-721baf8348694147aa01226b65259bda", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-a18c98007089466da26292a37ced5d36", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-00d0a1f6c8694cfbaef2a91acddeac5f", "references": ["yes"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task242-4a5228ebf4eb47769f5816744c045b43", "references": ["no"], "task_id": "task242_tweetqa_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task620-a534520209c34fe080422a214e4f4c51", "references": ["Allied Health Personnel", "Electric Countershock", "Emergency Medical Technicians", "Human", "Support, U.S. Gov't, P.H.S.", "United States."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-504837e1543a418495ad2783d1315f75", "references": ["Antidepressive Agents, Tricyclic", "Arrhythmia", "California", "Electrocardiography", "Emergencies", "Emergency Service, Hospital", "Female", "Human", "Length of Stay", "Male", "Prognosis", "Retrospective Studies", "Tachycardia, Sinus"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-1fb36db8f548493da15cd5876be2ed72", "references": ["Adult", "Amyotrophic Lateral Sclerosis", "Animal", "Antibodies", "Chickens", "Electrophoresis, Polyacrylamide Gel", "Human", "Male", "Multiple Sclerosis", "Muscle Proteins", "Myasthenia Gravis", "Nervous System Diseases", "Rats."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-7317fa8548d34844a9ca9221a3146fad", "references": ["Adolescence", "Adult", "Age Factors", "Child", "Child, Preschool", "Clubfoot", "Female", "Flatfoot", "Foot", "Foot Deformities, Acquired", "Foot Diseases", "Human", "Infant", "Male", "Nails, Ingrown", "Osteochondritis", "Pain", "Shoes"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-4f611302a08e439bbe42ae8cb4816e36", "references": ["Aged", "Animal", "Cats", "Diarrhea", "Drug Evaluation", "Female", "Head and Neck Neoplasms", "Human", "Male", "Middle Age", "Mitoguazone", "Nausea", "Support, U.S. Gov't, P.H.S.", "Vomiting"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-b45f9d90f2f94480a29fb9d5b31d994d", "references": ["Abdominal Injuries", "Adult", "Case Report", "Erythrocytes", "Hemophilia", "Hemorrhage", "Human", "Male", "Technetium", "Tomography, X-Ray Computed", "Wounds, Stab"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-d3c6b1cbc2954d78b21f703289340731", "references": ["Case Report", "Cauda Equina", "Human", "Male", "Middle Age", "Myelography", "Neurilemmoma", "Peripheral Nerve Neoplasms"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-72ecae15534a4a9fa35eae5be036a125", "references": ["Alcohol Drinking", "Case Report", "Emergencies", "Esophageal Diseases", "Human", "Male", "Middle Age", "Rupture, Spontaneous", "Syndrome", "Vomiting"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-26e6b851147e40f8b471607bbf258b95", "references": ["Bone Marrow", "Child", "Child, Preschool", "Drug Administration Routes", "Human", "Infant", "Needles."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-cede83485caf4f02be6497098f52c0a1", "references": ["Abdomen", "Administration, Inhalation", "Adolescence", "Adult", "Alopecia", "Case Report", "Cocaine", "Drug Contamination", "Human", "Hypertension", "Male", "Pain", "Substance Abuse", "Thallium"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-6cba1bcc22df45cdb3dbaddcec371d81", "references": ["Administration, Oral", "Adolescence", "Burns, Chemical", "Case Report", "Colloids", "Epiglottis", "Human", "Male", "Pharynx", "Salicylic Acids", "Suicide, Attempted", "Tongue"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-fb1d5f4c69cc488fb847f03607ef88a2", "references": ["Animal", "Bladder", "Cyproheptadine", "Dose-Response Relationship, Drug", "Female", "Human", "In Vitro", "Ketanserin", "Methysergide", "Muscle Contraction", "Muscle, Smooth", "Serotonin", "Serotonin Antagonists", "Support, Non-U.S. Gov't", "Swine", "Urethra"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-520c5c56583d4517a371b65628037987", "references": ["Aluminum", "Animal", "Bone and Bones", "Comparative Study", "Deferoxamine", "Drug Screening", "Female", "Hemodialysis", "Minerals", "Osteomalacia", "Rats", "Rats, Inbred Strains", "Support, Non-U.S. Gov't", "Support, U.S. Gov't, P.H.S.", "Time Factors."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-618d5b8c4759495a95edaadf817a893d", "references": ["Air", "Diaphragm", "Human", "Peritoneal Dialysis, Continuous Ambulatory", "Posture."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-54b94058c9794f5fa53beb9731cd5bac", "references": ["Cellulose", "Complement Activation", "Hemodialysis", "Human", "Leukocyte Count", "Membranes, Artificial", "Platelet Count", "Surface Properties."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-de46871b297245dab9bbcc3d26c20480", "references": ["Anesthesia, Inhalation", "Atracurium", "Child", "Child, Preschool", "Human", "Infant", "Isoflurane", "Isoquinolines", "Kinetics", "Liver", "Liver Diseases", "Nitrous Oxide", "Support, Non-U.S. Gov't."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-a47d3e0376d4449ebfcccb0d7ff4384b", "references": ["Anesthesia, Intratracheal", "Atracurium", "Biliary Atresia", "Drug Administration Schedule", "Female", "Human", "Infant", "Infant, Newborn", "Intraoperative Care", "Liver", "Male", "Neuromuscular Blocking Agents"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-299c037e04f44ac2b55c892377268cfa", "references": ["Atracurium", "Child", "Child, Preschool", "Female", "Histamine", "Histamine Liberation", "Human", "Infant", "Male", "Neuromuscular Junction", "Time Factors", "Vecuronium"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-7e7c49099d8c48d48b402161994ea35b", "references": ["Adrenocorticotropic Hormone", "Anesthesia, Inhalation", "Catecholamines", "Child", "Child, Preschool", "Halothane", "Hormones", "Human", "Hydrocortisone", "Infant", "Midazolam", "Postoperative Period", "Premedication", "Pulmonary Gas Exchange", "Respiration", "Support, Non-U.S. Gov't", "Surgery, Operative."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-90e0805293bb4319a0837521c907d343", "references": ["Anesthesia, General", "Anesthesia, Inhalation", "Comparative Study", "Enflurane", "Female", "Human", "Male", "Muscle Contraction", "Neuromuscular Blocking Agents", "Nitrous Oxide", "Thiopental", "Transducers", "Vecuronium", "Visual Perception."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-7a0130fa163d4ef390ab0565e72590f5", "references": ["Antibodies, Fungal", "Antigens, Fungal", "Counterimmunoelectrophoresis", "Cross Reactions", "Dermatomycoses", "Deuteromycetes", "Human", "Hyphomycetes", "Immunoelectrophoresis, Two-Dimensional."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-21dc698548af4bc5a52e7102426a9b97", "references": ["Administration, Topical", "Alkaloids", "Animal", "Colchicine", "Male", "Medulla Oblongata", "Nerve Degeneration", "Peripheral Nerves", "Rats", "Rats, Inbred Strains", "Strychnine", "Time Factors", "Vinblastine", "Vincristine"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-9773bdfef9244277bffc169cbacc7bb1", "references": ["Blister", "Burns", "Case Report", "Cicatrix", "Human", "Infant", "Male", "Microscopy, Electron."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-4f547d20f8cd4e85a39821ccd281aac6", "references": ["Amniotic Fluid", "Comparative Study", "Female", "Fetal Blood", "Fetal Membranes", "Human", "Interferon Type I", "Placenta", "Pregnancy", "Support, Non-U.S. Gov't."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-f68072a46b3542999cee70299f7e1193", "references": ["Convergence, Ocular", "Electrooculography", "Eye Movements", "Form Perception", "Human", "Pattern Recognition, Visual", "Support, Non-U.S. Gov't", "Support, U.S. Gov't, P.H.S.."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-001c933aaf494f7fa719a9fe3ae27d29", "references": ["Cataract Extraction", "Human", "Lens Capsule, Crystalline", "Lenses, Intraocular", "Surgical Instruments"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-2b9aca9407554a51823d6d83a39545b6", "references": ["Capillaries", "Case Report", "Choroid Neoplasms", "Fluorescein Angiography", "Human", "Male", "Melanoma", "Middle Age", "Neovascularization", "Retinal Vessels", "Support, Non-U.S. Gov't."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-44e4a84e2caf45aead57d017e04da79c", "references": ["Fundus Oculi", "Human", "Lighting", "Photography", "Retina", "Retinal Vessels"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-cf27668305eb4327b719c8fa680aa5a8", "references": ["Dose-Response Relationship, Radiation", "Eyeglasses", "Human", "Lens, Crystalline", "Pupil", "Retina", "Sunlight", "Ultraviolet Rays"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-359809db3f454fb8b731300555eb93ae", "references": ["Antibodies, Monoclonal", "Antibody Specificity", "Enterotoxins", "Immune Tolerance", "Mitogens", "Neutralization Tests", "Peptide Fragments", "Staphylococcus aureus", "Structure-Activity Relationship", "Support, U.S. Gov't, P.H.S.."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-3429a376c0bc41578378ca0fbd5340c6", "references": ["Anemia, Macrocytic", "Animal", "Bone Marrow", "Cell Differentiation", "Cell Division", "Colony-Forming Units Assay", "Growth Inhibitors", "Growth Substances", "Hematopoiesis", "Mice", "Mice, Mutant Strains", "Radiation, Ionizing."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-55784b70ae2e44ec9aeb57d5124a3f17", "references": ["Cell Membrane", "Fetal Blood", "Human", "Infant, Newborn", "Lymphocytes", "Microscopy, Electron, Scanning", "Microvilli", "Support, Non-U.S. Gov't", "Support, U.S. Gov't, P.H.S.", "Wiskott-Aldrich Syndrome"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-a85eb7f6f04246bba9c814fa3b745209", "references": ["Erythrocyte Deformability", "Erythrocyte Membrane", "Erythrocytes", "Heinz Bodies", "Human", "Membrane Proteins", "Microscopy, Electron", "Molecular Weight", "Oxidation-Reduction", "Phenylhydrazines", "Support, Non-U.S. Gov't", "Support, U.S. Gov't, P.H.S.."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-28cdf17bc3884f11abb80d6e3f3d6ebb", "references": ["Consanguinity", "Fetal Hemoglobin", "Gene Amplification", "Globin", "Haplotypes", "Homozygote", "Human", "Pedigree", "Support, Non-U.S. Gov't", "Support, U.S. Gov't, P.H.S.", "Thalassemia"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-0e8ee3e527fe45ad92a447b872b763ea", "references": ["Cell Differentiation", "Cell Division", "Cell Line", "Dimethyl Sulfoxide", "Drug Resistance", "Granulocytes", "Human", "Neuraminidase", "Sialic Acids", "Sialoglycoproteins", "Support, U.S. Gov't, P.H.S.", "Thioguanine", "Tretinoin"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-a8ee73527244468280b864ac15328148", "references": ["Adrenal Cortex Hormones", "Adult", "Androgens", "Case Report", "Dexamethasone", "Diagnosis, Differential", "Drug Resistance", "Female", "Human", "Hydrocortisone", "Male", "Menstruation Disorders", "Pedigree", "Virilism"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-1b13da695044431492d08b2c20d6593c", "references": ["Actuarial Analysis", "Adolescence", "Blindness", "Cardiovascular Diseases", "Child", "Child, Preschool", "Diabetes Mellitus, Insulin-Dependent", "Diabetic Angiopathies", "Diabetic Retinopathy", "Energy Metabolism", "Evaluation Studies", "Exertion", "Female", "Follow-Up Studies", "Human", "Infant", "Infant, Newborn", "Male", "Pennsylvania", "Regression Analysis", "Risk", "Sports", "Support, U.S. Gov't, P.H.S.."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-da4fa14fc7644a4da4f6035d6b9c98f5", "references": ["Azathioprine", "Chlorambucil", "Connective Tissue Diseases", "Cyclophosphamide", "Drug Resistance", "Human", "Hypoxanthine Phosphoribosyltransferase", "Immunosuppressive Agents", "Lymphocytes", "Mutation"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-41142cef31a24953b9cdeee355738380", "references": ["Antilymphocyte Serum", "Arthritis, Rheumatoid", "Combined Modality Therapy", "Hemoglobins", "Human", "Immunosuppressive Agents", "Leukocyte Count."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-e04c76bded9a4f7b8ec6cddc6ddd0374", "references": ["Carpal Tunnel Syndrome", "Electrodiagnosis", "Human", "Median Nerve", "Neural Conduction."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-b77ad609b87545c08823f2e0328fe68d", "references": ["Cardiovascular Diseases", "Comparative Study", "Estradiol", "Estrogens, Synthetic", "Ethinyl Estradiol", "Follow-Up Studies", "Human", "Male", "Neoplasm Metastasis", "Orchiectomy", "Prognosis", "Prostatic Neoplasms"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-5726b7733c3c49b6bb08641c0b7a1631", "references": ["Female", "Fetal Blood", "Human", "Hypersensitivity", "IgD", "IgE", "Pregnancy", "Retrospective Studies", "Smoking", "Support, Non-U.S. Gov't."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-595a9a7fbfe742aa97e1c9fd954422eb", "references": ["Bacterial Infections", "Burns", "Female", "Human", "Male", "Rosette Formation"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-50d954a1ecaf42f7a011ad3102ce2038", "references": ["Accidents, Occupational", "Adult", "Australia", "Burns", "Burns, Chemical", "Eye Burns", "Female", "Human", "Male."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-4b3f99fe43564161b133416646839439", "references": ["Accidents, Home", "Body Surface Area", "Burn Units", "Burns", "Child, Preschool", "Female", "Human", "Infant", "Intensive Care Units", "Length of Stay", "Male."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-a349acd5883248cbb50509feebc357af", "references": ["Burns", "Hand Injuries", "Human", "Occupational Therapy", "Play and Playthings"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-b0e9da43d8bf45069600fbdb0e41daf4", "references": ["Anterior Eye Segment", "Choroid", "Eye", "Human", "Light", "Pigment Epithelium of Eye", "Retina", "Vitreous Body"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-cde34f6048d343e2aff98b4c43c6714e", "references": ["Equipment Design", "Human", "Irrigation", "Suction", "Support, Non-U.S. Gov't", "Vitrectomy"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-e183933ace20490cab64705ab6771c3d", "references": ["Abducens Nerve", "Adolescence", "Adult", "Aged", "Case Report", "Child", "Cranial Nerve Diseases", "Evaluation Studies", "Human", "Middle Age", "Oculomotor Muscles", "Oculomotor Nerve", "Paralysis"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-a8c5f3662e87410894a1fb9d5f08cc7c", "references": ["Carbamazepine", "Case Report", "Fasciculation", "Female", "Follow-Up Studies", "Human", "Middle Age", "Oculomotor Muscles"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-d74098c9d2774c37bd042ebf052bad26", "references": ["Acute Disease", "Acute Phase Proteins", "Adolescence", "Adult", "Animal", "Antibodies, Bacterial", "Campylobacter fetus", "Campylobacter Infections", "Cattle", "Disease Outbreaks", "Enteritis", "Food Microbiology", "Human", "Immunity, Active", "Milk", "Support, Non-U.S. Gov't", "Support, U.S. Gov't, Non-P.H.S.."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-d675415b61e54d8186976a91aff2a47a", "references": ["Accidents, Traffic", "Behavior", "Data Collection", "Human", "Legislation", "New York", "Seat Belts"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-bc2787845dfc4da39c1b15c3116af1a4", "references": ["Antineoplastic Agents, Combined", "Bone Marrow", "Chromosome Aberrations", "Female", "Human", "Karyotyping", "Leukemia", "Male", "Middle Age", "Myelodysplastic Syndromes", "Neoplasms", "Prognosis", "Radiotherapy"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-db41109b56b846be92254151d04ef9dc", "references": ["Antineoplastic Agents", "Combined Modality Therapy", "Female", "Human", "Infant", "Infant, Newborn", "Male", "Rhabdomyosarcoma", "Support, U.S. Gov't, P.H.S.."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-c9c15943c6a346e188648608affc17b2", "references": ["Evaluation Studies", "Gallium Radioisotopes", "Human", "Lymphography", "Lymphoma, Non-Hodgkin's", "Retrospective Studies", "Support, Non-U.S. Gov't", "Tonsillar Neoplasms"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-791463d0f0f74bc5ad2aaf455fc51b5d", "references": ["Carcinoma", "Case Report", "Human", "Laryngeal Neoplasms", "Laryngectomy", "Male", "Neoplasm Staging", "Tomography, X-Ray Computed"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-ef8285db125040bb8bc8472813dfd362", "references": ["Age Factors", "Biopsy", "Hematuria", "Human", "Infant", "Infant, Newborn", "Kidney", "Kidney Diseases"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-4b5ea85cbcf748adbf458ea4a0a9db3c", "references": ["Catheterization", "Human", "Male", "Neoplasms", "Peritoneum", "Pleural Effusion"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-a07196637ba84ccba7aa7cc51429ba37", "references": ["Coronary Disease", "Heart Failure, Congestive", "Heart Valve Diseases", "Heart Ventricle", "Human", "Hypertension"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-383067da2db34cfeb73cf6cfbbbc3ed1", "references": ["Cardiopulmonary Bypass", "Heart Failure, Congestive", "Heart Ventricle", "Heart-Assist Devices", "Human", "Intraoperative Complications", "Lung", "Pulsatile Flow."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-741d3eddab7448bbb221687641717065", "references": ["Heart Failure, Congestive", "Heart Ventricle", "Human", "Intraoperative Complications", "Support, Non-U.S. Gov't", "Vascular Resistance."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-2eb533978bbd4ab694c6f3d055ee5e1d", "references": ["Blood Flow Velocity", "Comparative Study", "Echocardiography", "Fetal Heart", "Gestational Age", "Heart Ventricle", "Human", "Prospective Studies", "Reference Values", "Stroke Volume", "Support, U.S. Gov't, P.H.S.."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-8a1e64b2a3884df5ab4b1da2fbccafad", "references": ["Cineradiography", "Comparative Study", "Coronary Disease", "Coronary Vessels", "Evaluation Studies", "Follow-Up Studies", "Heart Catheterization", "Human", "Image Processing, Computer-Assisted", "Methods", "Random Allocation", "Retrospective Studies", "Risk", "Support, Non-U.S. Gov't", "Support, U.S. Gov't, P.H.S.", "Time Factors."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-372e9d6c4e2c4f48a5f7c5b8ec413ba4", "references": ["Angina Pectoris", "Angina, Unstable", "Angioplasty, Transluminal", "Coronary Vessels", "Electrocardiography", "Follow-Up Studies", "Human", "Myocardial Infarction", "Recurrence", "Time Factors."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-d5074c435ee443a89c031af0a39429a0", "references": ["Comparative Study", "Embryo Transfer", "Female", "Fertilization in Vitro", "Human", "Male", "Menstrual Cycle", "Oocytes", "Peritoneoscopy", "Pregnancy", "Prognosis", "Prospective Studies", "Sperm Count", "Sperm Motility", "Spermatozoa", "Support, Non-U.S. Gov't."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-a0be5d0d42ea4f11a2a857b27747fabd", "references": ["Cardiac Output", "Cardiomyopathy, Congestive", "Coronary Disease", "Diuretics", "Heart Catheterization", "Heart Ventricle", "Hemodynamics", "Human", "Prospective Studies", "Rheumatic Heart Disease", "Vasodilator Agents"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-95b8f48aba974bd9b11c416ae96720bb", "references": ["Aortic Valve", "Aortic Valve Insufficiency", "Aortic Valve Stenosis", "Heart Catheterization", "Heart Valve Prosthesis", "Hemodynamics", "Human", "Mitral Valve", "Mitral Valve Insufficiency", "Mitral Valve Stenosis", "Postoperative Period", "Prognosis", "Stroke Volume", "Systole."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-b3527ae2148d49b79fbd22ac748b5ecb", "references": ["Cardiac Pacing, Artificial", "Cardiopulmonary Bypass", "Cineradiography", "Comparative Study", "Computers", "Electrocardiography", "Electrodes", "Evaluation Studies", "Heart Catheterization", "Heart Ventricle", "Human", "Intraoperative Care", "Support, Non-U.S. Gov't", "Tachycardia"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-3eb812ef2e964ab69cff699a63597396", "references": ["Acetylprocainamide", "Digoxin", "Glutethimide", "Hemoperfusion", "Human", "Ion Exchange Resins", "Poisoning", "Polystyrenes", "Polyvinyls", "Procainamide", "Theophylline"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-4fef6347c47e47ca9910afc0692771ae", "references": ["Angioplasty, Transluminal", "Animal", "Arteries", "Blood Platelets", "Heart Diseases", "Indium", "Microscopy, Electron, Scanning", "Radioisotopes", "Support, Non-U.S. Gov't", "Swine", "Thrombosis", "Wounds, Penetrating"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-b6f76ecb1d884a1599ee714f6952e673", "references": ["Analysis of Variance", "Animal", "Blood Flow Velocity", "Coronary Circulation", "Dogs", "Dose-Response Relationship, Drug", "Flowmeters", "Heart", "Hemodynamics", "Iohexol", "Support, Non-U.S. Gov't", "Support, U.S. Gov't, Non-P.H.S.", "Time Factors."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-dae3ec6c734a48e29e0add6de769fa4d", "references": ["American Heart Association", "Caloric Intake", "Cardiovascular Diseases", "Diet", "Human", "Nutritional Requirements", "United States."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-f363bda4c8cf4e04bb60b97ac83eaafc", "references": ["Educational Measurement", "Schools, Medical", "Students, Medical", "United States."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-b0ed416a17dc4fdfa099f58ceee1f577", "references": ["Drowning", "Human", "Methods."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-7668355f09634113b9c6766f3a6360c8", "references": ["Electric Countershock", "Emergency Medical Services", "Human", "Life Support Care", "Resuscitation", "Support, Non-U.S. Gov't", "United States."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-bee6066da09c423d8ba907961d297c54", "references": ["Airway Obstruction", "Anesthesia, Inhalation", "Case Report", "Human", "Intubation, Intratracheal", "Male", "Middle Age", "Obesity", "Pulmonary Edema", "Risk."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-db3c038e882445f9a121fed24505b849", "references": ["Heart Arrest", "Human", "Respiration, Artificial", "Respiratory Insufficiency"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-26d94cf5a5a9462dba2d571c118b4a2e", "references": ["Ethmoid Bone", "Human", "Orbit"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-d1c5c2370f884e7fb9d5b2f7f03439ad", "references": ["Antithrombin III", "Blood Coagulation", "Female", "Fetal Blood", "Fibrinogen", "Fibrinopeptides A", "Human", "Labor", "Pre-Eclampsia", "Pregnancy."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-cf6f8d7224cb4c4dbaf8dc22f9c1a6af", "references": ["False Negative Reactions", "Human", "Knee Injuries", "Knee Joint", "Ligaments", "Movement", "Physical Examination", "Support, Non-U.S. Gov't", "Tibia"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-8beeab66f8f743cf8190357e0c6bdb8f", "references": ["Exertion", "Fractures", "Human", "Male", "Military Personnel", "Pain", "Physical Education and Training", "Prospective Studies", "Technetium Tc 99m Medronate", "Tibia"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-98c6c1d534214388a51f4086d1e3ddd3", "references": ["Double-Blind Method", "Exudates and Transudates", "Follow-Up Studies", "Human", "Injections, Intra-Articular", "Knee Joint", "Methylprednisolone", "Movement", "Pain", "Support, U.S. Gov't, Non-P.H.S.."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-09ae185b70ee495096ed054ed2e70145", "references": ["Bone Cements", "Bone Resorption", "Case Report", "Follow-Up Studies", "Hip Joint", "Hip Prosthesis", "Human", "Male", "Middle Age", "Osteoarthritis", "Reoperation."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-a1181cb8acdd487f8865ddc71657e9be", "references": ["Acetabulum", "Caucasoid Race", "Female", "Forensic Medicine", "Human", "Ischium", "Male", "Sex Determination"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-1289d9f6c4a94bb0a2bc99ae2f84eb11", "references": ["Achilles Tendon", "Adult", "Case Report", "Female", "Follow-Up Studies", "Human", "Rheumatic Fever", "Rupture", "Steroids", "Tendon Injuries"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-d3300a8817954c8b813d21224461f85b", "references": ["Biomechanics", "Diastole", "Heart Surgery", "Heart Ventricle", "Hemodynamics", "Human", "Intraoperative Period", "Myocardial Contraction", "Pericardium", "Regression Analysis", "Support, Non-U.S. Gov't", "Support, U.S. Gov't, Non-P.H.S.."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-5b586d7292db40cca4feffd238f80632", "references": ["Cartilage, Articular", "Glycosaminoglycans", "Human", "Stress, Mechanical", "Support, U.S. Gov't, P.H.S.", "Temporomandibular Joint", "Temporomandibular Joint Syndrome"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-60b80b5b5e96400ea5816352045d916c", "references": ["Attention", "Attention Deficit Disorder with Hyperactivity", "Child", "Female", "Follow-Up Studies", "Human", "Language Development Disorders", "Male", "Psychiatric Status Rating Scales", "Social Behavior."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-b0814cad09d24489a48a8a80bcd7f005", "references": ["Bronchoscopy", "Case Report", "Child", "Diagnosis, Differential", "Female", "Foreign Bodies", "Human", "Infant", "Male", "Respiratory Tract Infections", "Thoracic Radiography", "Trachea"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-e385104b1b4d459cab70400c638ade3b", "references": ["p-Aminohippuric Acid", "Adult", "Blood Pressure", "Comparative Study", "Dopamine", "Female", "Heart Rate", "Human", "Hypertension", "Infusions, Intravenous", "Inulin", "Kidney", "Male", "Middle Age."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-2909d2415c5343e28d5269211b58020e", "references": ["Acetylation", "Adult", "Amrinone", "Human", "Infusions, Intravenous", "Kinetics", "Male", "Phenotype."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-43828cac0b414d0d973453fec2851bfa", "references": ["Blood Proteins", "Chromatography, High Pressure Liquid", "Female", "Fetal Blood", "Fetus", "Human", "Infant, Newborn", "Kinetics", "Maternal-Fetal Exchange", "Pregnancy", "Ritodrine", "Support, Non-U.S. Gov't", "Support, U.S. Gov't, P.H.S.."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-cb96c319d2d74ee0b949e9e187e3197e", "references": ["Absorption", "Alkalosis", "Animal", "Bicarbonates", "Biological Transport", "Body Water", "Chlorides", "Glomerular Filtration Rate", "Kidney Tubules, Proximal", "Rats", "Rats, Inbred Strains", "Support, Non-U.S. Gov't", "Support, U.S. Gov't, P.H.S.."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-cacf7ae8d5d84b688a4fd5e9b7ae3dd9", "references": ["Electroconvulsive Therapy", "Female", "Human", "Patient Care Team", "Pregnancy", "Pregnancy Complications", "Psychotherapy", "Psychotic Disorders", "Psychotropic Drugs", "Recurrence."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-4ac49b304ec244feb4c9720a6d210485", "references": ["Dopamine", "Drug Interactions", "Human", "Intestinal Absorption", "Kinetics", "Schizophrenia", "Tranquilizing Agents, Major"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-ab5d24047657438ba152a07c63ed32b6", "references": ["Dermatitis, Contact", "Female", "Human", "Male", "Occupational Dermatitis", "Patch Tests", "Quinazolines"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-526277b40fbc49a5ad1ccb0cd3ded0ed", "references": ["Color", "Dermatitis, Contact", "Hemoglobins", "Human", "Patch Tests", "Skin", "Skin Tests", "Spectrophotometry", "Support, Non-U.S. Gov't."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-440b4c30675044de95a27839ff6de106", "references": ["Dermatitis, Atopic", "Dermatitis, Contact", "Female", "Food Service, Hospital", "Hand Dermatoses", "Human", "Maintenance and Engineering, Hospital", "Male", "Occupational Dermatitis", "Patch Tests", "Personnel, Hospital", "Psoriasis"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-266f7dc5261d48b6bd93b9bcad23a8fc", "references": ["Acne", "Adolescence", "Adult", "Ceramides", "Child", "Esters", "Human", "Linoleic Acids", "Sebaceous Glands", "Skin", "Support, Non-U.S. Gov't", "Support, U.S. Gov't, P.H.S.", "Waxes"], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task620-26f83024a50e4847b91cb8a453f2a721", "references": ["Asia", "Cobalt", "Construction Materials", "Human", "Nickel", "Occupational Dermatitis", "Solubility", "Spectrophotometry, Atomic Absorption", "Support, Non-U.S. Gov't."], "task_id": "task620_ohsumed_medical_subject_headings_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task1159-1f85921a609441d3b7a93581c1ccd801", "references": ["sack"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-f613ef25e6374d4bad74b925497bbf05", "references": ["bag"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-da6b7e27ea6f40199ffedfcc546a6569", "references": ["jar"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-0f1a96aed0f44f0da71ac6ada14f4614", "references": ["bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-36faa2a14fe84bc598aad0ad66e68f0a", "references": ["jug", "bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-75b4f7702e5e4317a38f7f130fb3d781", "references": ["can", "bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-e421ebb3803f4f38857b61858c3cc52c", "references": ["box"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-f74ce7625f18458db48b8e0a02ebc3be", "references": ["wallet"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-88c25277e44c4457b5436c39e93acedf", "references": ["tube"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-cf823092ac064b289120420ebc1720cf", "references": ["frame"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-06387c66e26143ae89f967eb74aa978a", "references": ["drawer"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-44f722056f8342deb93be0800a17fe46", "references": ["closet"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-8a7fe1c3fd204c71bc372de8787d6f11", "references": ["dresser"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-d8dc9230a13541b9896c18ea9783e3ef", "references": ["cupboard"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-ad0896029b49437f92f9f4951ac3341f", "references": ["sack"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-6728b2d70e764ae794c5a4d72adb5696", "references": ["bag"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-9f7408b60893452f881f7f3a7102fa02", "references": ["jar", "bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-2025cdde7934498b9b4395fb6bd63267", "references": ["bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-69c06a225f23462195aae35278033ce1", "references": ["jug", "bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-65afee9eeb3345cbae7040ccbda78f22", "references": ["can"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-de27884a15e94bef99e81ab75e0faa41", "references": ["box"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-29b908c63bea43a9ba38c3284bdfa76a", "references": ["wallet"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-edd034fd50264cf397e4027d5464bb81", "references": ["tube"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-0bd3aaaab575463a963fa9d453e399f7", "references": ["frame"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-e3dc764f79de4ff78bf34e25f44da0f3", "references": ["drawer"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-f9947d31f61f4e37a311777a0d6980b7", "references": ["closet"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-b9204244bef54551a45fee7b62ff6686", "references": ["dresser"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-ed7f28bbc86f4d2a98389f984757c3c5", "references": ["cupboard"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-69881722de394befbf0971e7e8379265", "references": ["sack"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-b295cb340d1b4a49a9c15b80d177b293", "references": ["bag"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-014cf8363ba74542906f50cfafe5d93f", "references": ["jar", "bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-7cc537d6e3b741e1b0950fea289d52be", "references": ["bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-336f27edc0ee4bbe99c961e0e1f977c8", "references": ["jug", "bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-0755904985dc4ce1bd389d199f17ef7d", "references": ["can"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-b2f44b47075d4334ad12cae05a678f68", "references": ["box"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-b94378684817485895d7f1ad02326078", "references": ["wallet"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-d6bbc9fa94394fc98bf81bbc36f1c612", "references": ["tube"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-d3b0308f75984aa0b481c1905431a9d9", "references": ["frame"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-be9c6becd42e4ba894e7dac737357133", "references": ["drawer"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-1286e99aa0f64d6a863eb5e5539839a4", "references": ["closet"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-c472d31f32d542ac98ee7564fd545564", "references": ["dresser"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-66fe2a29b71c431f98cdf57bc1a4689c", "references": ["cupboard"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-fddd649dbc7344509883015f19ff3cb1", "references": ["sack"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-c944b91c01aa48d7bc0345d9218b12dd", "references": ["bag"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-84a2cf42f1fe49b4870a4192b86638fe", "references": ["jar"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-34f80a42bd9a42b189f48255fe09577e", "references": ["bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-35711ad5482e4e8898cbfc0377cc84f0", "references": ["jug", "bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-e0ad0a51d5364f008183dd2431bccef3", "references": ["can", "bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-9f999671e7da4844bdfc09432a30b383", "references": ["box", "bag"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-bf2eb051c96d49c8a8d57e045b928dc1", "references": ["wallet"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-b6c13b5642f74c0a9dbcded5910ab8f2", "references": ["tube"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-defd6c7195454b2bb67514e2daa0bce8", "references": ["frame"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-f47d00e09dc8423f94e4d502446064d7", "references": ["drawer"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-a907d737e5f34915aa48adb85c9cc9b6", "references": ["closet"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-dc0173a6941945359e0c3b9f95f49c64", "references": ["dresser"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-7ff78f2ddb954504aef21c68f5ef15ad", "references": ["cupboard"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-747e174dfd7a413a9d4ba299a7116413", "references": ["sack"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-5f8b5e26ea9c4b15bdf9fefa8139aee1", "references": ["bag"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-e8bc5b08947041d9ab1050e3f18979a0", "references": ["jar", "bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-98cd2930776841cbba1aaa56700afd23", "references": ["bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-0fef2fc3e1934a77997cadb31c6400a5", "references": ["jug", "bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-25bcdb70d1ac477987e1a215d03b0e7d", "references": ["can"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-e5efe6079dba45a085f093581c9fbe53", "references": ["box", "bag"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-0bc437a84a1e4e5ea2ff98460fa471af", "references": ["wallet"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-5cf7bad1013547b1b757c2f22735a275", "references": ["tube"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-e7e20d3f1e874268b6a5efeaaaf0ad39", "references": ["frame"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-1f3e06b710d34b9188a59754e9b40164", "references": ["drawer"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-53a6c0b366764c258f0fe7ac04885583", "references": ["closet"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-c863967e680d47adb6c00485d6c220cd", "references": ["dresser"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-40300494c91b4c57b0a81d1a61d8a701", "references": ["cupboard"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-5c4649cf24d648f9904cacc34d8abd52", "references": ["sack"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-8f2241e33eb346aaa3943f8b67d88f21", "references": ["bag"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-1e50dae17f3440a0a343ca0b53115792", "references": ["jar"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-da119a40728148d197310090e916c1d8", "references": ["bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-49f5a81a3f71471d99fe8733bc2082ea", "references": ["jug", "bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-c648c13d3c8842eaa279eb06858ce9dc", "references": ["can", "bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-9d27917c08b14ddc983fc102f55146f1", "references": ["box"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-14bd9697c93d4a948d082bf41869c0dd", "references": ["wallet"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-9c8c4f952be84b4e83609fba4f50fe11", "references": ["tube"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-19efb53afc424c39b7c0035fa5276e47", "references": ["frame"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-1888789768c14b1baa2ccb0b6253d2c4", "references": ["drawer"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-87a0cbca86374ba8a07b97191ed37a95", "references": ["closet"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-8f532101b028479f8522ad19ea2fa407", "references": ["dresser"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-a15d8d6f2984420a882cae2eb8d22176", "references": ["cupboard"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-40ac86cb99144b02ae8408818b97462d", "references": ["sack"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-14b5d657d0a144d78404a0b7d9900bad", "references": ["bag"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-4bf359be787d42bf88263f219e4bd2f9", "references": ["jar"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-b9d5f4bd02f045e280b1f75fdcaf2d3d", "references": ["bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-c114e86891ed4d5892989203c9e92e30", "references": ["jug", "bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-96306cbc58d44cbbba6e7aae9a4ddde0", "references": ["can", "bottle"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-275101a78e994b5a80415b110d5ea866", "references": ["box", "bag"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-17bdacb6fedc4817a708d3319f1f4377", "references": ["wallet"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-3692e77921904dac856f9ab4488c597a", "references": ["tube"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-5ff56e7c28a74294a5fc174ed9ff7ad1", "references": ["frame"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-6920bf17f83d42cc84eb0096e7250821", "references": ["drawer"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-a1a598070d404a36b06ee27bfbda7a09", "references": ["closet"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-04699a84d714496ea1fe8bdd2981fa85", "references": ["dresser"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-784835d98fa94d568f76363ef8e61a9f", "references": ["cupboard"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-04ea436165eb4e76afd168fd1cfa37a2", "references": ["sack"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1159-029c6a2bcee0438d9ac7737d54a23a8e", "references": ["bag"], "task_id": "task1159_bard_analogical_reasoning_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task500-9bf82d580dfe4f0291a93151c5b3ea59", "references": ["giving my cousin my current xbox and replace mine with a new one after spilling water on his"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-5f529b1c588a4ae1ba242200ef3fdb6e", "references": ["dating a man 28 years older than me? my parents are coming apart at the seams"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-d41517850a1e4e29a3d0f04d8175bc2c", "references": ["not visiting my mentally ill sister"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-c4617559fb00414ea70034a161c3eb42", "references": ["wanting to explain"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-602d163321004f86a5a40eab978dc3ac", "references": ["standing up for myself at work"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-3202c920edd44f4ea7287756fe6a4d70", "references": ["buying $4 shirts as gifts"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-b71705b98a064c2e8b442cfdb9888f6c", "references": ["outing someone but"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-a1a8ee90e007476f976eef063b0bc1f8", "references": ["wanting to skip Thanksgiving and go to the hospital to see my boyfriend"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-7cc4242b588345fcb5fbfc6baa86582a", "references": ["potentially breaking my two friends up because I don't think she deserves him"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-49998ce2246947f99de5989efcdd79b8", "references": ["arguing over a window"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-a2105328a3e245f6a5b49fb6f052d88b", "references": ["taking over an account that someone used my email to create"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-cfaf3ee994044b4e939a151c6d695de7", "references": ["watching Harry Potter"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-3e39d5a367234b0da68f07f5cabe8551", "references": ["telling my parents I don't want to visit them until next year"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-26be00eb544b4799b8c0dee1e361c157", "references": ["asking someone to move their shopping cart"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-65e27805363146c3977799c88bc9804b", "references": ["asking a co-worker to stop chewing with their mouth open"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-82f685e62aca4cd290d0205ac4fbd942", "references": ["ending a close relationship to get over a crush"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-fd09bd58ef1c4ea78d3bf9cb615604ce", "references": ["ending a friendship"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-a4670a5c678f48f7b4efd2e752f9fd41", "references": ["being uncomfortable about a work situation"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-30d9efc504854e7180734a72a066b609", "references": ["requiring my next door neighbor to purchase my mobile home with certified funds"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-738f5b478ff04574a7ea4c07ba79302e", "references": ["telling my mentally ill mom to stop parenting me"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-06428f4e7c1042b98b5d875ed233ade3", "references": ["not changing how I describe disabled people"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-fda70a2e28a8468eb124c46158b1c4f4", "references": ["keeping my relationship going despite knowing it won't last"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-be4677a981764b11ac3b7b3621f0bbdc", "references": ["not wanting to pay for tickets on a car in my parents name"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-cd458cdf047b4ad4990541a44eded640", "references": ["staying sexually active with my ex"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-325cfe67cfd74650a32802e93a39f509", "references": ["being sick and not doing the dishes"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-b8915b20f21c48eeb8ce1b55cf821a41", "references": ["not telling my mom about my trans boyfriend"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-31f03ae3b5434cc89dab8121108abc64", "references": ["asking my housemate to clean up her dog's mess"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-6030c96ce3414bdc920757fe82a931f3", "references": ["not being more quiet after 10pm"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-c48971524a4d49ad8a74cd170c287be5", "references": ["making the bride refund me for my bridesmaid dress after she kicked me out of the wedding"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-34a7c8f1081a41eeb35bd4782d909a14", "references": ["trying to have sex with my female friend"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-6e4d2950f06e4802877cade05b6ef553", "references": ["not wanting to talk to my friend"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-e71ad7eb8f8f4e3c858eb9fb1d3837e8", "references": ["wanting to quit a job where I'm treated like shit and leave crying most days"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-4a48aa2dba0b45028645a4ff4e97bdae", "references": ["standing up to my dad about my life choices but making him violent towards my mom"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-c966b1141b424ecd97e1916f50532728", "references": ["telling my girlfriend that I don't like the way she acts when she drinks"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-e835902a31274ead805fd8747fde0351", "references": ["getting into a fight with my gf about sticky fingers"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-2831bc1c6e5f4203b94f18c49b0cc36f", "references": ["being pissy with my roommate/friend/ co worker"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-5533da5c98914b56b0e7ee27cb1b3d06", "references": ["wanting my mom's boyfriend to move out of the house"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-adf98b02c5f74542a73c87cd2fd3890c", "references": ["leaving my job"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-8b6cd321f2e9499383d2415399ac9e54", "references": ["not spending time with my dad on New Year's Eve"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-7509f85018834e7c9806735ce7c4ee7d", "references": ["not inviting my friends"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-66e9946e9c114242871df0427e68fb89", "references": ["not following the rule \"speak only in superlatives or do not speak at all\" after someone's death"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-59840ddb654c4afe89e36f39d16db52d", "references": ["not wanting to share my tv account passwords"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-b347be225fd74030aa59785ba1aa88a9", "references": ["not asking more about my friend coming out"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-a0acf21c87fc44afa2495938fc6cf136", "references": ["not being in the mood for sex"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-c03f785b64154db898f27e003c6f69fc", "references": ["thinking my (now ex) boyfriend was selfish"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-c38bf8784bdc4793941b0d73b834a9ac", "references": ["telling my girlfriend that when her gay friend tried to kiss her he was maybe doing it out of curiosity"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-352ee09b9d1d43fda4aaced48055cc7c", "references": ["getting my girlfriend a gym membership for her birthday"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-04525aaabcba450c8b5364517dfa36dc", "references": ["moving 6 states away and leaving my husband to pack up the house"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-2333c9ad1ba44612aa9e3d933a213b4a", "references": ["expressing an ultimatum: go to therapy, or this relationship is over"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-93538b7a7c994d7284ad4df6f9d76a98", "references": ["making my own plans even though I carpool with a colleague"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-e76bf4efad7c4b90b78aaa2e5ed7796d", "references": ["taking a new job"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-3a6760fdda014573b63ece46a94c6097", "references": ["asking a subway performer to stop singing"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-cba59f01390847f68c8c01c404bccde8", "references": ["expecting a cut of a referral bonus"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-87394aebb0fa458598072d363f0215c2", "references": ["not wanting to walk my elderly neighbor's severely overweight dog"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-6f605dd325004235b8aa70c196cd3f7a", "references": ["telling a girl that I think sex is an important part in a relationship"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-400b9445b6234491aad216e5c8aca332", "references": ["not spending time with my my girlfriend"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-cf25d3cc570142aeb783e0acd0303077", "references": ["pressuring my gf to tell her family we are dating"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-007a2b407749460fbd2c75c1e69f135c", "references": ["kicking my cousin who may or may not have post-partum depression out of my wedding party"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-ba313683c265444eb6fbd9e6a2141926", "references": ["gatecrashing a friends party after being purposely bounced around from person to person instead of being told not to go"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-ed5b3b3ad1e44a66b22f5e0f1f5fad6e", "references": ["not taking my roommate (who has no car) grocery shopping"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-1248cc0472184357b51e7702bf971a2b", "references": ["telling my close friend that I'm getting an abortion"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-e08b2b732e974a02b1ee406bc5cb8d62", "references": ["being resentful of my dad supporting my brother's mental health crisis"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-a0e6240b93554d2b84cf41aa7aa9610e", "references": ["leaving my friend when she was dealing with depression"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-abaca461922542498169dcdfb57795e1", "references": ["not putting up with someone being extremely annoying and lashing out at him for it"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-87c0fb1c2eb84dae9944fdf09911e309", "references": ["telling everyone my girlfriend cheated on me"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-fafc2bf5107d4010abb9ea0624bbe341", "references": ["cutting off all contact with a girl because she \"just wanted to be friends\""], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-9544de7e5a454091a8c8629ce0514458", "references": ["not wanting to celebrate my therapist's birthday"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-1e0b7265f4e342919f32440b1b72cec0", "references": ["asking an old fling out"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-99d21eb4d11840818ffa7872f3ebadba", "references": ["lying about my mental health"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-8fd5439f201b45279985e7bbc81eed8a", "references": ["not wanting to take care of dogs"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-82acf775d9f745018643ffb367135a34", "references": ["potentially ruining my boyfriend's relationship with his mother"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-dc1dba12b7584a8e87ddb374609acdf2", "references": ["not wanting to hangout and avoiding my \"friend\""], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-43630b01deec461dbb3df525a1b0d8a8", "references": ["making my friends get off their phones"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-8c6d7db8ecd9479b894026e884d22c95", "references": ["advising my boss that he needs to fire several employees"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-28f06e77d4ae4c56a92af9f67e6a62cd", "references": ["telling my newly pregnant girlfriend that she needs to consider my opinion on whether to keep the baby"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-2be70355abbd41deac01925a59ba3bb1", "references": ["wanting to order delivery in a blizzard"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-870b3dd34fa946dbbd56b5247b8974be", "references": ["being pissed that my boyfriend won't get a job"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-5369734c79be492aa5b2ffe930180f23", "references": ["taking her money"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-24ddccc73c884a969dfc014b07f5c862", "references": ["using the handicapped door opener when pushing a stroller"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-dae04084a764448a903f5391665dd71d", "references": ["telling my roommate he needs to go"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-3311b3493f014ca495c9c4d54a1663d1", "references": ["suggesting to my ex who wanted to remain friends that we should spend some time not contacting eachother"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-17a07c1460aa44c09e311b4b062431b8", "references": ["not wanting to hang out that much with my very extroverted roommate"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-04e589ea1c1c4b71b236eeb31625de00", "references": ["accidentally telling my girl churchmate why she's being ghosted/ignored"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-098d5584bc284b6593f951116612ef69", "references": ["unwittingly revealing my cousin is adopted"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-ec4a885fb2084c90b0d648c763af17df", "references": ["denying to do a job for my uncle"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-ae5cb9b6c53648c491e09002751e073b", "references": ["getting upset about the time its taling to repay a debt owed to me by my roomie"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-a2f1b5d92246405587f9fdf658d9757a", "references": ["feeling like a rebound because of this"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-b8ea9c61afbf4dca87166f5527c5f741", "references": ["refusing to take my mother's sick cat to the vet to be with my boyfriend during his father's surgery"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-f90c0806ca4c4658ac2df034e7916894", "references": ["not taking a \"friend\" to a concert anymore"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-d48c9d90c73a48f496e7036786bbb6c5", "references": ["wanting more romance in my relationship"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-eea1e64b257f4a6d9a517ae05b30c473", "references": ["dating my friends ex girlfriend"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-22ad7ff6547b438e8082d5f30dca096d", "references": ["being and at my friend for bailing on a trip to Europe"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-0f7fa1b8e4cd434fae969f4e36fd9c9a", "references": ["yelling \"fuck you!\" at a tow truck driver"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-3a4622148391445a834b761a948feeac", "references": ["shutting down an all girls club at school"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-caddaeb2a4fe4383a735c50628e7d268", "references": ["thinking my aunt in recovery's life is too dramatic for me"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-8846f4ce0de845a3bf9cc9381a3bdbe9", "references": ["not letting my neice bring her friends into my home"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-eaa3bea8bb384f8bb76c8b259b3fbdfe", "references": ["being angry at my ex for dating somebody to soon"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-4d167170490e493482864487755b7c60", "references": ["wanting to temporarily cut all contact with my best friend after his girlfriend based our friendship on lies"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-4f9c57fe206641f7ac5a0f083b1a4821", "references": ["being irritated with my roommate for banging his girlfriend while I worked from home"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task500-d4cae3e3595447da8b7739fdce78fee7", "references": ["trying to buy an item with $100 in coins"], "task_id": "task500_scruples_anecdotes_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task890-469dc422fb50410fa8c8cc10a99b5008", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-9c8cd9a925884d1cbc18b9304e7289d7", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-cced117ab70d434f8536cda4d1321c31", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-37efbe4a9c3d4bca87bd28259c4af55a", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-78f5b574edad491f9cbbca6ddbfce704", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-6b4f755b8978431fba6eaadc013291d3", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-b9576d9aec664042942fddc01b474c0b", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-4398e7696c1d419eae1fcc64ca0c9b1f", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-9f375dd2d5b341d8ad6a8a1a91b0ca09", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-cf679c77b3864cbf8813509397e8485e", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-09be46691bb544dd8795095cba723af0", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-1c1839419df84fd18bfc0b97675faf51", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-fa886a0f98e14788af95170c15fe07cc", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-fa9df8ff200540fe9141e46c22769cde", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-a818d012e4434637a5d425cf3f1e081a", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-5694647a4f1542129f529116107d0fa5", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-c2493e7bfd4c4a5fad8679fbbd40de94", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-f88e457cf6df44b99f6e3d20d49cc0dc", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-56dd2048dc9047fba552a9d5cb35e1fe", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-3598acc5afc541b982970e739dc21c27", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-f774af57eb0b4e97a8068d7adfd3225b", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-a2ee7b6374034ab1b7efa6c8fd9ff44d", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-22fbd6e2f5284d3d8900a88ca9dda10e", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-bfcf578809d24960830aa4e6da4f728a", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-6869b5617a8a4421bbdba215086defc9", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-7c2e3b3489454614aeeec956eb98367b", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-a3267aacd2914442a047bb8fdf23b6c4", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-07edc1b170394da4a8df1f5ce1121f59", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-f8385ad84c1846baac4f0d4b7c70d73c", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-3c3bebe2252d4cb387e9d00ff9670253", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-df8fd577637c450aa09b0081c321f85e", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-8f470352625b4855ba6eb8cd16930425", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-d12710735bd3471b98a5b027cb365cb2", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-1f95eaaa45cb4fefb48c47117600083b", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-3027e6af64d543ffa56f9427132b1c68", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-f6da3c37394e462d8b3e70bf3a5f38cd", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-c074e58fdb9941d2b90a587bf5b1f341", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-9f3ceaaafe314edc8946a8ab92b75fdd", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-663ea1a6da2b4c6894c96cfcbd2dbed6", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-ae21e3f026e347e2b91479b6fdd5ed37", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-c2d328cbf0f041749862d539fbed5a5c", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-f8386c7cbaf3473cb8a17772c9a26cf9", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-2fe4e3960a3e4f71881cef2c88ce161c", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-f899a524865e40c39bfbb7c443b0d262", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-ba429df1052740978b3a1109d4e7297a", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-1f3489688954440db5aabff68f1de6f8", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-a4501425226647f0a8a9bdb1fabc5b8f", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-55cc939cc4b345dab275dc5428b643d2", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-c5de57ce8fe34870823490bcf4b87f93", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-f5a3fe7f812344ea872fcd412ffaa18a", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-e585dcb97df949dbb5904e9e574c093e", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-34fa73f21fc74be48a6ab782568dbff0", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-e17321bea740459ea10fef58b218324a", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-aed60e586c3a4fc2a81541adb580a524", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-33b55d9d98784f82b40a2be5cb5a9c18", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-adbf7eef414245adbd979195f2c3fd75", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-4de4e5e257fa47dbafd207cff26040be", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-9f714e549a754ed4b217564125b46cc9", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-b0b63d3dbb8643acb22fc1656cd9101e", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-3475f733d6f84bceb6067ee76a205326", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-aee141f6b0bb415d862212dab9c9dc32", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-07483c86bbc64aea964c9836a8e194ab", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-addc51efc01448e9a08487943a59ba20", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-0b332ed43941431cb5d3c90050a65813", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-45bd050adcf44c39bbb993bd19012473", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-5c94d752fbfc4d72a852fb224309bb7b", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-222ea4afd67842238faade5d5bdfe488", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-65c4a1760fbf42dfb38b39b7fa627556", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-1eb89ba900c742539c7ca06006b99a9e", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-64fd7426bea5493b9c04862d1b890932", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-eb9675cca5c349c2a44a8d7441d200a7", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-41d27464886148239bd784cf94963f21", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-71479be924e24326b8cb0e98cc93d950", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-d83db51f22fc44d2be23834baf06c6fc", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-c76772d79382462d9c15c4b40e70bbd0", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-579d460c208d487c9be9026b22b608ee", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-d72ac8b7017f49879e008f1ec08d5039", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-634b50b712704090bc74efb20a23db3f", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-c9f16a0056564737875d15874fce0e1f", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-67ded18f776341a69d82796045cbd912", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-c22e3522ee514215b5b3dce05ede2cef", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-2a575258008e4d54905e7f4824ae328a", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-d048cb495f154814904ace140693d356", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-cadc886a1cbc42ab9e819eb83165da20", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-5ca8b794944f45ddaf5dbaf4a36a1d57", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-b06eaa45a72547dba0477a11327cf54c", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-188fa774087e49f8bab83450a645d489", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-22468310eaff462c848a82d2fa876478", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-c37476ea4cbf4a13ba8e1e6e6c6cab4c", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-f40af1d2e03a4c67934e54d3e40bf76b", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-f02ec9d151d44bc4bb0106cc6e50edd5", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-ed4b0ed530ce48d8943cb089ffd22310", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-2bd0cdc83db04662a47cd28285870eb1", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-8ff57c008c224ddb84bfa1bb3fabea5b", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-61f233ced59b464ba6063bd9d19970ec", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-4400e0e8821b482b92d16c91d41a5b27", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-91e33c66e7084698b3eb0fa9de4e9586", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-083170c6adfc418ba624cd23e8b590bc", "references": ["disagrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-5b616d7e45e84bbd9bc7b4238c4f5bd7", "references": ["neutral"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task890-c9d739655fe94bd78ced76d9f1f6f1fa", "references": ["agrees"], "task_id": "task890_gcwd_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task039-eb722ae4605f46f1afd5c61fb226948c", "references": ["host"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-df7e3ec4320c4b01a23752494a9a0735", "references": ["organisms"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-82675480b48f491f93cc5a2a79528a0e", "references": ["converted", "electric", "energy", "heat", "kinetic", "motor"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-e3dc338c59bd4ad7b7a51060bac6b81a", "references": ["Plants"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-e92ac418d7c14263aa628ed0b0ac6554", "references": ["animals", "environment", "leave"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-df900b4be0d243739baaea09b677846b", "references": ["migration"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-eb4cbc9f48014114ab0c99cb762d9c12", "references": ["weight"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-49719b4865d047dfa8a6eacf2b8776ae", "references": ["temperature"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-8358d71c29ca41d687597ac2e440952e", "references": ["living", "prehistoric", "things"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-81244bac1cbc464e96c37e0a337cc4a3", "references": ["satellites", "take", "used"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-343bbcfd529944cf83be5266c9d6e2e1", "references": ["habitats", "native"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-40253f1e6db7466da21697c33497ac44", "references": ["bacterium"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-1dba752760014890be4188a623793bad", "references": ["prey"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-b71d5fbb8b59482d8458d0196972ccff", "references": ["grass"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-ebb5ce023fdf448ab50a11b25d180e16", "references": ["Cytoplasm"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-a8071840887946308010d4624ad49903", "references": ["therian"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-eaf8e0346a8d45fd8492142d21777e5a", "references": ["conserving"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-ef521cac483240afbbd4c0f45f27fdcb", "references": ["Amphibians"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-957898ed947a472db76161f3ea111761", "references": ["audible", "wave"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-e4814dcc931c4dfe8c9077f3b80feb5f", "references": ["Forests", "stabilize"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-59a83eb935544d2da484764caeaef9df", "references": ["Shivering"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-2a36095feade494c88d84a6eb2cfaddc", "references": ["Bacteria"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-92457a9d1041487cb9cae69847ed57b9", "references": ["puberty"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-3d2e2d5df1cc40e5b8262637b593b9a0", "references": ["predicted"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-844225628a0c41bcaa033d471c2e22b9", "references": ["HIV"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-94dec0ef2a2d4abda2a988dcc99b7f0b", "references": ["cloudy", "rainy", "weather"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-3e877a42a5f34be386ea00b170e9fefc", "references": ["Sun"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-19cc23a0858f44338a75fc806d8459bf", "references": ["hornworts"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-6f6907b7c5c14fe9bc2d6b2312c8e121", "references": ["hold", "nutrients"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-e8e8c28ec40749c48a6d0c71bf31f3fe", "references": ["Montana's"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-82afa389537a4f6d8d3498329b0240f8", "references": ["stove"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-bd6a18ae7f7d4fd7b388d7693bb2f34c", "references": ["communication", "people", "telephones", "transmitting", "two", "used"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-2b3f08405e2c48f8b9e1324a66f1c522", "references": ["bulb", "light"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-507e85189165473499e145529904b62f", "references": ["algae", "bacteria", "food", "make", "sunlight", "use"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-2d90829b960642f180f7caec3d492fe6", "references": ["dormant"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-f2bba717ba0a4f5abae008c94ae74333", "references": ["an", "for", "growth", "organism", "requires"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-a8fdc3abf49b455091b252a62cf9b132", "references": ["Mating"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-8bd3e08c0ba74a2e8c208f361853061e", "references": ["control", "endocrine", "glands", "other"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-d0e0da2e5fd048899c44a53bea49cb43", "references": ["speed"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-55a320bada484715ad21f76cf83ae29c", "references": ["energy"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-9f32bc53a1d24815bca21ecf25abe373", "references": ["Animals"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-eb00960d60ad425d82a6b3989f79c3e6", "references": ["graphite"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-6c20f7ec11874883b695db25f6c032bb", "references": ["Oil"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-d504d908f7c244c6894215fe7bcd6444", "references": ["Most", "animals", "eat", "live", "rocky", "shores", "that"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-388eaa6527644c308c0bba21fd0dc1d5", "references": ["dispersal", "seed"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-ca9410d61f6c4d0b86a02d9ffe001f3d", "references": ["dialysis"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-a769a05e1966480dba47d8ecc0ca313b", "references": ["Antibiotics", "effect", "no"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-bc74dc92a3d24d268c51167252f59efb", "references": ["HCl"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-2ecd5d0d681941ed89cefe3d9f122138", "references": ["Lymphocytes"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-bbf49bc494c14d5a91c19c55fe7ac18c", "references": ["rely"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-328a397c748540e9995d657340e6287d", "references": ["hinged", "joint"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-de79bd00b9084210832f2aed157107c7", "references": ["Grapevines"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-602e0ad47bc946a1b247a8e80d7ca19b", "references": ["change", "characteristics", "living", "over", "things", "time"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-e88a757137de4ee082303180599d94ed", "references": ["sexual"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-026c658c6eac42469708685acdf2c688", "references": ["gametophytes"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-11f81ea0be3a472dba978f1268d14de9", "references": ["global", "warming"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-13b8aa0d5ba24b38a50598050d358b85", "references": ["ocean", "salt", "water"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-e2d129d258aa4b3a8a5966ace1620756", "references": ["breathing", "converts", "gill", "oxygen", "water", "when"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-30859f9179da463caf2afe446b9548f6", "references": ["changes", "material", "passes", "speed", "type"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-9ebf7bc1cb5c417ea69ba40f253ae061", "references": ["organs", "transplanted"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-868bbb8d75d54435a698db99c3f1e1e3", "references": ["earthworm"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-9ea623df02ee43d69b14d65f61da0256", "references": ["octopuses"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-66b1f50e8a7943b4b6ae05278f2a3e04", "references": ["terrain", "unstable"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-ef65c2af4ac847c79aeed1db6bb284af", "references": ["places", "remarkable"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-91a0deb3440541f1a391d0200b485b73", "references": ["snowing", "winter"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-0b9766c7387f46f7b83e8f36b3486303", "references": ["condenses", "night", "over", "vapor", "water"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-ebbaf3b6b34f4060ad3c49e3328b5177", "references": ["pedals"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-00ac7d12394d486fb333dd2556d6355a", "references": ["forming", "injury", "plates", "prevent", "protective"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-9488941ed4c240ca944cf110f4ac41f7", "references": ["life"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-572cf7a152e244a99261aed306c9f93e", "references": ["animal"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-83771dc889484c83933eca06fcc430d8", "references": ["lab"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-e12964c82c3d44fea3025b3dea1973fa", "references": ["electrolytes"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-39c6950fa39c46ccaebee8976542abcc", "references": ["Lipids"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-06950b02116f460187e5f3718cf84a01", "references": ["faster", "through", "water"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-f96c87aba4c54a579c2ef70c5fc82bdd", "references": ["burning", "coal", "coal-fire", "power", "produce", "stations"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-77dc494620ef40b78f3aa58061a9ce5a", "references": ["fires", "forest"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-8da6dc58835c4f93b8690ad298506daf", "references": ["healing", "requires"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-dc26096bb9c94626a31d81f7bfeffd03", "references": ["rate"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-fd2bc0d23e48490f89d02226fe0c4236", "references": ["dew", "formed"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-d682351370a1446f9f77948457da69cf", "references": ["center", "heat", "removes"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-c4d66163b32f4d26825ead0b7b3a788b", "references": ["shock"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-b55e66a39caa4db09ed55d6d65d3c63e", "references": ["Fertilization"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-80a3466743224d4e8cc6c07c46d86bde", "references": ["called", "collagen", "contains", "protein"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-425ca7ea8907476ca0fc1fce8ff3077e", "references": ["Clams", "oysters"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-f6c93ac400fb4061b4a64ef6a32b46d6", "references": ["cold", "food", "keeping"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-512c8acb1d8c47749875b33c7f808832", "references": ["hazard", "health"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-d9543785d7314f129036d3b37ea63956", "references": ["eukaryotes"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-149a609cbdfb4fec8cf9198994b8f50f", "references": ["divide", "uncontrollably"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-31bfcaaed7dd4fad899d140757dd2ffe", "references": ["insulation"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-42aeed14ab974125b750cad840883d25", "references": ["magnetism"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-7e3fe96b2a1b4de291e13d50d458cb40", "references": ["feed", "matter", "nonliving", "on", "organic", "plankton"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-d1afff3ab3f241cc948c21fc5a9c8ee3", "references": ["plants"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-1883fb26b2c345d8a2f3f50e90d50c0f", "references": ["Beavers"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-0e8af8fa8fba4f1b9272fde0b69e03dd", "references": ["fumes", "toxic"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-388ae2e1def2499982f4b3a263df94b7", "references": ["egg", "survive"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-08b7a31cbf0e4e479c80651cafb4dab2", "references": ["avoiding"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-075f19a799694eea8c9d3b6ac4621aa7", "references": ["clouds"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-f283bce9a38249aba401ca33ca5194c8", "references": ["Orchids"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-5703f1a4d5dd4dcba098bfd704e615a6", "references": ["'s", "a", "flower", "purpose"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task039-dd6000a15b994080af9dd9672cc08b03", "references": ["food"], "task_id": "task039_qasc_find_overlapping_words", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task1154-13494aea7eef47df9a85b33354c44491", "references": ["bicycle"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-d4d784b7803448ae89403a45a531067b", "references": ["taxi"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-bc756a8091a6405a877c6dd8356e1877", "references": ["airplane"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-8be13c1cc0454959a6eb7432365e292e", "references": ["boat"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-81f9102500db470697fea38f2fd5d43c", "references": ["car"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-da9622cefa9b4ebab1da9c045d938778", "references": ["bus"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-777eb8c372bb48cd883d5c8d0cda691a", "references": ["train", "airplane"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-82b4d47948664e1ba99b08aabc76e430", "references": ["rocket"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-70a5e58aa5004e0e92ef421460c1d954", "references": ["subway"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-d9e3f4319fff4bf288145281d11ca977", "references": ["hearse"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-b35036a130b44ed2a3217a846fbe8c97", "references": ["spaceship"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-6280fc68862647948b4ff5767a04da9e", "references": ["ambulance"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-f9db4944a76b4cdeba82fe93670370d1", "references": ["bicycle"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-133e10b837a4484994e6b2b2bc9be35c", "references": ["taxi"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-2fc966df0c274e08b911621405b6072f", "references": ["airplane"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-f74faad8ed1440338e2f5f1bd5d12e3c", "references": ["boat"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-d50b69cb983d414581d24c7ecf6032ff", "references": ["car"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-b75b01d407334896877820e977b3040e", "references": ["bus"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-3440f0dde86843d9b39ddd6e54246e3c", "references": ["train", "airplane"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-11a19c5a72ba461583c2e93013b0cc24", "references": ["rocket"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-1041e42320e84eebbc33fc44374a26d3", "references": ["subway"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-f0764dd34c3c466c91f4785834af6783", "references": ["hearse"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-6ee84efd31f84063bfc0c13255009e84", "references": ["spaceship"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-56afac536d784c75b45feffb50dd434e", "references": ["ambulance"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-f9cdaaee897c4758a6656e376aaa16eb", "references": ["bicycle"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-2ea64d82280b400bb49dcd4bd9428fba", "references": ["taxi"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-7dfcd9343e52401c8665e2d0cf6cb57b", "references": ["airplane"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-ae4893dee69047f590dce984df1ce9b7", "references": ["boat"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-efc3dc45c84644b29bcf2bc6ff7e5fe0", "references": ["car"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-b69b6d6443fb45ed9d94b95c94e4fdd6", "references": ["bus"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-10db4a7e7b3845bab3000c3c014d5806", "references": ["train", "airplane"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-4b723710b3f64b91921e30d375cd571d", "references": ["rocket"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-80751fa305c6456f957362d485387e6a", "references": ["subway"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-0c013ffb05f1463c93f24d932998fda9", "references": ["hearse"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-c869324e91c14449a195380823661af9", "references": ["spaceship"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-09250ec371174d86be3b1d6876dc02f5", "references": ["ambulance"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-efc4ec7b2e9047d4ada556742df6c069", "references": ["bicycle"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-db7cc9bb93f54ecea19456156f6443a3", "references": ["taxi"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-f8faaab7932c49f3bf9a55c8d84c962a", "references": ["airplane"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-67e7921acfb74ee082e6a4eaa78ccec4", "references": ["boat"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-3a305a5b0cb84cd4b0e71bbd00c7f7e6", "references": ["car"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-e3570ea7562a4db1bb5d427bfcf75aed", "references": ["bus", "bicycle", "skateboard"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-8248c1adbc53430980676dbc4b42d3d1", "references": ["train", "airplane"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-416e1bf47c1c43cdaa87e1a9f48817d9", "references": ["rocket"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-d40026841e004939939de71d67ae62ca", "references": ["subway"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-53b1b0f5d950465f927eb7a991014126", "references": ["hearse"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-294e83469e24499b93862a9aabbd82fd", "references": ["spaceship"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-62d29f27eaeb4883afea04e2ae6b4073", "references": ["ambulance"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-85a35013e23643a68c87c358004540de", "references": ["bicycle"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-56d2bfdc5981430b9ead89f743ed3ba7", "references": ["taxi"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-1cc276f97d2543a9ba8bf74bea782288", "references": ["airplane"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-1f58185bf97d4a04acbfd1bb88c07762", "references": ["boat"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-98d76d3213f74287bc25f39b33dabfba", "references": ["car"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-87ca19f425e74d5285312ff19dfdb53e", "references": ["bus"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-6d1674dbf2514060ac7dd19f939731d6", "references": ["train", "airplane"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-5133d221358e44df8403304264f2fa70", "references": ["rocket"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-4315669a42d548ef85124b27e8aebae0", "references": ["subway"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-0100cd06cc714823ba9dfd4cd1013469", "references": ["hearse"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-a291c77d9d604bac8206126e5a132418", "references": ["spaceship"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-2c9712e503b44d93bd6d537410c976ac", "references": ["ambulance"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-3b7df0afc8ba49b5b9efe62b3baf1805", "references": ["bicycle"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-c4fa40f598444ee79b5af779b6c4ee30", "references": ["taxi"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-1ed0f110789d4a7cb7e6d8c3995f59e5", "references": ["airplane"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-426c531014f742e39a9929d8c0f47a27", "references": ["boat"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-d920e8e1e32444d3bb6794dfcae96127", "references": ["car"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-750cb539d9fe4095bffb5da6d86e8448", "references": ["bus", "bicycle", "skateboard"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-1a18181954354f62947560ee44412157", "references": ["train", "airplane"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-2935275d19994337b3335bebf0d188bb", "references": ["rocket"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-4ead7de587cb4aeb9fc5a2bc77ecbd82", "references": ["subway"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-0d6d5045e6474d4e98c8badef1b8c59d", "references": ["hearse"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-98ecf75852a24927a5f0c3c4e141cfbc", "references": ["spaceship"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-eb6602a7d89d4a2b91eee9417b1b94e0", "references": ["ambulance"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-0730caa6e99f4b6e972b4e2e0e24cfca", "references": ["bicycle"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-f9cafbc5777045e8bc1602616154d0b5", "references": ["taxi"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-43db58bd1bcb441f80530de98bd924ba", "references": ["airplane"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-9217500892ec499682dcf6cdb7a73225", "references": ["boat"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-f0a92b008b00431eb05b91038c4382b7", "references": ["car"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-0acc93ec39ca41efb664db7b89ea9a49", "references": ["bus"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-d553294f7eea4161aeb9a1247a43154b", "references": ["train", "taxi", "car"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-5c139d8b744d45d6bfcf5dc50d7fdb53", "references": ["rocket"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-f5762fd7b5c54e8282d16b0de768b92e", "references": ["subway"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-116741b143344bcaa00b901f880a849b", "references": ["hearse"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-4065fc63a72d4abd9221912f0f49e95b", "references": ["spaceship"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-07ebfce57d794303a85c80279755d820", "references": ["ambulance"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-86b4c27f47654b45803d8bb83dfcaeb2", "references": ["bicycle"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-5849d1c3937545cabaca0cdfe131cab7", "references": ["taxi"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-32e6897b0f4644748243d5b5ed7b3ca8", "references": ["airplane"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-458b115b8e9942f9bff9103695d118d3", "references": ["boat"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-7f1c76e3dd794c56895b137abfd33ab7", "references": ["car"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-7ccf12d215d549839b896d8c284cbe53", "references": ["bus"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-639071e277074c42834492d2d2774aa9", "references": ["train", "airplane"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-c329d3ad7b2f43bfb05492c9e74dd8b8", "references": ["rocket"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-368ab3d8f540496b826d163114cf0247", "references": ["subway"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-6bef0d47fe1a4b4aa7fd3d9fff8f6552", "references": ["hearse"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-30bfc47c8060417ab4214452ac4b016f", "references": ["spaceship"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-1c727db17f794ca5afe0f1c6400c66ac", "references": ["ambulance"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-4b9b5c18819a42a38cd86b52b39f9634", "references": ["bicycle"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-36b8add7397642479885040bbb29e34a", "references": ["taxi"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-267ac9b2bc3d4e42928e0669bcb11ac3", "references": ["airplane"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1154-4f27c5d5747248f29ba7cda0dc9ef9d3", "references": ["boat"], "task_id": "task1154_bard_analogical_reasoning_travel", "task_category": "Word Analogy", "track": "default"}
{"id": "task1612-1adf6d58e352499baaa714f1a3379fd0", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-ff9f26e84ff248fdb22c4193ca92bea4", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-fa094d9d1c324d41956bb7ee5a3fe725", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-7a75eb20aaa84c34b37e14891cb65ba6", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-d1a1d36ed934409ca2f04ac9ca82257f", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-9055ce29aa394e04921601a29d8374fe", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-c7565e7ebfc14a70a420d6cf453cfeee", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-8db1ebeb894b4471ab239343c0ebead9", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-226715852a4b4e05890a9e8d31fd4783", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-15eecbd681e14808b562f30306fcdbd2", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-73e94ad9867e452cb3c8d664e05c13fa", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-09c12b3692e4436c81184b60ecf1c8fc", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-6e8f9d8fdd5e4efe98c3b23ab6392b04", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-fbb03ff36be44b41ba68e342d50c5d06", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-2fc42727ac0b45f5bf24099e68dd27f6", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-c790f60234b64730918ec41e78f34dc9", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-5bc6d01daf7d4c8b8247fd360e7ec756", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-cde6709b9f194a64b6df833973066dce", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-63ec9269f1bf4764b56e281287196444", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-8e08fd4bded84b78be7cb1fd2e22d903", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-27fac74ccd974a8cab41db4de115d827", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-8420118afa4c48b692b3515f7c340239", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-143e786358f44656bd55f74e327e40c4", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-bede9997dc9e479b81d217bc801b3f4a", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-f5b6484e46de4f17a365617348b50bb1", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-971ca52acf624886ada515384c4ff0df", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-10f833a98a3b483c999d6877af115237", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-56a6a86d4f1f4436b912aa06431f105b", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-8216f950f6214fb49d6a2fa2f9c6cd74", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-3d14014a026d4ce28c56d3352c2a7edd", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-e3f26e1c9bb3492591b544fa5c80c10f", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-40f4dc9aa7a24bd9a50a961f6495ce63", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-bfeb4f7656624cc297de4d9f648155a3", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-5c2859f149ae470eb6c3576ff1bd0baa", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-f35918c74b414a49936fb44ce29be165", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-628adb256dbd4cebab5a2f884e5e229f", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-ae29cecf286940f9a2434602680002a6", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-067f618d45af4a9ba821d329b6b01a44", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-2b6adb236b83410fb0e0b39a648e60ba", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-393794765c764ad9a747fca6fa7669d9", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-7d0156c2101b470286dafd882bf7e2ae", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-b5af051879f84dfd8335686a0566f38e", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-3ac976b320534efa91bb5e3f7380b947", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-51836e9fbabd45db88aa29c370c3081b", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-e759c1f0f14b4d21a22ac2e9c9091a52", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-c83f6de99dbe4dbdbab11636d8d3444e", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-8c41d8f663964798922dc0f48b162c16", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-9cefa1afbc66449fac87811f1266cd40", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-a4dd42ad0a944d5cb806b57ae4723906", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-ab2a017954c744e0a061e20a5c1bd7a4", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-7d037d0416ea45d0bbdea23cb32c7e93", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-8f03f50d8d8f4602b405f707ae6cf438", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-784ffb37b015430cbf9493c32a186d30", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-b1f0b50fb0a341cf888095c9e5b3a366", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-cd6414b22a524698831cc9a93d2ede9d", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-88449be2a6b14e82888ffbc330239acc", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-1d35e42a67ae48ba9fe74181f486c5fc", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-07890e269af041799af4230bc444f7cf", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-c0c022b68fb144eca69a275801c1a0ab", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-c855decfd884495d802eb70b77382b1b", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-45a02e69f51a4fae88266e75d87a45fa", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-37e5fd1597354c75adffe69a998ca624", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-d026942d562a49cabde0ee89e034579b", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-a858b54614ae4d38b05c1bd77a0c9529", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-8d665fc5cb274e13b3e29a785577d6bf", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-ec685bc6789d4c98a219c02b6931d9e0", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-79d96f347c56410499b95b647730fe09", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-33c682e633304ffc98af55461cb880b2", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-8d1da43daeec4677926e336e780955cd", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-ccbb8ff2848c4914a2130bbd641bc08f", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-3abe5484578045f4b285fc64be075811", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-0dee3e7a1b0d4a88a76255d5816b08d9", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-b6e4e7b2b054421dbc838d29d5c1a7ad", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-b60cda1428804fa7bf8a4ecff7a15abf", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-15797acc710d423c90219fc4a6538428", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-6823a1d9765f41329e0c3ef869502484", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-735e768e8e964f63a1f8a83aec7fa543", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-f88fbd8b6fd145edb59461282559984e", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-37de4afb11144aa8855bac493164b2c6", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-4986fa0684a4492d9afeeb6009140730", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-1b40566505fc40ce8425bae2e5c141ed", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-ef7be7a949bb40bf8c4d743bac92225c", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-061d550d74264fb9a382c7da075acf3d", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-6276edeedd8b444795e51af84e4655be", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-2256deca1f4745b887e55f098cc4e20e", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-4d5ab84898f44e1b88ccaf3c61f58857", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-6156dd63690940869b76f8a6992771d5", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-8a1ff75fb51b4ba486643458c9e8bd42", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-1278b79c4fd84a8abb6ce2772ba0e685", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-370bc9844b9f445c9bc425f1ef925c8f", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-89a3706d55aa4859b5b4239ce5ff3035", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-1b0fa5fe302444d9b1b511d95c8f7119", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-e615183d96e14602bd2dbae6a5324d2e", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-6be2647e549d40dc98f59c181650dee5", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-89e5cf751c5948738c74da9ee67c1d75", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-c310914992494fb6976d4d4ea21eb2b3", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-b3745d7fa197436586de0a257c761fe3", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-02da19267f16428f9d8a982bd0b70408", "references": ["1"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-955dfd5c518e4029859826ee13be0a13", "references": ["0"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1612-559024bea6a546859688b237b0a0e4b7", "references": ["2"], "task_id": "task1612_sick_label_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1442-3bac5d5e3e614ad4b7f34517ad82892c", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-79ba91a685e547d88896d23c86fbac52", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-89a098a151a34b808aa81e64b6e6fb7e", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-c6b5b10b9ef24a2296929cb33630197d", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-d94f70ebe66e4da19669d4a526a1ea7b", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-c116f66ff71f405abe2e7f80c1284134", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-1ceda55b071c466a9f85ad9350a8b1b2", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-904daf36cee64279843da685962f827c", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-07bda515004649d689e13d1927174454", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-d5c6965fba384fc2ac2f47791393a0d8", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-f75deefcf0e74a27859aa1f875796674", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-35e71e70983746b6a395273aeb705b43", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-6873646dfa034e1092ffd78cc1c91b16", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-97beb3917b3140fcb421d5fbcdd0340d", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-02a0008f57a644968298cba352742fdb", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-d8727a37a42647b6a716b34959514b24", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-ee99f9ebde30435eac7e23b5a12a2c81", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-1618d86ceb024cd39fe98647d53b3432", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-63f59caf655f4be19f367f42d3cfba5e", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-decaa582f81c4d669eb428335656a564", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-f8056cce462c4000b189964dfeed05a9", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-67a98662a8924ee5adc23b0be17d12da", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-3ef4ed00b8df4301bd7ba6483a881861", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-55ee08c59e814fc493f89945c80a9bb4", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-dd0606abf6bb45bdaa2804691e31adc2", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-35c2750480654b95be817c7e83d7cff4", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-bb27806bbe48456baf9b54698862e4e4", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-7024c918eb154901b06366e15d884c9c", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-e4e3a52b034a460d95f74e242baff418", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-c4aa3bf0143d42a4805cfdf87be2f355", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-7a228eff0227483cada5ac1d2fa84e11", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-d9536dd984514084a8a60b598f4c95fd", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-c6b5d8563fdd4811828828c093258563", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-f06d53e0128a495e8a3aba2ff6af6fd7", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-f691bc387ff2439a9f3ff2599382cd7a", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-d6a1d43ea730416ea438f568496217a9", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-91132e35127c497d8581857f8bf1ba0b", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-85975cfc2a014a7ca2d1203197e26805", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-36e7aac00b2b47dfaa11f8b32eb57203", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-2c46ccc9733b43bf9815b14a8a4167bc", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-d07bf6d07d12433495aa1044925acf51", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-cfd2f327f5594412bf62700de8ea566f", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-4e3c3f890a01405396857ee0b6e5b450", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-0e38eaf9a2724e26aa52ed1cca1a3087", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-a5b731c3f1074b6aadc8a80ad0bc28bf", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-b7c18dcfe49d48cfb270710d68738504", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-143ae272c56a4dc2a213fd094e5ffd6e", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-bc5bc8c24a3d4e81ae9118737608dc2e", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-d6cd660df93f4d92a2311301ddd5f2d1", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-684a36247e804cd1b82096b719bf6605", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-3fcf7413530d4eca871f997623fbb9ee", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-f3a266e6b297497a92afdead54abbafa", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-241d29f0cb8f449e98cf2a6a8d710571", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-07a44b8363554887acd0c1f0382863b9", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-20d9f9aba9b14bfa8c004458a953ade7", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-5a79782eb7ce4ed9b7f4b1ebec85a31d", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-7961eabef56f467a80ecf106afeca78b", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-cc076823ad474e1d9a09282eb32ce3db", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-6f5067511cf145ae94657a284b33272b", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-e2355896ccd04218a6445f2dd916efeb", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-508f58d65c9d4ab398fc11af9ed3488f", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-9a3aeeb0a61b4c5aa939560376378362", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-ea2aa8f8fde2497d94930e8e801f9de0", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-c28ddbdd553d4a79b0e95c742e3e085f", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-c3012cf42b0b41de8951e354e27e9737", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-fe4262dc2be14cda988ed4f39553cc77", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-d225d91ea5ff4adabffb39c8c6d9b53a", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-018470917a11484bb43e88c2beba1ad0", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-67c561034ef447a3997dd1cc9ed17231", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-7cf2fed6731a4f33872779d105f8474a", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-f1c06a3506b646049c6299dc8524d42f", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-0322052cfa3942b4854b0171be83cd58", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-45ef8af5e9184374845929dd503c1794", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-4fa970d2d3454520adcc73730a6637d9", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-3304847a1f8049f782244da1e9a3f9f8", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-08a3f3e044a24563aa2a30690f6082f6", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-e0492928b594400daf5ca147c6e95089", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-dd34d25528d94605a6a6f093d849ee89", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-1978fa078a1041ec8ccd8e9421c63593", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-71691a69d8d94f9196f17e3afc3a3c6f", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-5e914f2f40ab48ebb67dca5a82ec9b23", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-f085a79324404a42a23f8e4924e2dd0e", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-784badc4f84848e9844e8f0af86b7a92", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-7b8832512eb14b7faa260ba5b4a80426", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-d3c7791a68c94d44ad32c547a6b29b8e", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-b6115c480a934966a1a0a1beddc399e6", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-194b53d1358d4e39b532504f6d9297aa", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-6f5ca090d30a4a9eaa2b0cda2d1f4e4d", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-43418ab8de134a4dbe5e920f07f2d371", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-904185db552648b9b6d74279cad6d941", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-06184e43fcca4ca49b4ea737e2a192e6", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-3b0bd12d9baf45b9bfa0f858eed6056e", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-c1f49e8ab3fd4fa5871dd8b9ddb8525d", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-0849bbdd1eed46899f81b5d7436153f5", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-5cccb5769349408f88e24887cbaa1be6", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-b9c7d459228b4f4e92d66f87845f07f8", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-cf9a34623cfb43b0b1c4776666d2f597", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-6972377c12b8493baeb933c94ae78696", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-1d08baceea0e4ab4bff1f60a2b15a31f", "references": ["Yes"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1442-16a39126a71e4397a1acd43e910b6a02", "references": ["No"], "task_id": "task1442_doqa_movies_isanswerable", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-7092b71d05764c7fb97a8add0b21c2a0", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-68335e3e999647a6bcdafb1fb7c1da9f", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-e6292c291ee14229a3060ccdc46f0249", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-92e11574528d479fb15f18a79db6e1a7", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-30d7be5623ec4a3cabe8015539cfc758", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-365b9bf2d5dd4cec8957606bce5abae1", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-f5f6813b56dc41c594da94ed619692b9", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-66ea3c615cef461bae55457e0cda019d", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-d89fb3b3cf0549e59466a063ae997fdb", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-0edfecd0734f4a9086572c57d693ccfc", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-5a3cd7a650bc4b3a8e2491af935191dc", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-4670784fe95949a8a31b10089389e9d3", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-466ad65709ee4565998632e9badde332", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-7167dfa8e705434692c3ff5800ffccef", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-c2d6694592e34fe78c8539df3dedfa36", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-62c89d95cd7f40318366fb7910841866", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-29015ee6dcf148a09a169a523311e875", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-e0ccf8c6859546b795f04b3c9f70d026", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-043bf9f2427d4c2fa13274776c8bef9e", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-f41fc2bacf2c4b3abba1fa66cfec2d7d", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-aeb48588ea714e46a2288dd866b5cedc", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-56789dcda39342189dcac9ad7f79d573", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-cf71776a77804c31a85e33a0687ee727", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-bdf2bc420a1542179bda82a1116a7ee9", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-0ea7d3ae609946a9ba02d8cf410250b2", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-29092e23290d44929cc8312cb940ecf1", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-71b40377bd104a3b9f41ee208d041fb3", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-2e5a272f4e8c4300a120caf43a1c8699", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-f974e8d2ea78410e8770f78992d96150", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-717a6a3156bb4d56af6ce9325201423f", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-c1b674930a7049e08d9b66f601784ca5", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-72e4da910833452ba97ca557e64d5f6a", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-3e8e740d2b9a46b2b9256b2f172cacf1", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-71580e3d5c5f425096a8b5b1f3d2a3e9", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-9fbade90ec7d4683b577f3f08f61e49b", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-697e17bad0494462a78fa1477305e317", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-4f4fc1953e8f4ff6ad41391b21e445fe", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-2564d9d83e524f53a1caa2f8c4cf6d5c", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-80aaa40b2f2c408ba7b809fd4152de1c", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-61ce0130ab1343ef92ebf1ede6753a8a", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-ab782db93d7a4e089ae49e9b97e60712", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-9dbb33fb5e7b4c34a93b51400219fcd8", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-90e49cd037184c258f742072e936a0a2", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-bff5ea2de23e4afbb033953670c2c160", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-159e949c46be4144827808262dc60b3f", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-e056e36ebb6e453bb555245a1645d9f7", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-dfdb9d9e32f341e1a76f20c4b6f6dc5c", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-8b4b48aa0c2c49ca85d7e15cf6294967", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-f5f453c1041148bc8a66d9de1b4bafdd", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-0cbe2ad52c174530937312a65ef3cab7", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-83bc006998bb430898b754cc02458d39", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-b7e4d8485fa0445bb0b1f3fbc727ea30", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-9a64b68980af4617a886bc53e4402d14", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-5918b7da94344ba599c1ebb3f0dc65c9", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-b3a23fe863544a508885cd8206c094c4", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-ab36ecc542ac4df980174865097b409d", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-da69c709507348f48f7a00f01e50dca8", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-c7a0c39bd32a42ef9870c705640d557a", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-2bdc2000230e46519e946d235155d544", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-70bb33d13d2249ddab3a5ace43d37e59", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-cf7cdc427c3442ce9a41f6d26bea1177", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-5e7e8abdd0854ac5a23218f7b857f283", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-bb71c4e56f9d4248bb918987cf6a16f9", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-fadbcdc1b6e341d48176c1cc9be84811", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-81a1681fdb0945fd96cd536ffb074231", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-2280fc8f89fc4cd9b466750b79a215b1", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-6f8b36b655534ef3ae854c8ea0dafccf", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-6077ea66fd864f08a4633971bffa5576", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-ec72206249c24428af5f61550d771eda", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-046f927e8a66405bbe2dd1825f8d80e0", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-79020a5928ab4994aca0fb021e5865ea", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-38b673c142f2419b88272c63f1341ca7", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-69f7c8e2dde847468e308f3ecdc89834", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-abceccea153d41f1b7bf0a4d834564eb", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-0518fb2fce104ff5a6c00f280c72670e", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-fbc544148b9540a285e3ce00b7bb5a20", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-e8762710d66c40e680448761ec293468", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-32c2f168ae65409dbf008887056b9c66", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-e216ad6893ac494eaee56f2f4276cc38", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-386c2e7667de4cce8906ed2d6e488829", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-5bfc8d801e0d4b1b99add5138f03b68a", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-9bbaef5aa5f84cc48c6f8971415e836e", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-85aeded3b5a4437f828344e4696c63d4", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-276729a2cd3242f19562420221f48924", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-6dff42fa221d404fa57b2baa19e0c87e", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-e54f3d47b8524137943258f64fc41483", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-48e08d34ccfe46a9a7e0b8599b8ac29f", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-d2a69d91545e4aafa7509d07da0b5076", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-f92d0b12dbbe4d6e961f66d011623f8f", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-16df779ebb1d480f82eb1321bc88e857", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-3922e41373b3488982dcdd41dbe504d6", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-bafce4fb93f04593bcd6acb472b52977", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-eb0710d3699f4bc3b4576f432e537c27", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-737dabf1908b4f8883fb41899162a796", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-048e4d2b134c4c74b1951bc4bb5297de", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-597cb886e9ac49a6bd5d29996b6e1d5d", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-a05c36a53adb4f718151b02b1013a203", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-b525f812bf79489384d3488c59b291d6", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-25b205901fbe485fa19a2a6836e345c1", "references": ["b"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task233-322abc351c36496b8a0249af13c57c51", "references": ["a"], "task_id": "task233_iirc_link_exists_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task936-79ad1df6ad4347a290c232b20d8c7a5a", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-06dfed5d966d421a961fff7604eb69e2", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-023653d33ef34f9db4f387481c3a931e", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-f48832c9b2a34f3087aacc4607309f1d", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-8f8f91f714dd49f18e111e67a8623b8f", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-9f8265f4a6474085a51e2ccf871cd45f", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-fe4fe9abe4ae466f9070b38ede4e7853", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-66e42620316349239ab1e225477251fc", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-a67e27d58eb94b4da6ed162f2a2812fe", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-318e006fa9914cd08737e7c445fd00e6", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-76dd2852a5154e2093c55f0b52ce584d", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-5f9ccce50e0e40cc9658ac38318386b4", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-4326ecda5e56411f85948faf44e7691a", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-491b51392af546b6ad403125bb6c7754", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-6667d7b4f51f4602bbb3fc1e252cc7f8", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-dd04042c543846fe93f74cfe4d1dc609", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-89afc2c76d5d44b6a4b64f885db97a71", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-e2c5873996864f5380ec314411394a25", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-cedc53fafb6d44b0a2754db5b318c459", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-990528f469324c3e9139f7d5215ed96b", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-b05021f35383426aa4e0b33049f67f48", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-e947449ff52e41e68119716844d6f1b0", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-0f74f2ff81cd4ee989bf24c9f5967a34", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-ccd8729fcd0b41e0aeee2bf4094c3f99", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-843b6da5b4874837b5fea76a91ec0039", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-8a4986ea63e3491b87b786483d9f5192", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-464d13a233ec4f22bb304f4bdc5c341e", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-4e27098ccc60422e99b1d852d9edc3a0", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-89ce71af8be546489f4fc323d88d49ae", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-70b55f80690546319b80f5abd52868e6", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-7cb5aaf8e65b4fbeb4f0a115a4df3071", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-e612907acf354183bbbc2aa065a5c712", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-dbae68007f1c48e6b6ed04a9eb8bf5d1", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-3d495e5ae3c24147afc2d6d1ddc75e26", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-46ab2ae5999e48a4a0d75cc2e1743a43", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-560904e24dc34b9bb1af8675267509b6", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-43dc4c659b11413d88c760093a7d09e3", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-fc3201d1315a4acf984e57a9c5adcf43", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-5f4a0ccdefe64e16a513aa79463dbd88", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-dd55ba1e9dca4e228064bbea82e314e0", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-daed61d228de4918b5fab789a7df6b23", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-31c4da55534f4ef6a0ea7a1ee0d897df", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-ea661dabad0e4feebe8c23f16c56fb52", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-4e2a3caf6cf64af2888544c0eb3dadaf", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-5b45aeb23b824a1f8bbef69201f51976", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-90f7f7f423784d889e22ca74b74f671e", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-2bf0043a9e3f47fc833045124ad3f7df", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-f8bd1d4a00ec4002b06717e7c37b68ab", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-cf4918cde5b145beaaccfdddea3fae74", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-57cabafa08a142b7a2b12ad2642f4716", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-074c18db16f2441c922b874f6d228c9c", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-8386dafe2bc4458c9a9084408da67576", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-e713c752a33a483683ce3dfdb505eb00", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-2c6564f7258347e8afa55cb41ec022d2", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-62fd23823c2845ec821d9a30678729f6", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-898c57b966f8488593fa3b9c4ce4be7c", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-980d908844fa40e483c4f9b2b688a232", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-fcd85005e6f04e4b8face6e4f250c972", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-f28248bae6754898aed5d2135331411b", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-1c1cca7c95a749c28dc739ccb79698b4", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-614372595e4c41a88db049fb7695a3b7", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-de9383c5163149d9b7363e80fb1b4cc4", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-068664935700487795d368ad4efd3dd9", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-f3a8229114ee4fbca2c080fd761a36b1", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-dcb97d6063704603b839da05f65e4c6f", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-e0ffc602eae34ac1ba4ce0007aedbb6a", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-3a641b249b5840d1928863bdc679d367", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-b54aa5a654f948aa8965cc8af4c99f24", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-906051b8b2034128aa09fdf62150b126", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-5940ef4589254be9afa832776a7b43fe", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-0013dce0ea814b14a2b42781d7dc6d09", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-16f326d6374b47bb82db6b090ca51959", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-8a1a739f62354e8f9d177dcba913594e", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-08ccdf2b876b4db1b3a8dd31b6000e13", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-1e7e45f2bad64f6baa1ce6e73fe0eaf7", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-382eb3b36be249c6ae67900783a55364", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-5b4707b72003427bac7c2babefcece72", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-b7a096d1d20c460d9fc6e2e7cd593419", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-fa5f3a1b27f44adab4effbfa0bebe628", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-a3e3fb2988bb429ba7e6f5bb2fe363db", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-34a4db4598e44ff0bfb4e6da88763ecc", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-9e42d609dfe4403cb6ed66bc10c6e32c", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-7470653f15cb4a0a929a94171a7ea28b", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-57985aa5ee22454cb79a348eb081a805", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-21e29b00427542f6b51a3f2a05680789", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-634ed6a3b17d40389719a9e71cb4c0bf", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-d3f87be530fd4f04b34d462abdb60704", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-419a8f7eee2a42b2a57cb16b20952962", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-ef7b2ada561a4440b6b031097a96a534", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-17fa0672a978404a9a35bf9bf995c3b8", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-6aaf3772050d4de089010980840d1520", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-92660d8c162a465084230197069a5b15", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-7730f55d6b7f4289baa9acb5ced14a43", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-18065bc8ba0847f39835ba4152273230", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-4c884113b6f343cb95ecfa4c32000fdd", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-384844149e344e4fa96abbd34e16432c", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-1f4a3f802c3443b2a0597227e112920d", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-a4f18fe449c64fdcab921f0a11e1d2db", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-4b7c38f71b1142aa88d8fcb05e019c99", "references": ["weakener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task936-7a4c2ccbc4eb4e27a1b6b02284194898", "references": ["strengthener"], "task_id": "task936_defeasible_nli_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-ab55b89b7a1b4641b08dd056bf028c40", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-a79c4e73e2dc4a55a6a49c43c0acd75c", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-be857f94f343419ea3db17ccabd3edc5", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-af0d8d807f2b4f1194fb30371ff088d7", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-c0e95c2ca4b640438e6b8c9cddd48aea", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-8b217d767b5c4d23a414bde1a2c5f684", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-24e64b975198459c9d65baa36afa2962", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-413dd60ae7424897910239b1d4871653", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-14a880793f6d4f05b4bd091777a4b28c", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-6a222cbf668c4fc791da2daa028edbc0", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-433f01e81ca74ee2920a1dc0ae3377eb", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-50d53e87b2aa4a9c9ba5599da9e094aa", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-3a3c39bcbf974417aded91a8fb2aab89", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-e37aba889368426caf8a0cfdc6ec3fa6", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-d19989e7af1741e4a66d0996c3c4854c", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-9f03679bf9544394845c766b27f5b36e", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-c2d519f0e9f34e86a96a8e69cf04f7d1", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-f03d07a6f30145a28f4efa272e3d629f", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-0d7641544cb04c93a13618085a2be231", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-67ad11054d154562b6a505a6d398f8b9", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-581945f53dfa4741a8f860e7a4dc6a52", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-e90487b5651f4239a5589bb437766591", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-8e7fc9ff3f194589b40fa172eb1132b7", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-d941c473437c4533885089a78836aef1", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-18f41a15972045f2a972d9e6acd85184", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-3741e4f7933f4ac1a57a87642bb0c752", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-6817cd9189fa46c5bd7630aab88ee83c", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-3b76f328b0c94ecdb03bb37486a73e31", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-1ce1183915984ab5a1e8f8b40eaeb4ce", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-f66cf9fa1ccd498aae72a0e2830e8589", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-34d43499d4794b659b8f31437476ef31", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-90a2120411b549d8b1e5460e8bec3521", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-f994ae0f00024c7ca78d05fda88e3953", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-ad34a0e7f6b14fd88f4ef855af27abb7", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-accc3b6a18eb4702841622f2fa69595b", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-efc3dd7243344ecc9704abba637989e2", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-64900233a2ba42d5811df419fd69afa2", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-ec4cd199573541b28327147d7ddb824e", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-342a33ec87354382b248c5b46478ee86", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-6c663319c7a3426a9ec85081f629660a", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-2d3244655c58422b85a1f2330fa407b3", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-bcb074a6bd6849e98d14f50b9fa2ec51", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-85de012564864d15ac65056993eb933e", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-7ede77b7649c44bdb1050dba4fcfb5c7", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-e8d9d7836d724bfdb0d7e306bbd77b42", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-0bc90dca10604d96b0eb86b37ced6a75", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-86b5568c8e7e45b1a45ebbf1a34bb39f", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-0accb611422049ec8949d42f4a67a9c9", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-98de0bb62c264d139525d2233c806edf", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-893d5ecdb7a64cd5bce5719e185c5841", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-be1e10e748184d1a88dc91df597a41df", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-e543f42f0d04482db00c7fd96172725b", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-5d4924aa7ae746abb317b43d5c4c1f4e", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-429856cbbef140778ed5243e775dee1c", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-66047db18c1a4ab7a9c3b0b6d782b601", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-ca3b5844bab9425e8ce4005e4bc8ca3b", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-5a80454442fb4def924a202652736f92", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-05b41dfb099a473785e7118147d886ae", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-fd3bb049b7cf4799892cb48d7cc69d65", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-5a7a7c7c878c48cc83747b72e6830683", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-8fd159df87884804baf6336e4d664da6", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-487e5fca75214112a90ec6b7a15ff3c2", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-82164ca640c94d04b9015a82ec216839", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-86f37e2d296440259f4a49ac0e177d8f", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-0e9931b62a794422892869ed103a2738", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-4b69881494fb416599830955c80ebe01", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-09757b4b3afb49148b2dc6785d33154f", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-137053453ea447cdbd6727a1709e446b", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-0dfb736a0dd8403b910f7378f7e3de04", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-7442c352e4bc444c92a1512f020d7425", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-af5e8af01f544745999e8c6a8b470cad", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-53fd58f6abba4d4db4e7222837c47b61", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-2c38bef37667456dbce2bb2d08d55cee", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-8bfe8926de7748079a761658f66ce35e", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-2f0d85d9952f445cb97ea3ddc2219528", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-e653888a578d45c38346111cc5651781", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-532a669e0db94fce96a6eee88bfb3121", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-e276eb7f5f79404cb31c66266515ae36", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-22d580be0be849beaf6bf73cde3b7800", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-d889f5953c18432295e426a6a024fcc7", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-442cb644979b4fe7b0fd8290dbd0fee4", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-76da12d69160420d8f3fe6fcf5c4c5ca", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-3af0b97d158d492fa5d0ecf1769ffd6a", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-e85213ac5085489e8392e67584f41222", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-f728f4da4b2741ed914fa26708caa3c8", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-8e3a7a06eace43a590f2f818ae4718cd", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-8facaa5022384616948d430f633c244e", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-aaf7c0954f6442c7b26c2f3674cd84de", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-639eb09f67a6482b91979a7ed458ec14", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-892c47fa106a49019557b5564328e8fe", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-76043a3076df4364b480348ca3e66299", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-c0e85ff7394445b1a03bd4fa0d67fed2", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-bf6253f64c1a4dae9049f93fc10b16c5", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-b172488bb93e47669fb115cbd3186111", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-fbaf9d1499e94349b37d6fa7c5f08986", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-514cf35ffd9440a3819d1a1a98809396", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-6ee0784138a749e5aa4c44de5eb028ca", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-ea2c4e005601401fb88ebc7d1afd11c9", "references": ["Entailment"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-d37fce607544400caf4d3bc3c98efbe9", "references": ["Contradiction"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1386-78107a64e97c46d3a46220d680f214f3", "references": ["Neutral"], "task_id": "task1386_anli_r2_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1152-43a0232a708b4dda93feca216d38b340", "references": ["bleed"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-1e35a29f146844b2bb42b06d9568b066", "references": ["grow"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-add7bd0e7afe4463bee5cd9ce9127aea", "references": ["break"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-c92212764e5a4a60b7316ca90adbe267", "references": ["die"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-b44825665cfc43d2a10868c284896621", "references": ["burn"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-f0e26f205a9941a98df33d23ca489030", "references": ["shatter"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-1bece39fc679441fb168a2ac6ada36f6", "references": ["stumble"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-df61c971074842dfaf20b3770dcd1a9f", "references": ["see"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-cabdf2d6b67348ccb8a64ef3c24bf6ad", "references": ["hear"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-c4fd58b49e9e418f87a1d21cd50f3628", "references": ["catch"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-ce6b2843706f4c4f8fccaaa04478cf0f", "references": ["attain"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-b04f5cc6723a4631897e260bac0ca791", "references": ["flee"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-f87b5cfdbcb54af1a6c10e4fc63ba737", "references": ["arrive"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-471328d8697b4da49ad56007bbf8ba52", "references": ["fly"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-70d628d262484145be397c8216b76ac4", "references": ["fall"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-f5881fe34f324093aab223e09847a073", "references": ["bleed"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-1afcbf0c5dda42daa87e8788aa789fd9", "references": ["grow"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-acd3cf07f9a148db8d93b35f2c72dd07", "references": ["break"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-7e4165e921d1430fb86ed963d0e38a57", "references": ["die"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-06d1346db6a041149ed747ed61acb6ee", "references": ["burn"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-65b8ae20dc4d40a7a3781963f4f45473", "references": ["shatter"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-9e8c4ba215674fc5a2ca51ee3f44b5bd", "references": ["stumble"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-c33161fefa6c4392bcd1660a012f0d87", "references": ["see"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-532a6de38fc64fc1989f4f316a18b8f2", "references": ["hear"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-7b21800200fd43419ffac8a9ab266976", "references": ["catch"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-dcd4d439b8e2441c8aefb57220200d1f", "references": ["attain"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-fdc7398afba947ecaef36945de9f88e2", "references": ["flee"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-d168dfbc3b6649d1ac832433fb324636", "references": ["arrive"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-372126ec7b7f45928fdd6caac0cd46d6", "references": ["fly"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-4d4a23d517934e54960b352da3484043", "references": ["fall"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-a9a6d738015f4c8eb614aa1fc9d12829", "references": ["bleed"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-ce4ca98b39fa44a9a374e12b0a99fe74", "references": ["grow"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-f202a04add284c8f81850e59fff64969", "references": ["break"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-c8878264987242629fc760924a006d4c", "references": ["die"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-325ba59ac3794df38f8889eabfa2c4e5", "references": ["burn"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-11e66bee172f4c2ca13d827aa60c3f49", "references": ["shatter"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-5411ac42abc345f397fb1430e1e6701a", "references": ["stumble"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-9a6d7c5612e048cdb6c3827253f8c833", "references": ["see"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-1284e2e8174649e29b90c70a562fa322", "references": ["hear"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-7fd2a3ad7a4c464e8e027d3792f9fc3f", "references": ["catch"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-aa3b47e0ea63414584e0292d29164740", "references": ["attain"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-679aa568279f4f2c8324302e818e83e4", "references": ["flee"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-91b9758750984ee598f86c997879d81e", "references": ["arrive"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-4f1e92d1effe44a98d74149bcba42bfa", "references": ["fly"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-2e40a686f0e24029b9ce0d4394b6dd8b", "references": ["fall"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-4bbe27fd0ebe465db3944e41f345e640", "references": ["bleed"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-edb5ae78b5c1497f99ed36a4f370a252", "references": ["grow"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-33b4d60828b44c108988492d2d7023da", "references": ["break"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-e33838e7d9da4b9bb82396b68a897e94", "references": ["die"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-49ca1901cfb847aebbeb89c75ad46f46", "references": ["burn"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-f0acee8d9f4242d2adddf165f029cd04", "references": ["shatter"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-a7dd5ef149364c698554de9cd6fd6ff8", "references": ["stumble"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-f976b77e287c462fa21c442eaace4e1a", "references": ["see"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-030bddd67f5e4af2971c18626ceb985f", "references": ["hear"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-8a8cf292331d46939c0eaa2e1dbb91b4", "references": ["catch"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-437980aa710247528cdd252482f44d77", "references": ["attain"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-f089da836ef841a29ea184319b3e7148", "references": ["flee"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-c29220c4994c4be8b0b9df796c086640", "references": ["arrive"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-7ec8652a26d54d1381557ebb214182e1", "references": ["fly"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-ca9a7bcb73364296a02719b0af86b029", "references": ["fall"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-96f273184c7549339e0fa021f5ecf667", "references": ["bleed"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-b33899d210474015b46bb2d713c5eb76", "references": ["grow"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-5a70209f89ab4476b23c351d946b212b", "references": ["break"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-26524e84dbfc4d67b4aa41c7d5ed77de", "references": ["die"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-f37c3d44341c48c5a7b818cb87476a88", "references": ["burn"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-4211fd4f512a4ae9889280232a5bb71f", "references": ["shatter"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-8052152f369d413599942c81532c4755", "references": ["stumble"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-2ef219802cdc4c52b4146a51270b6dba", "references": ["see"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-fe13054e4e194fc19be86bdeb42f4cd9", "references": ["hear"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-0596d30976624751b94b281367135477", "references": ["catch"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-f1cf85e1c3b0435c8a71fb78cdfe85a0", "references": ["attain"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-0b577d84fdfd4c5c85ec9a446d019b55", "references": ["flee"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-6831d30cd2a844558c446becea6df26f", "references": ["arrive"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-780952db27814506a173ad54d2c7b973", "references": ["fly"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-8427e2eec1434baea21534398c1f2438", "references": ["fall"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-cc333cf1c27e46dbb483d57d77d68be0", "references": ["bleed"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-9271eaf6066a4938a89174200681bebf", "references": ["grow"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-745eb271a56c4d19bcd2f525d0824081", "references": ["break"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-dc6beca8910d4c05bc261660787fb3c8", "references": ["die"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-58db6f046fb84c84ac6f319eada5fb47", "references": ["burn"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-4371a856c53f4f96b7028d56796f0bad", "references": ["shatter"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-3a3f4b8bb4c449e3a77427b321f894af", "references": ["stumble"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-482b294f854249b0b104ee75244324fe", "references": ["see"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-8f188c5945bb47518faba734c6960b69", "references": ["hear"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-bb996cbef0e1481381b1cf8f47d56faa", "references": ["catch"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-67a38861362c44349d8803db401b2be0", "references": ["attain"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-a20aa7f811e24eaab151c5cb7a68cb0a", "references": ["flee"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-78358cb501014bb3b8b76312644c91df", "references": ["arrive"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-abc97f36dadd482eacdaa579d94c8b8b", "references": ["fly"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-e42c631b36b94e809d7c0bb62f36b225", "references": ["fall"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-1f3e4a42c66340f5bc79f3d4786eb3f2", "references": ["bleed"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-3256db66159c4361a7bb50bee1885833", "references": ["grow"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-de2b6bf9801d475585d1e950fe661506", "references": ["break"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-162f6c7984b342febf969174027582aa", "references": ["die"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-cc021fc9a56c4de89cc710b187a217e1", "references": ["burn"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-b1fe1a27ef16417a80ca2e2ce1a5b549", "references": ["shatter"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-6766e242bff94463bb7864816fd769a8", "references": ["stumble"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-9a4e587e35f9465e97839d4745a12e0d", "references": ["see"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-d09f3858851a473590c7583ff09a75ee", "references": ["hear"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task1152-27882a8487f34a3896fe5acf3c01a7fe", "references": ["catch"], "task_id": "task1152_bard_analogical_reasoning_causation", "task_category": "Word Analogy", "track": "default"}
{"id": "task290-b6832d0787fb4a5ab61fdede15f85a68", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-dc1509be581c48f99d25e8a728c2ba1a", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-1e9259a87aec43739bc22fe528ee6934", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-f1a84c19c3064575b704421c872a80d8", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-5d2d0b5c68714b4ab30ecfb61b7022d2", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-1dafd1f90d9e4113b6c39de63fb865e7", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-3168d7fad90f4cbb96542da2eb0210ef", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-10812226828549c0a5c08b9f409e4fda", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-badcc84d2bd244b7a0ed20c879d23339", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-573b89c4ea534a0f83dcb71b45962740", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-b10b4837415042b4a4ac0a284a6b0e1d", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-86c722d1df014245a477d159214655cb", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-0c1986387f1f4a8faf7d05bb49f50bb3", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-aadef1c15e2846a6854175825917d3cd", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-430b133d904942edbe90516ee308b7b1", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-1233e6ba503e4c16bbd50470b7647420", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-b87e972f3a2b4720af85aacbc58ce514", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-07debe4168ab49d1a8d0b51eb5345358", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-ee3b2ad15ddf4191b679b866f6243cbf", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-88a83ccd13e0448b849cb131a7358ec6", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-9aa7a1fd45d0436aae31634fa254bb61", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-ad483ac976c0469e95a1a50eb8dd001e", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-da895e87d6af4660a58828716db80e02", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-68be26445c2743f89a7b0c097eb14f91", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-61594a11d4e44c95bc8db876f0089d4d", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-a00c47f2125943939f5b630f7a38148d", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-bad5a5bd81754a5bb716c9a3f4249ec8", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-8373802e45bc4ea384ee9ca576f0859a", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-01230ea609e24dccada48a5c6d89dc57", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-1ad830fc74674743b9c5fe4ba23dd725", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-0cec04f6f4d9420789bf64a81654d9a4", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-d88fa24dcb6d48b28b219a5fb2db7ab9", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-0edca9cc732942509fdbb1cb86869c99", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-aec97e73048549a781c716b84a0f3b11", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-91b26db486b4434b8ed7db9090a3fb36", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-aacbc39f76ff4bc78411e3adabc18399", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-6c7605ca795849d3aa291fd112a32645", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-8b8e5e5692e6423b8d8b251aab59582b", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-101cf58a46c14cf58325ec8de0c9a3a9", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-aeb6c0a4b8a84e34ad8efe806bd5cdec", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-1ef549fafd404178804c36151afcf403", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-7ef2084c90b546f2937daa57e467c191", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-bd4f12d9c220486382070404efb2c43c", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-7c133d2f433c41f8910e84e47721c3c0", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-c2054840a8934725ab863cf1155f5cd4", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-09d922b1fd304c75bd9fe29336726f80", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-9c701c45ac7c45a09159dbef8a4b09b9", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-d8946e6749ce4637b2a26f75b487db62", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-19f270ba44b34f02b806812575569ebe", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-80fc6dde9d164c8eabe2b545e8ec8ed9", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-923e08256a5c47648952d66e15c1e614", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-be62847d8cc94554b7429484d43410c8", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-3ab4f735b405404c8225d8a0d11b2f54", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-59ededaf6a3b44ffa1c53e61bf546c05", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-46142ef507a54f6389358cf9135337de", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-65fbabaeaabe4dcd970fdca79eb7a1b9", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-e5e5eb48c5f348d89d65148c453db389", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-25e14e316a804b63934f0171a6c58baf", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-f0000bedbf324e94b352a18d20436915", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-182cc83702bd43b6b82f50d81975f051", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-ce3f258f7c074277890148298aa9c42c", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-54cccd7818bd4b19a6013903bb877d6d", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-8f062b3cf4c5457a9b9f3e0b487b8d76", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-f0fed7eb2372486da6ed2feaf5e29bf1", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-3ab3d3a4388f454184d09d1f6440c5c9", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-cbb550eb545444018f81ee69091d92ea", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-49d17120625e48bfa13a08913546f107", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-42dbdd3e40ac4fd3b13e630aaccc6224", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-1b0a43b4664946a4ae73586ce6839aad", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-17b3e3510f244324b37aed803510e181", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-75a12a0a4ecc43ce8f88917e5127c0a4", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-55afc9d199f049bfb2917b510ba4287b", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-056e481a3828413bbe90449bb47a25d9", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-fbb2a9acc3ac406bb4e207eb2a601c35", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-d972f82d71b544e4bd9fec62c627a7b2", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-67d3056abaef4beb9ac8806d26bad5aa", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-ad361c4445784cca9227122b0d35638e", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-5f3b6f87612a4bf393b14446cf110f16", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-9dbbeb163a5548e6a913c845506d678b", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-9d56552b350449a9a580fb1fc6f9f24d", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-10978cb89fd04aee98fc87701c05801d", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-96ba066138074ed4b376aa49bd47a225", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-5e0b245d8532411ea0d5dd1af786a5a8", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-b2f748a1724241d596ec68ff7c3c3f7e", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-ff108cff1b65416eb82c1fea6e468d36", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-e6190f19a7fe40ce9a8b85122f0b7a2c", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-fc303979d2a941d2aefae3c1f62a7259", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-5cf334dc1e8a433c9d1547ff39d5b2bf", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-4f475e11e39744778e3f5ddfc7c78be8", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-ce6a9b6d2b324bf685d8e98ff2e9a676", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-c194bb1775004f7b95704ef6db42c963", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-a3e7680c387446b2ab94c4ce127b3dc8", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-fbe7e519b5934c7896eddfe0679fbcfe", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-b20d90378e2e414ab8f4f1fcf11c2633", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-496d772ec8e043e2b336a25b7a22890b", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-e2f87465b06648cea7300b2e8c2f56a9", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-ef96bfb0508e4b3d8218f3bae46a6dcd", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-5192f6054ea44ea1b8d25dc11b5d0756", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-d4096a8897da4cdb8af7381b41d2af2d", "references": ["Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task290-b4209cb7873c41a1b185b62c209c2084", "references": ["Not Answerable"], "task_id": "task290_tellmewhy_question_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task304-266b6d5cf3974a298742c930445f8b26", "references": ["REFERENCE set"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-5c81ac569924427d9ce740cc3400573c", "references": ["REFERENCE car"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-108fb8934dba457bbab9b054df3f0e35", "references": ["REFERENCE submarines"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-aa72bff818474a96a4e56de56941c8ad", "references": ["REFERENCE book"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-4efd8495baae4ccf827536f9fad5d2b8", "references": ["YEAR"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-6aa720800e944245aba0dabdeed56b16", "references": ["REFERENCE thunderstorms"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-08b6082c71194a6b95747a2418a5bb1c", "references": ["REFERENCE battalion"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-7931a4987ed741888f70ffa84b5c5193", "references": ["OTHER"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-45bcdd3e304149b3bf63ac5536c3c06b", "references": ["REFERENCE move"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-40e31ec0efdb494bbc3ef24d2a19d86f", "references": ["REFERENCE guard"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-961eb203005746bbaa14a43a2947e989", "references": ["REFERENCE wife"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-e41b79b2df0547a792ed144bc2dddde0", "references": ["REFERENCE people"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-703492e3019748cab79af31096599c10", "references": ["CURRENCY"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-28007458b8f24c1fbcd1c0852729b8fb", "references": ["REFERENCE bells"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-a030511edf2a4005a86ca717ed238790", "references": ["REFERENCE cane"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-f8b33ce04ca24f8d8eb793512a1ef5f1", "references": ["REFERENCE Markers"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-f6d6acf8797a40f7ab867ff04cb1432a", "references": ["REFERENCE days"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-e509a555f94846728281f3c33ada1210", "references": ["REFERENCE nature"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-2be4dde1f0734d988010051746bc8748", "references": ["REFERENCE kids"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-fb9e71e3e6c9438185f6bb6ec64bbb67", "references": ["REFERENCE man"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-fa991cba4d7243cf93598dd3342159c7", "references": ["REFERENCE life"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-d615e3dab058457394af9e96cd863ca1", "references": ["REFERENCE episodes"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-36090f4be75045f689638b91407b98b1", "references": ["REFERENCE rattlesnake"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-bbc3a8d70bed4c93bf76c97429a49124", "references": ["REFERENCE eye"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-0f31ab4494a7460abf6a57258dd52553", "references": ["REFERENCE thought"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-064753aa15d645d4abdeeff78b2e923b", "references": ["REFERENCE pack"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-d9ea22a679384b5395a62bc417f653f0", "references": ["REFERENCE soul"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-0af186ea2fdc4f12990bed12a499c71f", "references": ["REFERENCE Pounds"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-70843359fc1745daa7ed3b6571f67dfe", "references": ["REFERENCE cameras"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-87ea9f8a523c4e57bf92239780ed56c2", "references": ["AGE"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-4a1a12db548246ec905b6c573217e9f0", "references": ["REFERENCE drop"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-ca8a386a08c740f587d9ecb85d8c61d4", "references": ["REFERENCE damsel"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-1721fcf0b93b4784b41c12880eee6718", "references": ["REFERENCE gun"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-6d9558c04638454197e3583d8bd405c3", "references": ["REFERENCE cascade"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-d482c8f1638c40e493c5f3cf239d2fd3", "references": ["PEOPLE"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-3cf083648926407c95888a33eaceb023", "references": ["REFERENCE motivator"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-199eef851bea4936814775c911746f93", "references": ["REFERENCE penises"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-48dc8ad8c79c4437946386953fb6e3a4", "references": ["REFERENCE explanation"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-89e510f5feb84057b2d30abae30d3a61", "references": ["REFERENCE guys"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-b340e6511e02493a866680600f6d9410", "references": ["REFERENCE day"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-c98489f43d5242ca9da58be59178fd73", "references": ["REFERENCE Fly"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-a7733b96772d42efba045856bac83340", "references": ["REFERENCE criminals"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-581fd909a4a3445c86da969815c10491", "references": ["REFERENCE feeding"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-a6d75eb8512f42d7b516b6592a56e3d2", "references": ["REFERENCE doves"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-cf7069c3d2e14abba79bedb0f0d22d1c", "references": ["REFERENCE things"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-721aabbfe1314791aa9c24e1ba731340", "references": ["REFERENCE distortion"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-db936529c5664456afce9940c2b4192a", "references": ["REFERENCE batteries"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-1d2bc3b049884b1bbb9d063921d03bfd", "references": ["REFERENCE family"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-e938978ee2824ddba3f54e0dcdb23843", "references": ["REFERENCE victims"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-237db4de73434ffc9101a908ffb1f2b7", "references": ["TIME"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-9497003497ee46a9bb7d7d3ede91a0aa", "references": ["REFERENCE pounds"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-de99070e35ef4ab591095a834039e4a5", "references": ["REFERENCE leg"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-9222bf86e7f049e6a831042f5d44546a", "references": ["REFERENCE alibis"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-f56c72d8c7074466828af61d83f1bb27", "references": ["REFERENCE lawyers"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-ccf40efa436142e485ba20c006612e59", "references": ["REFERENCE sausage"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-2e1c500ebd6940038eda5e2b364a9c74", "references": ["REFERENCE Highlander"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-695e0b7ce83f476abccdaeb0e0c6c23c", "references": ["REFERENCE necklaces"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-99dbecb044f546a798c5f9a45c37ddab", "references": ["REFERENCE CDs"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-17b16cee7239402bbb5d04e968a1f077", "references": ["REFERENCE fight"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-1ea62dea0fc441e6932ffaf07d11de10", "references": ["REFERENCE questions"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-52fd97478e9c4d5c9df41a978842949a", "references": ["REFERENCE soldier"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-b90fb577f29c40c489d8df9d14df6936", "references": ["REFERENCE companions"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-3d8826deacc34263891bfc0bc55e6c8e", "references": ["REFERENCE degrees"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-530dd60df6d547b3a1e426e01a0b67fe", "references": ["REFERENCE moon"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-d7f4f8600c6141798eb7469565c0356e", "references": ["REFERENCE incubator"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-9360a0264b2a4863afdaa3174392bc14", "references": ["REFERENCE future"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-72ba868b3795469f88005f8a7edfe02f", "references": ["REFERENCE list"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-1958f03fef494180858d8315478f25d0", "references": ["REFERENCE answers"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-5aa7827664a6401b8ccd94114b97ef8e", "references": ["REFERENCE circumstance"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-57890b93c4ee4cf4bf0ce9a0b138f5ed", "references": ["REFERENCE parachute"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-414cda49e61246acb06f31a9de32d3ef", "references": ["REFERENCE reasons"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-f41a6c7bbff74ff2a118fccab1d8ff67", "references": ["REFERENCE world"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-affc31ae235c4f73a69381b7376fb1a6", "references": ["REFERENCE guy"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-4c9a71d4f5e247ea82b3a11ed5bf0bbe", "references": ["REFERENCE v\u00e1se"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-c80d4cac6cab43a8b289ad5ea6e5e25f", "references": ["REFERENCE Faces"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-367d046ba7614132b47c1366e354914f", "references": ["REFERENCE game"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-e848f6419445403fa84759ffdfc59cb6", "references": ["REFERENCE bucks"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-32f7bae8ca424276a7ee7592b019d2d6", "references": ["REFERENCE disaster"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-4a155e1305bc4a2ea59ce3b4c7673098", "references": ["REFERENCE theory"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-ab78a539ed1347ca94b5e5422802132d", "references": ["REFERENCE airplane"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-ee211fa920f04c73b8f77e29dac40739", "references": ["REFERENCE championship"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-9f3896a03bdc499d9e52f088ad06b2b3", "references": ["REFERENCE babies"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-4f9f5e21213f4c7b921530305336d9e6", "references": ["REFERENCE bed"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-01b2b534f91f4047bfb90484bd93ff29", "references": ["REFERENCE limits"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-6a8ca3d42e7c48ceb51bab5be83138c0", "references": ["REFERENCE T"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-214b8a77711a452cb2b1cd90ae2424a3", "references": ["REFERENCE boxes"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-5ac9c7ee51f7420390bf455b9c65bb4b", "references": ["REFERENCE example"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-bfe3675b676c488b856eb9095e3b4a7c", "references": ["REFERENCE models"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-5d4e856aabb541f983e0eabb7346a82d", "references": ["REFERENCE dame"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-e950a2c8535c4bb29c646bb1e68fc746", "references": ["REFERENCE office"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-61bccd4253404ca1a435b9d39eb6bb2e", "references": ["REFERENCE barber"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-701f64d790204c5d9d4395ed5058c6de", "references": ["REFERENCE monster"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-817ed2cc2ee8445b8dddc352012c6b7c", "references": ["REFERENCE scope"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-e03b2bb0d2f04870bc937d06e18051c8", "references": ["REFERENCE medium"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-34cfef76bc5946a68257038b750f6aba", "references": ["REFERENCE worlds"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-fb1ad9215ddd4b33a4a857e292187cfd", "references": ["REFERENCE windows"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-71694fbefdab45ac9ca326254c281742", "references": ["REFERENCE times"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-64b806a37350431fa11f135e5bb703e2", "references": ["REFERENCE t"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-310c3488bed040bc826a166941550597", "references": ["REFERENCE grift"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task304-658123d2dc1c41849a4c3ba4a11ba98b", "references": ["REFERENCE fire"], "task_id": "task304_numeric_fused_head_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task760-becf7feb899d41f88987026901e67250", "references": ["Passage:  The player named Tyler Houston, plays as Pick-2.Tyler Houston plays for the team Atlanta Braves, at position C. Tyler Houston belongs to the school Valley HS (Las Vegas, NV). The player named Ben McDonald, plays as Pick-1.Ben McDonald plays for the team Baltimore Orioles, at position RHP. Ben McDonald belongs to the school Louisiana State University. The player named Roger Salkeld, plays as Pick-3.Roger Salkeld plays for the team Seattle Mariners, at position RHP. Roger Salkeld belongs to the school Saugus (CA) HS. The player named Jeff Jackson, plays as Pick-4.Jeff Jackson plays for the team Philadelphia Phillies, at position OF. Jeff Jackson belongs to the school Simeon HS (Chicago, IL). The player named Donald Harris, plays as Pick-5.Donald Harris plays for the team Texas Rangers, at position OF. Donald Harris belongs to the school Texas Tech University. The player named Paul Coleman, plays as Pick-6.Paul Coleman plays for the team Saint Louis Cardinals, at position OF. Paul Coleman belongs to the school Frankston (TX) HS. The player named Frank Thomas, plays as Pick-7.Frank Thomas plays for the team Chicago White Sox, at position 1B. Frank Thomas belongs to the school Auburn University. The player named Earl Cunningham, plays as Pick-8.Earl Cunningham plays for the team Chicago Cubs, at position OF. Earl Cunningham belongs to the school Lancaster (SC) HS. The player named Kyle Abbott, plays as Pick-9.Kyle Abbott plays for the team California Angels, at position LHP. Kyle Abbott belongs to the school Long Beach State University. The player named Charles Johnson, plays as Pick-10.Charles Johnson plays for the team Montreal Expos, at position C. Charles Johnson belongs to the school Westwood HS (Fort Pierce, FL). The player named Calvin Murray, plays as Pick-11.Calvin Murray plays for the team Cleveland Indians, at position 3B. Calvin Murray belongs to the school W.T. White High School (Dallas, TX). The player named Jeff Juden, plays as Pick-12.Jeff Juden plays for the team Houston Astros, at position RHP. Jeff Juden belongs to the school Salem (MA) HS. The player named Brent Mayne, plays as Pick-13.Brent Mayne plays for the team Kansas City Royals, at position C. Brent Mayne belongs to the school Cal State Fullerton. The player named Steve Hosey, plays as Pick-14.Steve Hosey plays for the team San Francisco Giants, at position OF. Steve Hosey belongs to the school Fresno State University. The player named Kiki Jones, plays as Pick-15.Kiki Jones plays for the team Los Angeles Dodgers, at position RHP. Kiki Jones belongs to the school Hillsborough HS (Tampa, FL). The player named Greg Blosser, plays as Pick-16.Greg Blosser plays for the team Boston Red Sox, at position OF. Greg Blosser belongs to the school Sarasota (FL) HS. The player named Cal Eldred, plays as Pick-17.Cal Eldred plays for the team Milwaukee Brewers, at position RHP. Cal Eldred belongs to the school University of Iowa. The player named Willie Greene, plays as Pick-18.Willie Greene plays for the team Pittsburgh Pirates, at position SS. Willie Greene belongs to the school Jones County HS (Gray, GA). The player named Eddie Zosky, plays as Pick-19.Eddie Zosky plays for the team Toronto Blue Jays, at position SS. Eddie Zosky belongs to the school Fresno State University. The player named Scott Bryant, plays as Pick-20.Scott Bryant plays for the team Cincinnati Reds, at position OF. Scott Bryant belongs to the school University of Texas. The player named Greg Gohr, plays as Pick-21.Greg Gohr plays for the team Detroit Tigers, at position RHP. Greg Gohr belongs to the school Santa Clara University. The player named Tom Goodwin, plays as Pick-22.Tom Goodwin plays for the team Los Angeles Dodgers, at position OF. Tom Goodwin belongs to the school Fresno State University. The player named Mo Vaughn, plays as Pick-23.Mo Vaughn plays for the team Boston Red Sox, at position 1B. Mo Vaughn belongs to the school Seton Hall University. The player named Alan Zinter, plays as Pick-24.Alan Zinter plays for the team New York Mets, at position C. Alan Zinter belongs to the school University of Arizona. The player named Chuck Knoblauch, plays as Pick-25.Chuck Knoblauch plays for the team Minnesota Twins, at position 2B. Chuck Knoblauch belongs to the school Texas A&M University. The player named Scott Burrell, plays as Pick-26.Scott Burrell plays for the team Seattle Mariners, at position RHP. Scott Burrell belongs to the school Hamden (CT) HS."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-132dd620d18f4475b5afaa945b2ccebc", "references": ["Passage:  The club named Cordoba CF is at position 1. Cordoba CF played 30 matches, won 16 matches, lost 6 matches and had 8 draws. Cordoba CF scored 48 goals for, 22 goals against. Total points for Cordoba CF are 40. The club named CD Malaga is at position 2. CD Malaga played 30 matches, won 14 matches, lost 6 matches and had 10 draws. CD Malaga scored 52 goals for, 36 goals against. Total points for CD Malaga are 38. The club named Granada CF is at position 3. Granada CF played 30 matches, won 15 matches, lost 9 matches and had 6 draws. Granada CF scored 48 goals for, 34 goals against. Total points for Granada CF are 36. The club named UD Las Palmas is at position 4. UD Las Palmas played 30 matches, won 15 matches, lost 10 matches and had 5 draws. UD Las Palmas scored 47 goals for, 39 goals against. Total points for UD Las Palmas are 35. The club named Recreativo de Huelva is at position 5. Recreativo de Huelva played 30 matches, won 13 matches, lost 10 matches and had 7 draws. Recreativo de Huelva scored 43 goals for, 42 goals against. Total points for Recreativo de Huelva are 33. The club named Levante UD is at position 6. Levante UD played 30 matches, won 14 matches, lost 12 matches and had 4 draws. Levante UD scored 49 goals for, 42 goals against. Total points for Levante UD are 32. The club named Hercules CF is at position 7. Hercules CF played 30 matches, won 14 matches, lost 12 matches and had 4 draws. Hercules CF scored 55 goals for, 46 goals against. Total points for Hercules CF are 32. The club named Real Murcia is at position 8. Real Murcia played 30 matches, won 12 matches, lost 11 matches and had 7 draws. Real Murcia scored 40 goals for, 35 goals against. Total points for Real Murcia are 31. The club named Real Jaen is at position 9. Real Jaen played 30 matches, won 14 matches, lost 13 matches and had 3 draws. Real Jaen scored 58 goals for, 42 goals against. Total points for Real Jaen are 31. The club named Cadiz CF is at position 10. Cadiz CF played 30 matches, won 12 matches, lost 14 matches and had 4 draws. Cadiz CF scored 43 goals for, 52 goals against. Total points for Cadiz CF are 28. The club named CD Cartagena is at position 11. CD Cartagena played 30 matches, won 13 matches, lost 15 matches and had 2 draws. CD Cartagena scored 45 goals for, 56 goals against. Total points for CD Cartagena are 28. The club named CD Mestalla is at position 12. CD Mestalla played 30 matches, won 11 matches, lost 14 matches and had 5 draws. CD Mestalla scored 50 goals for, 49 goals against. Total points for CD Mestalla are 27. The club named Albacete Balompie is at position 13. Albacete Balompie played 30 matches, won 10 matches, lost 13 matches and had 7 draws. Albacete Balompie scored 27 goals for, 32 goals against. Total points for Albacete Balompie are 27. The club named CD San Fernando is at position 14. CD San Fernando played 30 matches, won 11 matches, lost 14 matches and had 5 draws. CD San Fernando scored 37 goals for, 47 goals against. Total points for CD San Fernando are 27. The club named Atletico Ceuta is at position 15. Atletico Ceuta played 30 matches, won 8 matches, lost 15 matches and had 7 draws. Atletico Ceuta scored 33 goals for, 48 goals against. Total points for Atletico Ceuta are 23. The club named CD Villarrobledo is at position 16. CD Villarrobledo played 30 matches, won 4 matches, lost 22 matches and had 4 draws. CD Villarrobledo scored 26 goals for, 79 goals against. Total points for CD Villarrobledo are 12."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-175ea253a14e4d3680d2ece872a0a375", "references": ["Passage:  The model named 'Remington-Beals Army Model Revolver', with Large frame, was manufactured around the period 1861-1862. 'Remington-Beals Army Model Revolver' has .44 caliber(s), and the number of production units are 1,900 (estimated) and 8 inch octagon barrels. . The model named 'Remington-Beals Navy Model Revolver', with Medium frame, was manufactured around the period 1861-1862. 'Remington-Beals Navy Model Revolver' has .36 caliber(s), and the number of production units are 14,500 (estimated) and 7 1/2 inch octagon barrels. . The model named '1861 Army Revolver (Old Model Army)', with Large frame, was manufactured around the period 1862. '1861 Army Revolver (Old Model Army)' has .44 caliber(s), and the number of production units are 6,000 (estimated) and 8 inch octagon barrels. . The model named '1861 Navy Revolver', with Medium frame, was manufactured around the period 1862. '1861 Navy Revolver' has .36 caliber(s), and the number of production units are 7,000 (estimated) and 7 3/8 inch octagon barrels. . The model named 'New Model Army Revolver', with Large frame, was manufactured around the period 1863-1875. 'New Model Army Revolver' has .44 caliber(s), and the number of production units are 122,000 (approximately) and 8 inch octagon barrels. Used for factory conversions in .46 RF & .44 Remington. The model named 'New Model Navy Revolver', with Medium frame, was manufactured around the period 1863-1875. 'New Model Navy Revolver' has .36 caliber(s), and the number of production units are 28,000 (approximately) and 7 3/8 inch octagon barrels. Used for factory and U.S. Navy conversions to .38 RF & CF. The model named 'New Model Single Action Belt Revolver', with Large frame, was manufactured around the period 1863-1875. 'New Model Single Action Belt Revolver' has .36 percussion and .38 CF caliber(s), and the number of production units are 2,500 - 3,000 (estimated) and 6 1/2 inch octagon barrels. Factory conversion production started in 1873. The model named 'Remington-Rider Double Action New Model Belt Revolver', with Large frame, was manufactured around the period 1863-1873. 'Remington-Rider Double Action New Model Belt Revolver' has .36 percussion and .38 CF caliber(s), and the number of production units are 3,000 - 5,000 (estimated) and 6 1/2 inch octagon barrels. 1863-1865 available with fluted cylinder, conversions had two-piece cylinder. The model named 'New Model Police Revolver', with Medium frame, was manufactured around the period 1865-1873. 'New Model Police Revolver' has .36 percussion and .38 RF caliber(s), and the number of production units are 25,000 (estimated) and 3 1/2, 4 1/2, 5 1/2, 6 1/2 inch octagon barrels. Conversions all believed to be rimfire only. The model named 'New Model Pocket Revolver', with Medium frame, was manufactured around the period 1865-1873. 'New Model Pocket Revolver' has .31 percussion and .32 CF caliber(s), and the number of production units are 25,000 (estimated) and 3, 3 1/2, 4, 4 1/2 barrels. Majority produced as conversions or cartridge."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-fd9c3a0b404940ad92a05b5fa7b04aac", "references": ["Passage:  The nation, Algeria, has ranked 1. Algeria won 8 gold medals, 3 silver medals, and 0 bronze medals. Algeria won 11 medals in total. The nation, Tunisia, has ranked 2. Tunisia won 4 gold medals, 3 silver medals, and 5 bronze medals. Tunisia won 12 medals in total. The nation, Egypt, has ranked 3. Egypt won 3 gold medals, 3 silver medals, and 3 bronze medals. Egypt won 9 medals in total. The nation, Cameroon, has ranked 4. Cameroon won 1 gold medals, 1 silver medals, and 5 bronze medals. Cameroon won 7 medals in total. The nation, Morocco, has ranked 5. Morocco won 0 gold medals, 1 silver medals, and 5 bronze medals. Morocco won 5 medals in total. The nation, South Africa, has ranked 6. South Africa won 0 gold medals, 1 silver medals, and 1 bronze medals. South Africa won 2 medals in total. The nation, Angola, has ranked 7. Angola won 0 gold medals, 1 silver medals, and 0 bronze medals. Angola won 1 medals in total. The nation, Burkina Faso, has ranked 7. Burkina Faso won 0 gold medals, 1 silver medals, and 0 bronze medals. Burkina Faso won 1 medals in total. The nation, Gabon, has ranked 7. Gabon won 0 gold medals, 1 silver medals, and 0 bronze medals. Gabon won 1 medals in total. The nation, Madagascar, has ranked 7. Madagascar won 0 gold medals, 1 silver medals, and 0 bronze medals. Madagascar won 1 medals in total. The nation, Nigeria, has ranked 11. Nigeria won 0 gold medals, 0 silver medals, and 3 bronze medals. Nigeria won 3 medals in total. The nation, Senegal, has ranked 12. Senegal won 0 gold medals, 0 silver medals, and 2 bronze medals. Senegal won 2 medals in total. The nation, Congo Republic, has ranked 13. Congo Republic won 0 gold medals, 0 silver medals, and 1 bronze medals. Congo Republic won 1 medals in total. The nation, Ivory Coast, has ranked 13. Ivory Coast won 0 gold medals, 0 silver medals, and 1 bronze medals. Ivory Coast won 1 medals in total. The nation, Guinea, has ranked 13. Guinea won 0 gold medals, 0 silver medals, and 1 bronze medals. Guinea won 1 medals in total. The nation, Niger, has ranked 13. Niger won 0 gold medals, 0 silver medals, and 1 bronze medals. Niger won 1 medals in total."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-8de9a0cdb1b342a19dd857fabc6a40db", "references": ["Passage:  The Great Britain athlete named Tommy Green is ranked  globally. Tommy Green completed the marathon in 4:50:10 seconds. The Latvia athlete named Janis Dalins is ranked  globally. Janis Dalins completed the marathon in 4:57:20 seconds. The Italy athlete named Ugo Frigerio is ranked  globally. Ugo Frigerio completed the marathon in 4:59:06 seconds. The Germany athlete named Karl Hahnel is ranked 4 globally. Karl Hahnel completed the marathon in 5:06:06 seconds. The Italy athlete named Ettore Rivolta is ranked 5 globally. Ettore Rivolta completed the marathon in 5:07:39 seconds. The Germany athlete named Paul Sievert is ranked 6 globally. Paul Sievert completed the marathon in 5:16:41 seconds. The France athlete named Henri Quintric is ranked 7 globally. Henri Quintric completed the marathon in 5:27:25 seconds. The United States athlete named Ernie Crosbie is ranked 8 globally. Ernie Crosbie completed the marathon in 5:28:02 seconds. The United States athlete named Bill Chisholm is ranked 9 globally. Bill Chisholm completed the marathon in 5:51:00 seconds. The Estonia athlete named Alfred Maasik is ranked 10 globally. Alfred Maasik completed the marathon in 6:19:00 seconds. The Canada athlete named Henry Cieman is ranked  globally. Henry Cieman completed the marathon in  seconds. The Greece athlete named John Moralis is ranked  globally. John Moralis completed the marathon in  seconds. The Italy athlete named Francesco Pretti is ranked  globally. Francesco Pretti completed the marathon in  seconds. The Switzerland athlete named Arthur Tell Schwab is ranked  globally. Arthur Tell Schwab completed the marathon in  seconds. The United States athlete named Harry Hinkel is ranked  globally. Harry Hinkel completed the marathon in  seconds."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-6652fe996df64880a302a6f26dcb9a67", "references": ["Passage:  The population for Brasov was 283,901 according to 2002 census, 277,945 according to 2007 estimation, and 253,200 according to 2011 census. The total area of Brasov is 267.32km2, and the density is 1,204 pop/km2. The population for Codlea was 24,256 according to 2002 census, 24,550 according to 2007 estimation, and 21,708 according to 2011 census. The total area of Codlea is 132.79km2, and the density is 182 pop/km2. The population for Sacele was 30,044 according to 2002 census, 31,796 according to 2007 estimation, and 30,798 according to 2011 census. The total area of Sacele is 320km2, and the density is 93 pop/km2. The population for Ghimbav was 5,100 according to 2002 census, 5,357 according to 2007 estimation, and 4,698 according to 2011 census. The total area of Ghimbav is 28.08km2, and the density is 181.62 pop/km2. The population for Predeal was 5,625 according to 2002 census, 5,174 according to 2007 estimation, and 4,755 according to 2011 census. The total area of Predeal is 58.4km2, and the density is 96.14 pop/km2. The population for Rasnov was 15,436 according to 2002 census, 16,055 according to 2007 estimation, and 15,022 according to 2011 census. The total area of Rasnov is 164.36km2, and the density is 94 pop/km2. The population for Cristian was 3,952 according to 2002 census, 4,300 according to 2007 estimation, and 4,490 according to 2011 census. The total area of Cristian is 27.73km2, and the density is 142.51 pop/km2. The population for Sanpetru was 3,401 according to 2002 census, 3,759 according to 2007 estimation, and 4,819 according to 2011 census. The total area of Sanpetru is 30.74km2, and the density is 110.63 pop/km2. The population for Halchiu was 4,072 according to 2002 census, 4,560 according to 2007 estimation, and 4,218 according to 2011 census. The total area of Halchiu is 56.67km2, and the density is 71.85 pop/km2. The population for Tarlungeni was 7,413 according to 2002 census, 7,996 according to 2007 estimation, and 8,320 according to 2011 census. The total area of Tarlungeni is 135.66km2, and the density is 54.65 pop/km2. The population for Prejmer was 8,323 according to 2002 census, 8,876 according to 2007 estimation, and 8,472 according to 2011 census. The total area of Prejmer is 60.48km2, and the density is 137.61 pop/km2. The population for Harman was 4,437 according to 2002 census, 4,775 according to 2007 estimation, and 5,402 according to 2011 census. The total area of Harman is 52.79km2, and the density is 84.05 pop/km2. The population for Bod was 3,942 according to 2002 census, 4,173 according to 2007 estimation, and 3,994 according to 2011 census. The total area of Bod is 33.56km2, and the density is 117.46 pop/km2. The population for Total was 399,902 according to 2002 census, 399,316 according to 2007 estimation, and 369,896 according to 2011 census. The total area of Total is 1,368.58km2, and the density is 270 pop/km2."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-82ae9d23ceea445eb8f7b26ab7d13605", "references": ["Passage:  The institution named Clemson University is located in Clemson, South Carolina. Clemson University has 20,576 students enrolled. Clemson University's nickname is Tigers, and has 19 varsity sports teams. Clemson University has been playing rugby since 1967. Clemson University's head-coach is Justin Hickey. The institution named Maryland is located in College Park, Maryland. Maryland has 37,641 students enrolled. Maryland's nickname is Terrapins, and has 20 varsity sports teams. Maryland has been playing rugby since 1968. Maryland's head-coach is Jeff Soeken. The institution named Navy is located in Annapolis, Maryland. Navy has 4,576 students enrolled. Navy's nickname is Midshipmen, and has 30 varsity sports teams. Navy has been playing rugby since 1963. Navy's head-coach is Mike Flanagan. The institution named North Carolina is located in Chapel Hill, North Carolina. North Carolina has 29,340 students enrolled. North Carolina's nickname is Tar Heels, and has 28 varsity sports teams. North Carolina has been playing rugby since 1966. North Carolina's head-coach is Pete Earsman. The institution named North Carolina State is located in Raleigh, North Carolina. North Carolina State has 34,767 students enrolled. North Carolina State's nickname is Wolfpack, and has 25 varsity sports teams. North Carolina State has been playing rugby since 1965. North Carolina State's head-coach is Jim Latham. The institution named University of Virginia is located in Charlottesville, Virginia. University of Virginia has 20,895 students enrolled. University of Virginia's nickname is Cavaliers, and has 25 varsity sports teams. University of Virginia has been playing rugby since 1961. University of Virginia's head-coach is Jon Roberts. The institution named Virginia Tech is located in Blacksburg, Virginia. Virginia Tech has 30,379 students enrolled. Virginia Tech's nickname is Hokies, and has 21 varsity sports teams. Virginia Tech has been playing rugby since 1891 / 1968. Virginia Tech's head-coach is Andy Richards. The institution named Wake Forest is located in Winston-Salem, North Carolina. Wake Forest has 7,079 students enrolled. Wake Forest's nickname is Demon Deacons, and has 18 varsity sports teams. Wake Forest has been playing rugby since 1973. Wake Forest's head-coach is Patrick Kane."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-483bcbe43622459ba12494d412903134", "references": ["Passage:  The program named The Morning Click is available on Monday-Friday at time 9am-10am. The Morning Click is hosted by Jamie Colby (M,T), Harris Faulkner (W-F). The Morning Click is about Current events.. The program named  is available on Monday-Friday at time 10am-10:30am.  is hosted by Lauren Green (M), Uma Pemmaraju (T), Gregg Jarrett (W), Arthel Neville (F), Rick Folbaum (F), Heather Childers.  is about Current events.. The program named FBN Live is available on Monday-Friday at time 11am-11:30am. FBN Live is hosted by Lauren Simonetti. FBN Live is about Business news and information.. The program named On the Hunt is available on Monday-Friday at time 12pm-1pm. On the Hunt is hosted by Jonathan Hunt. On the Hunt is about Current events.. The program named Entertainment Hour is available on Monday at time 2pm-3pm. Entertainment Hour is hosted by Ashley Dvorkin. Entertainment Hour is about Entertainment news, musical performances.. The program named  is available on Tuesday at time 2pm-3pm.  is hosted by Kimberly Guilfoyle.  is about Crime and legal news.. The program named Defcon 3 is available on Wednesday at time 2pm-3pm. Defcon 3 is hosted by Kathleen Troia McFarland. Defcon 3 is about Military news and information.. The program named Fox Car Report is available on Thursday at time 2pm-3pm. Fox Car Report is hosted by Gary Gastelu. Fox Car Report is about Automotive news and information.. The program named Spirited Debate is available on Friday at time 2pm-3pm. Spirited Debate is hosted by Lauren Green. Spirited Debate is about Discussion on religious topics.."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-f876cc4b98ca41ce8ef7e569dc235218", "references": ["Passage:  Tiffany Brien from Belfast was Miss Northern Ireland in the year 2012. Tiffany Brien was placed as a Top 30 at Miss World. Tiffany Brien was Top 10 of Beach Fashion and 1st runner-up of Sports & Fitness at Miss World 2012. Finola Guinnane from Drumbo was Miss Northern Ireland in the year 2011. Finola Guinnane was placed as a Non-Finalist at Miss World. Finola Guinnane was Top 20 of Beach Beauty and Top 77 of Beauty with a Purpose at Miss World 2011. Lori Moore from Belfast was Miss Northern Ireland in the year 2010. Lori Moore was placed as a Top 25 at Miss World. Lori Moore was Winner of Sports at Miss World 2010. Cherie Gardiner from Bangor was Miss Northern Ireland in the year 2009. Cherie Gardiner was placed as a Non-Finalist at Miss World. Cherie Gardiner was . Judith Wilson from Enniskillen was Miss Northern Ireland in the year 2008. Judith Wilson was placed as a Non-Finalist at Miss World. Judith Wilson was Top 19 of Talent at Miss World 2008. Melissa Patton from Belfast was Miss Northern Ireland in the year 2007. Melissa Patton was placed as a Non-Finalist at Miss World. Melissa Patton was . Catherine Jean Milligan from Newtownards was Miss Northern Ireland in the year 2006. Catherine Jean Milligan was placed as a Top 17 at Miss World. Catherine Jean Milligan was Winner of Miss Talent at Miss World 2006. Lucy Evangelista from Portglenone was Miss Northern Ireland in the year 2005. Lucy Evangelista was placed as a Top 15 at Miss World. Lucy Evangelista was Later Miss United Kingdom 2005 and Miss Universe United Kingdom 2005 2nd runner-up. Kirsty Anne Gabriel Stewart from Enniskillen was Miss Northern Ireland in the year 2004. Kirsty Anne Gabriel Stewart was placed as a Non-Finalist at Miss World. Kirsty Anne Gabriel Stewart was . Diana Sayers from Belfast was Miss Northern Ireland in the year 2003. Diana Sayers was placed as a Non-Finalist at Miss World. Diana Sayers was . Gayle Williamson from Lurgan was Miss Northern Ireland in the year 2002. Gayle Williamson was placed as a Non-Finalist at Miss World. Gayle Williamson was Later Miss United Kingdom 2002. Angela McCarthy from Belfast was Miss Northern Ireland in the year 2001. Angela McCarthy was placed as a Non-Finalist at Miss World. Angela McCarthy was . Julie Lee-Ann Martin from Belfast was Miss Northern Ireland in the year 2000. Julie Lee-Ann Martin was placed as a Non-Finalist at Miss World. Julie Lee-Ann Martin was ."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-767c06ec58b04c66b66d55414ab255ed", "references": ["Passage:  The club named Biarritz Olympique, played 26 matches. Out of it,  19 matches resulted in win,  resulted in draw, and 7 matches were lost. The total points the 26  club scored is 694. Total points scored against the 26  team is 350. The team scored 14 bonus points. Total points scored by the 26 club this season this 90. The club named Stade Francais, played 26 matches. Out of it,  19 matches resulted in win,  resulted in draw, and 7 matches were lost. The total points the 26  club scored is 633. Total points scored against the 26  team is 437. The team scored 13 bonus points. Total points scored by the 26 club this season this 89. The club named Stade Toulousain, played 26 matches. Out of it,  19 matches resulted in win,  resulted in draw, and 7 matches were lost. The total points the 26  club scored is 713. Total points scored against the 26  team is 427. The team scored 12 bonus points. Total points scored by the 26 club this season this 88. The club named USA Perpignan, played 26 matches. Out of it,  18 matches resulted in win,  resulted in draw, and 8 matches were lost. The total points the 26  club scored is 671. Total points scored against the 26  team is 398. The team scored 12 bonus points. Total points scored by the 26 club this season this 84. The club named SU Agen, played 26 matches. Out of it,  15 matches resulted in win,  resulted in draw, and 11 matches were lost. The total points the 26  club scored is 655. Total points scored against the 26  team is 540. The team scored 10 bonus points. Total points scored by the 26 club this season this 70. The club named CS Bourgoin-Jallieu, played 26 matches. Out of it,  14 matches resulted in win,  resulted in draw, and 12 matches were lost. The total points the 26  club scored is 591. Total points scored against the 26  team is 516. The team scored 11 bonus points. Total points scored by the 26 club this season this 67. The club named Castres Olympique, played 26 matches. Out of it,  13 matches resulted in win,  resulted in draw, and 13 matches were lost. The total points the 26  club scored is 685. Total points scored against the 26  team is 559. The team scored 14 bonus points. Total points scored by the 26 club this season this 66. The club named ASM Clermont, played 26 matches. Out of it,  14 matches resulted in win,  resulted in draw, and 12 matches were lost. The total points the 26  club scored is 577. Total points scored against the 26  team is 569. The team scored 7 bonus points. Total points scored by the 26 club this season this 63. The club named CA Brive, played 26 matches. Out of it,  10 matches resulted in win, 1 resulted in draw, and 15 matches were lost. The total points the 26  club scored is 431. Total points scored against the 26  team is 553. The team scored 9 bonus points. Total points scored by the 26 club this season this 51. The club named RC Narbonne, played 26 matches. Out of it,  11 matches resulted in win,  resulted in draw, and 15 matches were lost. The total points the 26  club scored is 533. Total points scored against the 26  team is 775. The team scored 3 bonus points. Total points scored by the 26 club this season this 47. The club named Montpellier RC, played 26 matches. Out of it,  9 matches resulted in win, 0 resulted in draw, and 17 matches were lost. The total points the 26  club scored is 574. Total points scored against the 26  team is 659. The team scored 10 bonus points. Total points scored by the 26 club this season this 46. The club named Aviron Bayonnais, played 26 matches. Out of it,  8 matches resulted in win, 1 resulted in draw, and 17 matches were lost. The total points the 26  club scored is 514. Total points scored against the 26  team is 669. The team scored 9 bonus points. Total points scored by the 26 club this season this 43. The club named Section Paloise, played 26 matches. Out of it,  9 matches resulted in win, 0 resulted in draw, and 17 matches were lost. The total points the 26  club scored is 476. Total points scored against the 26  team is 790. The team scored 4 bonus points. Total points scored by the 26 club this season this 40. The club named RC Toulonnais, played 26 matches. Out of it,  3 matches resulted in win, 0 resulted in draw, and 23 matches were lost. The total points the 26  club scored is 332. Total points scored against the 26  team is 837. The team scored 7 bonus points. Total points scored by the 26 club this season this 19."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-2f0050d2ee0b4cfa997bc4cdca059ebd", "references": ["Passage:  The home city of the team 'Barak Netanya' is Netanya. The capacity at stadium Yeshurun in Netanya is 1,000. The last season played was 9th. The home city of the team 'Bnei HaSharon/Herzliya' is Herzliya. The capacity at stadium HaYovel Herzliya in Herzliya is 1,750. The last season played was 10th. The home city of the team 'Hapoel Gilboa Galil' is Gilboa Regional Council. The capacity at stadium Gan Ner Sports Hall in Gilboa Regional Council is 2,400. The last season played was 2nd. The home city of the team 'Hapoel Holon' is Holon. The capacity at stadium Holon City Arena in Holon is 2,850. The last season played was 5th. The home city of the team 'Hapoel Jerusalem' is Jerusalem. The capacity at stadium Malha Arena in Jerusalem is 3,000. The last season played was 4th. The home city of the team 'Ironi Ashkelon' is Ashkelon. The capacity at stadium Ashkelon Sports Arena in Ashkelon is 3,000. The last season played was 6th. The home city of the team 'Maccabi Ashdod' is Ashdod. The capacity at stadium HaKiriya Arena in Ashdod is 1,260. The last season played was 7th. The home city of the team 'Hapoel Eilat B.C.' is Eilat. The capacity at stadium Begin Arena in Eilat is 1,100. The last season played was 8th (as Habik'a B.C.). The home city of the team 'Maccabi Haifa' is Haifa. The capacity at stadium Romema Arena in Haifa is 5,000. The last season played was 11th. The home city of the team 'Maccabi Rishon LeZion' is Rishon LeZion. The capacity at stadium Beit Maccabi Rishon in Rishon LeZion is 2,500. The last season played was 3rd. The home city of the team 'Maccabi Tel Aviv' is Tel Aviv. The capacity at stadium Nokia Arena in Tel Aviv is 11,700. The last season played was 1st. The home city of the team 'Hapoel Tel Aviv' is Tel Aviv. The capacity at stadium Beit Maccabi Rishon in Tel Aviv is 2,500. The last season played was 1st (Liga Leumit)."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-ee34add0c3dd4cd8b43013cbda176e99", "references": ["Passage:  The title track 'Dig a Hole' from the album 'Non-album single', was released in the year 1990. Dig a Hole's chart positions in US Air, US Main, and US mod were --, --, and -- respectively. The title track 'Mister Love' from the album 'Velvet', was released in the year 1993. Mister Love's chart positions in US Air, US Main, and US mod were --, --, and -- respectively. The title track 'Mister Love' from the album 'Rubberneck', was released in the year 1994. Mister Love's chart positions in US Air, US Main, and US mod were --, --, and -- respectively. The title track 'Possum Kingdom' from the album 'Rubberneck', was released in the year 1995. Possum Kingdom's chart positions in US Air, US Main, and US mod were 40, 9, and 4 respectively. The title track 'Away' from the album 'Rubberneck', was released in the year 1995. Away's chart positions in US Air, US Main, and US mod were --, 23, and 28 respectively. The title track 'Tyler' from the album 'Rubberneck', was released in the year 1996. Tyler's chart positions in US Air, US Main, and US mod were --, --, and -- respectively. The title track 'Backslider' from the album 'Rubberneck', was released in the year 1996. Backslider's chart positions in US Air, US Main, and US mod were --, --, and -- respectively. The title track 'Push the Hand' from the album 'Hell Below/Stars Above', was released in the year 2001. Push the Hand's chart positions in US Air, US Main, and US mod were --, 34, and -- respectively. The title track 'No Deliverance' from the album 'No Deliverance', was released in the year 2008. No Deliverance's chart positions in US Air, US Main, and US mod were --, 38, and -- respectively. The title track 'Song I Hate' from the album 'No Deliverance', was released in the year 2009. Song I Hate's chart positions in US Air, US Main, and US mod were --, --, and -- respectively. The title track 'Summer of the Strange' from the album 'Play.Rock.Music', was released in the year 2012. Summer of the Strange's chart positions in US Air, US Main, and US mod were --, --, and -- respectively."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-58df3a6a6727493589cd11b9ff087102", "references": ["Passage:  The driver with number 5, Fernando Alonso, from Renault constructor, finished at position 1. Fernando Alonso completed 56 laps in 1:31:33.736 time and earned 10 points. The driver with number 16, Jarno Trulli, from Toyota constructor, finished at position 2. Jarno Trulli completed 56 laps in +24.327 time and earned 8 points. The driver with number 8, Nick Heidfeld, from Williams-BMW constructor, finished at position 3. Nick Heidfeld completed 56 laps in +32.188 time and earned 6 points. The driver with number 10, Juan Pablo Montoya, from McLaren-Mercedes constructor, finished at position 4. Juan Pablo Montoya completed 56 laps in +41.631 time and earned 5 points. The driver with number 17, Ralf Schumacher, from Toyota constructor, finished at position 5. Ralf Schumacher completed 56 laps in +51.854 time and earned 4 points. The driver with number 14, David Coulthard, from Red Bull-Cosworth constructor, finished at position 6. David Coulthard completed 56 laps in +1:12.543 time and earned 3 points. The driver with number 1, Michael Schumacher, from Ferrari constructor, finished at position 7. Michael Schumacher completed 56 laps in +1:19.988 time and earned 2 points. The driver with number 15, Christian Klien, from Red Bull-Cosworth constructor, finished at position 8. Christian Klien completed 56 laps in +1:20.835 time and earned 1 points. The driver with number 9, Kimi Raikkonen, from McLaren-Mercedes constructor, finished at position 9. Kimi Raikkonen completed 56 laps in +1:21.580 time and earned  points. The driver with number 12, Felipe Massa, from Sauber-Petronas constructor, finished at position 10. Felipe Massa completed 55 laps in +1 Lap time and earned  points. The driver with number 19, Narain Karthikeyan, from Jordan-Toyota constructor, finished at position 11. Narain Karthikeyan completed 54 laps in +2 Laps time and earned  points. The driver with number 18, Tiago Monteiro, from Jordan-Toyota constructor, finished at position 12. Tiago Monteiro completed 53 laps in +3 Laps time and earned  points. The driver with number 21, Christijan Albers, from Minardi-Cosworth constructor, finished at position 13. Christijan Albers completed 52 laps in +4 Laps time and earned  points. The driver with number 2, Rubens Barrichello, from Ferrari constructor, finished at position Ret. Rubens Barrichello completed 49 laps in Handling time and earned  points. The driver with number 6, Giancarlo Fisichella, from Renault constructor, finished at position Ret. Giancarlo Fisichella completed 36 laps in Collision time and earned  points. The driver with number 7, Mark Webber, from Williams-BMW constructor, finished at position Ret. Mark Webber completed 36 laps in Collision time and earned  points. The driver with number 11, Jacques Villeneuve, from Sauber-Petronas constructor, finished at position Ret. Jacques Villeneuve completed 26 laps in Spun Off time and earned  points. The driver with number 3, Jenson Button, from BAR-Honda constructor, finished at position Ret. Jenson Button completed 2 laps in Engine time and earned  points. The driver with number 4, Anthony Davidson, from BAR-Honda constructor, finished at position Ret. Anthony Davidson completed 2 laps in Engine time and earned  points. The driver with number 20, Patrick Friesacher, from Minardi-Cosworth constructor, finished at position Ret. Patrick Friesacher completed 2 laps in Spun Off time and earned  points."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-96029627b61343d786beb0d64e672ce9", "references": ["Passage:  The person named Trisha, finished at position Winner and won $26,000 prize money. Trisha is a 28 years old Female from Santa Monica, CA.  Trisha  works as a Artist. The person named London, finished at position Lost and won $0 prize money. London is a 46 years old Male from Mt. Holly, NJ.  London  works as a US Coast Guard (Retired). The person named Adria, finished at position 7th Captured (by Ricky Ortiz) and won $0 prize money. Adria is a 25 years old Female from Seattle, WA.  Adria  works as a Bartender. The person named Lucas, finished at position 6th Captured (by Kim) and won $0 prize money. Lucas is a 32 years old Male from Carlsbad, CA.  Lucas  works as a Student. The person named Andrew, finished at position 5th Captured (by Kim) and won $0 prize money. Andrew is a 21 years old Male from Redondo Beach, CA.  Andrew  works as a Student / Lifeguard. The person named Tracy, finished at position 4th Captured (by Grant) and won $0 prize money. Tracy is a 30 years old Female from Mililani, HI.  Tracy  works as a Student. The person named Lynda, finished at position 3rd Captured (by Icey) and won $0 prize money. Lynda is a 59 years old Female from Los Angeles, CA.  Lynda  works as a General Contractor. The person named Darin, finished at position 2nd Captured (by Wong) and won $0 prize money. Darin is a 46 years old Male from Fontana, CA.  Darin  works as a Sports Official. The person named Evan, finished at position Opted Out and won $2,000 prize money. Evan is a 29 years old Male from Long Island, NY.  Evan  works as a Teacher. The person named Ameenah, finished at position 1st Captured (by Grant) and won $0 prize money. Ameenah is a 34 years old Female from Atlanta, GA.  Ameenah  works as a Drummer."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-eba518b557344ea89aa8170518c3c312", "references": ["Passage:  The U-209 was a VIIC type submarine. The U-209 submarine,  commanded by Captain Heinrich Brodda, was sunk on the date 4 May 1943 by the Catalina Flying-boat of No. 5 Squadron RCAF submarine. It results in 46 casualties. The U-638 was a VIIC type submarine. The U-638 submarine,  commanded by Captain Oskar Staudinger, was sunk on the date 5 May 1943 by the HMS Sunflower submarine. It results in 44 casualties. The U-531 was a IXC/40 type submarine. The U-531 submarine,  commanded by Captain Herbert Neckel, was sunk on the date 5 May 1943 by the HMS Vidette submarine. It results in 54 casualties. The U-192 was a IXC/40 type submarine. The U-192 submarine,  commanded by Captain Werner Happe, was sunk on the date 6 May 1943 by the HMS Loosestrife submarine. It results in 55 casualties. The U-125 was a IXC type submarine. The U-125 submarine,  commanded by Captain Ulrich Folkers, was sunk on the date 6 May 1943 by the HMS Oribi, HMS Snowflake submarine. It results in 54 casualties. The U-630 was a VIIC type submarine. The U-630 submarine,  commanded by Captain Werner Winkler, was sunk on the date 6 May 1943 by the HMS Vidette submarine. It results in 47 casualties. The U-438 was a VIIC type submarine. The U-438 submarine,  commanded by Captain Heinrich Hensohn, was sunk on the date 6 May 1943 by the HMS Pelican submarine. It results in 48 casualties."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-a285dbb3e4694005b6004a77444ba0f9", "references": ["Passage:  On May 29, 2011, there was Hair based wager on the professional wrestlers, row[1] and Super Crazy at the location Mexico City, Distrito Federal.  row[1] won the match , and row[2] lost the match. Additional details: Six man tag team steel cage Masks vs. Hairs match, where Los Perros del Mal (Crazy, El Hijo del Perro Aguayo and Damian 666) faced Los Psycho Circus (Psycho Clown, Monster Clown and Murder Clown).. On July 10, 2011, there was Hair based wager on the professional wrestlers, row[1] and Coco Rojo at the location Tlalnepantla de Baz, State of Mexico.  row[1] won the match , and row[2] lost the match. Additional details: Steel cage match, where Los Perros del Mal (Halloween, Damian 666 and Ek Balam) faced Los Payasos Tricolor (Coco Rojo, Coco Azul and Coco Amarillo) and Los Psycho Circus (Monster Clown, Murder Clown and Psycho Clown).. On July 31, 2011, there was Hair based wager on the professional wrestlers, row[1] and X-Fly at the location Guadalajara, Jalisco.  row[1] won the match , and row[2] lost the match. Additional details: Six man tag team steel cage Masks vs. Hairs match, where Los Perros del Mal (X-Fly, Damian 666 and Halloween) faced Los Psycho Circus.. On August 28, 2011, there was Title based wager on the professional wrestlers, row[1] and Monster Clown at the location Naucalpan, Mexico.  row[1] won the match , and row[2] lost the match. Additional details: Four tag team steel cage match, where Los Psycho Circus put their IWRG Intercontinental Trios Championship on the line against the hairs/masks of Los Perros del Mal (Damian 666, Bestia 666 and X-Fly), Los Temerarios (Black Terry, Durok and Machin) and Los Villanos (Kortiz, Ray Mendoza, Jr. and Villano IV).. On October 9, 2011, there was Hair based wager on the professional wrestlers, row[1] and Halloween, Damian 666 and Nicho el Millonario at the location Monterrey, Nuevo Leon.  row[1] won the match , and row[2] lost the match. Additional details: Steel cage Masks vs. Hairs match.. On August 5, 2012, there was Hair based wager on the professional wrestlers, row[1] and Joe Lider at the location Mexico City.  row[1] won the match , and row[2] lost the match. Additional details: . On October 7, 2012, there was Hair based wager on the professional wrestlers, row[1] and Halloween at the location San Luis Potosi, San Luis Potosi.  row[1] won the match , and row[2] lost the match. Additional details: Domo de la Muerte, where Los Perros del Mal (Halloween, El Hijo del Perro Aguayo, Psicosis and Teddy Hart) faced Dark Cuervo, Cibernetico, Dark Ozz and Jack Evans.."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-361bcee5d8494ad29ef8e7ac239466bf", "references": ["Passage:  The artist Brian & Roger gave a live performance at RTE Studios in the city of Dublin on 15 January.  The artist Brian & Roger performed (1) We Will Rock You. Additional details: . The artist  gave a live performance at  in the city of The Late Late Show on (2) We Are the Champions.  The artist  performed . Additional details: . The artist Brian gave a live performance at The O2 in the city of Dublin on 31 January.  The artist Brian performed (1) Bohemian Rhapsody. Additional details: Matinee and Evening performances. Roger attended but did not perform.. The artist Brian & Roger gave a live performance at Dominion Theatre in the city of London on 10 May.  The artist Brian & Roger performed (1) Bohemian Rhapsody. Additional details: 8th anniversary.. The artist Brian gave a live performance at Beatrix Theatre in the city of Utrecht on 3 September.  The artist Brian performed (1) Bohemian Rhapsody. Additional details: . The artist Brian gave a live performance at Dominion Theatre in the city of London on 4 September.  The artist Brian performed (1) Bohemian Rhapsody. Additional details: Matinee and Evening performances. The artist Brian gave a live performance at Cirkus Arena Restaurang in the city of Stockholm on 12 September.  The artist Brian performed (1) Bohemian Rhapsody. Additional details: . The artist Brian gave a live performance at Theater des Westens in the city of Berlin on 21 October.  The artist Brian performed (1) Bohemian Rhapsody. Additional details: ."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-a437472c960b4712a69474994c5b815a", "references": ["Passage:  Anabelle Rodriguez , currently serving as Associate Justice, was appointed  by Sila Maria Calderon in the year 2004. The current age of Anabelle Rodriguez  is 64, and has 6 years until mandatory retirement. Edgardo Rivera Garcia , currently serving as Associate Justice, was appointed  by Luis Fortuno in the year 2010. The current age of Edgardo Rivera Garcia  is 59, and has 11 years until mandatory retirement. Erick Kolthoff Caraballo , currently serving as Associate Justice, was appointed  by Luis Fortuno in the year 2009. The current age of Erick Kolthoff Caraballo  is 53, and has 17 years until mandatory retirement. Federico Hernandez Denton , currently serving as Chief Justice, was appointed  by Sila Maria Calderon in the year 2004. The current age of Federico Hernandez Denton  is 70, and has 0 year until mandatory retirement. Liana Fiol Matta , currently serving as Associate Justice, was appointed  by Sila Maria Calderon in the year 2004. The current age of Liana Fiol Matta  is 68, and has 2 years until mandatory retirement. Luis Estrella Martinez , currently serving as Associate Justice, was appointed  by Luis Fortuno in the year 2011. The current age of Luis Estrella Martinez  is 43, and has 27 years until mandatory retirement. Mildred Pabon Charneco , currently serving as Associate Justice, was appointed  by Luis Fortuno in the year 2009. The current age of Mildred Pabon Charneco  is 57, and has 13 years until mandatory retirement. Rafael Martinez Torres , currently serving as Associate Justice, was appointed  by Luis Fortuno in the year 2009. The current age of Rafael Martinez Torres  is 55, and has 15 years until mandatory retirement. Roberto Feliberti Cintron , currently serving as Associate Justice, was appointed  by Luis Fortuno in the year 2011. The current age of Roberto Feliberti Cintron  is 51, and has 19 years until mandatory retirement."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-7e1115681dc34529b9448204ec488635", "references": ["Passage:  The species Bubas bison originated in the country of France, Spain. The species was first released in April 1983, and last released in 1996 (not by CSIRO) in the WAareas, making the total count of release to 1,613. The species was established in NSW, SA, WA areas. The pasture type for the spices is Winter rainfall. The species Copris elphenor Klug originated in the country of South Africa. The species was first released in January 1977, and last released in May 1983 in the QLDareas, making the total count of release to 2,287. The species was established in QLD areas. The pasture type for the spices is Summer rainfall. The species Copris hispanus Linnaeus originated in the country of Spain. The species was first released in October 1983, and last released in June 1994 in the WAareas, making the total count of release to 294. The species was established in WA areas. The pasture type for the spices is Winter rainfall. The species Euoniticellus africanus Harold originated in the country of South Africa. The species was first released in October 1971, and last released in February 1984 in the NSW, QLD, SA, Tas, Vic, WAareas, making the total count of release to 49,009. The species was established in NSW, QLD areas. The pasture type for the spices is Summer rainfall. The species Euoniticellus fulvus Goeze originated in the country of France, Turkey. The species was first released in March 1978, and last released in February 1983 in the NSW, SA, Tas, Vic, WAareas, making the total count of release to 76,944. The species was established in NSW, SA, Tas, Vic, WA areas. The pasture type for the spices is Winter rainfall. The species Euoniticellus intermedius Reiche originated in the country of South Africa. The species was first released in November 1971, and last released in February 1984 in the ACT, NSW, NT, QLD, SA, Vic, WAareas, making the total count of release to 248,637. The species was established in NSW, NT, QLD, SA, Vic, WA areas. The pasture type for the spices is Summer rainfall. The species Euoniticellus pallipes Fabricius originated in the country of Iran, Turkey. The species was first released in March 1977, and last released in September 1982 in the NSW, SA, WAareas, making the total count of release to 46,642. The species was established in NSW, SA, Vic, WA areas. The pasture type for the spices is Winter rainfall. The species Geotrupes spiniger Marsham originated in the country of France. The species was first released in April 1979, and last released in December 1983 in the ACT, NSW, Tas, Vicareas, making the total count of release to 12,082. The species was established in ACT, NSW, SA, Tas, Vic areas. The pasture type for the spices is Winter rainfall. The species Liatongus militaris Castelanu originated in the country of South Africa (via Hawaii). The species was first released in January 1968, and last released in November 1979 in the NSW, NT, QLD, WAareas, making the total count of release to 70,450. The species was established in NSW, NT, QLD areas. The pasture type for the spices is Summer rainfall. The species Onitis alexis Fabricius originated in the country of South Africa. The species was first released in August 1972, and last released in February 1984 in the NSW, NT, QLD, WAareas, making the total count of release to 186,441. The species was established in NSW, NT, QLD, SA, Vic, WA areas. The pasture type for the spices is Rainfall, summer rainfall, winter rainfall. The species Onitis aygalus Fabricius originated in the country of South Africa. The species was first released in January 1977, and last released in January 1982 in the NSW, SA, WAareas, making the total count of release to 18,682. The species was established in NSW, SA, Vic, WA areas. The pasture type for the spices is Winter rainfall. The species Onitis caffer Boheman originated in the country of South Africa. The species was first released in October 1979, and last released in April 1984 in the WAareas, making the total count of release to 8,738. The species was established in QLD, NSW, SA areas. The pasture type for the spices is Summer rainfall, winter rainfall. The species Onitis pecuarius Lansberge originated in the country of South Africa. The species was first released in November 1976, and last released in May 1979 in the NSW, QLDareas, making the total count of release to 11,395. The species was established in NSW, QLD areas. The pasture type for the spices is Summer rainfall. The species Onitis vanderkelleni Lansberge originated in the country of Kenya, Rwanda, Zaire. The species was first released in October 1974, and last released in February 1982 in the NSW, QLDareas, making the total count of release to 10,852. The species was established in QLD areas. The pasture type for the spices is Summer rainfall. The species Onitis viridulus Bohemann originated in the country of South Africa. The species was first released in September 1976, and last released in July 1980 in the NSW, NT, QLDareas, making the total count of release to 8,008. The species was established in NSW, NT, QLD, WA areas. The pasture type for the spices is Summer rainfall. The species Onthophagus binodis Thunberg originated in the country of South Africa. The species was first released in October 1971, and last released in February 1982 in the NSW, QLD, SA, Tas, Vic, WA, Norfolk Islandsareas, making the total count of release to 173,018. The species was established in NSW, QLD, SA, Tas, Vic, WA, Norfolk Islands areas. The pasture type for the spices is Winter rainfall. The species Onthophagus gazella Fabricius originated in the country of South Africa. The species was first released in February 1968, and last released in February 1984 in the ACT, NSW, NT, QLD, SA, Tas, Vic, WA, Norfolk Islandsareas, making the total count of release to 420,415. The species was established in ACT, NSW, NT, QLD, SA, WA, Norfolk Islands areas. The pasture type for the spices is Summer rainfall. The species Onthophagus nigiventris d'Orbigny originated in the country of East Africa. The species was first released in May 1975, and last released in March 1983 in the NSWareas, making the total count of release to 29,960. The species was established in NSW areas. The pasture type for the spices is Summer rainfall. The species Onthophagus obliquus originated in the country of Nigeria, Senegal, Zaire. The species was first released in January 1976, and last released in November 1977 in the QLD, NTareas, making the total count of release to 9,300. The species was established in QLD areas. The pasture type for the spices is Summer rainfall. The species Onthophagus sagittarius originated in the country of Sri Lanka (via Hawaii). The species was first released in January 1968, and last released in March 1977 in the NSW, NT, QLD, WAareas, making the total count of release to 9,075. The species was established in NSW, NT areas. The pasture type for the spices is Summer rainfall. The species Onthophagus taurus Schreber originated in the country of Spain, Greece, Italy, Turkey. The species was first released in February 1975, and last released in January 1984 in the NSW, SA, Tas, Vic, WAareas, making the total count of release to 164,499. The species was established in NSW, SA, Tas, Vic, WA areas. The pasture type for the spices is Winter rainfall. The species Sisyphus rubrus Paschalidis originated in the country of South Africa. The species was first released in March 1973, and last released in February 1980 in the NSW, QLD, WAareas, making the total count of release to 85,933. The species was established in NSW, QLD areas. The pasture type for the spices is Summer rainfall. The species Sisyphus spinipes originated in the country of South Africa. The species was first released in March 1972, and last released in December 1978 in the NSW, NT, QLD, WAareas, making the total count of release to 36,125. The species was established in NSW, QLD areas. The pasture type for the spices is Summer rainfall. The species  originated in the country of Total:. The species was first released in , and last released in  in the areas, making the total count of release to 1,680,399. The species was established in  areas. The pasture type for the spices is ."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-d655a07ec6044870b340c63bf2b51f84", "references": ["Passage:  The song Caravaggio by the artist Claudia Faniello finished at 2nd position in the national finals, and at 13th position in the international finals by earning total 53 points.  The song Caravaggio is in the language English from the country Malta. The English translation of the song is -. The song Parparim by the artist Bo'az Ma'uda & Oshrat Papin finished at 3rd position in the national finals, and at 8th position in the international finals by earning total 90 points.  The song Parparim is in the language Hebrew from the country Israel. The English translation of the song is Butterflies. The song Flyer by the artist Sergey Lazarev finished at 4th position in the national finals, and at 7th position in the international finals by earning total 119 points.  The song Flyer is in the language English from the country Russia. The English translation of the song is -. The song One on One by the artist Rolf Junior finished at 4th position in the national finals, and at 21th position in the international finals by earning total 0 points.  The song One on One is in the language English from the country Estonia. The English translation of the song is -. The song I Feel The Same Way by the artist Sandrine finished at 2nd position in the national finals, and at 10th position in the international finals by earning total 64 points.  The song I Feel The Same Way is in the language English from the country Belgium. The English translation of the song is -. The song Dojdi do mene by the artist Risto Samardziev finished at 3rd position in the national finals, and at 19th position in the international finals by earning total 7 points.  The song Dojdi do mene is in the language Macedonian from the country Macedonia. The English translation of the song is Come to me. The song Viva la Musica by the artist Man Meadow finished at 3rd position in the national finals, and at 3th position in the international finals by earning total 155 points.  The song Viva la Musica is in the language English from the country Poland. The English translation of the song is Hail to the music. The song Milloin by the artist Mikael Konttinen finished at Unplaced position in the national finals, and at 14th position in the international finals by earning total 46 points.  The song Milloin is in the language Finnish from the country Finland. The English translation of the song is When. The song Hinterm Ozean by the artist Carolin Fortenbacher finished at 2nd position in the national finals, and at 6th position in the international finals by earning total 129 points.  The song Hinterm Ozean is in the language German from the country Germany. The English translation of the song is Beyond the ocean. The song Until We're Satisfied by the artist Kendra Lou finished at 4th position in the national finals, and at 18th position in the international finals by earning total 21 points.  The song Until We're Satisfied is in the language English from the country Denmark. The English translation of the song is -. The song Always and Forever by the artist Kostas Martakis finished at 2nd position in the national finals, and at 4th position in the international finals by earning total 140 points.  The song Always and Forever is in the language English from the country Greece. The English translation of the song is -. The song Zavet by the artist Beauty Queens finished at 3rd position in the national finals, and at 9th position in the international finals by earning total 68 points.  The song Zavet is in the language Serbian from the country Serbia. The English translation of the song is A pledge. The song Andjeo by the artist Emilija Kokic finished at 6th position in the national finals, and at 15th position in the international finals by earning total 31 points.  The song Andjeo is in the language Croatian from the country Croatia. The English translation of the song is Angel. The song Troy on Fire by the artist Aiste Pilvelyte finished at 2nd position in the national finals, and at 17th position in the international finals by earning total 26 points.  The song Troy on Fire is in the language English from the country Lithuania. The English translation of the song is -. The song Not Crazy After All by the artist Leona Daly finished at Unplaced position in the national finals, and at 12th position in the international finals by earning total 56 points.  The song Not Crazy After All is in the language English from the country Ireland. The English translation of the song is -. The song It's You by the artist The Revelations finished at 3rd position in the national finals, and at 16th position in the international finals by earning total 29 points.  The song It's You is in the language English from the country United Kingdom. The English translation of the song is -. The song Porto de encontro by the artist Lisboa Nao Sejas Francesa finished at 9th position in the national finals, and at 20th position in the international finals by earning total 6 points.  The song Porto de encontro is in the language Portuguese from the country Portugal. The English translation of the song is Meeting harbour. The song Empty Room by the artist Sanna Nielsen finished at 2nd position in the national finals, and at 1th position in the international finals by earning total 269 points.  The song Empty Room is in the language English from the country Sweden. The English translation of the song is -. The song Am I Supposed To Love Again by the artist Veronica Akselsen finished at 4th position in the national finals, and at 5th position in the international finals by earning total 139 points.  The song Am I Supposed To Love Again is in the language English from the country Norway. The English translation of the song is -. The song Samara by the artist Brigita Suler finished at 3rd position in the national finals, and at 11th position in the international finals by earning total 57 points.  The song Samara is in the language Slovene from the country Slovenia. The English translation of the song is -. The song Todo esta en tu mente by the artist Coral finished at 2nd position in the national finals, and at 2th position in the international finals by earning total 178 points.  The song Todo esta en tu mente is in the language Spanish from the country Spain. The English translation of the song is Everything is in your mind."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-6fcc909cdd5445f5a4c7d01dc3afec08", "references": ["Passage:  China finished at th position in the Host Country Qualification tournament. The sailor was . Brazil finished at 1th position in the 2007 Worlds Qualification tournament. The sailor was Robert Scheidt, Bruno Prada. France finished at 2th position in the 2007 Worlds Qualification tournament. The sailor was Xavier Rohart, Pascal Rambeau. Great Britain finished at 3th position in the 2007 Worlds Qualification tournament. The sailor was Iain Percy, Andrew Simpson. New Zealand finished at 4th position in the 2007 Worlds Qualification tournament. The sailor was Hamish Pepper, Carl Williams. Italy finished at 5th position in the 2007 Worlds Qualification tournament. The sailor was Diego Negri, Luigi Viale. Poland finished at 6th position in the 2007 Worlds Qualification tournament. The sailor was Mateusz Kusznierewicz, Dominik Zycki. Germany finished at 7th position in the 2007 Worlds Qualification tournament. The sailor was Marc Pickel, Ingo Borkowski. Sweden finished at 8th position in the 2007 Worlds Qualification tournament. The sailor was Fredrik Loof, Anders Ekstrom. Australia finished at 9th position in the 2007 Worlds Qualification tournament. The sailor was Iain Murray, Andrew Palfrey. Portugal finished at 10th position in the 2007 Worlds Qualification tournament. The sailor was Afonso Domingos, Bernardo Santos. United States finished at 11th position in the 2007 Worlds Qualification tournament. The sailor was Mark Reynolds, Hal Haenel. Switzerland finished at 4th position in the 2008 Worlds Qualification tournament. The sailor was Flavio Marazzi, Enrico De Maria. Croatia finished at 12th position in the 2008 Worlds Qualification tournament. The sailor was Marin Lovrovic, Sinsa Mikulicic. Ireland finished at 14th position in the 2008 Worlds Qualification tournament. The sailor was Max Treacy, Anthony Shanks. Austria finished at 16th position in the 2008 Worlds Qualification tournament. The sailor was Hans Spitzauer, Christian Nehammer."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-98cfaa3ea8404bd891961b8d23524338", "references": ["Passage:  The player named Shawn Gifford was picked at 25 position. The player plays at position OT in the team Montreal Alouettes from the college Charleston Southern. The player named Kevin Eiben was picked at 26 position. The player plays at position S in the team Toronto Argonauts from the college Bucknell. The player named Nick Tsatsaronis was picked at 27 position. The player plays at position RB in the team Winnipeg Blue Bombers from the college Memphis. The player named Ryan Donnelly was picked at 28 position. The player plays at position OL in the team Hamilton Tiger-Cats from the college McMaster. The player named Peter Moore was picked at 29 position. The player plays at position DL in the team Montreal Alouettes from the college Syracuse. The player named Andrew Carter was picked at 30 position. The player plays at position OL in the team Calgary Stampeders from the college Bishop's. The player named Steven Maheu was picked at 31 position. The player plays at position WR/QB in the team Montreal Alouettes from the college Simon Fraser. The player named Kelly Bates was picked at 32 position. The player plays at position OL in the team BC Lions from the college Saskatchewan."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-19ffe37882dd4d639342a91df46f1361", "references": ["Passage:  The player named Nicholas Addlery with jersey number 11, plays at position FW. There were total 0 playoffs,  0 CFU Club Championships, 1  CONCACAF Champions Leagues. Total 10 matches. The player named Jonathan Fana with jersey number 9, plays at position FW. There were total 0 playoffs,  4 CFU Club Championships, 0  CONCACAF Champions Leagues. Total 8 matches. The player named David Foley with jersey number 7, plays at position FW. There were total 1 playoffs,  0 CFU Club Championships, 0  CONCACAF Champions Leagues. Total 7 matches. The player named Hector Ramos with jersey number 24, plays at position FW. There were total 0 playoffs,  2 CFU Club Championships, 2  CONCACAF Champions Leagues. Total 7 matches. The player named Jay Needham with jersey number 33, plays at position DF. There were total 0 playoffs,  0 CFU Club Championships, 0  CONCACAF Champions Leagues. Total 3 matches. The player named Gregory Richardson with jersey number 22, plays at position FW. There were total 0 playoffs,  1 CFU Club Championships, 1  CONCACAF Champions Leagues. Total 3 matches. The player named Josh Hansen with jersey number 20, plays at position MF. There were total 0 playoffs,  2 CFU Club Championships, 0  CONCACAF Champions Leagues. Total 2 matches. The player named Own Goal with jersey number , plays at position . There were total 0 playoffs,  2 CFU Club Championships, 0  CONCACAF Champions Leagues. Total 2 matches. The player named Richard Martinez with jersey number 3, plays at position DF. There were total 0 playoffs,  0 CFU Club Championships, 0  CONCACAF Champions Leagues. Total 1 matches. The player named Anthony Vazquez with jersey number 16, plays at position DF. There were total 0 playoffs,  0 CFU Club Championships, 0  CONCACAF Champions Leagues. Total 1 matches. The player named Noah Delgado with jersey number 5, plays at position DF. There were total 0 playoffs,  0 CFU Club Championships, 0  CONCACAF Champions Leagues. Total 1 matches. The player named Jarad Van Schaik with jersey number 14, plays at position MF. There were total 0 playoffs,  0 CFU Club Championships, 0  CONCACAF Champions Leagues. Total 1 matches. The player named Tyler Wilson with jersey number 17, plays at position MF. There were total 0 playoffs,  0 CFU Club Championships, 0  CONCACAF Champions Leagues. Total 1 matches. The player named Justin Fojo with jersey number 24, plays at position MF. There were total 0 playoffs,  0 CFU Club Championships, 0  CONCACAF Champions Leagues. Total 1 matches. The player named TOTALS with jersey number , plays at position . There were total 1 playoffs,  11 CFU Club Championships, 4  CONCACAF Champions Leagues. Total 48 matches."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-db1f9d517b0d41f3a1369262856bb1f5", "references": ["Passage:  The locomotive BL26 named Bob Hawke and with serial no. 83-1010, entered the service in the month of March 1983. It has Standard gauge, and Pacific National blue & yellow livery. The locomotive BL27 named  and with serial no. 83-1011, entered the service in the month of August 1983. It has Standard gauge, and Pacific National blue & yellow livery. The locomotive BL28 named  and with serial no. 83-1012, entered the service in the month of September 1983. It has Standard gauge, and Pacific National blue & yellow livery. The locomotive BL29 named  and with serial no. 83-1013, entered the service in the month of October 1983. It has Broad gauge, and Pacific National blue & yellow livery. The locomotive BL30 named  and with serial no. 83-1014, entered the service in the month of December 1983. It has Standard gauge, and Pacific National blue & yellow livery. The locomotive BL31 named  and with serial no. 83-1015, entered the service in the month of November 1983. It has Standard gauge, and Pacific National blue & yellow livery. The locomotive BL32 named  and with serial no. 83-1016, entered the service in the month of February 1984. It has Broad gauge, and National Rail orange & grey livery. The locomotive BL33 named  and with serial no. 83-1017, entered the service in the month of April 1984. It has Standard gauge, and Pacific National blue & yellow livery. The locomotive BL34 named  and with serial no. 83-1018, entered the service in the month of June 1984. It has Broad gauge, and Pacific National blue & yellow livery. The locomotive BL35 named  and with serial no. 83-1019, entered the service in the month of July 1984. It has Standard gauge, and Pacific National blue & yellow livery."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-e2ebfe2491564a2c9ae99f85dbf68ef9", "references": ["Passage:  The team Paulistano finished at position 1. The team Paulistano played 18 matches, won 14 matches, and lost 2 matches. Total points earned is 30 The team Palestra Italia-SP finished at position 2. The team Palestra Italia-SP played 18 matches, won 14 matches, and lost 1 matches. Total points earned is 29 The team Corinthians finished at position 3. The team Corinthians played 18 matches, won 12 matches, and lost 2 matches. Total points earned is 26 The team Ypiranga-SP finished at position 4. The team Ypiranga-SP played 18 matches, won 11 matches, and lost 3 matches. Total points earned is 25 The team AA Sao Bento finished at position 5. The team AA Sao Bento played 18 matches, won 7 matches, and lost 2 matches. Total points earned is 16 The team Santos finished at position 6. The team Santos played 18 matches, won 6 matches, and lost 1 matches. Total points earned is 13 The team SC Internacional de Sao Paulo finished at position 7. The team SC Internacional de Sao Paulo played 15 matches, won 3 matches, and lost 5 matches. Total points earned is 11 The team Minas Gerais finished at position 8. The team Minas Gerais played 15 matches, won 4 matches, and lost 2 matches. Total points earned is 10 The team AA das Palmeiras finished at position 9. The team AA das Palmeiras played 15 matches, won 3 matches, and lost 0 matches. Total points earned is 6 The team Mackenzie finished at position 10. The team Mackenzie played 15 matches, won 1 matches, and lost 0 matches. Total points earned is 2"], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task760-682053d9b4b342f5aa0bf6855b5237ed", "references": ["Passage:  The player Barry Bonds from the team San Francisco Giants has scored total 762 home runs. Barry Bonds has played in the seasons: 1986-2007. Barry Bonds  had completed 500 home runs on date April 17, 2001. The player Hank Aaron from the team Atlanta Braves has scored total 755 home runs. Hank Aaron has played in the seasons: 1954-1976. Hank Aaron  had completed 500 home runs on date July 14, 1968. The player Babe Ruth from the team New York Yankees has scored total 714 home runs. Babe Ruth has played in the seasons: 1914-1935. Babe Ruth  had completed 500 home runs on date August 11, 1929. The player Willie Mays from the team San Francisco Giants has scored total 660 home runs. Willie Mays has played in the seasons: 1951-1952, 1954-1973. Willie Mays  had completed 500 home runs on date September 13, 1965. The player Alex Rodriguez from the team New York Yankees has scored total 654 home runs. Alex Rodriguez has played in the seasons: 1994-. Alex Rodriguez  had completed 500 home runs on date August 4, 2007. The player Ken Griffey, Jr. from the team Cincinnati Reds has scored total 630 home runs. Ken Griffey, Jr. has played in the seasons: 1989-2010. Ken Griffey, Jr.  had completed 500 home runs on date June 20, 2004. The player Jim Thome from the team Chicago White Sox has scored total 612 home runs. Jim Thome has played in the seasons: 1991-2012. Jim Thome  had completed 500 home runs on date September 16, 2007. The player Sammy Sosa from the team Chicago Cubs has scored total 609 home runs. Sammy Sosa has played in the seasons: 1989-2005, 2007. Sammy Sosa  had completed 500 home runs on date April 4, 2003. The player Frank Robinson from the team Baltimore Orioles has scored total 586 home runs. Frank Robinson has played in the seasons: 1956-1976. Frank Robinson  had completed 500 home runs on date September 13, 1971. The player Mark McGwire from the team St. Louis Cardinals has scored total 583 home runs. Mark McGwire has played in the seasons: 1986-2001. Mark McGwire  had completed 500 home runs on date August 5, 1999. The player Harmon Killebrew from the team Minnesota Twins has scored total 573 home runs. Harmon Killebrew has played in the seasons: 1954-1975. Harmon Killebrew  had completed 500 home runs on date August 10, 1971. The player Rafael Palmeiro from the team Texas Rangers has scored total 569 home runs. Rafael Palmeiro has played in the seasons: 1986-2005. Rafael Palmeiro  had completed 500 home runs on date May 11, 2003. The player Reggie Jackson from the team California Angels has scored total 563 home runs. Reggie Jackson has played in the seasons: 1967-1987. Reggie Jackson  had completed 500 home runs on date September 17, 1984. The player Manny Ramirez from the team Boston Red Sox has scored total 555 home runs. Manny Ramirez has played in the seasons: 1993-2011. Manny Ramirez  had completed 500 home runs on date May 31, 2008. The player Mike Schmidt from the team Philadelphia Phillies has scored total 548 home runs. Mike Schmidt has played in the seasons: 1972-1989. Mike Schmidt  had completed 500 home runs on date April 18, 1987. The player Mickey Mantle from the team New York Yankees has scored total 536 home runs. Mickey Mantle has played in the seasons: 1951-1968. Mickey Mantle  had completed 500 home runs on date May 14, 1967. The player Jimmie Foxx from the team Boston Red Sox has scored total 534 home runs. Jimmie Foxx has played in the seasons: 1925-1942, 1944-1945. Jimmie Foxx  had completed 500 home runs on date September 24, 1940. The player Ted Williams from the team Boston Red Sox has scored total 521 home runs. Ted Williams has played in the seasons: 1939-1942, 1946-1960. Ted Williams  had completed 500 home runs on date June 17, 1960. The player Willie McCovey from the team San Francisco Giants has scored total 521 home runs. Willie McCovey has played in the seasons: 1959-1980. Willie McCovey  had completed 500 home runs on date June 30, 1978. The player Frank Thomas from the team Toronto Blue Jays has scored total 521 home runs. Frank Thomas has played in the seasons: 1990-2008. Frank Thomas  had completed 500 home runs on date June 28, 2007. The player Ernie Banks from the team Chicago Cubs has scored total 512 home runs. Ernie Banks has played in the seasons: 1953-1971. Ernie Banks  had completed 500 home runs on date May 12, 1970. The player Eddie Mathews from the team Houston Astros has scored total 512 home runs. Eddie Mathews has played in the seasons: 1952-1968. Eddie Mathews  had completed 500 home runs on date July 14, 1967. The player Mel Ott from the team New York Giants has scored total 511 home runs. Mel Ott has played in the seasons: 1926-1947. Mel Ott  had completed 500 home runs on date August 1, 1945. The player Gary Sheffield from the team New York Mets has scored total 509 home runs. Gary Sheffield has played in the seasons: 1988-2009. Gary Sheffield  had completed 500 home runs on date April 17, 2009. The player Eddie Murray from the team Baltimore Orioles has scored total 504 home runs. Eddie Murray has played in the seasons: 1977-1997. Eddie Murray  had completed 500 home runs on date September 6, 1996."], "task_id": "task760_msr_sqa_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task035-735e789a2282422589808af8754a2bc0", "references": ["PersonX learned new organizational skills from PersonY because _ 's day schedule was very efficient."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-78cf6177c9ce45528bbcd0b92644e56c", "references": ["PersonX gave valuable pre-natal advice to PersonY since _ had no experience with coping with pregnancy."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-695583fecac34c37bb89806944cf4f01", "references": ["PersonX was accepted by PersonY for her club, so _ had a new member at last."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-3f07bba2879d4dc69170337a29436b54", "references": ["PersonX had been abused badly by PersonY , so the pain _ gave was recurring every day."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-56be5ef47d7e4f079d54e713dde14e4b", "references": ["PersonX was no longer interested in working for PersonY , so _ accepted her notice to resign."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-049d3e868f1146349a74400f0bf9612a", "references": ["PersonX caught more fish than PersonY , so _ was disappointed with their fishing trip."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-0dd8ee97f55749dd8edecbd422255f41", "references": ["PersonX gave lessons to PersonY , as _ had so little piano playing knowledge and experience."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-8b6b83650d5347fab88495e9473c7d1c", "references": ["PersonX acted superior and confident around PersonY , since _ had much too low self-esteem."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-bf7ef1ed829a4f47a3d0dbd80a316e39", "references": ["PersonX declined to invite  PersonY to go on a long hike, as _ prefers to be indoors."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-52ca1383bf954a709ce65ee4cd8124be", "references": ["PersonX gave PersonY plenty of business advice, since _ had so little experience with company growth."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-0ec3a1600ca14cfd918c486b7918be22", "references": ["PersonX gave her fries to PersonY to eat, since _ absolutely loved the taste of fried potatoes ."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-0e71cbe9f3ea42699dd3654c9a345f01", "references": ["PersonX was learning to bake from PersonY since _ was an expert at this art."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-9596efaa1cc9452db83877d8635b5fc3", "references": ["The school chose PersonX over PersonY to be a teacher, since _ conflicted with kids ."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-d91bed7d23314e5481bfeda1588068cd", "references": ["PersonX wanted to buy a new pet from PersonY so _ showed what tropical fish she had for sale."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-799ce834a2384c798d62ba2cb8146fac", "references": ["PersonX wanted to keep PersonY from entering the house, but _ got to the door before her."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-fdcd14b4540e41dbb9d4854a8873a685", "references": ["PersonX was very civil when dealing with PersonY so that _ would be impressed by her."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-56d5e301837d4bdb8fd6729b5a20e68f", "references": ["PersonX had a much better style than PersonY , since _ had no taste in designer clothes."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-7e3b434788774679bb62ebec9f18f242", "references": ["PersonX got a lower rating on their restaurant's health inspection than PersonY because _ kept the place clean ."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-7f9bbd0d45ac4675854c0250d22be771", "references": ["PersonX could lean on PersonY because _ offered some strong support during these hard times."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-037974c75c4440ad8aaf4966a148eff4", "references": ["PersonX helped PersonY memorize their lines, since _ was not familiar with the part ."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-ed42ce861b5f406883534fcbc21d56e2", "references": ["PersonX was a better student tha PersonY so _ got into a good college on the last application."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-bdf92efc1ea14b83a90afa360b98a1e4", "references": ["PersonX went to the store for PersonY since _ is currently the sick one at home."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-c58ccca7b4744764b9f430a6e8510d1c", "references": ["It was very hard for PersonX but not PersonY to plan a funeral because _ had no deaths in the family."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-aaab83ee86c1413a9246895833a1b9fb", "references": ["Baking bread was always a favorite of PersonX's compared to PersonY, so _ was disinterested in being a baker."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-436451411506471bb1ad643a005f4efc", "references": ["PersonX sat at the window and watched for PersonY to arrive because _ had been gone on a trip."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-e3bcc7c84b184df7ba942a5e3ffd9eb3", "references": ["PersonX hit his leg on the leg that PersonY stretched on the floor and fell down because the _ is sitting."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-c354abf005594d1283a9e0f03fff6895", "references": ["Since PersonX drove PersonY to the gym then went home, _ spent the next two hours at the gym."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-06045815050142b79edc31610d71e571", "references": ["PersonX had a terrible memory while PersonY remembered everything, so _ was in charge of the plans."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-4ebad081ef1048f5bf6403931f2ea908", "references": ["PersonX asked PersonY how to grow lettuce and a variety of other vegetables as _ was a professional gardener."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-c2e298cb8b3549d2b5f5af21fd559aff", "references": ["The summer went too quickly for PersonX.  PersonY was excited for school, and _ wanted to go back."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-d0ecf751a97648509a657986315cf778", "references": ["PersonX had a sun burn on their skin but not PersonY because _ shielded them self to the sun."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-8d31d6936ced4fb497ede490763611e7", "references": ["PersonX got a new iron for Christmas, but PersonY didn't because _ already had a new one."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-e23e8297913e435781f0c336cafba8c5", "references": ["Acting suits PersonX very well but not PersonY because _ is not a very outspoken person."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-4883186df8f8414a94a6d950c6c1dc18", "references": ["PersonX left the sunroof open in the car while PersonY always closed it. _ was anxious of getting mold and mildew in the seats."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-f72373027cc444e79e5d6dcf62eb900d", "references": ["PersonX went to the local bank to open a checking account and added PersonY as an authorized user. _ had no paycheck."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-18bb90f124f24d41a4b79de6f50aa960", "references": ["When PersonX declined PersonY's invitation to go out on a date, _ realized that they already had a girlfriend."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-f234d4d959a14781b9763288198a290c", "references": ["PersonX was very religious but PersonY was more spiritual. _ spent a good amount of time reading New Age books not the Bible."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-a66c167afe6e4a0f941b02ef2a361532", "references": ["PersonX wasnt as prepared for the mountain as PersonY because _ snored loudly the night before."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-567ba6caf8f74fafbed56f22dda3829f", "references": ["PersonX needed help from PersonY to get his nervous cat into the carrier to take to the vet, and _ was happy to help."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-257662ad82b54c21b614813b18229cb0", "references": ["PersonX cleaned their scraped knee but PersonY didn't clean theirs. _ got an infection on their wound."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-fe2d823f73004e4ea6ebff420a01f222", "references": ["PersonX's hair always got big when it was humid. PersonY bought a dehumidifier to help. _ was proud of the gesture."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-269e3379c3024c7f98616343080e33a8", "references": ["In tense situations PersonX did not scare easily but PersonY did because _ was very cowardly."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-6878e977730240e59d8388258f6a7078", "references": ["PersonX recognized an inherent talent in PersonY, so _ was asked to star in a new movie."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-97e0d645cb5649c290fbdbe4a78b346b", "references": ["PersonX did not spend a lot of time choosing fabrics unlike PersonY because _ had plenty time to shop."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-57c98706acd64ccd937344d1e1d291ee", "references": ["PersonX could only climb beginner walls while PersonY climbed advanced ones because _ was very strong."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-b8651d39d4fa43bba36fa17a336ccd03", "references": ["Photographs taken by PersonX are not as good as a PersonY because _ is a professional photographer."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-0793d09feaf547e78abc21ae7cef60f5", "references": ["In the newsroom, PersonX demands to know the source for PersonY's article. So, it's probably that _ is the writer."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-bce8c73e56d84bbbbbb43118a0c61b9b", "references": ["PersonX did not value the antique pictures as much as PersonY because _ was a history buff."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-fa55de662cd54204b661bb65e38fc70c", "references": ["PersonX needed PersonY's help swimming in the pool, because _ was comfortable in the water."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-49079470746445688020920018c1f2c2", "references": ["PersonX is a loan officer, PersonY is a gardener due to that _ is not better at helping to procure a loan."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-3175e87d09724602b2479dd3cb088b41", "references": ["Although PersonX was overall a nicer person than PersonY, _ often did practice humility very regularlly."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-7217839d01f34d7b856364bc5ccdb521", "references": ["At the bakery, PersonX comforted PersonY after flour was spilled all over the floor because _ was angry."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-a877a2aef718439f991d5c922ad9eb4b", "references": ["PersonX used a razor but PersonY used a pair of scissors so the hair in _ beard was longer."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-0c3c9a0b208c4926a964ff9e5ffd8e72", "references": ["PersonX is lazy and does not move around much, PersonY does and that is why _ is skinny."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-0ba9f86d77b141a3b8488821a7df657e", "references": ["The brewery manager position was better suited for PersonX than PersonY because _ had limited knowledge of craft beer."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-827c30d4e79e4f74819a546a09286061", "references": ["PersonX decided that they would treat PersonY to a massage at the spa because _ was so helpful."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-5bda9bfe0fed421c8c052b7264765d7d", "references": ["Intermittent fasting worked great for PersonX but not PersonY, as _ suffered badly from low blood sugar."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-98a4075efe444ad782a489f822edeb11", "references": ["PersonX said that the execution of the task was as important as completing the task itself. PersonY was lazy so _ didn't follow the advise."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-b69eff007882464d875b27ddb02f676a", "references": ["PersonX tried to make PersonY more comfortable but _ was sick last week and wanted to help."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-55c1a6e5fa964b17b133b224f55ce6ae", "references": ["While shopping for shoes, PersonX asked for PersonY to show them some Nikes so _ went to get them."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-d4c6176ecbcb419bbdabca6ff50cefe2", "references": ["PersonX was a better museum docent than PersonY because _ never read up on the new exhibits."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-2662965605794d7b985bc5c3a1ee0e27", "references": ["PersonX owned fewer cookbooks than PersonY, so _ made a different meal for every church potluck while the other dd not."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-d5a2069f68394be0a5a66680d745afc6", "references": ["Getting ripped off was something that happened to PersonX regularly but not to PersonY because _ always paid attention to their store receipts."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-c4db2583c43f41ffb8fc8d06d96475c9", "references": ["PersonX's braces looked better than PersonY's because _ never flossed after a meal and before bed."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-48d7185d37dd403590a41ad240474864", "references": ["Following instructions came naturally to PersonX but not PersonY because _ did not have any self discipline."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-1c9055ee0bd64d90af89623437c759fb", "references": ["When PersonX's company hired her, she became PersonY's manager. _ asked for some changes to improve efficiency."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-c9eb94d7e56241c6876961880db15a0f", "references": ["PersonX often had a foul smell coming from their armpit but PersonY didn't, so _ had to go to Target and buy some shoes."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-11d65ac58ee54c0f9d8cfadeeaad4fd8", "references": ["PersonX was helping PersonY to stretch at the gym because _ is a new member."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-711a9c4952d6480f8e191ca47c47067c", "references": ["PersonX gave PersonY a head start in the race they were having as _ was a very slow runner."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-9f163c1e8d484bb781b2c7163e8e0018", "references": ["PersonX had a lot of fear of heights while PersonY was fearless, and _ wanted to go skydiving."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-2976458fdad34b03870e7f504a5fef69", "references": ["The dentist was happy with PersonX's teeth but not PersonY's. _ had a big buildup of plaque."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-29e200c7aabd4cce9eb0c8a021420b78", "references": ["The project meant PersonX collaborated with PersonY, who was not excited. _ hated working with others."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-aa8c83ca7292427fa2195b97a7ce42d6", "references": ["PersonX caught their flight while PersonY missed theirs, so _ spent the next hour on the phone rescheduling."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-ff01e5e99b7d4d47b4fc13961366504e", "references": ["PersonX was better able to communicate their ideas to the group than PersonY, because _ was shy."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-2ad82a6ec60d4a2e8b9729fcaba6573d", "references": ["PersonX chose to wear a faux fur coat unlike PersonY, because _ was unconcerned about the reaction from animal rights activists."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-52b8368757c9441bb3ba2c26472f447c", "references": ["PersonX stayed in bed and slept for longer than PersonY although _ was feeling more sick."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-642c199493704095815bde4c90376ab3", "references": ["PersonX is known to socialize a lot with others, but PersonY doesn't. This is because _ is shy."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-8c602a177d194ffca3b09bbdba34a857", "references": ["PersonX sneezed more than PersonY did because _ had kept their pet out of the house."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-6afcbd4dd1394eac98720b486b620214", "references": ["PersonX needed to use a moisturizer after taking a shower but not PersonY because _ had oily skin."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-258f13580c4a4891b25e59d7bc706c80", "references": ["PersonX broke the smaller items that needed cleaning in the house so PersonY ended up doing it since _ is competent."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-740e841e085c43d19b8fa93d5d8b802b", "references": ["PersonX loves to drink sweet drinks unlike PersonY because _ is health conscious and a diabetic."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-43a7dbda1b6d4c1a99f123d31238d28b", "references": ["PersonX suspects she might have cardiac arrest and goes see her doctor PersonY, because _ is a doctor."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-16dcf02440df41cbbde73b5970bbf8e0", "references": ["PersonX was a much worse baseball player than PersonY because _ had control over the ball."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-30488b709cc646439bc1b7f36895f779", "references": ["The jeans PersonX gave to PersonY is dragging on the floor when he wore it because _ is short."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-6b1c41aa8d2d43f0b6b77a0a40292ec5", "references": ["PersonX tried to step in and prevent PersonY 's fall, but _ was too fast."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-e32a1ab4336e4f3e99e648fc466f9d90", "references": ["Even though PersonX lost more weight than PersonY, _ still ate fewer sweets overall despite this."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-c6d358873697432e86068ac79e5886e2", "references": ["PersonX was annoyed that PersonY kept insisting she wear a costume to the party, because _ adored dressing up for Halloween."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-a19ecb2c3dd445a59e63435027d6777e", "references": ["PersonX experienced a lot of heartbreak after PersonY left town due to _ not being in love."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-9dd17ef525f14541a2391651d3e47e82", "references": ["The kids of PersonX are being watched by PersonY because _ is a good babysitter."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-68ba55ed9e0344ce8857af482164fa26", "references": ["PersonX is afraid of caterpillars but PersonY is not. So _ helped her remove the caterpillar."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-00edaf241f99491896bb0dc5928d66ab", "references": ["Canada was an ideal place for PersonX to live but not for PersonY because _ loved warm weather."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-9ee198fcd5b048b48404af886a9cca28", "references": ["As they got older, PersonX chose to dye her hair but PersonY chose not to, so _ had grey hair."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-c4888aff3b45491ba4c7552d7e6fcc07", "references": ["PersonX had been feeling weird about bonding with PersonY, so _ said they wanted to stop spooking them."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-eb03d4354eb14021860aef9aae5d00d5", "references": ["PersonX is very creative, PersonY is not therefore _ does not want to be a graphic designer."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-5af75dd772f04759887c5d5cd9f08732", "references": ["The death of the actor was felt more by PersonX than PersonY because _ was just an acquaintance."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-c7900ed1d4d24a41b054296f4c5488c6", "references": ["PersonX was eating more than PersonY was eating for dinner because _ had a big lunch."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-675139262e394bddb95c48c6289ee331", "references": ["Last week, PersonX chewed out PersonY for shooting his BB gun at the squirrels, because _ was only doing it for sport."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-60527cb1c83b4a178e5c5fd3bfff4630", "references": ["PersonX was afraid of the dark, while PersonY was not, so _ 's anxiety was unaffected by the blackout."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-ea87f3a5c3bf4b84ad4ec65fd9b298c3", "references": ["PersonX had very little money and PersonY received a recent inheritance and was very rich, so therefore _ purchased a very expensive car."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task035-ed012e8cb92348d7bbaf06f51146a44e", "references": ["PersonX went to the courthouse to file a civil lawsuit against PersonY because _ had been negligent."], "task_id": "task035_winogrande_question_modification_person", "task_category": "Question Rewriting", "track": "default"}
{"id": "task569-9c8f4cfd5eaf4b3dbca298b5b8b70e70", "references": ["Tomato Bread"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-2d57fa938134432baa0db2bbb1364f87", "references": ["Fruit Ball"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-51588f59306f46628fefafbd6832f9f5", "references": ["Sweet Potato Bundt Cake"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-5656a426dbbe4b57b67747381f03d20c", "references": ["Easy Beef Stew"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-1f5b44b3f1994ae087cab3b49c1810b0", "references": ["Graham Cracker Cookie Bars"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-9a47cfa1ae6242ac8be4a300a1d18e0e", "references": ["Vegetarian Chili"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-50af92d27fe5419d94332df01853fd20", "references": ["Crawfish Broccoli Rice Casserole"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-8a1a2c39c68f434782936fcfcb347413", "references": ["Gemelli With Shrimp And Tomato Cream"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-c6393d28d4e44774aa0d94ed9be403ba", "references": ["Zucchinni Surprise "], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-27c06a39a6fd4a9ebf2162fa3c3e7a4a", "references": ["Sesame Beef Bing"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-2116040c90a94fb7b43a9bd6437db1eb", "references": ["Octopus Salad"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-a210e25e8273410a930f2472ba423e51", "references": ["Creamy Mango Pastry Stack"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-7384241bc833414ea9fe6619a10dc38d", "references": ["Apple Pound Cake Made with One Egg"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-31bed9df49eb4636921c07e9e9412606", "references": ["Orange and Honey Pull-Aparts"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-5f226d59b87946ee85db712d406b48ea", "references": ["Prosciutto With Roasted Pears "], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-f85debc0f6aa4faeaf7e5de3a1a3bc29", "references": ["Onion Blossom"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-01eebcaa1d444fb983bde762ae5243f0", "references": ["Cream Cheese Cookies"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-6d0b16ce3f804ad1a24955bbbd76c2e0", "references": ["Peach Pickles"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-f74f0c80079c4558a5251f2ee8113340", "references": ["Roast Chicken with Lemon and Thyme"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-9bbd6945d4fa41ebbdc97e4560cd5f2b", "references": ["Pineapple Cake"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-d57b12a7590b4df08b4e24992b03d395", "references": ["Taco Salad"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-d0e3f5767b0a48f4a4846423dec0cfce", "references": ["To Go Baked Oatmeal"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-3fed2224d7bf408384818484a98b8911", "references": ["White Chocolate Ramen Bark Recipe"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-3bd343b2aa7c47b99a78b61bb995b716", "references": ["Soft Molasses Cookies"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-1dc8397c5de144ccb18cb0cd6bbdf283", "references": ["Orange Nut Bread"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-18749fde9f66489e854ed38d5367752a", "references": ["Mom'S Vegetable Beef Soup"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-1a106b993ccd487b800e56e0453b27ed", "references": ["Garlic Bread Sticks"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-4b41fdf0fd534bfea30ab1d880e87315", "references": ["Oatmeal Cake"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-2b4b02d4029d44b3b4afa800a7c12e9d", "references": ["Easy Triangle Cookies"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-795987e67aae42b48917c736878ba8be", "references": ["Creamy Lamb Stew"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-7d4908a9186f4d749fddfc47708928bb", "references": ["Corn Machu With Pork"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-d594881026a04485832f56a05a775e9f", "references": ["Brenda'S Lasagna"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-993d04c59b744ad89c9bc07f79b66194", "references": ["Cheesy Chicken Rollups"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-d683d7fecee14faebd9aabaa73a679f0", "references": ["Onion Burgers"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-9b688400a58246c7a0848d4a662683d3", "references": ["Cheesy Tater Topper"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-7bdefb5f932844d59935905cde131dcd", "references": ["Three Bean And Sausage Casserole "], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-7c69062dde6c4b899fc506c40fdb2430", "references": ["Aubergine Lasagne Recipe"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-ec35ebe8df584406857410cef21d8e8f", "references": ["Marshmallow Fruit Salad Recipe"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-d563df09416f4457b8f4d1de964b06f4", "references": ["Jamie Deen'S Chicken Salad"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-f17a25e2c975480ca891665c99576553", "references": ["Cream-Style Corn"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-55e5c916b1f0428a9cb76d302ffe5637", "references": ["Pear & Gorgonzola Tart"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-88b750c4d46946ff8526828d2f1c98d6", "references": ["Easy As 1,2,3 Homemade Applesauce "], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-30da5b69389f400cace8fe3757f2a414", "references": ["Beer Dip"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-aa383b537c99491aa1abce7957b52e6a", "references": ["Banana Cake With Chocolate Frosting"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-6bb276707cbd43228d09e2620504baa5", "references": ["Easy-Bake Oven Cake Using Commercial Cake Mixes"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-5eb3746ddcbc44408b5ae62fb936b770", "references": ["Pecan Crispies"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-6a3e8ff5d9794f018fd274c76cbabd5e", "references": ["Joseph'S Trail Mix"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-71e60e28b98149c98dbd08bd8346bd47", "references": ["Lemon Tahini Dressing"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-cefb9af5950b49af8f7d68114eda3867", "references": ["Chicken And Wild Rice Soup"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-3fa5934571db4f548da171d053dd8d74", "references": ["Icebox Cake"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-f6005366b41d4a1baaf80784c57be7ad", "references": ["Cherry Almond Mousse Pie"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-3bda6e4a084140adb15fcc799b3739ee", "references": ["Granny Smith, Radish, And Radicchio Salad With Orange-Walnut Vinaigrette"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-bdddfa2a25d843f8bf21f97fa9f762e5", "references": ["Beef-Greens-Potato Green Beans"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-b6d22140ffc94e798226a034fe45b7f4", "references": ["Strawberry Delight"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-c6e08a0c16344f0cbfabf5004525dd2a", "references": ["Lobster And Chive Bisque"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-803da5dc83364c53a919b309c9b15c25", "references": ["Cilantro Chicken Salad"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-8f47087f8a4a48ac9eb14ba94232da65", "references": ["Blueberry Pie"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-a65653614d5244e7a5df8613a3a30802", "references": ["Mexicali Meat Pie"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-f22299bc2f874841be107ace3e4a0c60", "references": ["Frozen Iced Tea"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-8a23033b6f8147d087feb7a8bece50f2", "references": ["Broccoli Casserole"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-9b4f328ed4d843488022db3adcd4dc4f", "references": ["Garlic Black Bean & White Corn Salsa"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-ad5b42b5641044db86ef7c0116b3e374", "references": ["Sunday Sauce Recipe"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-e58c973b0cb84e729d94d95e5f4f3d0f", "references": ["Pumpkin Cookies"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-f0fc7a00ddcc45b7b92bf660112372cc", "references": ["New Orleans  Style  Italian  Salad"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-759e1675bc9c4faf803ca34e4e19c13c", "references": ["Breakfast Tortilla Roll"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-2ae441b8930343db92011582b7dd0b4b", "references": ["Starbucks\u00ae Caramel Frappuccino Copycat Recipe"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-ca98c301e32b443f9faaba44367f6d36", "references": ["Swedish Kladdkaka"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-ae257cbbecef42d5822e59dfa9611945", "references": ["Red Wine Chocolate Cake"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-b2d0e087bb76439c9b8507b9646f3177", "references": ["Harvest Pumkin Chocolate Chip Cookies "], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-b8253c148eb441b9ab4836502d5b7934", "references": ["Chicken Casserole"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-0001b3f4b2484d0a894e682e78518654", "references": ["Easy Chicken Noodle Soup "], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-cb3b397c3b494303ba4c44dd0e547fd9", "references": ["Splice"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-caaab27e8049472180a912eb32a49e31", "references": ["The Best Salad In The World Waiting For A Good Enough Name"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-7d4630b2306e44d4b90ecfd427b5db99", "references": ["Fragrant Pan-Fried Smelt with Roe"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-72b19c5d3e45455a9dc32f64d2e3daa3", "references": ["Meatless Meatballs With Mushroom Sauce"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-87503d09a3ac4726a74582e546704f97", "references": ["Swiss-Style Mocha"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-c6a9de84e6684ea89cd3bf66c194d612", "references": ["Chocolate Cream Pie"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-1dcaf0c4d98e4918abdf12d309a1a99e", "references": ["Easy Spanish Yellow Rice"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-32cd8f10ad4a4bba9b342b702294b386", "references": ["Pie Crust"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-9a046a26f2e849b4966e4a9e9bb12d35", "references": ["Soft Boiled Eggs With Dukkah Salt"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-bb00aba69eeb46d085b03c1ededd4be9", "references": ["Lemon Marinade For Chicken (Omac)"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-4f87b09d7f1641f69785493846a5be92", "references": ["Pot Roast"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-7de8fe2b8eee43428437ecdbad8f2039", "references": ["Homemade Ginger Beer "], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-086733a8a4124b28b2e2e9211c04ae26", "references": ["Portuguese Marinade Ii For Pork, Beef Or Fish (Vinha D'Alhos)"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-15241a07b775415fab081a5e3a28384a", "references": ["Chicken Salad"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-6b8479df51024b44b2c38f0f4cb797f6", "references": ["Chicken Stir-Fry"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-a428a6a2d411443287ad2479c684063a", "references": ["Ham Jambalaya"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-5d2e741759fd4553a268c8bcb9603838", "references": ["Baked Flounder In Hollandaise Sauce"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-5f9d775a9b7641289e4a0ec8ae6c4978", "references": ["Rose And Ginger Cupcakes"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-992391ccebeb4898bb008ce7c9b31f48", "references": ["Pecan Apricot Bread"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-d2a316c087f0459dab1ace58c155529e", "references": ["Apple-Oatmeal Cookies"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-445a21be6a10493e90dfa3b1397e1ee1", "references": ["Cherry Torte"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-4de295e1269a4991a60b42d2ca20c33b", "references": ["Wild Mushroom Risotto"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-30f14381ed204a9699c9548130426713", "references": ["Seared Elk Loin, Pommes Boulangere "], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-8904572929d44bffb73f93ce81515397", "references": ["Rocky Road Turtle Cake"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-88a40f819fd54375bdbb2b4abf9dbde2", "references": ["Apple Cobbler"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-28ed8714ebf948f29ec11864e0d590a6", "references": ["Jake Louthan'S Hot Rolls"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-d6ee11ae673b4b76b0f9b2e556ba2677", "references": ["Crumbly, Sweet And Buttery Cornbread"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-6bdabd1733c44ab981b401ccdf84d030", "references": ["Pumpkin-Sage Veggie Burgers With Green Onions"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task569-b23bb6ed4fc84a1e8b4ea5f6757a576e", "references": ["Amish Barbecue Chicken"], "task_id": "task569_recipe_nlg_text_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task391-cd9b17aa096b40838bcedf19e2a43937", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-18664b8f732e43eba693b2b3807346dd", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-2883cdea5daf4663964e6159f3e44b5d", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-bef39b050a2c4a169265bce4bce22840", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-f609022387f94802a621aec1368fcf5c", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-737332757cb24576b62a09dc3478ed5f", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-de54eea542324627a1589e1ff6de33c6", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-5c0978573b064b598db7494081e886a2", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-350d56b821e745e7a184c7259823cca0", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-295253ddc5b64363b24204d76346e4c7", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-ade19d4fef564e4e9814289958ad7ef8", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-3c904af18b2f45adb8463baa2faac473", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-6df0d7456e1f4cc2ae66e3c2bde5547f", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-e22731c0901e4634ae6fc6b16f11901d", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-a975349f9d3d444ca7bb24e82650e3e0", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-8bbf156f9f174363bfafc6eedcd14694", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-3233b8d93a9943b486a19864b6367679", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-1cd6ab0169e34c509467bae5b351569e", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-0a880be5563a40bca038d1a3f4c07498", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-026f3a9d1de34d27a0b0dbeac6a91b04", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-06b74fcd9d0949db9f6cbae9cb9cc591", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-1bf1e0c1b76b479f870e2c2c81bc78d0", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-ecee0cab443c4ef79d9306466daf3cff", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-dafa11a6e86c40feb5c8025598416bc8", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-ddbdd71a524a4e2fbb523d21595f4e19", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-53f09a7ef448489c8a6ac676f01c8158", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-7b934f7223b54fcca7e4a263311a6430", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-26c302b5eb7c4e18a354f363811fd53a", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-ff7ae3ca51a145d09a6eb075650cb08e", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-1662e17fc2b84556ae5d1912f0a195a5", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-e1ef4ba652ee4f10894c028a29acb2e9", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-cd61f71c36914b639a06a4188d61d1bf", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-72ac38efa0ea49ed82ff4f976ea3b7be", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-b0d0215078dc448e93d2f4e36406e63f", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-17bd206942354e329f0c43b5c3134b51", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-d288c4f0f248454f8c4d57a0bf8a062d", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-f6faebdb1e1347fea4262bb458a000b2", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-877861a243fd4fed97c89bbf350ae5a1", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-795048428bc6454794bec2eb57ee854f", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-9630e981f63a4c618a80a444d20e4df3", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-f483d3fc2cb14332a94d9af3abccd197", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-f9b540cb073b4eeebdc9519b44da8fc3", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-b5447b2c0fc543b3a18ce8901a4b5214", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-dcce2417d0c04e7bb84f5950a5945804", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-c851e99de72a42f0a0eab3d61e288662", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-a2228398af1a4fdd8c56398efd9c40b9", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-e1a9c8a4062d4130a6cd2ccd038b6b62", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-7af616ac9ae24c49b2ea42ff715149e7", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-07a2fdf176f14b1d98825afa022914b0", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-ec9fa331e74e46bb861bea9beb91ba11", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-a7fb35e6ca744b10939e4233db113a67", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-b383723158ce41f6b7ec310e7941454d", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-0831f86e9253493595bd26158cab5aaf", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-dbfca5d0791b425c961e03420c83c19a", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-89eefe774ea5418faa605326a6cec9a4", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-2fba1691cf5a4ec29a70b0ab7c5ae8f9", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-3e12d58c070b4401a603a637bd3a853c", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-91a0c96d70764f0f94f373a7dd558c10", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-0472ff1c231d4c778790718424d198e0", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-3195fccef96945ebac377f28fe62a781", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-58af70ef3466450a9c2abbc75c8bf67d", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-59f1eb7af3b34657b4c55895c3aea7e0", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-71cb06bc68544e658cdba8de5727703c", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-e93aa8474773478496ef495734cb355f", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-280678dfd51844888335fb5bdb4fd97d", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-9b0c47d3eb2c417996a8f0f975ae6be7", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-fabe8e2aeaac4069b68ee5215cdb51da", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-d2f562b5706447ccad08c028c7f0c04f", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-a490a23473c047599858aa08c1171806", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-b03838fc65894e2194e085e7697112b7", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-edfd76a20bcb48a5a5d990b198645bb6", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-68415401fc3d404ab4c8328623e8eb05", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-bcb4b71faff948a58da6e6714ce6d67a", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-34938ff138534fadb5cdd96b9adad9e2", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-6523400c59794bbabdf854a124d87b5e", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-27f6a6e8c3a64ff3976c6659e1fbf1d8", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-b08a9f86840c42adbbfabfee6ae44aa7", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-a1d7c75153ce417d82e93477bde158a1", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-b04354ea199143dcbefb8b5e40189c74", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-eb8934b868b64812a1aa3b6778fd2da9", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-9973f186cdee4bd5afb83dfa14d8f942", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-d806f8f5d02041eab000e305ddff542f", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-7a2bb1d6a9754b19a9ff4b666ecd9484", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-9ec04db7f6cb4aa5a256bc3f04a0fa81", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-a93456d1da6d436eaaf40c61fb15b1bd", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-5f8f243fb1ee485a9a8d3e990855b0a8", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-c0cbb7cd8ae349f19737502b80dded27", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-9a0576d92f144b999ef5ad7ea375b82f", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-cfa673cf8fc74c9bb3396812f09966a4", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-80cf6bc837df437197b7421d85525507", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-28ad714f373747d7abf352370c998578", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-2241f64686c54a3eb3fdaccf7b13d246", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-01548698816247aabdd4edbd7cad951e", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-75e2570c82fe4c4e90c2806b129d5ada", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-d55b68f499ac4e9ebf5393d9aec5629c", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-0e084f80858a4670a08d78cc534d067b", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-71879b738c4d4b5abb7fcbaad43b9fee", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-c83a01ab7d6a42fc99c26ed4665439fe", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-e7444a518117493191dfd98f3d1af861", "references": ["plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task391-73c316e9d0ab4e329342fbd77590f30c", "references": ["not plausible"], "task_id": "task391_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task891-1195b1c8d9e54beea6fec1cd20c044d6", "references": ["Katie Blair"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-aaf42030d1ae4dbdb037014af36b2075", "references": ["Tad Friend"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-befe3225b948410c87218d75b548ac27", "references": ["Rito"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-980f1d42677445628a20b11d216d7225", "references": ["William"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-d5839df8ea9c4de29ff08b0359f0e039", "references": ["Dan Bailey"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-499d0e3c65b248fd89e997447e83cb3a", "references": ["Marge"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-8cdfa6b610c84704a92240a5a2842a35", "references": ["Nora Wright"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-2947961fc3904537bbd757809bb7b991", "references": ["Charles"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-cd0afa2c48b64157b6fa6cfaea75b555", "references": ["Jefferson Davis"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-2c8ea21266474eb4a27276092a288e99", "references": ["Frei"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-4c2460cb6fdb4ac9b52c92597a12498b", "references": ["Leishman"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-d9deaf04269c4e4b84ce5233e0a62768", "references": ["Traci Peterson"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-3f92921b668641c291583e2bbba29fb7", "references": ["Starr"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-b8ebb06e9b614ed29dd6b441db1ac854", "references": ["Giggs"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-c8e47487195a4992bb635379f4328681", "references": ["William P"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-b6f003d4bf6d4b49be2aa8927b602d2e", "references": ["Eunice"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-e23f14d28a3545f091127df6fc56a548", "references": ["Montesquiou"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-728795babe234f269f48854707f98457", "references": ["Jay-Z"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-e73fa4fc30a74b249acdd58f3c63ce6b", "references": ["Steve Blizzard"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-881df338a5d64667b6c4a1a5d88131c9", "references": ["Halberstam"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-f888a4b884ba4257880be57bf6871db7", "references": ["Cilla"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-7495deda872843b5b2e5d893816ede21", "references": ["Andy Spade"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-803020fcf6a947968245841aea170c38", "references": ["Paige"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-cd81c9c02f6c4e76883cbd31690c4fe8", "references": ["Etta"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-277bdea30ffc4880a25ec533ddc6ff52", "references": ["Vic"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-0661a35f01ec45db8097c692015fd931", "references": ["Rosa Mendes"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-d30b074ecaa3499fb5351d13dd795ace", "references": ["Gardiner"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-4c7b8e1561f24d66af2f409056baa5ff", "references": ["Uzelac"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-c9084c393faf4ccbb905126529f7821c", "references": ["Isabel"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-c244d472458e464291c1d5b945f1d76e", "references": ["Chueathai"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-f3f481a019584abbb0902ee908f5eac1", "references": ["Edmund Calvert"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-4ed6c2e452c24c679a16bab0714b99ac", "references": ["Castillo"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-100cbf4fc18a4500aefc68668782093b", "references": ["Eckardt"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-287bc39447784340bf60159369289bda", "references": ["Tim Boswell"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-07d0ace3160e4d6db1514606ef69830b", "references": ["Tobin"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-c8f9ea0575364496a95b07e57084abb0", "references": ["Penny Unger"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-d1acd64f619447c0a76230b1f992e599", "references": ["King Edward"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-e5f5aee0ec484ac0ac8106fe7b0928ce", "references": ["Charlotte de la Marck"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-2d6513157a8944a5a4f9b38127cfde99", "references": ["Dyer"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-a10419cbf1be4307bf67b60e3e693e62", "references": ["Rudolf Friml"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-202437e66e3444d2964d9e5ea229265c", "references": ["Streep"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-3249deaf244147b78e2476f7d9356800", "references": ["Cecile"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-d1050d9485ea4b5a9c690c29f0282149", "references": ["Davidson"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-8f2da83fe5ca40fc8174cb1d3a8e4c5d", "references": ["Father Joseph"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-d4e1c64f2d3b4388a071f18e42f78f32", "references": ["Keys"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-c0fac71f663540e1abbea1cd9ef5e947", "references": ["Ceruti"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-a1e91229fe5041d9a47a5f23f4f65c36", "references": ["Panizza"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-2659c2dc89c0417dbee6dcc8707ee23f", "references": ["Pat Magnum"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-b8f5c696b49a44c3b0e091c0a5993184", "references": ["Dolly"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-4d247c49ccad4c53be5b9a9e17ff1097", "references": ["Tarde"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-e944f07613d046508ff4ecfef3666ebd", "references": ["Wollheim"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-49f90baa2edd4707a9911d4f36f6fa31", "references": ["Mark Ingram"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-2a041e3d1eb542c0be4242dde3fbd9dc", "references": ["Jones"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-e2cb73a8c6794ec8b37dd19d8d6d53ca", "references": ["De Graeff"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-f19e113705834bb0a427a524f63da419", "references": ["Richter"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-4bc539674e3341f9a82707d81b4f1884", "references": ["Helen"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-f347dee6c9114f7ea75c89bd9cf6ea73", "references": ["Celia"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-8943e7c0f94d4a2ab97bb2ecc3f2c68f", "references": ["Sharon Rich"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-90161451952a4147a4f8f87a6c48f681", "references": ["Ramsey"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-b2322ed11638423d8ca0da80895a0271", "references": ["Longstreet"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-c2979e7543794298944bf09d0e7f6f14", "references": ["Schwarzenberg"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-70d18885e7ad45a49c68ffe5bfc98e44", "references": ["Robert Langdon"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-960dc061046e4edf878e4875748ca292", "references": ["Malcolm Rifkind"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-b8605940689f47b4ab712b7676522171", "references": ["Susanna"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-3ff7f76cd57541b7b07507fbd76f96d5", "references": ["Zilic"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-2b9fcda6735d4b7ea1fde5fafb5932b6", "references": ["Raph"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-e7f0a5d2091c488d92cc5977bb9ec3c3", "references": ["Lennon"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-ae5a3f1771ab4166a42cb80df4c7b6be", "references": ["Adam"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-31e94c3751db473fb1c15394beefd831", "references": ["Vivian"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-470e4b2bcdbb487fa6383205499d57d0", "references": ["Tewodros"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-cc7d155dfd804a3083fbfa2eb27cc113", "references": ["David Onley"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-d7da0f7ed3bc4a0abbb7811674bbfac5", "references": ["Fuld"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-f43c77433d2b4da69308ba54a859455b", "references": ["Edward Baigent"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-f55d71068951447ea77ec9c9daee93b2", "references": ["Marcus"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-36c63abf48bb4f5b8b3e8d159a110e4e", "references": ["Father Boisot"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-b0a152722c02407684de4c228456f862", "references": ["Victoria"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-b6a2983a50154d9db7eeb771db80f0dd", "references": ["Leota"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-b29c825bc5b24db19323bc98aee4029a", "references": ["Ben"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-a52257f88c9745f38a29c9ce5d6c363e", "references": ["Anstruther"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-86e807a221874db5b0b8d642af3b2522", "references": ["Thomas"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-eb8f185474fd4216b75c4b5f87bbca35", "references": ["Siva"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-0a481e07e3724e94987da3fbccd65c6f", "references": ["Taylor"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-8454eaf7092e4fe6aa9cc2f3ab669e45", "references": ["Vickie Guerrero"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-02dd9caa63b14c8ea7e7a46880f8d4f2", "references": ["Amanda"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-957dbab0340a4c3980329bf574e82de4", "references": ["Smith"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-3375a930c68c4074a0f74eb6deae16bb", "references": ["Suzanne"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-ce4139ef9c264c9ba637194c5235ba30", "references": ["Barlow"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-00a1f2eca19247d0bbe4365c0610623f", "references": ["Rusedski"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-ef31dd052c68439cb6b312af74b0f307", "references": ["Kudryavtseva"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-8ed01c072ca9497eb9edba172e00fde3", "references": ["Temerlin"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-7dbd654478514b6e8e786d852c060650", "references": ["Baron Grenville"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-09a722fd654e4982b620a6c7481d624c", "references": ["Kylie"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-f7051609f03e4a02883853876770fd41", "references": ["McNeill"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-f86bdf4bc4e54589b3407f78d0ce2126", "references": ["Neff"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-58907d30c66c4a699760ef1b75c5976d", "references": ["Daisy"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-abb99da9ade8470b818108c241d48952", "references": ["McHale"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-de71d3edd9db4179b6581be73ef6c129", "references": ["Mermet"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-fdf27721c04044cc9a43d56468782131", "references": ["Alison Krauss"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-c7edef0f12604c4f8d26787e0688ffdf", "references": ["Anna"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task891-37c25542e7d34e979239ce525b2ecee8", "references": ["Emperor Taizong"], "task_id": "task891_gap_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1586-b785580022864765bc4de07fb9f54f5b", "references": ["BC1 RNA, the transcript from a master gene for ID element amplification, is able to prime its own reverse transcription."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-8a03a22e068f408291bde6f5cd53bf0f", "references": ["The DNA Methylome of Human Peripheral Blood Mononuclear Cells"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-1b86fa8bac7c404687d6244f90024a4c", "references": ["The human myelin basic protein gene is included within a 179-kilobase transcription unit: expression in the immune and central nervous systems."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-21eb67e150074bc996202f7d6d5fd5c0", "references": ["Targeting A20 Decreases Glioma Stem Cell Survival and Tumor Growth"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-50883590b7a242e0a7bfc0796319d344", "references": ["Efficient targeting of expressed and silent genes in human ESCs and iPSCs using zinc-finger nucleases"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-4daa563cf14c42bd93df445ee8bc718d", "references": ["Empirical Bayesian models for analysing molecular serotyping microarrays"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-fd593ab022fb42328f2538b73653489e", "references": ["Bayesian measures of model complexity and fit"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-adac2134b7a94bd9bc940e6751ad0f89", "references": ["Simplifying likelihood ratios"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-8a8c29a579a949d0b324b801c09b7013", "references": ["induction of early IFN-inducible genes in the absence of type"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-749616cd3c3d44d6a0387ff7339f67e7", "references": ["Arteriolar function in visceral adipose tissue is impaired in human obesity."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-9f0e49304d514608a0d398710737c3ae", "references": ["Common Carotid Intima Media Thickness and Ankle-Brachial Pressure Index Correlate with Local but Not Global Atheroma Burden: A Cross Sectional Study Using Whole Body Magnetic Resonance Angiography"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-29fdfe6240e04d18a2246e6cdd8a0939", "references": ["Loss of immune escape mutations during persistent HCV infection in pregnancy enhances replication of vertically transmitted viruses"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-d73441b6019d4396b15ded7d893aa628", "references": ["The journey of developing hematopoietic stem cells."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-48c0bbb1d80e4872b15be069d2743e24", "references": ["The sacroiliac joint in the spondyloarthropathies."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-67ef20547adb4249a4fb666467e20d50", "references": ["A rapid method for extraction of cotton (Gossypium spp.) genomic DNA suitable for RFLP or PCR analysis"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-aa0d681fc23f4c4f885a574b8dad94d6", "references": ["Cross sectional stature and weight reference curves for the UK, 1990."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-ebfa69505c04448eaad7c793fb917a06", "references": ["The suture provides a niche for mesenchymal stem cells of craniofacial bones"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-97b513b39c0446cea32038e5b99dfada", "references": ["Cardiac neural crest cells contribute to the dormant multipotent stem cell in the mammalian heart"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-a99ca5fe6df94478a25dd4ba53fea6f0", "references": ["The mammalian target of rapamycin signaling pathway mediates epileptogenesis in a model of temporal lobe epilepsy."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-2619b60bce0f4da290de6c5e2829d8ce", "references": ["Acute administration of recombinant Angiopoietin-1 ameliorates multiple-organ dysfunction syndrome and improves survival in murine sepsis."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-2cb596d141fa4fccbc4d880557578ead", "references": ["Mechanisms linking obesity to insulin resistance and type 2 diabetes"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-43bb07b6cb074c3aa5b92272b48299e5", "references": ["Tracking the fate of glomerular epithelial cells in vivo using serial multiphoton imaging in novel mouse models with fluorescent lineage tags"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-9c3f990d3875459ca582b5a79c7f5493", "references": ["CTCF binding at the H19 imprinting control region mediates maternally inherited higher-order chromatin conformation to restrict enhancer access to Igf2."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-c5b54def6071465e9d53cb83855d5fd4", "references": ["De novo assembly of a PML nuclear subcompartment occurs through multiple pathways and induces telomere elongation."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-f841a3c7534b4243a5e902ef17433bc2", "references": ["Effects of an opal termination codon preceding the nsP4 gene sequence in the O'Nyong-Nyong virus genome on Anopheles gambiae infectivity."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-e762be7b33e14078aff12de64e6dcea3", "references": ["Down-regulation of a host microRNA by a Herpesvirus saimiri noncoding RNA."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-b1060550584f4170a146f0fd770b69d3", "references": ["HTRF: A Technology Tailored for Drug Discovery \u2013A Review of Theoretical Aspects and Recent Applications"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-0ba1d40e070c4be4a0029a444e403a03", "references": ["Chk1 inhibits replication factory activation but allows dormant origin firing in existing factories"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-bb9e185355a5474cb48e4e28eb597864", "references": ["Fra-1 protooncogene regulates IL-6 expression in macrophages and promotes the generation of M2d macrophages"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-cd8df4d5ac2f473595aa8e26b4d57191", "references": ["Analysis of SiO2 nanoparticles binding proteins in rat blood and brain homogenate"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-2b2d7b07162447b2a4141573eaa88786", "references": ["Inferring nucleosome positions with their histone mark annotation from ChIP data"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-2ff7e9c590b54f33b6990767e453bfaa", "references": ["Generation of large numbers of dendritic cells from mouse bone marrow cultures supplemented with granulocyte/macrophage colony-stimulating factor"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-fa337b0a9bee4da89c3d0a08ffa00f59", "references": ["Insulin action and resistance in obesity and type 2 diabetes"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-b8bf358a9d8a46f392285c0207d768b4", "references": ["Effect of lowering blood pressure on cardiovascular events and mortality in patients on dialysis: a systematic review and meta-analysis of randomised controlled trials"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-7a14debccf63432b8101386f6b57ad05", "references": ["Interaction of an adenovirus E3 14.7-kilodalton protein with a novel tumor necrosis factor alpha-inducible cellular protein containing leucine zipper domains."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-3f746269656e4387bdd119cc1130fe5d", "references": ["PML induces compaction, TRF2 depletion and DNA damage signaling at telomeres and promotes their alternative lengthening."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-bb71c72f33a44bc5aa1fba324321430d", "references": ["Gr-1+CD11b+ myeloid cells tip the balance of immune protection to tumor promotion in the premetastatic lung."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-75506a1dd2884b449aa3d9c70a3fed01", "references": ["Activin/Nodal signalling in stem cells."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-579d3e51c309462f9d4e23a2d26388d4", "references": ["Distinct RNA-dependent RNA polymerases are required for RNAi triggered by double-stranded RNA versus truncated transgenes in Paramecium tetraurelia"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-5855329e487140c88ff8c92d2166336e", "references": ["Mechanisms of Renal Cell Apoptosis Induced by Cyclosporine A: A Systematic Review of in vitro Studies"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-e0ad55507ef749d69ca112bf4dd8195e", "references": ["Blood stem cells emerge from aortic endothelium by a novel type of cell transition"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-6f8774d1ab7c47409f3f0ca4f023b241", "references": ["Probable person to person transmission of novel avian influenza A (H7N9) virus in Eastern China, 2013: epidemiological investigation"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-bd29a529557e4126b0b6053b79345b0a", "references": ["Genetic Variation in the Interleukin-28B Gene Is Associated with Spontaneous Clearance and Progression of Hepatitis C Virus in Moroccan Patients"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-609d473000e241219f7b85e9937e5a11", "references": ["CD127 expression inversely correlates with FoxP3 and suppressive function of human CD4+ T reg cells"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-ecc2417c33f943c6b4d86386316c39a4", "references": ["Environmental factors that influence the cutaneous production of vitamin D"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-1ee80f86cc9942c39db296e2c6ceb31e", "references": ["Genomic imprinting in development, growth, behavior and stem cells."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-c47fad4f85a243f8959c715c65ee3865", "references": ["Quantitative Detection of Hepatitis C Virus RNA by Light Cycler PCR and Comparison with Two Different PCR Assays"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-52d11313194f43b1a962d45422165440", "references": ["Targeting arginine metabolism pathway to treat arginine-dependent cancers."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-b084e24ad017400cb30ba1659f4c8e96", "references": ["Rank Signaling Links the Development of Invariant \u03b3\u03b4 T Cell Progenitors and Aire+ Medullary Epithelium"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-a4ab36375c634efcb3112b35e488b150", "references": ["Long-term immune deficiency after allogeneic stem cell transplantation: B-cell deficiency is associated with late infections."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-3c1d94a543424bf3ae71209d3bb52ba8", "references": ["The stimulatory potency of T cell antigens is influenced by the formation of the immunological synapse."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-afe87d40c8824bf785f8ce90927aa09d", "references": ["Control of glutamate clearance and synaptic efficacy by glial coverage of neurons."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-3b886c26224e49c1995738727dc67a3a", "references": ["Duration of androgen suppression in the treatment of prostate cancer."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-3110731755c948b5b6377ba3b902b026", "references": ["A Functional Neuroimaging Study of Sound Localization: Visual Cortex Activity Predicts Performance in Early-Blind Individuals"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-eb7f2e7a28fb47e3a4fd4dc90f2f6fe4", "references": ["High-Infiltration of Tumor-Associated Macrophages Predicts Unfavorable Clinical Outcome for Node-Negative Breast Cancer"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-251309241e0b4cc0ad481a783da77d47", "references": ["Murine Dishevelled 3 Functions in Redundant Pathways with Dishevelled 1 and 2 in Normal Cardiac Outflow Tract, Cochlea, and Neural Tube Development"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-38328af04c9b4599815821c806c99a4a", "references": ["Dynamics of adherens junctions in epithelial establishment, maintenance, and remodeling"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-a1a1b475cbdf4dd1bfd85955cedd5652", "references": ["Long-term outcome of patients with asystole induced by head-up tilt test."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-374a1e3aa9ca446ea420d21782e86714", "references": ["ZINC 15 \u2013 Ligand Discovery for Everyone"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-fc4d42e8d988430c96e0da9e3a68ef94", "references": ["Condensin-mediated remodeling of the mitotic chromatin landscape in fission yeast"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-725280cc7de74989a630cb88c5a15973", "references": ["Efficacy of the 6-month thrice-weekly regimen in the treatment of new sputum smear-positive pulmonary tuberculosis under clinical trial conditions."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-21df0b6d3abd458db749c7529630db09", "references": ["Curcumin attenuates inflammatory response in IL-1beta-induced human synovial fibroblasts and collagen-induced arthritis in mouse model."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-1f115570cfc54630b55f03e1ae1af41f", "references": ["Loss of IGF-IEa or IGF-IEb impairs myogenic differentiation."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-19d49404f67145f9a95a54292ab58547", "references": ["Antimicrobial peptide pleurocidin synergizes with antibiotics through hydroxyl radical formation and membrane damage, and exerts antibiofilm activity."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-2630a2ab1427438c9a358e475bd7e435", "references": ["Outcome of pregnancy in women with moderate or severe renal insufficiency."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-3ace12a19bba4406abed98767d564066", "references": ["High-mobility group box-1 protein induces osteogenic phenotype changes in aortic valve interstitial cells."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-e25c813a30f34ab7a12e5f6dd5d3370f", "references": ["RiboSys, a high-resolution, quantitative approach to measure the in vivo kinetics of pre-mRNA splicing and 3'-end processing in Saccharomyces cerevisiae."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-6b41f30db77e4b6caa8dc3e51a11bac1", "references": ["Generation of mice with a conditional allele for the p75(NTR) neurotrophin receptor gene."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-eab48284389146239d50f6677d25982d", "references": ["A Triple-Stain Flow Cytometric Method to Assess Plasma- and Acrosome-Membrane Integrity of Cryopreserved Bovine Sperm Immediately after Thawing in Presence of Egg-Yolk Particles1"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-125a336e36d449afa63810ded15d234a", "references": ["Universal voluntary HIV testing with immediate antiretroviral therapy as a strategy for elimination of HIV transmission: a mathematical model."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-f13c91a086a841b79d7681fd8d6f1fcd", "references": ["Common variants in DGKK are strongly associated with risk of hypospadias"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-23ba30d2b1c54a82bbdb0d34577cff4f", "references": ["Neutrophils Suppress Intraluminal NK Cell-Mediated Tumor Cell Clearance and Enhance Extravasation of Disseminated Carcinoma Cells."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-6e0fb4184a2d45ee90cd77ac97da51d4", "references": ["High Km soluble 5'-nucleotidase from human placenta. Properties and allosteric regulation by IMP and ATP."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-ddbefcd1e39f41639a9d27d28ba44671", "references": ["Histones are incorporated in trans during reassembly of the yeast PHO5 promoter."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-81cd3ffb89b543a9b9e964da4123d856", "references": ["National study of physician awareness and adherence to cardiovascular disease prevention guidelines."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-1dba205d17594baa9f3ee7f19cb3d061", "references": ["TECHNICAL ADVANCE Floral dip: a simplified method for Agrobacterium-mediated"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-2ec2ff409e894556a0d0bc05256822e8", "references": ["Pathway connectivity and signaling coordination in the yeast stress-activated signaling network"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-8d4e765843554bd097eeb0f8a59b3c5a", "references": ["Premigratory and migratory neural crest cells are multipotent in vivo."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-df882ea624184793869d2fd91a033c72", "references": ["Implementing the ESHRE 'poor responder' criteria in research studies: methodological implications."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-df755185d88846b891623313c132b2e0", "references": ["Generation of gene-edited rats by delivery of CRISPR/Cas9 protein and donor DNA into intact zygotes using electroporation"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-312977c24871429d846ddca40449cd74", "references": ["Uridylation of miRNAs by HEN1 SUPPRESSOR1 in Arabidopsis"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-cebc841f42654f04a92934736dd0ca10", "references": ["Pre-Exposure Prophylaxis to Prevent HIV Infection: Current Status, Future Opportunities and Challenges"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-5cc709cff5f940c0b6f71c721fb861b8", "references": ["Prepregnancy BMI and the risk of gestational diabetes: a systematic review of the literature with meta-analysis."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-86992138d6174953bc3ee8cd0f014abc", "references": ["Multiple risk behaviour: increasing socio-economic gap over time?"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-8b80eb3565a64b8892bc98e350aafce3", "references": ["Variations and Trends in Health Burden of Visual Impairment Due to Cataract: A Global Analysis."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-e02cba7f1008463591ee5013e8623d33", "references": ["Multiplex genome engineering using CRISPR/Cas systems."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-7ba6590e9cc3476691d94d821065f291", "references": ["Transformation of intact yeast cells treated with alkali cations."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-7e35e2d2334c4020a51ce9ca6f7b66e5", "references": ["Health-Related Quality of Life as Measured with EQ-5D among Populations with and without Specific Chronic Conditions: A Population-Based Survey in Shaanxi Province, China"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-b5ff56c3a4d54523954ec47dae065266", "references": ["Memory and Modularity in Cell-Fate Decision Making"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-bb21a0501fa246dd84c864dde6aa53e6", "references": ["Lpd depletion reveals that SRF specifies radial versus tangential migration of pyramidal neurons"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-de8cfb855e7749ffbc48414a223030f7", "references": ["Identification of p18 INK4c as a tumor suppressor gene in glioblastoma multiforme."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-7bb49be6e29f4ea2b69051d90f09b928", "references": ["Two-stage control of an oxidative stress regulon: the Escherichia coli SoxR protein triggers redox-inducible expression of the soxS regulatory gene."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-ad0d2ce69a4e4c37a3691077d0e13419", "references": ["Air pollution and cardiovascular disease: a statement for healthcare professionals from the Expert Panel on Population and Prevention Science of the American Heart Association."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-3d6c04a6f7b94f6787e2ca2042595b81", "references": ["Non-invasive ventilation in chronic obstructive pulmonary disease patients: helmet versus facial mask"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-14c4a5a341154ca0bf0825dd632569f1", "references": ["Case Management and Client Access to Health and Social Services in Outpatient Substance Abuse Treatment"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-fe8480ac842b48aaaabea02a03c869d3", "references": ["Evidence that the Ipl1-Sli15 (Aurora Kinase-INCENP) Complex Promotes Chromosome Bi-orientation by Altering Kinetochore-Spindle Pole Connections"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-277965a2fbf847b18c299f953234aa9c", "references": ["CRITICAL REVIEW Cutaneous Vasculitis Update: Diagnostic Criteria,"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-b7cbef38338b421fa7fcd12e2c1dfb0e", "references": ["E2F-dependent histone acetylation and recruitment of the Tip60 acetyltransferase complex to chromatin in late G1."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-da7e7896b96f4fb58b16db1ecf16a1cb", "references": ["Chemical approaches to stem cell biology and therapeutics."], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1586-1394f1327f15438899319c1afe3a305c", "references": ["Genome-wide maps of chromatin state in pluripotent and lineage-committed cells"], "task_id": "task1586_scifact_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-274c8eff6f7e46d2bf95af987e3422b6", "references": [" Gameplay"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-c8f926ba45924b499cbf3af609140a12", "references": [" Plot"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-f6edef6643614755abb507088a7e335b", "references": [" Release"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-98ba4414cd744513ba81498b2271836a", "references": [" Reception"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-d7dd69c1787a429e92a78e34a97e7618", "references": [" Legacy"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-5a38e04d02f9415e88007cfab106d9aa", "references": [" Adaptations"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-d17e1040a84640bf845ee6837b04edc0", "references": ["Tower Building of the Little Rock Arsenal"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-c271fb9d697a406bba2555e0c02984a0", "references": [" Construction"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-c2341568b6fe4b458fe4a66f6341a47c", "references": [" Civil War"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-0d8b1127409f483b9556a2d645b634a3", "references": [" Decommissioning"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-4bdc6a87d13941368247e9817160f444", "references": [" \u00c6sthetic Club"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-6acdfca157cd46f9b5128d44d7da87ff", "references": [" Public use"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-f886b5f9d7e44b2a81ec94e62a630dcd", "references": ["Cicely Mary Barker"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-bb1155d832cb4e709cad37d52ae57eba", "references": [" Early life"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-41e937c617eb41e18c96f5906b6dc39b", "references": [" Art education and first professional work"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-067fb6cd57c34050a3f76d7a0a42421a", "references": [" Flower Fairies of the Spring , 1923"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-dc22aea72b9543449f45c3df0732c7d0", "references": [" The Waldrons"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-dfc79da971954b1e91422ee24fd2fe32", "references": [" Middle years"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-9ccbd1ce336f45209014bd99c3716b9f", "references": [" Later life and death"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-46e8c91f390340f2b0078b5e4c5f350f", "references": [" Art"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-06ec391b2a874860822a1170f9a62ced", "references": [" Depictions of children"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-7df7ad24a1be4623bd1ea36aa7217796", "references": [" Christian @-@ themed works"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-a28aee4afa314839adddde8791aff54d", "references": [" Works"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-859fccaed01a482ebf76560b1edd98c4", "references": [" Cards"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-acbf8f1dce064418bfd5b6180676161d", "references": [" Books"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-f3e17d0169574ffea8e5739fffe40dc3", "references": [" Posthumously published"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-ceaca8189fbf4cf3b7c61574fb17a438", "references": [" Book covers"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-3696e1d018994616833f653ccb649131", "references": [" Religious works"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-d7a480d016114118b5ac5a40a14691a0", "references": ["Gambia women 's national football team"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-a1ddad5107ca4317ac7210175f14f7d0", "references": [" The team"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-f84626e653bb4118b65bdd3fa04b2f67", "references": [" Background and development"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-26a7c8db6fbc44e0abb13467f850a978", "references": ["Plain maskray"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-17bbf7059a6549a6a3c30c85ddb513c0", "references": [" Taxonomy and phylogeny"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-24e6f2f575da46e99c4286eb4500708d", "references": [" Description"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-e5cab4ec15c04ebab849b6fdc8aeb1c8", "references": [" Distribution and habitat"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-a3a149f8ed874b43b2de02f0355ad04f", "references": [" Biology and ecology"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-5aa5ae90f72e4d88857d5bad6e24fd2f", "references": [" Human interactions"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-36f0ae2c75bd4fcf99d0ed3c6cea8c99", "references": ["2011 \u2013 12 Columbus Blue Jackets season"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-fc7ad68aebf04c50ae3a79954d1f77e5", "references": [" Off @-@ season"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-8fd68764f6214b3aba37b0be64eadc96", "references": [" October \u2013 December"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-f04b2f9674fb44ca9f23e62ca036b250", "references": [" January \u2013 February"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-b26ca2884a014e408d01b5747aa14155", "references": [" March \u2013 April"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-5abb28c9ab3c4339804ecf7ecf6316e4", "references": [" Post @-@ season"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-78131c9121f3469ba3b8dd1358ad980d", "references": [" Standings"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-182313f96e9c487aa8c85a48a05eac8d", "references": [" Regular season"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-0a6f8333a2554f4884542b7aaf6634db", "references": [" Player statistics"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-53e2a903aa8f42e3b3a0f041eefdef3c", "references": [" Skaters"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-d78a90843b274d2e8b18322add3fdfb5", "references": ["Position ; GP"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-f898b97435ac4186a6fecd6fcd1cdfb7", "references": ["Goals ; A"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-5eec3caf37c240cc9042cc5455975198", "references": ["Points ; PIM"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-287972cb02e345509b1011d55349fd7f", "references": ["Plus / minus  "], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-00fa8363fd574114abda91f7d83a312c", "references": ["   Note : GP \n"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-606d674e70104074973ab9f8b76bddf5", "references": ["\n Time On Ice ( minutes ) ; W \n"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-c94926d0682249f28fdc0616e417e86d", "references": ["\n Losses ; OT \n"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-ad29dae3a79647f0a5f26f9981e0ced0", "references": ["\n Goals Against ; GAA"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-d5df040bf6644e7193147c86d013da01", "references": ["Shots Against ; SV \n"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-3c7b50f65f2846eea2ae27c9d4ec4f33", "references": ["\n Save Percentage ; SO"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-c55337da35a14e00b9dbbe80ea62255a", "references": [" Milestones"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-59ec05166e774f42a9b13efc6205daf9", "references": [" Transactions"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-cb2841f2b5ef412c84005d161e6060fa", "references": ["Gregorian Tower"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-23629c4a84f54c91a6506fb7952e2c16", "references": [" Early history"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-3dc9773be83d49469cf4b8f8ef33d7ea", "references": [" Second stage"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-ad7d00ffa78f48c99734284237db22b1", "references": [" Third stage"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-aea8f244482b439c9e06f36ff33441fe", "references": [" Fourth stage"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-38fc703bfeeb47288a833e2da3be2576", "references": [" Features"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-25de7bf917fa457bb6f9549a0b685a9f", "references": ["There 's Got to Be a Way"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-82192efbc15e49c28834df78b0b41f31", "references": [" Background and release"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-e14204b876b748eb9ab2114db68190ca", "references": [" Composition"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-db3d251cafbb4433a0b4d35d9ce48d2d", "references": [" Critical reception"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-b99bb73c0eda46d38d9e93fc8f430928", "references": [" Music video"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-f2115163e1994952a24aa1872376e9b9", "references": [" Track listings"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-f9d1929b7bae497084f04508980301d0", "references": ["Nebraska Highway 88"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-618c7b60e3014cbc907dc3d3e082fa71", "references": [" Route description"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-7f7fa84f39e7402eae7b4618b6a790e6", "references": [" History"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-9038a714f266468fa71b7439fe626aaf", "references": ["USS Atlanta ( 1861 )"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-f1ca01e7a18040f8a86c57d80b8ac755", "references": [" Description and career as Fingal"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-2d59fa3197d244458158e3c97f352613", "references": [" As Atlanta"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-13f6cd260bac4c329403451d6ccffb66", "references": [" In the Union Navy"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-aa29c2833e5445838575b0592e6ef19b", "references": ["Jacqueline Fernandez"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-45649803218a4cada1255090017c369e", "references": [" Early life and modeling career"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-91509a178d7242198fe4944d6c0be0d9", "references": [" 2009 \u2013 2013"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-b656130adad24b98adbe30bedee5f4bc", "references": [" 2014 \u2013 present"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-ab2adeebdd3f45a996bd411d2796b2d0", "references": [" Personal life and other work"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task602-087d83d3700a46789db00066a97ff572", "references": [" In the media"], "task_id": "task602_wikitext-103_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1195-864855ea376c41e78cabc8398abe40da", "references": ["What do lobates feed on?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-de26309c202f4021b6da3550a7e38045", "references": ["Some theories argue that civil disobedience is justified in regard to?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-741a34e3af7b4d75bf9c54c4ef643dfe", "references": ["Who runs the Sonderungsverbot?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-4f99d95922e347d49537470d17736d5a", "references": ["What bridge did the Germans fail to demolish?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-585f0bbb3baf4f05800ceedb17c8bf53", "references": ["What led to Newcastle's fall from power as military advisor?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-5ace033b19b84cf8ae5c11975e37a72d", "references": ["How long has the Rhine coastline been in the same location?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-7de408f6fa3c48b2a096c73313d7ff65", "references": ["What Governor wasn't in charge of New France died in 1752?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-ed6ee17bc87a4377be9b0e89b130cdc2", "references": ["Which company provided streetcar connections between downtown and the hospital?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-d607ebbd4ad645f28297fd80ab4e240f", "references": ["Whose activities were the French able to gain knowledge of?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-bf2d1861a9114b549edc86bcf3862055", "references": ["What was Old Briton's response to Celeron?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-55919ecb6bf343409421693ced35538c", "references": ["What was the stretched sequence in the Grand Canyon named?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-7c597fbadf844dfaa6c097e61bb34f6b", "references": ["Where was Francis Heisler taken after the protest?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-d849a16d82b345a281f791217d9d4b30", "references": ["Proper valuations happened in a government project because of what by the owner?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-2ca911dc51384eaa87ad3c2b93a2f3e1", "references": ["What does colonialism lack that imperialism has?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-8e516bdce7c943a6aeb887c62abe53e4", "references": ["What does the statocyst split to connect with?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-d12df37497334629b4754a6df01af784", "references": ["What modern city did Khanbaliq become?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-8c8c5b45bb664cc1b7e29a42fec072f2", "references": ["Who had Toghtogha tried to defeat?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-b33732d4e04d40e59c40ec636308e09d", "references": ["What did the non-Afghan veterans returning home have in addition to their prestige?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-a55571563fc44189b48de4a2da9b15d9", "references": ["What is the largest port in Europe called?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-80f6737fdb784bbebf18a21185d4ec17", "references": ["What was the first Internet2 Network created with NLR?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-ab66a4a6ce0f4272ad8eed3351619c1b", "references": ["Who invited Washington to fight with him?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-b4afdf0e695c436fa403fe2a09c1fa03", "references": ["In what other way can disobedience be applied to international organizations and governments?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-aa4e4a77eb134c68980f4e6c17b380f5", "references": ["How many bits are typically used in the key exchange logarithms for for the Diffie-Hellman key exchange?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-43bbe4087f9b4aa28ef26911593abbfb", "references": ["What are antimicrobial peptides that evolved as immune defense in eukaryotes called?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-fdd14964dafb45fb8ceaff14af19ba49", "references": ["What type of venue is the Teatr Wielki?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-d33c75b0a42f4d52bd384a1539a92756", "references": ["Who added to Dioscorides' book in the Islamic Golden Age?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-739d477fb0a64bb4872c6087702a47b0", "references": ["Which Fresno district is the center for the LGBT community?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-5b314332221e4dd8a6e27d0ccf5a7346", "references": ["In addition to English, what language is also often taught in Nepalese private schools?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-4e5c7a3d7c3b405e90350cce1358eb2d", "references": ["What happened to the Bank of Italy Building?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-d69a5d40dbb84a7eb8d537113f5372fd", "references": ["Which group sets the time agenda?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-861cd4cd357641b4808c84b482bc61ab", "references": ["What was the network operated by the Duct PTT Telecom?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-95d037ceb55348e29c945aa06490e35e", "references": [" What did Mongols refuse to worship?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-9d84862bb3624cb188e48c6c8eeb5f15", "references": ["Who demonstrated how to create an infinite number from a Mersenne prime?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-3a3fb45dbddb453a848763b5df4ee02b", "references": ["What are government schools also called in India?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-b26528c2018e4d69a363a5c5ebfd588a", "references": ["How many different network technologies where there before 1973?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-61a88ba8187d4f7eb268d4430338cff2", "references": ["What would change the rotational inertia of a body under Newton's First Law of Motion?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-f5f3245d332e435e9c1ed6cfab39b3d6", "references": ["Who wrote the paper that the \"Millennial Northern Hemisphere temperature reconstruction\" graph was based on?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-319c14eae5b64628bbbbc0ad476725ef", "references": ["What was Berlin the most diverse of in Poland?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-33a3a9f41bb344008b26ed56604f4072", "references": ["During 2012-2013, how many student were able to take the Core classes at a single time?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-1f06a3209e434d7493d9c1bcc496cb8c", "references": ["Which country today is a remnant of the Ottoman empire?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-1a3f50d195ee49d3b77ccd8895182ff9", "references": ["What must be specified in order to account for their effects on the motion of the head?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-6329e5f356af491e93dbdcda50ec5a44", "references": ["Who is usually hired for a design-bid-build contract?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-e25fd65ed5c24b08a39e172698803ad5", "references": ["How many casualties did British get?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-28245f4d682a4693b85e0b4c39ccd99c", "references": ["WHat do x.25 and Frame Relay both require"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-3a1e5236804a49e8884f59f0bd233af7", "references": ["How was france the same as Britain in managing its colonies?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-ebed6887cd5f45a1a131f73e11aeb9c9", "references": ["Where are some physicians permitted to prescribe and give out medications within their practices?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-64fe6bebb28b4631974c247362df35f9", "references": ["Where was Shirley planning an expedition?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-971bdcf9cac54b08b843a36b2bda72bd", "references": ["What is not a condition that causes immunodeficiency?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-855147a8d20142aa868a248dcf5cc883", "references": ["What did Harvey Wheeler direct the Hungarians to do?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-0eb5504311614f829cb870c4d871da27", "references": ["What is the largest main branch of the Rhine?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-6c125a6fca1e4c718bc98a41e30724f2", "references": ["How many branches does the Zuider Zee brackish lagoon have?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-3e665cafa24e4ba484ef786d825f7e21", "references": ["What areas are pharmacy informatics prepared to work in?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-268905c7902b40999268c5727b0dbe53", "references": ["Which boulevard can you find many majestic homes in the area?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-c229f707f6724ace93a3bafd591c5c8c", "references": ["What do petrologists use electron microprobes in the laboratory for?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-1682989a580c4b06b9856e5e7fbab601", "references": ["Apart from its mathematics and postgraduate schools, what else doe the university run?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-18154808a23d450fa27a34b89650d780", "references": ["What did Guo Shoujing do for calendars?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-fda547293ce64319bec002427475d6d8", "references": ["Other than the steamboat, what modern form of travel brought visitors to Florida?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-ffe94429752341639d152bb46ff0b7c4", "references": ["What was the average family size with no wife present?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-afab0b04614e4b8db46114bea0cb3d90", "references": ["What are auricles?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-13c85cc25fa6439db1f3c80534bc4db0", "references": ["Whose theory was the theory of continental drift?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-2bddda2556dc42b0a915b200d70737dc", "references": ["What year was the first class taught at the University of Chicago?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-33723d2e9f8a4710920c2b0d32338a0e", "references": ["WHat does UserDatagram Protocol gaurentee"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-cd519a287754424ca67f58ec68590c97", "references": ["As of February 2011, how many numbers has Goldbach's conjecture been proven to?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-e774950940f2436495b59178a94a8cdb", "references": ["To whom did William Maclure submit the map?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-84493568fb704a51bb5ff21f2a076b5b", "references": ["What exchange in Warsaw is one of the most important for Central and Eastern Europe?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-9a9f12a1f678477e8c6d35d7326494e3", "references": ["The college's University is divided into how many divisions?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-176dfb185c1f4f7486b52522710439be", "references": ["What is the fastest growing area in the pharmaceutical industry?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-5697e3f2958a440fa49ef5e9a7b6d61b", "references": ["What was Tugh's Chinese-style name?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-b319620ac1434b9685db5eaeee03bd28", "references": ["What event does the Oxford-Cambridge rivalry culminate in?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-ed6be43194064c838a869254fd198215", "references": ["Gamma delta T cells rearrange TCR genes to produce what?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-5ce4c172fe854c5e9ff22f053a0e3bfd", "references": ["What does not compete with commensal flora for food and space?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-264680c2b3ac447cbbec7cc8d9033337", "references": ["What evolved in later vertebrates?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-e94866e9c680458085137a1247116e2c", "references": ["How is X.25 connection-oriented at layer two?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-1fd8276f008448d38b272ba4e6e40130", "references": [" Germany doesn't have an imperialistic future until when?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-9fc05f5b30224c4fb02af6f1b10d69c4", "references": ["What was the attack on the British weakness?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-6bb920756f7d4598a718a9000fd6bd16", "references": ["A decline in Scottish nationalism during the 1960s fueled what?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-94b67fb500c8430896c25c361e3e1c71", "references": ["The Maroons are apart of what association?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-d5a1a7049e5a41e089d62a5701ed4ace", "references": ["What university was John of London an alum of?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-48928bc35fac4a21a1332fd76cc35a85", "references": ["What is the largest city directly linked to an interstate?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-56655d09fba1475db99e4bfc3a71287a", "references": ["The \"It's Scotland's oil\" campaign resulted from the discovery of what in the South Sea?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-989e0aa693cd40cb99278fd5dc94ab30", "references": ["How much land does Cambridge own in Allston?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-bf913226234b40bc9e899359437547f5", "references": ["Ctenophores are less complex than what other group?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-1ea90eab57f64e4f817af6302bf52ebb", "references": ["Who did the Mongols give control of Korea?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-b950a23715d048428740f4d174f7bb20", "references": [" Which country besides the Cuba did the United states not try to annex in 1898?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-7f8dfa72145e475d956b4fc0a923e9ca", "references": ["What geometric shape is used in equations to determine net force?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-8ed4a6de7d8d4ec5b3ba1bbab0e89cb7", "references": ["When did England formally declare war on France?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-2a3bcdd7e3574e8f8123d36e73899cca", "references": ["What did Kublai's government have to balance between?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-f41d5e261c0f49e3b191acbe4affa0ee", "references": ["In which direction does the west side of Fresno neighborhood lie to the 99 freeway?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-534c11e4a88d4ff39ee7e1af565e2591", "references": ["What index is an indicator of the effects of taxes applied to social spending?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-19c49bea16604800ac11434b16b05aff", "references": ["Who discovered pottery found on Black Hammock Island?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-38ae2451f1f545d080add0f7b58ed85d", "references": ["What is subtracted to its constituency seats?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-2d94736fc0ff4318b69f7c8d092b6ade", "references": ["How long is Olive Avenue?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-6041b9149edb4312b5632a9148d34e0d", "references": ["A higher capacity network was formed by what project? "], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-6af89910a48a4d0fa991472107fd86a2", "references": ["What has a bigger impact on the United States' economy more than trade?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-f52bf8040ed542daa5c72f39c00205f6", "references": ["Do adults or juveniles secretions luminesce brighter?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-52f31d0f6b244035b118994e83979874", "references": ["What has happened to the the rock in the Grand Canyon that is the oldest rock in the world?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-7ee282066fdf409e86279e2f34556679", "references": ["What do engineers offer themselves as for a project?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-08afb73e9de7401f95c365672013bb5f", "references": ["Where is the Santa Fe Railroad Depot located?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-09a782595177464d978583c4caccd3f3", "references": ["What area has become attractive for restaurants?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1195-f580103e5f944eb990b211e82c7826da", "references": ["According to Ellen Churchill Semple what type of climate was unnecessary for humans to become fully human?"], "task_id": "task1195_disflqa_disfluent_to_fluent_conversion", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1409-fc6edfc5959e4cdc872168a4dcc96243", "references": ["For cheap French food go to The Waterman in the city centre. They have a 5 out of 5 rating and are not family-friendly.", "The Waterman in the city centre offers cheap French food. It is not family-friendly and has a customer rating of 5 out of 5."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-422ab81d19cf4c66880b08bd223ae998", "references": ["Ayam penyet is a dish from the region of Singapore and Indonesia. The main ingredients of Ayam penyet are \"squeezed\" or \"smashed\" chicken served with sambal. It also has fried chicken in it.", "The fried chicken dish Ayam penyet hails from Indonesia and Singapore. The chicken is \"squeezed\" or \"smashed\" and served with sambal.", "The fried chicken dish Ayam penyet is a dish from Indonesia and Singapore. The chicken is \"squeezed\" or \"smashed\" and served with sambal."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-e030de93a5ff4f49bb57afc9cf06fc60", "references": ["The ratings for episode 11 was 15.6."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-22bf974870534e04ad5b32eafe2cb73e", "references": ["A setter on the Estonia men's national volleyball team weighs 81"], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-37c3b1afd33249e4990dbfa701b7432d", "references": ["The Punter is a Japanese restaurant with a moderate price range"], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-e09244c1c2344a94ac02783bb72711b3", "references": ["Try Browns Cambridge located near the Crown Plaza Hotel. It is a family friendly, Italian restaurant and coffee shop with a low customer rating."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-be13e45f015a4769bb0c0f07159174a5", "references": ["A pub named Clowns has an average customer rating and a high price range."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-96a90d8b98f444df99fd182b34a5205d", "references": ["There are 20,118 active unaffiliated voters and 7062 inactive ones in Stamford Connecticut."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-cd9f2739771e4987aa6fa97ccca19303", "references": ["The winning party in the Pennsylvania 6 district's democratic."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-a3a7f861ee1a4bfb80ec1784ad9b09c0", "references": ["Aromi is a three star coffee shop. It is family friendly."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-81673b0a54664ebda90a3ddbec13ad8d", "references": ["In 1987 season, Chicago Cubs won by Sanderson (5-6) on August 9 and the opponent team was @ Mets."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-0a2ff36a435f4dac8e08c3b66b1d15a5", "references": ["The real estate agent from is salt lake city, utah."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-f0d2259d9c634eb8b12ad46910772bf5", "references": ["In the high price range, The Punter is a coffee shop offering Indian food. It is not children friendly and has a customer rating of 1 out of 5. It is located near the Caf\u00e9 Sicilia.", "There is a coffee shop near Caf\u00e9 Sicilia, The Punter, that serves expensive Indian food and has customer rating 1 out of 5. It's not children friendly.", "The Punter is an expensive Indian coffee shop near Caf\u00e9 Sicilia that is not children friendly. Their customers have rated 1 out of 5.", "Located near the Caf\u00e9 Sicilia, The Punter coffee shop offers Indian food in the high price range. It is not children friendly and has a customer rating of 1 out of 5."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-1f55bee51d5448cab88390e49c90c325", "references": ["Manhattan is part of New York City, which was part of New Netherland, and is the location of Asser Levy Public Baths.", "The Asser Levy Public Baths are in New York City, Manhattan ( a part of New Netherland).", "The Asser Levy Public Baths in Manhattan, New York City are part of New Netherland."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-478199111efa4c47aa53dfa6d2dc4a9f", "references": ["Balder is a comic character created by Stan Lee and the American, Jack Kirby. His alternative name is Balder Odinson.", "The comic character of Balder is also known as \"Balder Odinson\". It was created by Stan Lee and the American, Jack Kirby.", "The comic character, Balder has the alternative name of Balder Odinson . It was created by Stan Lee and the American, Jack Kirby."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-65b19c9d0b834770baecac210bbef835", "references": ["The Golden Curry serves Chinese in riverside near Caf\u00e9 Rouge. While not family friendly it has a customer rating of 5 out of 5.", "The Golden Curry is situated near Caf\u00e9 Rouge and is near the riverside. It is not family friendly and has a customer rating of 5 out of 5. It sells Chinese food.", "The Golden Curry provides Chinese food. It has a 5 out of 5 customer rating. It is in the riverside area near Caf\u00e9 Rouge and is not family friendly."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-caf71d305d58409fb733eb474cf77a92", "references": ["In 1996-97 FA Cup, Chesterfield at home played against Nottingham Forest with score 1-0"], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-a891ea7dc9df4a218b664e6b9a2068c2", "references": ["Jamie McMurray won november 2 the race."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-8174c984717048c8991b61ca64ac79a8", "references": ["On September 15, 1982, 18-year-old Mary Bridget Meehan went missing."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-ba6c7268eca147178460ce3d9c89c758", "references": ["Near to the Rainbow Vegetarian Caf\u00e9 there is a Japanese restaurant called Strada which is child friendly with a high customer rating of 3 out of 5 and a average price range of food."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-85c947bdc78746cb9c7b69c2349f4d54", "references": ["W 24-21 was the result of the week 3 game."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-1da5f3967a9748138e77d13a925615b7", "references": ["The Chicago Road Informational Designation is located in the Bronson Township."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-9297769d1c924cde83132205f1d36628", "references": ["Zizzi is an adult-only 1 starred pub serving traditional British food", "Zizzi is a 1 starred pub, serving British food. Not family friendly"], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-2056444d5ae9449d996222340c2f8b27", "references": ["Chicken is an ingredient in Batchoy which is eaten in the Philippines.", "Chicken is an ingredient in Batchoy which comes from the Philippines.", "Batchoy is eaten in the Philippines, is made with chicken."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-83f99ab0c36247458d1e0e5204b31f48", "references": ["Asam pedas is a food found in Malaysia, where the capital is Putrajaya and where Arifin Zakaria is the leader.", "Asam pedas is a food found in Malaysia, which leader is Arifin Zakaria and its capital is Putrajaya."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-3ffbe7696ea44995bb4b9bbe47cf0e5e", "references": ["Alan Bean was born on the 15th of March 1932 and selected to be part of NASA's Apollo 12 mission in 1963. Apollo 12 was commanded by David Scott and included backup pilot Alfred Worden.", "Alan Bean (born on March 15, 1932) was chosen by NASA in 1963 to be crew member of Apollo 12 and Alfred Worden as a backup pilot.The operator of Apollo 12 was NASA under command of David Scott.", "Alan Bean was born on the 15th of March 1932. He joined NASA in 1963 and became a crew member on the Apollo 12 flight mission along with Alfred Worden as backup pilot and David Scott as commander."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-b40d22f8f6e04882a4945e517a8167fa", "references": ["The Waterman offers English food in \u00a320-25. It is located at riverside. Although it is not kids friendly, yet it is rated high, by the customers.", "The Waterman, with a price range of \u00a320-25, is not kid friendly, is near the riverside, serves English food and has a high customer rating.", "The Waterman is not a child friendly English food place, which is by the riverside has a high customer rating and a price range of \u00a320-25.", "The Waterman at riverside provides English food in the price range \u00a320-25. Its customer rating is high, however it is not kids friendly."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-8bcaa9d028d04a128518251186b05241", "references": ["The record when the Bruins had 41 points was 2-0-0."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-216a4e56737c4fe7a6b6dcbf0887ed68", "references": ["Fitzbillies is an Indian coffee shop with a moderate price range, located in riverside. The customer ratings are 1 out of 5 and is kid friendly.", "The moderately priced Indian coffee shop, Fitzbillies, is located in riverside. It has a rating of 1 out of 5 and it kid friendly."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-695a0814e9a04df7b1e33648597f976a", "references": ["Pork belly is one of the ingredients of the dish Bandeja paisa. This is a traditional dish from the Paisa region and is part of Columbian cuisine.", "Bandeja paisa, containing pork belly, is a traditional Columbian dish from the Paisa region.", "Bandeja paisa is a traditional Columbian dish containing pork belly from the Paisa region."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-838ff94eb2594f1a832b8af91798af65", "references": ["Cocum is a child-friendly pub.   The prices are above average and it has a low customer rating."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-6c6b8ee81eae4bdca2c3844d38cd8d57", "references": ["The order number where the original artist was The Doors was n / a."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-1923890b24d14fd1818fe703c92391b6", "references": ["Wildwood is a pub that serves sushi.  They are high priced and are rated 3 out of 5."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-87539532b2444a0eb988cb28fce214a0", "references": ["The Wildwood is a French coffee shop near the Ranch. It has a high customer rating in the average price range."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-e980605c3a7b45849aef946cf01c765c", "references": ["Finland lost five of its matches and came in last place amongst the 7 teams."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-c6da9cb4eb884433b1384900ddd29a22", "references": ["The album Love Session by Silk was released in 1999."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-4be7023024fe4eeb901eca91900bcd9a", "references": ["Cocum is a coffee shop that offers a family friendly environment and 5 star quality."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-9051095002ef4b26a90fe99dc0cc7d8d", "references": ["The Iowa Hawkeyes lost (7-24) the 1981 game against Illinois at Memorial Stadium, Champaign, IL."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-26c1d40c7bac4e9ca755003cc583d39f", "references": ["Dell Curry played for Toronto during 1999-2002."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-3ff2279201eb41b4b33bba7d49c842de", "references": ["A family-friendly coffee shop serving Italian food with an average customer rating is Browns Cambridge near the Crowne Plaza Hotel.", "Browns Cambridge near Crowne Plaza Hotel is a family-friendly coffee shop that serves Italian food and has an average customer rating."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-3e3407eb1b3c42edadfc861859ae772a", "references": ["Previously owning club Varese Calcio S.S.D., Rolando Maran now manages club Associazione Calcio Chievo Verona.", "Rolando Moran manages A.C. Chievo and is in Varese Calcio S.S.D. club.", "The manager of A.C. Chievo Verona is Rolando Maran who used to be at Varese Calcio S.S.D."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-07c9c18a6ca341b697a7d585e6bd7e40", "references": ["Washington National's Nolasco seized the win against the Marlins on July 2nd."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-f2246caa61784aca82825656980d4881", "references": ["In the city centre near to The Sorrento is a Chinese called Browns Cambridge. It is very family friendly."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-aa6602e6c34d44988e6a7158edc7701a", "references": ["The Wrestlers provides English food for \u00a320-\u00a325. It is child friendly and has high customer rating."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-e541c3668f2943bc84a25c171de13e48", "references": ["The Green Man is located on the riverside near All Bar One.  It serves cheap, family friendly English food.", "Green Man is family friendly and is located in the riverside area near All Bar One.  It serves English food and has a cheap price range.", "The Green Man in the riverside area is near the All Bar One. It is family friendly, serves English food and the price range is cheap", "The Green Man is family friendly. It is in the riverside area near the All Bar One. It serves English food and the price range is cheap.", "Green Man serves English food, has a cheap price range, and is family friendly.  It is located in the riverside area near All Bar One.", "The family friendly Green Man serves cheap price English food and is located near All Bar One by the riverside.", "The Green Man offers cheap, family friendly English food.  It is located near All Bar One on the riverside.", "The Green Man is a family friendly establishment serving English food at cheap prices.  It is located at the riverside near All Bar One."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-6e8a7de88e7842bfa250a170bbaf8ba7", "references": ["An alternative name for the comic character Bolt is Larry Bolatinsky.", "Larry Bolatinsky is the alternative name for the comic character, Bolt."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-1d431678636c42689673ab3a5cc610d8", "references": ["The Cambridge Blue is a pub near Caf\u00e9 Brazil which serves high price Indian food.", "Located near Caf\u00e9 Brazil there is a high price range Indian pub called The Cambridge Blue.", "Near Caf\u00e9 Brazil is a pub that serves expensive Indian food, called The Cambridge Blue.", "The Cambridge Blue is a Indian pub near Caf\u00e9 Brazil, with a high price range.", "The Cambridge Blue is a high priced pub that serves Indian food near Caf\u00e9 Brazil.", "The Cambridge Blue is a pub that serves Indian food in a high price range. It is located near Caf\u00e9 Brazil.", "Near Caf\u00e9 Brazil there is a pub Called The Cambridge Blue. Its provides Indian food but it has a high price range."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-f90483d0f26a4e888588d76feefee2ee", "references": ["In 2004, Mauli Dave was a dance contestant on the show Boogie Woogie-Chalo America."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-c4399c66944c4521b9dbbb2747d91da2", "references": ["A Japanese pub with a price range less than \u00a320 is The Cambridge Blue which is around Caf\u00e9 Brazil.", "The Cambridge Blue is a pub near Caf\u00e9 Brazil with Japanese food that is less than \u00a320.", "There is cheap, Japanese pub near Caf\u00e9 Brazil named The Cambridge Blue.", "The Cambridge Blue is a Japanese pub located near Caf\u00e9 Brazil that serves great food for low prices of less than twenty dollars.", "The Cambridge Blue is a pub serving Japanese food. It is located near Caf\u00e9 Brazil with a price range less than \u00a320.", "The Cambridge Blue, located near Caf\u00e9 Brazil, is a pub with prices ranging less than \u00a320. It serves Japanese food.", "The Cambridge Blue is a cheap, Japanese pub located near Caf\u00e9 Brazil.", "Near Caf\u00e9 Brazil is The Cambridge Blue which is a Japanese pub with a price range less than \u00a320.", "The Cambridge Blue pub, cheap Japanese food, close to Caf\u00e9 Brazil"], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-849e10740df342eb9f7be724035d2ac3", "references": ["The Dumpling Tree Pub would like to welcome you and your family to try our Indian cuisine. We are near The Portland Arms", "The Dumpling Tree is a pub that provides Indian food. It is located near The Portland Arms.", "The Dumpling Tree is a pub that provides Indian food It is near The Portland Arms.", "Welcome to The Dumpling Tree Pub where you can bring your children to try our Indian cuisine. We are near The Portland Arms."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-024f908c946243b4a7a1a25a457e3885", "references": ["The Standard reference for nbsp is HTML 3.2"], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-7be636413e4542d7b51b38b1a6f4432a", "references": ["The United States Air Force, which fought in the Korean War, operates the Al Asad airbase. Among the USAF attack aircraft are the Lockheed AC-130, which also serves as a transport plane , and the General Dynamics F-16 Fighting Falcon.", "Al Asad airbase is operated by the United States Air Force who fought in the Korean War. They deploy the Lockheed AC-130 on their aircraft carriers, the Lockheed C-130 Hercules as a transport aircraft and the General Dynamics F-16 Fighting Falcon as a fighter."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-9439c5d2bafe46bd84f22f7d737179da", "references": ["The episode with the no. 54, directed by Charles Haid, is written by sean jablonski & brad falchuk."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-6ea1ccb5e2ac43169947d0b3668fe2ac", "references": ["The Cricketers coffee shop is a family friendly establishment offering a five star rating with good quality food, situated near The Portland Arms"], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-8e07437eb74b422588dc12393ea70768", "references": ["The Cricketers is an Indian coffee shop, it has average ratings, but it can accommodate kids. It is near The Portland Arms.", "The Cricketers is a coffee shop providing Indian food It is near The Portland Arms. Its customer rating is average.", "The Cricketers is a coffee shop providing Indian food It is near The Portland Arms. Its customer rating is average.", "The Cricketers is a coffee shop providing Indian food It is near The Portland Arms. Its customer rating is average."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-3d6b16e945c2423495e50b01e1ab1ab6", "references": ["Larry Perkins and Tomas Mezera from team Perkins Engineering use car Holden VL Commodore SS Group A SV"], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-6eea7e195c824fbdb2f8dc947d81f4da", "references": ["The Rice Boat provides Indian food which costs less than \u00a320. It has a customer rating of low. It is located near the riverside. It is not family friendly. It is near the Express by Holiday Inn.", "The Rice Boat serves Indian cuisine at prices under \u00a320. Customer rating is low. The Rice Boat is located near the riverside and Express by Holiday Inn. Sorry, no family's are allowed.", "The Rice Boat is a place with cheap Indian food located at the riverside near the Express by Holiday Inn. It is not family friendly and is rated low by customers.", "At the riverside near the Express by Holiday Inn is a cheap, lowly rated Indian food place called The Rice Boat. It is not family friendly."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-2e3f91142deb417e9b97395bc19b2e04", "references": ["Tapioca is an ingredient in bakso which is made in China.", "Bakso is from the chinese cuisine and contains tapioca."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-6e4e1e33787b4c3fa384d61051cc9b61", "references": ["The New Model Police Revolver was manufactured from the years 1865-1873"], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-1d57b85043f948b1a696ddfe8fdd459d", "references": ["korea republic member association has 14 clubs"], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-00139a31bb004b2cae11cce11d704ad6", "references": ["The Cambridge Blue is a three star restaurant located in the middle of nowhere", "The Cambridge Blue is a three star rated restaurant that sells sushi.", "There is a three star rated restaurant called The Cambridge Blue that sells sushi.", "The Cambridge Blue is a three star restaurant located in the middle of nowhere", "The average customer rated restaurant is The Cambridge Blue.", "The Cambridge Blue restaurant has been rated 3 out of 5 stars. It offers wine, spirits, and appetizers.", "The Cambridge Blue is a 3 star restaurant that offers a selection of wine, spirits and appetizers.", "The Cambridge Blue restaurant, food and drink. Rated three stars."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-834a4858278b4f78b51935484a8316f8", "references": ["The Golden Curry offers  Italian food. It is family-friendly and customers rated it average. It is located near Caf\u00e9 Rouge in city centre."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-d5638a227cbd40ed9ecb9c9f86bceff9", "references": ["If you're looking for a cheap, family-friendly Fast food coffee shop, with an excellent customer rating of 5 out of 5, then go to The Eagle located near Burger King in the city centre.", "In the city centre near Burger King there's a cheap fast food coffee shop named The Eagle.  It is family-friendly with a customer rating of 5 out of 5.", "The Eagle is a cheap family-friendly fast food coffee shop in the city centre near Burger King.  They are family-friendly with a 5 out of 5 customer rating."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-b1f08d990ddb45e7adc4fd27a4eb2800", "references": ["Dick Campbell had 15 wins."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-b6aa8e467cfc482fab8e66057b9131a7", "references": ["The value of Bada 0.05% was on july - 13."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-448d142519834d36832811300ee30f47", "references": ["Akita Museum of Art is located in Akita, Akita which is part of Akita Prefecture, Japan.", "The Akita Museum of Art is located in Akita, Akita Prefecture in Japan."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-daa492d2a9e444ce9eb939eef335a579", "references": ["The AWH Engineering College in Kuttikkattoor, Kerala, India was established in 2001 and it has 250 academic staff.", "The AWH Engineering College in Kuttikkattoor, Kerala India was established in 2001 and has a staff compliment of 250.", "AWH Engineering College is located in Kuttikkattoor city, kerala state in India. It was established in 2001 and has an academic staff size of 250."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-8e3054d4a8694a29bdac349f5392406f", "references": ["Loch Fyne provides Chinese food It is located in the riverside. It is near The Rice Boat. Its customer rating is 3 out of 5.", "Loch Fyne provides Chinese food It is located in the riverside. It is near The Rice Boat. Its customer rating is 3 out of 5.", "Loch Fyne serves Chinese food, 3 out of 5 people like it, its in riverside, near The Rice Boat.", "Loch Fyne provides Chinese food It is located in the riverside. It is near The Rice Boat. Its customer rating is 3 out of 5."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-63deaa57b5fc459b87ebacf88324e2c7", "references": ["Jordan was the cover model of Playboy when the centerfold model was Shallan Meiers."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-a9473bb108564dd9be6879bfba95d171", "references": ["In the 1991 NCAA Division I men's soccer First-Team All-America teams, Alexi Lalas from Rutgers, Mike Lapper from UCLA, and Cam Rast, Santa Clara were selected for defenders while Gerell Elliott from Fresno State, Henry Gutierrez from NC State, Manny Lagos from Milwaukee, and Dante Washington from Radford were selected for forwards."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-8d93a2eaa26246b084879277089d448b", "references": ["The Mill is a high-end Italian pub in riverside.", "There is an above average pub serving Italian food in the riverside area called The Mill", "An Italian pub called The Mill is in riverside and costs a moderate amount.", "The Mill which is Italian pub that costs a moderate amount is in riverside.", "Italian food is served at moderates prices in the The Mill Pub by the riverside.", "If you are looking for Italian food priced around \u00a330, or a bit more, down by the riverside, I would recommend The Mill. It has a pub atmosphere, so it is a relaxing place to be and eat.", "The Mill a low budget pub on a riverside serving Italian food.", "The Mill is a pub in the riverside area that serves Italian food", "The Mill pub in Riverside serves excellent Italian food but is quite costly.", "The Mill is a pub providing upscale Italian fare.  Located on the riverside it provides views that are worth the upscale price of the food."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-557d090292b1448d8d5c89e20a4d9240", "references": ["With a customer rating of 3 out of 5, The Wrestlers is a French, kid friendly, restaurant with prices ranging \u00a320-25.", "The Wrestlers is a kid friendly French restaurant with a price range of \u00a320-25 and a customer rating of 3 out of 5.", "The Wrestlers is a French, kid friendly restaurant, with prices from \u00a320-25 and a customer rating of 3 out of 5.", "With a price range of \u00a320-25, The Wrestlers is a kid friendly French restaurant with a customer rating of 3 out of 5."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-76c48ff8265643ffa00dff3e924201c5", "references": ["Why Can't This Be Love, peaked at 3rd."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-5c18122cdbec42b093f9ef4aef79fdd8", "references": ["The webkit version when the minor version was 3.1.2 is 525.21.."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-88d56296e8e84c24a7ee513299b8d1ef", "references": ["Poorly rated Fast food store in riverside is The Rice Boat.", "The Rice Boat, located in the riverside area, has a low customer rating but is adult oriented and serves fast food."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-1b57d43a09b64466b5ffc31d6b848e57", "references": ["The region 4 (Aus) date associated with a region 1 (US) date of January 16, 2007 is april 1, 2010."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-6f05c0d9879e416fae92347d52d81c37", "references": ["1634 The Ram Rebellion was written in the U.S. where Native Americans live and Barack Obama is the leader.", "1634 The Ram Rebellion comes from the United States where Barack Obama is president and Native Americans are one of the ethnic groups.", "1634 The Ram Rebellion comes from the United States where the leader is Barack Obama and the Native Americans are an ethnic group."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-39ecf0e3f0b14895b050d15fc573b6a2", "references": ["Curitiba is located in Brazil.", "Curitiba is in Brazil."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-88d1a4366dc644c49c88fa8123897ceb", "references": ["The Italian restaurant, The Rice Boat, is kid friendly and has a high customer rating. It is located in the riverside area.", "Situated by the riverside The Rice Boat is a child friendly restaurant that serves Italian food and has a high customer rating.", "The Rice Boat is a kid friendly Italian restaurant in the riverside area with a high customer rating.", "The Italian food restaurant located in Riverside called The Rice Boat is children friendly and because of that they have very high customer ratings.", "The Rice Boat is a kid friendly restaurant in the riverside area that serves Italian food and has a high customer rating.", "The Rice Boat is an Italian restaurant located by the riverside. It has a high customer rating ans is child friendly."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-bd93901079454c978fd2fc5f89aa3728", "references": ["The Waterman provides a French menu in the riverside area, but has a low customer rating, a high price range and is not child friendly.", "The Waterman serves French food costing more than \u00a330. It is in the riverside area. It is not children friendly and has a low customer rating.", "In the riverside area there is a place called The Waterman which serves French food costing more than \u00a330. It is not children friendly and has a low customer rating."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-9f92e53b7b264f6ca3ea3aa107f506c3", "references": ["In city center, The Waterman providing Indian food in less then \u00a320. Customer rating is low and marked as no family friendly."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-13c07c8a2dc84a35be5a33481c6f7ddc", "references": ["The Eagle is a Japanese coffee shop located near Burger King, in the riverside area. It is not family-friendly and it is rated and priced averagely."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-a0ce01a5508e4869beeaa86e40fc034f", "references": ["Zizzi public house serves British food, is family friendly and has 5 out of 5 stars.", "Customers have rated Zizzi public house 5 out of 5.  It serves British food in a family friendly environment."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-b60e05edaa0e4d1c864e1295bf294856", "references": ["The Cricketers is a fast food, coffee shop, near The Portland Arms,  Kid Friendly, and customers give it a 3 out of 5", "The Cricketers, Ever see a fast food, coffee shop. Kid friendly, Near The Portland Arms, With a rating of 3 out of 5.", "Near The Portland Arms, there is a coffee shop named  The Cricketers with a Fast food type of food,  Customer rating of 3 out of 5  and  it is kid-friendly.", "The Cricketers is a type of coffee shop that serves Fast food, kid friendly and has a Customer rating of 3 out of 5, and also near from the famous The Portland Arms."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-3de3a67ea86a45ab9e03b507c167c9ea", "references": ["Now retired, Alan Bean was born on Mar 15, 1932 and hired by NASA in 1963.", "Alan Bean, who was born on 15th of March 1932 and selected by NASA in 1963, is now retired.", "Alan Bean (born on 1932-03-15) was selected by NASA in 1963 and is now retired.", "Alan Bean(born on 1932-03-15) was selected by NASA in 1963 and now is retired."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-e2370b50a2f242d5aa114fa7c7e93c02", "references": ["Cotto is a average rated family friendly Chinese restaurant near Ranch."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-820dc098591b4fa382c456f9f8e15e3f", "references": ["Child friendly pub Zizzi has a high customer rating."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-d8a82a2ab4da494787635d3a79df71eb", "references": ["Alimentum is a restaurant providing family services in the high price range. It is located in the north of city ."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-b07a23d86028481ea75ede39384f928b", "references": ["The 7th president of the US, Andrew Jackson, was from the Democratic Party."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-693db24051ef4e5b8eec491a6a357513", "references": ["Al Asad airbase is operated by the United States Air Force who were involved in the invasion of Panama. They deploy the Lockheed AC-130 on their aircraft carriers, use the Boeing C-17 Globemaster III as transport aircraft, and the McDonnell Douglas F-15 Eagle aircraft fighter."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-8b8bcda0521843fbb4502372cc1154bb", "references": ["Alcatraz Versus the Evil Librarians is from The United States where the English language is spoken and one of the ethnic groups are the Native Americans.", "The US is home to Native Americans and the origin place of Alcatraz Versus the Evil Librarians.", "Alcatraz Versus the Evil Librarians is from The United States where one of the ethnic groups are the Native Americans and the English language is spoken."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-11061972421d4f739794dbe9f9e9fd1a", "references": ["The Golden Curry is a non friendly, English serving restaurant. It has a low rating, but is located in the center of the city near Caf\u00e9 Rouge."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-207175ad8e5c4ac68ffd4de52d55cfe6", "references": ["Jay-Z was a guest star at the Suicide Tour at the Old Trafford Cricket Ground in September 2009."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-110a9717a9534ce6b153e6e1dd52420c", "references": ["A high priced Italian food place, named The Wrestlers, is children friendly and has a customer rating of 1 out of 5.", "The Wrestlers serves expensive Italian food. It is family friendly but poorly rated."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-47d06da12e654e289fd4cad8e826c175", "references": ["The Phoenix has French food at a moderate price in the riverside area with a customer rating of 1 out of 5."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-21618a4bea054393b1531139954b0da6", "references": ["Jupiter (F85) was completed on 25 June 1939."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-0c928da3beb74a3a8197c803c2d8921c", "references": ["The Al Asad Airbase is situated in the Al Anbar Province, Iraq.", "Al Asad airbase is located at Al Anbar Province, Iraq.", "Al Asad Airbase is located in Al Anbar Province, Iraq."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-2fe583de43094fe782e00607b73653e4", "references": ["The leader of Catalonia is Carles Puigdemont.", "Carles Puigdemont is a leader in Catalonia.", "Carles Puigdemont is the leader of Catalonia."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-26b34bb4763047ad95f3121a9b456c55", "references": ["A.F.C. Blackpool's has 1500 members and their ground is The Mechanics. A.F.C. Blackpool's full name is \"Association Football Club Blackpool\" and they played in season 2014.", "A.F.C. Blackpool, full name Association Football Club Blackpool, plays at the Mechanics and has 1500 members. They competed in the 2014 season."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1409-225a6997be374f41a9741df5f7f12dfe", "references": ["Browns Cambridge is our new kid friendly, Italian coffee shop located near Crowne Plaza Hotel in our beautiful riverside community. With high customer ratings this place is sure to be a favorite.", "There is a coffee shop that is children friendly in riverside. The name is Browns Cambridge. Their customer rating is high and they serve Italian food. They are near the Crowne Plaza Hotel.", "Browns Cambridge is a riverside coffee shop, near the Crowne Plaza Hotel. It serves Italian food, is highly rated and child friendly.", "Come visit our new, kid friendly Italian coffee shop Browns Cambridge. Located in the riverside area near Crowne Plaza Hotel. High customer ratings will make this a favorite.", "There is a coffee shop that is children friendly in riverside. The name is Browns Cambridge. Their customer rating is high and they serve Italian food. They are near the Crowne Plaza Hotel.", "Near the riverside area is a place that serves Italian food. It is by the Crowne Plaza Hotel. It has a high rating and there is a coffee shop. It is called the Browns Cambridge and yes, it is kid friendly.", "Near the Crowne Plaza Hotel by the riverside, there is a highly rated coffee shop called Browns Cambridge which is child friendly and serves Italian food.", "Near the riverside, is a place that serves Italian food. It is near the Crowne Plaza Hotel. It has a high rating. There is also a coffee shop called Browns Cambridge. Yes, it is kid friendly.", "There is a child friendly Italian highly rated coffee shop in the riverside area near Crowne Plaza Hotel.  It is named Browns Cambridge.", "In riverside - near the Crowne Plaza Hotel - there is an Italian coffee shop named Browns Cambridge. It has a high customer rating and is kid-friendly.", "There is a child friendly Italian highly rated coffee shop in the riverside area near Crowne Plaza Hotel.  It is called Browns Cambridge."], "task_id": "task1409_dart_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task033-9ac98f6efd4c45108f0e6b3d601f7615", "references": ["unicycle"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-ff85ac1c019b4235875fee2794a4198a", "references": ["salad"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-3427ab532cc94359a4247ad1f49259df", "references": ["fuel"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-3aabbeab6051480d842081250680c6ca", "references": ["fox"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-78652618700e4138be18bc525db8e621", "references": ["dress"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-d0a533ef719b486e85772e3c7ef3be43", "references": ["lease"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-99b3e1166dcd4d56ac0528a40bf8f793", "references": ["remedy"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-d1ff6fe6bb6d404aa75f65aac7c05d74", "references": ["field"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-3e94063eb607499899dfef5b145d0f4c", "references": ["orders"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-1bc09ea8a00c4d659cc2ee3f62f655c8", "references": ["mountain"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-012a5eb93378490aa3907b878d69d190", "references": ["compact discs"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-de16d8498fc245d9b0c51027eb859f38", "references": ["happy"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-7e45c371009546038131a6a1c2d25714", "references": ["candy"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-387a1d78a39942f894c1d7e8d292ee52", "references": ["eyelash"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-9ee30d85aac7499baa51fd924f8698d5", "references": ["mantle"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-542ff5f6d8a24ab59451422415fc8176", "references": ["store"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-e09782c9c0554244969779628a056cbc", "references": ["basket"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-98bb25eb3f0e4c36b9707b0d7e1f2ced", "references": ["bag"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-f5153d3a5d644f76bb818da29b0e104c", "references": ["purple finch"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-fd524c50837942b888dd0c6185271753", "references": ["exercise"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-4fe54fcf743841e5938e4a8e7e709da4", "references": ["budget"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-15333cb838e7455e826f518cef1c373a", "references": ["pollen"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-274145263f2140a28be636a4a0c509fb", "references": ["play"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-03f715eaf9a941c48b38f1cdc250c249", "references": ["canola"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-c908f4ce65d64d6a8dfb48155f2c0472", "references": ["beer"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-f7815ba12168442da60d033265740934", "references": ["spring"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-a9f5e250080e4910b14389bb25fb585d", "references": ["computer"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-e5c740a96b71427cb21f8c0f05760a2d", "references": ["handle bar"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-673a16e3be894216a227f151bdd4fbcf", "references": ["Chevy"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-d8af8d805a274ef2b313f75bd96798d8", "references": ["appendix"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-dca986680ae2433fbd5a1317ee4e54be", "references": ["scale"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-4ff0a2b5dc104c798020968f5c2a4987", "references": ["trial"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-9a362189d78b4e0080f5925364d0b957", "references": ["fortress"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-6ca3f442b55642208b46a1f4d8b037d8", "references": ["bedroom"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-251861ddb21f4071b70e0434c63215f0", "references": ["convertible"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-5cfd198505844f978e7f03bbdf61455b", "references": ["train"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-4a2f5bac74dc4640941256d28b5d9403", "references": ["pie"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-91cd67775c0a42e397fd17df4e3bf202", "references": ["cloth"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-6943b6785bd14f6bba6ce01ec83afe96", "references": ["tube"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-024f9418a5fa4184a5d5745bf2e0a2dc", "references": ["cream"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-3b328d2808a644cfa3e3d4ff3633cd3b", "references": ["snow"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-12e2a9f3e0f347709fea11b4bf2c0a90", "references": ["zuchinnis"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-813189a12b5b40c288b2fb306bc2d035", "references": ["certificate"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-24860e02b0e847b2b5d09c4deb7bedad", "references": ["needle"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-4dfb4bdb2ae645088305d2abd46f4616", "references": ["watches"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-95ea84cb0f134ccfaea19555019e6f37", "references": ["coffee"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-a46b59e47389473a8386f46d60810b61", "references": ["tarantula"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-3003768e4c0946bbbf897d04a93014ad", "references": ["yellow"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-fa6406aaa9374c499f5f362207eede8b", "references": ["floor"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-6a1d77bcbc8b41da8224b0d12c98aff8", "references": ["soda"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-9c70e4c9b96e4bb4b7069cb16ba173a8", "references": ["cafe"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-a4f8e5bf27124f729ae978a5f2a309e7", "references": ["shoulder"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-63c36f6e47ab49699e3c10e771483b88", "references": ["knife"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-95744b405ae74bd2ba1195d699f4824f", "references": ["credit card"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-3c6a29cd2c944282901cdb177dd83251", "references": ["truck"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-98d0722e0512457e9cbec283bbc2355e", "references": ["violence"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-3ae7744a6d644fe68595ee08e1815ef3", "references": ["belly"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-2b5d0a9dea3c43e48121a95711e8b6c3", "references": ["pose"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-7954cbae312a4a049503b6d6844d6ba5", "references": ["mask"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-a370a7eb43e14e4a933c92d976b5f318", "references": ["body wash"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-85f06bb7d71c42e2ab92bc42cd4958ac", "references": ["weights"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-3cf4e751620b49e79d55b627eb5c942c", "references": ["bar"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-c144c51c2fc24e28b07a85d1a5a8eaae", "references": ["bun"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-89e6b568950c483292605e7f8a8e2d67", "references": ["gum"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-b43469dd2cb54fc9a400d6805a28b844", "references": ["t-bone"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-01d4148fb6c34488a0077a3852b51397", "references": ["pallet"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-2b012ff2d6d64ec7b14f97d4d61717bb", "references": ["rocks"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-a362370917a74b3a9faac8fb033e1aad", "references": ["ghosts"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-b63cabe69eab4a0cbc532aa8063ab5d0", "references": ["jar"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-4de53af359484ece87d5ad5ec748b1c2", "references": ["tutor"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-8d3ccc4f4f274d968999c0527c13a64d", "references": ["cloak"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-8d8c2508aecd48b2a96a3cd0f219ce8d", "references": ["home"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-99212586c9ad4e60a7c845555125d411", "references": ["towel"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-3560a6d3d9de41db94bd9d2a9ca760b1", "references": ["ball"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-52e3cf44988548fa8e0ee38710f9e061", "references": ["mower"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-946933034a984e0db5614bc28fd6c60e", "references": ["lip balm"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-2d404bbf92944855811bf1c885a8c45d", "references": ["love"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-a99edaf578d34bb3861ce78f9d140492", "references": ["house"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-02fd452b37d64ae38f1fd111c89fe525", "references": ["shirts"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-1c74e584430c482d86ebc6a85e35bd4c", "references": ["cemetery"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-1339e5deb98a4f41a0a2f5fee07cc883", "references": ["action"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-de344f2f7360404b8fa36b7a7c74574d", "references": ["plan"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-9cf9bb75ee1941258497c9e623cbd038", "references": ["assets"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-04683e2644a34162bdb2dc121367abb0", "references": ["cigarettes"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-2cadd97b34be47318f65f9a05babeb80", "references": ["marker"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-a6218e3317c54e23936a6490df5a050d", "references": ["flowers"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-fa69ed51d1b9464daf4d7f022d8c7b67", "references": ["evidence"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-ec426289f22846feac9636a4d09b09af", "references": ["box fan"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-53a9d787abd348198bd83825196a434a", "references": ["vocabulary"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-23dca02e443a4059924f54bac77608d7", "references": ["bank"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-e35348ad7e01483eb99db2b5a1c92340", "references": ["umbrella"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-931560443e104a34a38a6903f30598f4", "references": ["water"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-ae5e94021c83436eabce8864c0647769", "references": ["sweater"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-562ab8a0c4f845bda1204b811db551aa", "references": ["shoe"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-1d5d8c0cbd4840feb6b15283c4f0109c", "references": ["painting"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-25eb4150cf824469bd04ac8fb07f94bd", "references": ["marbles"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-99f77773fa254aee83f58dc83cefb256", "references": ["grass"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-ffcdad6d7e4e40a68dc7a81fc1f53f69", "references": ["werewolf"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-0986c7aa3e23473db2d1af9fd74973cc", "references": ["movie"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task033-1cc9ab9d4088465ba1b1a1e245a348b2", "references": ["board"], "task_id": "task033_winogrande_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1407-ce3c3e005bc5474bb27e3213e3805263", "references": ["There is a family friendly coffee shop named _____ which also serves Indian food located at the city centre which has average ratings and cheap prices.", "_____ is a coffee shop that sells Indian food, price range cheap, average customer service rating, located in City Centre, family friendly", "_____ is a coffee shop that serves cheap Indian food in the city centre. It has an average customer rating and is family friendly.", "_____ coffee shop is a family friendly ,cheap Indian in the city centre with  average customer ratings."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-8e427e27b48b499f8c6035a002e42519", "references": ["Near The Six Bells there is _____, which serves Italian food. The price range is above 30 pounds.", "_____ serves Italian food in the higher price range and is located near to The Six Bells."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-3b71b72f008847b19922f3d6d36c306c", "references": ["_____ is a French eatery with a moderate price range. It is not kid friendly."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-50db437035e440328689500756fd013c", "references": ["_____ is an expensive, five star whole foods restaurant fun for the whole family."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-6be2408cad014ab4bb728092c8a5f21b", "references": ["First prize at the _____ won $41,500."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-41a9d993353440899fd0459455320f21", "references": ["_____  served as coach of the Alabama Crimson Tide baseball team for 1 year, with a 1-0 record."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-6cfde19052b94e4188f1cc782c5cf695", "references": ["Galina Zybina came in 3rd place with _____ as her best mark and 17.42 on Throw 4."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-2f5875078f3d45ddb6c6d3b95b58b2e0", "references": ["Elvir Balji\u0107 scored against Faroe Islands on _____ at Asim Ferhatovi\u0107 Hase Stadium, Sarajevo"], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-ae4c9f198d20486ca359dc471edae9d3", "references": ["_____ was the _____.", "_____ served as the _____.", "_____ performed as the _____."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-aca283cb550840ce9a3a4a47e98e25b3", "references": ["A Japanese restaurant called _____ is _____ly priced.", "There is a Japanese restaurant that is called _____  with _____ prices.", "There is a Japanese restaurant that has a _____ price range, it is _____.", "The Japanese restaurant _____ has a _____ price range."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-ad2876bd086443669538365360e765ee", "references": ["Moderately priced, _____ serves Japanese food in a city centre pub.", "_____ is a Japanese pub in the city centre with a moderate price range.", "_____ serves Japanese food in a city centre pub. It is Moderately priced.", "_____ is in the city centre. It has a moderate price range, and is a Japanese pub."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-1309ade05f7247239752f6770f38b197", "references": ["_____ is a moderately cheap coffee shop located in the area of Riverside, near the Raja Indian Cuisine. Fast food is sold there and the staff are friendly."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-13a26682598d47f3aa6eead417732ccd", "references": ["_____ is located in Oeste Potiguar at coordinates 6\u00b09\u20328\u2033S 37\u00b045\u203258\u2033W\ufeff / \ufeff6.15222\u00b0S 37.76611\u00b0W"], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-9ea5d888ebab4dcda014353edc6df9df", "references": ["The Native Americans are an ethnic group in the _____ where English is the language spoken, as in Great Britain. The book \"A Loyal Character Dancer\" was published in the US by Soho Press.", "A Loyal Character Dancer is published by Soho Press and it is based in the _____, where one of the ethnic groups of the United States and they speak English, same as in Great Britain.", "A Loyal Character Dancer is published by Soho Press, that is based in the _____, where people speak English, same as in Great Britain and there are an ethnic group called Native Americans."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-b918a54b37db4837802bbf0676b6a772", "references": ["In _____, one can find 182 species of amphibians, 904 species of birds, 241 species of mammals, and _____ species of reptiles for a total of 1,569 terrestrial vertebrate species."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-c289c84befa745b6baf89a9f2fd1e725", "references": ["Buss Aldrin is an American born in Glen Ridge, New Jersey in the _____ of which _____ is a leader.", "Buzz Aldrin was born in Glen Ridge, New Jersey as a _____ national where _____ is the leader.", "Buzz Aldrin was born in Glen Ridge, New Jersey and was a _____ national where _____ is the leader."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-73dd269eb0884675914453c3250f92ef", "references": ["Published by American Institute of Physics, _____ was edited by A.T. Charlie Johnson who's almaMater is Harvard University and who is the doctoral advisor for Michael Tinkham.", "_____ is published by the American Institute of Physics. It is edited by A.T. Charlie Johnson, whose alma mater is Harvard University, and whose doctoral advisor was Michael Tinkham.", "The editor of _____ published by American Institute of Physics is A.T Charlie Johnson who graduated from Harvard University and is the doctoral advisor for Michael Tinkham."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-5ef7fdc64ded4772b35261e65ce0e9ef", "references": ["The _____n rupee is the currency in _____ where the leaders are T. S. Thakur and Vajubhai Vala. Bhajji comes from the Karnataka region in India.", "Bhajji comes from the country _____ in the the Karnataka region. The currency of _____ is the Indian rupee and T. S. Thakur is the leader of India while the leader of Karnataka is Vajubhai Vala.", "T S Thakur is the leader of _____, which has the currency the rupee. The food bhajji comes from the Karnataka region who is led by Vajubhai Vala."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-fc79cccdd9e2442eaa92887004d1864f", "references": ["_____ is not children friendly and is located in _____.  It has a price range of more than \u00a330, a customer rating of 5 out of 5, and serves French food.", "_____ is located in _____, serves French food, and it not children friendly.  It has a price range of more than \u00a330 and has a 5 out of 5 customer rating."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-239129df868241ff876c68274128e0e1", "references": ["There is a Japanese coffee shop in riverside near Clare Hall called _____.  Clowns has a 3 out of 5 customer rating.", "_____ is a coffee shop that serves Japanese food and has a customer rating of 3 out of 5. It is located near Clare Hall in the riverside area.", "_____ is a coffee shop that serves Japanese food. It is located near Clare Hall in the area of riverside. Clowns has an average customer rating of 3 out of 5.", "_____ is a Japanese coffee shop in the riverside area near Clare Hall. This venue has a customer rating of 3 out of 5.", "There is a coffee shop that serves Japanese food named _____ that is in the area of riverside and is located near Clare Hall. It has an average rating of 3 out of 5.", "Near Clare Hall in the riverside area, _____ is a coffee shop that serves Japanese food and has a customer rating of 3 out of 5.", "With a customer rating of 3 out of 5, _____ is located near Clare Hall in the riverside area. This coffee shop serves Japanese food."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-93f40bb7566c4e748715b39492427a8f", "references": ["_____, born on October 17, 1933, was selected by NASA in 1963. He served as a crew member on the NASA Apollo 8 mission along with _____.", "_____, born October 17th 1933, was selected by NASA in 1963 and served on their Apollo 8 mission alongside _____.", "_____, was a member of the NASA operated Apollo 8's crew alongside _____. Anders was born October 17th 1933 and selected for NASA service in 1963."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-ba41effea81844ff8ba393fbdd317fef", "references": ["_____ had _____ total receiving yards, with an average of 13.1 yards/reception."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-0e1f01cd26934b67aa1ab67993757a1b", "references": ["_____ near Express by Holiday Inn in riverside serves a \u00a320-25 price range Japanese food ."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-b25aefefb31747b18d375e77a1b26019", "references": ["New Hampshire born _____ (Nov 18 1923) graduated from NWC with a M.A. in 1957 and died in California.", "_____ was born in November 1923 in New Hampshire and graduated from NWC with a M.A. in 1957. Alan Shepard passed away in California.", "_____, born 18th of November 1923 in New Hampshire, passed away in California. He was a graduate of NWC, M.A. (class of 1957)."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-3a99b059f5c7421f8ff4e04caf70b063", "references": ["Michael Jackson's _____ was the Billboard number one album on January 28, 1984."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-d5ed605a5b9f43f6a622d0a46bba2af4", "references": ["_____ is eliminated by Kaz with time 04:55"], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-b0aac55271084e729bc4528e56113982", "references": ["_____ charted as high as _____ in the US."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-4a9ee3a73ed44a90813f7a81284efcc1", "references": ["Odd came in second place in the _____ Main League."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-cc313bca7c724fd1a9a8ad5e51a200b8", "references": ["_____ is a cheap French restaurant in the _____.  It has a rating of 5 out of 5, but it is not recommended for families.", "_____ is a French restaurant, that is family-friendly, providing cheap food in the _____ and is highly rated 5 out of 5"], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-cf2da497e5d249c4891fe4a3019b3fdd", "references": ["The _____ was the _____nd division of United Soccer Leagues (USL)."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-083a5b60ca12439384498555f8f49c3f", "references": ["Robert Hayes, Matthew Illingsworth, Bryan Steel, and Chris Newton competed for _____ and ranked 10th with a time of _____."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-6c150217311649e5a66dff0cb669f3e2", "references": ["_____ is a pub near Caf\u00e9 Brazil which serves high price _____ food.", "Located near Caf\u00e9 Brazil there is a high price range _____ pub called _____.", "Near Caf\u00e9 Brazil is a pub that serves expensive _____ food, called _____.", "_____ is a _____ pub near Caf\u00e9 Brazil, with a high price range.", "_____ is a high priced pub that serves _____ food near Caf\u00e9 Brazil.", "_____ is a pub that serves _____ food in a high price range. It is located near Caf\u00e9 Brazil.", "Near Caf\u00e9 Brazil there is a pub Called _____. Its provides _____ food but it has a high price range."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-d6631f5e5fa740e1876e61a23b05f6b7", "references": ["_____ directed by  Patrick Henry is number 3 in series and number 1 in season"], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-204538dfc1084a898b55450485881c68", "references": ["The year when West Manila has a tariff increase of 6.5 was _____.0."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-905d8bac0c5f4d458b19641bcf3446f1", "references": ["Mumbai, in _____, is the most populous city in india, with population of _____2,478,447 in 2011, and 11,978,450 in 2001."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-52ef9bb3d4bd49acbba9c9695f0ad5b0", "references": ["_____ has an average review. It is Italian and is in the city centre", "Located in the city centre, _____ has an average customer rating for serving Italian food.", "_____ serves Italian food and has an average customer rating. It is located in the city centre.", "For Italian food there is _____.  It has 3 out of 5 stars and is located  in the city centre.", "_____ is located in the city centre serving Italian food.  It has a 3 out of 5 star customer rating."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-2700b55ad0c84e8d9789aef34fb987a2", "references": ["_____ is a restaurant that serves Japanese food for cheap.", "_____ restaurant provides Japanese food within the cheap price range.", "_____ is a Japanese style restaurant with cheap prices.", "There is a cheap price range Japanese restaurant called _____"], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-c783e33261264c848e0c7626c1714a1b", "references": ["croissants in the _____ which is rated 3 out of 5 stars in Riverside named _____"], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-2e2b2208db464d3d89904a5b562edcaf", "references": ["_____ provides Chinese food in the _____ price range. Its customer rating is 5 out of 5.", "_____ serving _____ Chinese food is rated 5 out of 5 by customer."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-ca3cf7775d61484590768320f3dabf93", "references": ["_____ is the language spoken in the _____.", "_____ is the language of the _____."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-826cdd86cdaf495687a72e5a9e583935", "references": ["In the city centre area is a Japanese restaurant named _____. It is a coffee shop with a customer rating of 5 out of 5."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-34a1c6a710b24bf58c42800e9d96a19c", "references": ["_____ is a Sushi restaurant located by the river and has moderate prices."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-cf8c05711d154fb1919a0dcde3c0e256", "references": ["Al-Taqaddum Air Base serves the city of _____ which is in Iraq.", "The Al Taqaddum Air Base serves the city of _____, Iraq.", "The Al Taqaddum Air Base serves the city of _____ which is in country of Iraq."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-20f0787c59c441b4baa6a43cd8b8beff", "references": ["John Benjamin Hickey role is \t_____ title name _____"], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-f230c3456f7f413090742590ac9722ff", "references": ["Tamera Mowery played the role of Emma C. Squared for _____ during the _____ season of The Adventures of Hyperman."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-d6c9e94b7d004bb9a37d219a730fa87e", "references": ["Bulgarian player _____'s loan started on 23 October and ended on 23 January"], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-6b747ce810164f99aadb5b8e313740e8", "references": ["There is a _____, French restaurant called _____ that is located near Clare Hall in Riverside."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-a95d1123a9c7449fbdafb5001d511da2", "references": ["_____ is a coffee shop that serves high priced fast food. Near _____ in City Centre, the place is children friendly and the customers rate it average."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-1b86e3321b214437a9f0132945c3278f", "references": ["_____ is in _____ County."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-14453ea2c0d649d9b1591e5993f1d62c", "references": ["_____ is a Chinese cuisine with moderate prices. Customer rating is 3 out of 5 and it is not kids friendly, located near Rainbow Vegetarian Caf\u00e9."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-8b3cfd5c9c02440bab92fd5d8e5c77fd", "references": ["_____ is a _____ coffee shop that is non child friendly with a high price range and a customer rating of 3 out of 5.", "_____ is a _____ coffee shop which is not child friendly. It has a customer rating of 3 out of 5. The price range is high.", "There is a _____ coffee shop named _____ that is non child friendly with a high price range and a customer rating of 3 out of 5.", "There is a _____ coffee shop named _____. It is not child friendly. It has a customer rating of 3 out of 5. The price range is high."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-472224aeb83847d4b9074ffb07a6395c", "references": ["I love eating at _____. The prices of \u00a320-25 are just right and the kids can run around by the riverside, while we are waiting for our table. After we are finished, we can scroll down near The Portland Arms for a walk.", "Situated in riverside, _____ is near The Portland Arms, and is in the average price range."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-b10a8f61f51f48e6bc00206199841c7d", "references": ["_____ is a pub that offers Indian cuisine for adults, that has a high rating of _____ from its customers.", "There is a very relaxing pub by the name _____ that offers Indian food at a _____ rating for all adults.", "_____ is a pub providing Indian food Its customer rating is _____."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-4a3cb0a27eda4dda857b814c8b2491da", "references": ["Amatriciana sauce can be found in _____, where Italians are from, and the capital is Rome. It is also where Sergio Mattarella and Pietro Grasso are leaders.", "Amatriciana sauce is a traditional sauce from _____, where italian people are from, rome is the capital, and Sergio Mattarella and Pietro Grasso are the leaders."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-5f7969035b2848f2860d560ff9c29731", "references": ["In riverside near The Portland Arms there is a 1 out of 5 customer rated, moderately priced Indian food coffee shop called _____.", "Located on the riverside near The Portland Arms is _____, an Indian coffee shop with a low rating, but moderate prices.", "_____ is a coffee shop providing Indian food in the moderate price range. It is located in the riverside. It is near The Portland Arms. Its customer rating is 1 out of 5.", "_____ is a coffee shop in riverside near The Portland Arms.  They have Indian style food at moderate prices.  Customers give it a rating of 1 out of 5.", "_____ is a coffee shop that provides Indian food for a moderate price. It has a 1 out of 5 rating, is located on the riverside and is near The Portland Arms.", "There is a coffee shop called _____ that serves Indian food, has a moderate price range, and has a 1 out of 5 rating. It is located on the riverside near The Portland Arms."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-c87540766b444bdfbf92bbef5b8d3f92", "references": ["Created in 1965 and withdrawn in 1993, the _____ has a UIC number of 352-007-9."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-0ca61eb0d54c4647862e5afeb04bef7e", "references": ["_____ is a _____ movie."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-0bb0c58eadf943bba9e3495c42b4cd11", "references": ["_____ serves _____, is kid friendly and the price range is moderate.", "_____ is kid friendly, served _____ and the price range is moderate."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-c28692c742e34db59227ff4b76218339", "references": ["Sergi Mattarella and Matteo Renzi are leaders in _____, where the capital is Rome, Italian is the language spoken and Amatriciana sauce is a traditional dish.", "Italian is the language spoken in _____, a country where Matteo Renzi and Sergio Mattarella are leaders and where Rome is the capital city. Amatriciana sauce can be found in Italy.", "Sergio Mattarella is a leader in _____ alongside Matteo Renzi. You'll find Amatriciana sauce here where Italian is the language spoken and the capital is Rome."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-bfd572afafd441c4b17fe44a78eab896", "references": ["_____ is a low rating French _____ located in the city center near The Rice Boat.", "In the city center, near The Rice Boat, there is a low rating French _____ named _____.", "The '_____' is a French food _____ near The Rice Boat in the city center with a low customer rating.", "_____ is a French _____ located in the city centre near The Rice Boat.  It doesn't get good ratings.", "It gets low ratings, but _____ is a French _____ near The Rice Boat in the city centre.", "In the city center there is a French _____ called the '_____' close to The Rice Boat, it has a poor customer rating."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-84c627e08ac64986a9f973f891768e7a", "references": ["_____ is a coffee shop offering French food at a moderate price. It is located in the _____.", "_____ is a coffee shop located within the _____ offering French food at a moderate price."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-07d9a7e0f7b54ee1bfb0310c170edd94", "references": ["_____, the manager of FC Magdeburg also plays for _____, SV Babelsberg 03 and he has played for FC Sachsen Leipzig."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-cf9519e4052146b58b1cb447a08d8458", "references": ["There is a coffee shop _____ in the city centre. They serve Japanese, it is moderately priced and has low customer rating.", "_____ is a Japanese coffee shop with low prices and low customer ratings, located in the city centre.", "_____, located in the city centre, is a low priced, low rated, Japanese coffee shop."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-112c9f85c46b4c22876f98dbe8e72c0d", "references": ["_____ played himself on _____."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-e34bf8e3cfdb4a74bbc785c21f95cc3a", "references": ["The domestic box office for \"_____ was _____\"."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-ad9f6af653d3440790539be5d87575fa", "references": ["There is a children friendly restaurant _____ located in the city center that provides average _____ in the high price range."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-580bb9ef65834b769866a67fa464e411", "references": ["Near Burger King in the city centre is French coffee shop _____ serving dishes less than \u00a320", "_____, a French coffee shop, serves dishes less than \u00a320 near Burger King in the city centre"], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-1964bcaaffca4000be6af5ada40da876", "references": ["The riverside Italian restaurant _____ is a family friendly restaurant conveniently located near the Express by Holiday Inn.  The restaurant is rated 5 out of 5."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-4cdb5c2e63b543dba334c0e395490c82", "references": ["_____ is an good French coffee shop offering a low price range menu and is _____t family-friendly."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-a6e570abb70f446e85b8080713134bd1", "references": ["_____ was released in July 1983."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-c9d966026df5420a8f03b9a5e05f2ed1", "references": ["John Clancy is a labor politican who leads Birmingham, where architect _____, who designed _____, was born.", "John Clancy is the leader of Birmingham where _____, the designer of _____, was born.", "Labour politician, John Clancy is the leader of Birmingham. _____ was born in this city and was the architect of _____."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-7c84c8a8af544c24acc4681a66ccb82b", "references": ["The character _____ ( Benjamin Urich ) was created by Gene Colan.", "_____'s full name is actually Benjamin Urich, the character was created by Gene Colan.", "_____, full name Benjamin Urich, was created by Gene Colan."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-d19c3f5202784d99bd0c7a974a4b7659", "references": ["In 2003 World Aquatics Championships _____ got total of 16 medals with 5 Gold medals"], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-14b7df4f0a1b43559f4b8f0319c7bccb", "references": ["_____ is a highly rated coffee shop with some amazing Italian food that is kid friendly and is on the riverside with generously sized meals costing between 20-25 Euros.", "_____ is high-rated coffee shop that serves Italian food for \u00a320-25 in Riverside. It is kids friendly."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-7a00a9918eae4693a99b720e632a5710", "references": ["Agremia\u00e7\u00e3o Sportiva Arapiraquense, whose ground is Est\u00e1dio Municipal Coaracy da Mata Fonseca, play in the _____ league in Brazil. Agremiacao Sportiva Arapiraquense has 17000 members and _____ have been champions of Campeonato Brasileiro S\u00e9rie C.", "Agremiacao Sportiva Arapiraquense's ground is the Estadio Minicipal Coaracy da Mata Fonseca, it has 17000 members and play in the _____ league, which is based in Brazil and the current champions are _____.", "_____ have been champions of _____. league in Brazil. Agremia\u00e7\u00e3o Sportiva Arapiraquense, who have 17000 members, also play in the league and have their home ground at Est\u00e1dio Municipal Coaracy da Mata Fonseca."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-2a60ceda628f45e1a5a3e21f85c6bbe4", "references": ["_____ restaurant has expensive prices and has received poor reviews.  It is located next to the Rainbow Vegetarian Caf\u00e9.", "Near Rainbow Vegetarian Caf\u00e9 is a restaurant named _____. This restaurant has a price range of over 30 Euros and a low customer rating."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-4a9e582d01fd4b2dbc8079c8b2250b76", "references": ["There is a low customer rated Japanese coffee shop in riverside called _____. It is not children friendly."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-24e9e0e6208f4f66b1eba66af31a341c", "references": ["_____'s drainage basin area is 164.6 km2."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-281a38ef0d3642998264f5703d1dc9e1", "references": ["There is a _____ place near Caf\u00e9 Rouge, _____, with customer rating 3 out of 5.", "_____ is a _____ venue near Caf\u00e9 Rouge with average customer rating.", "_____ provides _____ food It is near Caf\u00e9 Rouge. Its customer rating is 3 out of 5."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-cdfcf051b189475eacd3ae84f7a272c5", "references": ["The _____ in the city centre is low priced and not family friendly.", "In the city centre there is a low priced place called The _____ which is not family friendly."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-1e53e46624a34069ad4ee5971148a664", "references": ["The child Friendly _____ restaurant called _____ has a 5 out of 5 customer Rating. It is found in the riverside area near Caf\u00e9 Rouge.", "The child Friendly _____ restaurant called _____ has a 5 out of 5 customer Rating. Located in the riverside area near Caf\u00e9 Rouge."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-8e9d591c2c7c45088962c2139540b4eb", "references": ["The Federal Chancellor of _____ is called _____. The Accademia di Architettura di Mendrisio is located in Switzerland.", "_____'s leader is Federal Chancellor _____ . The country is the location of the Accademia di Architettura di Mendrisio,.", "The Accademia di Architettura di Mendrisio is located in _____. The country's leader _____ is officially known as the Federal Chancellor."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-4b914d0e75f94a2f9adc5e6dc50092fc", "references": ["The _____ is a high priced, children friendly _____ place. It is located near Rainbow Vegetarian Caf\u00e9 and is rated 1 out of 5.", "You can get high priced _____ at The _____ near the Rainbow Vegetarian Caf\u00e9. It is children friendly with a rating of 1 out of 5."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-8d5c013dd4974c548bce98f057f083e7", "references": ["There are 34 floors at _____ which is located in London, the leader of which is Boris Johnson.", "_____ which is located in London( the leader of which is Boris Johnson) has 34 floors.", "Boris Johnson is a leader in London where _____ with 34 floors is located."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-33b3f5bb82f64f2dba7d8e6f96bbec40", "references": ["Located near the river, _____ is a low-priced wine and cheese bar serving wine and cheese. It is not family friendly.", "Located by the riverside, _____ has a price range of less than \u00a320 ans is not family-friendly.", "_____ is not a family-friendly place with a price range of less than \u00a320 and is located by the riverside."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-583056b333644b4c9d98b12f159b7d17", "references": ["Andrews County Airport is located in _____, USA, the capital of Texas is Austin, people in Texas are called Texans and one of their languages is Spanish.", "The U.S.A.'s Andrews County Airport is found in _____ where Austin is the capital. Spanish is spoken in the state where the people are known as Texans.", "Andrews County Airport is located in _____,United States.The state's capital is Austin and its inhabitants are called Texans.Spanish is one of the spoken languages in Texas."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-b1931629757642eab7b0fb20a5ac5a65", "references": ["_____ is a coffee shop located near _____, at city centre.  It offers French cuisine and has average ratings.", "_____ is located at city centre, near _____. It's a coffee shop with average ratings, offering French food.", "_____ is a coffee shop that serves French food. It is near the city center and _____ and the customer rating are average.", "If you are searching for a French coffee shop, near the city center and _____, _____ might be for you. The customer rating are average."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-5e83f95c8e2d4d0ba98c0a9283f9e469", "references": ["A children friendly fast food restaurant near _____ is called _____."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-5b46268ec7f74c759d173952b0ad81d4", "references": ["_____ is a French restaurant with an average price range.", "_____ is a French restaurant with an average price range.", "An average-priced restaurant that sells French food is _____.", "For \u00a320-25 there is a French food restaurant called _____.", "A French restaurant in the price range of \u00a320-25 is _____"], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-5c207338c49a4c298de8f26c4afc8bc9", "references": ["The French restaurant in the city centre area near the Express by Holiday Inn is _____. They have a high price range with a customer rating of 3 out of 5 and not children friendly.", "_____ is a French restaurant with a high price range. The customer rating for The Rice Boat is a 3 out of 5 and is not children friendly. The Rice Boat is located in the area of the city centre near the Express by Holiday Inn."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-1fdadf6208464192bcd6ad9c8235c2fc", "references": ["Near the _____, _____ is a coffee shop style Japanese, low price, this restaurant is a good place for kids."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-f9cff556a84f447c937034cdd09b801a", "references": ["The Acharya Institure of Technology in _____ is affiliated to the Visvesaraya Technological University. It was also given the \"Technical Campus\" status by the All India Council for Technical Education, which is located in Mumbai. Bangalore's founder was Kempe Gowda I.", "The Acharya Institute of Technology which is affiliated with the Visvesvaraya Technological University was given Technical campus status by the All India Council for Technical Education in Mumbai. It is located in _____, a city founded by Kempe Gowda.", "The Acharya Institute of Technology in _____ is affiliated with the Visvesvaraya Technological University and was given the 'TechnIcal Campus' status by the All India Council for Technical Education. The council is in Mumbai and the founder of Bangalore is Kempe Gowda l."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-f05891c62f75406d938e29984a36297b", "references": ["_____ is a pub near Caf\u00e9 Brazil serving Italian food in the high price range.", "If you want Italian and the high price is no problem, _____ pub can be found near the Caf\u00e9 Brazil.", "_____ serves Italian pub-style food near Caf\u00e9 Brazil. Their price range is on the high side.", "There is a high priced pub near Caf\u00e9 Brazil called _____ which serves Italian food.", "_____ is a high priced pub near Caf\u00e9 Brazil which serves Italian food."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-87a8132bd73f412e8cab37da42a947a9", "references": ["_____, located near Express by Holiday Inn in city centre, serves English food and received a 5 out of 5 customer rating."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-9a0f3bb366504c52bef333634ddb5862", "references": ["_____ ranked 3rd at time 8:19.98 and he belongs to Kenya nation."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-85054fc552b541889e42aa18337ba060", "references": ["Batagor comes from Indonesia, it includes peanut sauce and is a variation on Shumai/_____.", "Shumai is a variation of _____ and they are both types of the same dish. Batagor is found in Indonesia and has peanut sauce as an ingredient.", "Batagor, a variant of Shumai and _____, contains peanut sauce and originates from Indonesia."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-b26eb034597b4bacaa9a181160418eb5", "references": ["_____ is a French coffee shop located in the city centre near Burger King. The average rating is 3 out of 5 with a price range from \u00a320-25 and it is not kid friendly."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-9dc4de7d9b01451f9cab7fe6f5809fb3", "references": ["The elevation above the sea level (in metres) of _____ is 23.0."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-c88a4ce121944208a5476bc79573528f", "references": ["In the riverside area, near Burger King, is a coffee shop named _____. The Eagle has Indian food, low ratings, not family friendly, and a price range of _____.", "_____ is a coffee shop with Indian food in the price Range of _____, with a low customer Rating and not family Friendly, in the riverside area near Burger King.", "_____ is a low rated coffee shop that serves Indian food in the riverside area near Burger King. The Eagle has a price range of _____, and is not family friendly."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1407-82e8ddb486fc42e1826cbae2a917b6a2", "references": ["_____ has a low rating and is a high priced _____.", "_____ is a high priced _____.  It has a low rating."], "task_id": "task1407_dart_question_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task402-cdf2947913984cab926b5bb4d6a0674e", "references": ["List the APIs which utilize [JSON].", "What apis have the protocol of [JSON]?", "Which apis use the [JSON] protocol?", "[JSON] is the protocol used by which APIs?", "What apis is the protocol of [JSON] included in?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-1dd37c8d7280414faf349d891592c2bd", "references": ["Can I have a list of [New Wave science fiction] books?", "What books are a part of the [New Wave science fiction] genre?", "What book falls into the genre [New Wave science fiction]?", "[New Wave science fiction] is genre for what books?", "The [New Wave science fiction] genre includes which books?", "Look for books of the [New Wave science fiction] genre.", "What books are in the [New Wave science fiction] genre?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-be056c6c670b4474b1cb0359bfcb4d31", "references": ["The cricket bowler that uses the [Slow] pace is?", "Which cricket bowler uses a [Slow] pace?", "What is the cricket bowler that uses the [Slow] rhythm?", "Who is know to use a [Slow] pas in cricket?", "[Slow] pace is used by which cricket bowler?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-d251283d057a4be4a67494e770ae2183", "references": ["What are the characteristics that define a person as being [Female]?", "Which well known personas are [Female]?", "Who is a [Female]?", "What person is of the [Female] gender?", "The gender of [Female] belongs to which person?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-652351b25483486ba204ed55d2e1375d", "references": ["The [Avanti Prima] is what type of bicycle?", "What bicycle uses the [Avanti Prima] model?", "[Avanti Prima] is the model for what type of bicycle?", "[Avanti Prima] is the model used by what type of bicycle?", "[Avanti Prima] model include what type of bicycle?", "What kind of bicycles do the [Avanti Prima] models use?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-1872e9163e264ff78551e8773d7ebb3f", "references": ["Can you tell me what type of award the [ALMA Award for Outstanding Comedy, Variety or Music Series/Special] is?", "[ALMA Award for Outstanding Comedy, Variety or Music Series/Special] is an award of what type?", "What does this [ALMA Award for Outstanding Comedy, Variety or Music Series/Special] prize refer to?", "[ALMA Award for Outstanding Comedy, Variety or Music Series/Special] is what type of award?", "[ALMA Award for Outstanding Comedy, Variety or Music Series/Special] is what kind of award?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-c382b0424fa8414d82b190665ea81a38", "references": ["The subatomic particle generation that has the particles of [Charm quark] is?", "[Charm quark] contains what subatomic particle generation?", "The particles of [Charm quark] are of which subatomic particle generation?", "[Charm quark] is what generation of subatomic particle?", "The particles of [Charm quark] are part of which subatomic particle generation?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-47688c9dac234305b95a0e05eb9d8d8b", "references": ["What department in France is in [Alsace]", "[Alsace] is home to which French department?", "[Alsace] is the location of which French department?", "The region of [Alsace] has which French Departments?", "Regarding French departments, which is in [Alsace]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-ec66deb3d185422d855dfd4b2e0975ea", "references": ["The museum director of the [Science Museum, London] is?", "what is the name of museum director [Science Museum, London]?", "Who is the director of the [Science Museum, London]?", "The [Science Museum, London] features who as its director?", "The name of the museum director of [Science Museum, London] is?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-d75b67a4bda04f4b80fbe0b50d1a5dde", "references": ["Founded by [Harmonia], what was the name of the fictional organization?", "[Harmonia] founded which fictional organization?", "What is the name of the fictional organization founded by [Harmonia]?", "[Harmonia] was the founder of which fictional organization?", "[Harmonia] was founded by which organization in fiction?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-dcad40878b30463cabca3b7823c1b9b5", "references": ["Which game uses [Strategy] as its subject?", "[Strategy] is needed to play which board game?", "[Strategy] is the subject of what game?", "What is a game of the [Strategy] genre?", "[Strategy] is the subject of which game?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-d0ade5b6f5dc459695c6666426a5eec2", "references": ["[What Where] was directed by which director?", "[What Where] was directed by who?", "Who was the director of [What Where]?", "Who is the director of [What Where]?", "What is the name of the director behind [What Where]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-80cc555ea2ee49cb9ca1fe832e0c080f", "references": ["What is the next model year for the [2016 Chevy Spark]?", "What will the next model year of the [2016 Chevy Spark] be?", "[2016 Chevy Spark] has what model before it?", "What is the next model year for [2016 Chevy Spark]?", "What model year is the next for [2016 Chevy Spark]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-db9ece3c11874e1da5cad842de970b77", "references": ["The [Second Monday in August] is known for being what holiday?", "[Second Monday in August] is the date of what holiday?", "What holiday is in the [Second Monday in August]?", "[Second Monday in August] is what holiday?", "The [Second Monday in August] is the date of which holiday?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-8d740875e4b14bed85a9741c2baf15dc", "references": ["Name the work of writing which is included in [Fixed series].", "What is the written work that is part of the [Fixed series]?", "What is a book from [Fixed series]?", "What is a written work that is part of [Fixed series]?", "[Fixed series] contains what written work?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-76bf145553e14141b51ed4ceca2d7fac", "references": ["What award category does [Buma Cultuur] belong to?", "[Buma Cultuur] is in which award category?", "[Buma Cultuur] is featured in which award category?", "[Buma Cultuur] is the award category of what?", "[Buma Cultuur] falls into which award category?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-09402559bcd04e3985e0a94871716e75", "references": ["[Sugar] is found in what products?", "[Sugar] composes what products?", "Give me some products that are made with [Sugar] in it.", "What products are made with  [Sugar].", "[Sugar] is used to make what products?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-dbb4b3ca3b7a48089ecf91befe382ae1", "references": ["What zoos are also considered to be [Public aquarium]s?", "Which zoo's category is [Public aquarium]?", "[Public aquarium] is a category of which zoo?", "What is the name of the  zoo that is in the category of [Public aquarium]?", "What zoo is a [Public aquarium]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-537b74f5941b4b36ae2ab51094a505e2", "references": ["What rail network does [Tonsley railway line] belong to?", "[Tonsley railway line] is featured on what rail network?", "[Tonsley railway line] is part of what rail network?", "[Tonsley railway line] is on the rail network named what?", "[Tonsley railway line] is included in which rail network?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-85cf828bb7c3469d9efb06078e601bfa", "references": ["[Nothing he knew of, enunciated life like death.] is a quotation that addresses who or what entity?", "[Nothing he knew of, enunciated life like death.] has what quotation address?", "[Nothing he knew of, enunciated life like death.] has what as the quotation address?", "What is the quotation addresses related to [Nothing he knew of, enunciated life like death.]?", "The quotation [Nothing he knew of, enunciated life like death.] addresses whom?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-f019e250647e48849b229e97e521d940", "references": ["What genome build was the result from the curator of the [National Center for Biotechnology Information]?", "Which genome build is curated by the  [National Center for Biotechnology Information] ?", "[National Center for Biotechnology Information] created what genome build?", "What genome build is  [National Center for Biotechnology Information]  known for?", "For which genome build is [National Center for Biotechnology Information] the curator?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-cb25ec53f48e45e7945202c0e171332d", "references": ["What is the opera performed in the [Sanskrit Language]?", "[Sanskrit Language] is the literary language of which opera?", "[Sanskrit Language] is the language of which opera?", "What opera has [Sanskrit Language] in it?", "[Sanskrit Language] is used in which opera?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-d231eab0f76c4e2b9b8412505dea2c3d", "references": ["Name a video game that [New System House Oh!] published.", "[New System House Oh!] publishes which video game?", "[New System House Oh!] is the publisher of what video game?", "The publisher [New System House Oh!] puts out which video game?", "[New System House Oh!] published what video game?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-fbd02744324141d3955db6945d1f09fb", "references": ["[Grand Duo concertant, op. 48, J 204: II. Andante con moto] was recorded by what recording engineer?", "[Grand Duo concertant, op. 48, J 204: II. Andante con moto] was engineered by which recording engineer?", "[Grand Duo concertant, op. 48, J 204: II. Andante con moto] was created by which recording engineer?", "[Grand Duo concertant, op. 48, J 204: II. Andante con moto] was created by what recording engineer?", "[Grand Duo concertant, op. 48, J 204: II. Andante con moto] is the responsibility of what recording engineer?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-88d1c4efe53e420fb6f58d0d0813de50", "references": ["[Sidewalk Talk] has what composition?", "[Sidewalk Talk] is what type of composition?", "[Sidewalk Talk] is a composition type of what?", "Name the type of composition of [Sidewalk Talk]?", "[Sidewalk Talk] is what kind of composition?", "[Sidewalk Talk] has what composition type?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-21603f4f83e0492f9e73f1018d414e97", "references": ["The [Battle between Macedon and Athens, Thebes, and allies] was created in what work of fiction?", "Which fictional event includes the [Battle between Macedon and Athens, Thebes, and allies]?", "What fact in fiction includes [Battle between Macedon and Athens, Thebes, and allies]", "[Battle between Macedon and Athens, Thebes, and allies] happened in what fictional event?", "Which event in fiction includes [Battle between Macedon and Athens, Thebes, and allies]"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-e930bab475634ee8b11270f03132552c", "references": ["[Leonis Adobe] is an example of which architectural style?", "[Leonis Adobe] is an example of which type of architectural style?", "The building [Leonis Adobe] is an example of which architectural style?", "What is the architectural style that [Leonis Adobe] is an example of?", "[Leonis Adobe] is a representation of which architectural style?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-70c892f46f9042a4bf1fd2a0d7d673d3", "references": ["What asterism is [Altair] in?", "[Altair] has which asterism?", "What asterism has [Altair]?", "[Altair] is a part of which asterism?", "[Altair] is located in which asterism?", "[Altair] is located in what asterism?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-41421d9e2d2542268d786e1818b8d751", "references": ["[Aloha Airlines Flight 243] occurred on an aircraft belonging to which airline?", "What is the name of the airline that has the [Aloha Airlines Flight 243] accident?", "What airline is the accident [Aloha Airlines Flight 243] attributed to?", "[Aloha Airlines Flight 243] was an accident with which airline?", "[Aloha Airlines Flight 243] was an airline accident by which airline?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-73e6472d9db44ccd886e74c902ccd08c", "references": ["Who was the 43rd President's [George W. Bush] vice president?", "Who served as vice president during [George W. Bush]?", "US president [George W. Bush] had whom as his vice president?", "Who was the US president [George W. Bush]'s vice president?", "Who was the vice president when [George W. Bush] was president?", "US president [George W. Bush] had who as his vice president?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-92e45689781149f1a1fea2d9bb4e13aa", "references": ["[UWH criteria iv] is a subcategory for which site listing category?", "[UWH criteria iv] is a subcategory of which site listing category?", "THE SUBCATEGORY OF [UWH criteria iv] IS IN WHAT SITE LISTING CATEGORY?", "[UWH criteria iv] is the subcategory given to which site listing category?", "The subcategory of [UWH criteria iv] is part of which site listing category?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-2a36794552fa4851a8186ecbb92949f3", "references": ["[French, Cajun Language] is a dialect of what major human language?", "[French, Cajun Language] are all dialects of which language?", "What language has [French, Cajun Language] as dialects?", "[French, Cajun Language] is the dialect for what langauge?", "Where are the [French, Cajun Language] dialects found in human language?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-f09962f98ab842e29a9971d18d9a882e", "references": ["[Deauville American Film Festival] is the name of what film festival event?", "What is the [Deauville American Film Festival]?", "[Deauville American Film Festival] is what film festival event?", "The [Deauville American Film Festival] is what film festival event?", "What events are at the [Deauville American Film Festival]?", "[Deauville American Film Festival] is the event of what?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-6489b0c6f4604de2a914aba9f1f09ee7", "references": ["[The Burghers of Calais] are an edition of what artwork?", "What artworks are edition of [The Burghers of Calais]?", "[The Burghers of Calais] is an edition for what artworks?", "[The Burghers of Calais] is the edition of what artworks?", "[The Burghers of Calais] include what artworks?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-1455c359a5384704b1ba86a529515f0e", "references": ["The government service of [Global Entry] is piloted by which government service channel?", "[Global Entry] has what government service channel?", "What is the name of the government service channel that [Global Entry] as a government service?", "[Global Entry] is the government service of which government service channel?", "What is the government service channel that [Global Entry] belongs to?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-9cc00c7a1fb040679e4d78a28ec04912", "references": ["What is the name of the government office of [Czechoslovakia]?", "[Czechoslovakia] has what government office?", "What is [Czechoslovakia]'s government office?", "[Czechoslovakia]'s government office is what?", "[Czechoslovakia] has which government office?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-e2c125990c0444ac932e1a6857c6899c", "references": ["The [International System of Units] uses what unit of electric current?", "What unit of electric current uses the measurement system of [International System of Units]?", "The measurement system of [International System of Units] is within what unit of electric current?", "What is the unit of electric current used by the measurement system of [International System of Units]?", "The measurement system of [International System of Units] is used by which unit of electric current?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-ba42d80785014abbbf70f9dd18b049b6", "references": ["[Five World Trade Center] was destroyed in what manner?", "How was [Five World Trade Center] destroyed?", "what method was used to destroy [Five World Trade Center]?", "How was the [Five World Trade Center] destroyed?", "What destruction method was used on the [Five World Trade Center]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-6c859a7279cb4fd185343008f5c0acaf", "references": ["[Drug physiologic effect] is the topic of what?", "[Drug physiologic effect] is of what topic?", "The topic of [Drug physiologic effect] is what?", "[Drug physiologic effect] falls under what specific topic category?", "[Drug physiologic effect] includes what topic?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-ec5a0bb8347e48ada1b57e219b694304", "references": ["[Military branch] is the parent category of which collection category?", "[Military branch] has what collection category?", "[Military branch] is the parent category of what collection category?", "What is the parent collection category for [Military branch]?", "[Military branch] belongs to what collection?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-8b17b2de645a47fe8485327f2675f3d4", "references": ["[11675 Billboyle] was discovered by which astronomical observatory?", "What observatory first found [11675 Billboyle]?", "[11675 Billboyle] was discovered by what observatory?", "which astronomical observatory made the discovery of [11675 Billboyle]?", "[11675 Billboyle] was discovered by which specific astronomical observatory?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-faca1c8df03a4426bdc049a9bb6a15eb", "references": ["[Alliance-Union universe] was created by whom?", "Who founded [Alliance-Union universe]?", "The [Alliance-Union universe] was created by who?", "What author created the [Alliance-Union universe]?", "Can you name the creator of the [Alliance-Union universe]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-85070b7d92bd4ce180cda9826456c295", "references": ["[Pan-STARRS] was involved in the discovery of what?", "What did  [Pan-STARRS] discover?", "[Pan-STARRS] surveys the sky to find what discoveries?", "[Pan-STARRS] made what discovery?", "[Pan-STARRS] finds what discovery?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-f88fdca02bf84e79ab052bbcd1a1873a", "references": ["What professional field does [Website content writer] belong to?", "[Website content writer] is what professional field?", "[Website content writer] is part of which professional field?", "A [Website content writer] is a member of what professional field?", "Which professional field does [Website content writer] pertain to?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-cf1dda8afe4e4ce8ba822d1466b5efec", "references": ["The play [Autant en emporte le vent] has which musical soundtrack?", "The play of [Autant en emporte le vent] is in which musical soundtrack?", "[Autant en emporte le vent] is featured in which musical soundtrack?", "What musical soundtrack played [Autant en emporte le vent] on it?", "[Autant en emporte le vent] can be found on which musical soundtrack?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-2aeb0cf4ad374e97ba9834acd43a5fd3", "references": ["Which school has a lowest grade level of [Ninth grade]?", "Which school's lowest grade that is taught is the [Ninth grade]?", "[Ninth grade], or freshman year is the lowest grade offered in which schools?", "In what school do they teach [Ninth grade] as the lowest grade?", "In which school is [Ninth grade] the lowest grade taught?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-8f5f926ee9954d099eb4fbe0f2734ef6", "references": ["[Chamossaire] is the parent of what organism?", "What organism does the parent [Chamossaire] have?", "[Chamossaire] is the parent of which organism?", "[Chamossaire] is a parent of which organism?", "[Chamossaire] is the parent of what organism type?", "[Chamossaire] is parent of which organism?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-e9f230fd93fb416a802e2b630b523433", "references": ["[Joule per kilogram] is the unit of energy in which system of measurement?", "The unit of energy called [Joule per kilogram] is used by which measurement system?", "[Joule per kilogram] is used in what measurement system as energy units?", "What measurement system is related to the [Joule per kilogram] energy units?", "[Joule per kilogram] is used by which energy unit measurement system?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-826e63f1104a41c08b23c2565666e202", "references": ["[Calcium sulfide] is a principal ingredient of what drug component?", "[Calcium sulfide] is in what drug component?", "[Calcium sulfide] is a component of which drug?", "[Calcium sulfide] is a part of which drug component?", "[Calcium sulfide] in in which drug component?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-93568c7d093140a49b1a817b4a942a5d", "references": ["[Mrs Adams] is a character in which fictional universe?", "[Mrs Adams] is part of what fictional universe?", "[Mrs Adams] is a character of which fictional universe?", "The character [Mrs Adams] is in what fictional universe?", "The character [Mrs Adams] is part of what fictional universe?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-bf281b924df14bd398a0f6ca65a662c9", "references": ["The league of [European Volleyball Confederation] played in what sports league championship?", "What sports league championship did the [European Volleyball Confederation] compete in?", "The league of [European Volleyball Confederation] participated in what sports league championship?", "[European Volleyball Confederation]  took part in what sport's league championship?", "[European Volleyball Confederation] league participated in what sports league championship?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-b037441c448547309a9bb569421b6671", "references": ["What province is [Suwon]  the capital of?", "[Suwon] is the capital of which South Korean province?", "[Suwon] is the capital of what South Korean Province?", "[Suwon] is which south Korean province capital?", "[Suwon] is the capital of what South Korean province?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-7573d285a4eb4c1393b631590636740b", "references": ["[Gold Coast Football Club] has what fight song?", "The [Gold Coast Football Club] features what tune as their fight song?", "What is the name of the fight song for the [Gold Coast Football Club]?", "What is [Gold Coast Football Club]'s fight song?", "The fight song called of [Gold Coast Football Club] is called what?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-2739ffe08fcf469d986c562a5b48af7c", "references": ["What privately owned vehicle does [John Lennon] possess?", "[John Lennon] privately owns what vehicle?", "What privately-owned vehicle does [John Lennon] own?", "[John Lennon] owns what privately owned vehicle?", "[John Lennon] owns which privately owned vehicle?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-ceb09a1824e341e19a9ee1411642943b", "references": ["The [United States Army] military unit is what?", "The [United States Army] features what military unit?", "The [United States Army] includes what military units?", "The [United States Army] uses what for a military unit?", "How are military units divided and classified in the [United States Army]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-e8859177c73d4235b3f29517bdd75805", "references": ["What position does [\u221a\u00c5ngel S\u221a\u00b0nchez] play?", "Where is the [\u221a\u00c5ngel S\u221a\u00b0nchez] located?", "What is [\u221a\u00c5ngel S\u221a\u00b0nchez] baseball position?", "[\u221a\u00c5ngel S\u221a\u00b0nchez] plays in which position?", "[\u221a\u00c5ngel S\u221a\u00b0nchez] is what position?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-68fc29c4ecd444d3bdae45db46b24fda", "references": ["The music of [Kim Wu-Cheol] is used in which films?", "The music created by [Kim Wu-Cheol] is in which film?", "[Kim Wu-Cheol] created music that was used by which film?", "Music by [Kim Wu-Cheol] was used in which film?", "The music of [Kim Wu-Cheol] features in which film?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-db3c275416c44a70a6fbab79c15d5c76", "references": ["[Indian English] is the main language used in what type of written work?", "[Indian English] is the language that which work was written in?", "[Indian English] is the language of what written work?", "The name of the work wrote in [Indian English]?", "Do you know of any [Indian English] written works?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-fbf57ef91a5b4d789c82d882d7bc5c87", "references": ["The band of [Human Cytogenetic Band 5q33.2] belongs to which genomic locus?", "[Human Cytogenetic Band 5q33.2] is a band located in what genomic locus?", "The [Human Cytogenetic Band 5q33.2] band belongs to what genomic locus?", "[Human Cytogenetic Band 5q33.2] is the band for which genomic locus?", "What genomic locus carries the band of [Human Cytogenetic Band 5q33.2]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-c166287e9ee04a0e8d69d257d6be032e", "references": ["What kinds of medical trials deal with [Role of Antibodies in Cognitive Dysfunction in Patients With Systemic Lupus Erythematosus]?", "what are the types of trials [Role of Antibodies in Cognitive Dysfunction in Patients With Systemic Lupus Erythematosus] in them?", "The [Role of Antibodies in Cognitive Dysfunction in Patients With Systemic Lupus Erythematosus] was a part of what medical trial type?", "[Role of Antibodies in Cognitive Dysfunction in Patients With Systemic Lupus Erythematosus] is found in which medical trials?", "[Role of Antibodies in Cognitive Dysfunction in Patients With Systemic Lupus Erythematosus] are in what medical trials?", "[Role of Antibodies in Cognitive Dysfunction in Patients With Systemic Lupus Erythematosus] is in what type of medical trials?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-dc5e96b1b6784852ac679a137240b70b", "references": ["A [Guinea pig] is included in what type of animal breed?", "The [Guinea pig] is of what animal breed?", "What is the animal breed of a [Guinea pig]?", "[Guinea pig] is what animal breed?", "The [Guinea pig] species has which breeds?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-83d58226943243e1982032105813922c", "references": ["What clouds are of the type [Altocumulus]?", "Give me a list of clouds that are members of [Altocumulus].", "What clouds are [Altocumulus]?", "Which clouds can be described as [Altocumulus]?", "What clouds are a member of [Altocumulus]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-3f6cf6b4f0414fc5b1e1b03d2700eeb6", "references": ["The [Texas Travel Industry Association] counts which zoo as one of its member?", "[Texas Travel Industry Association] has a member of which zoo?", "What is a zoo that is a member of the [Texas Travel Industry Association]?", "Which zoo is involved with [Texas Travel Industry Association]?", "[Texas Travel Industry Association] has which zoo as a member?", "[Texas Travel Industry Association] has which zoo as its member?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-067a1aeeb4d748e6ba086ad0f427df85", "references": ["[Radio DavidByrne.com - 128kbps Stream] is distributed by which broadcast distributor?", "[Radio DavidByrne.com - 128kbps Stream] is distributed by what broadcast?", "[Radio DavidByrne.com - 128kbps Stream] is distributed by what broadcast distributor?", "[Radio DavidByrne.com - 128kbps Stream] is carried by which broadcast distributor?", "Do you know the broadcast distributor who distributes [Radio DavidByrne.com - 128kbps Stream]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-4b0bd9ca59ae4e77b67ab7ebfdf78a28", "references": ["[Jordanian Premier Basketball League] counts which team as a member?", "What team corresponds the [Jordanian Premier Basketball League]?", "[Jordanian Premier Basketball League] features what basketball team?", "Select the basketball team that is part of the [Jordanian Premier Basketball League].", "IN THE [Jordanian Premier Basketball League] WHAT BASKETBALL TEAM PLAYS THERE?", "What basketball team located in  [Jordanian Premier Basketball League]?", "What team is in the [Jordanian Premier Basketball League]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-bda19c1cec2f4beb98a2dd5517a078b4", "references": ["[Allahabad High Court] oversees what inferior court?", "[Allahabad High Court] has what inferior court?", "What is the name of the inferior court of [Allahabad High Court]?", "What is the name of subordinate court of [Allahabad High Court]?", "Can you name [Allahabad High Court]'s inferior court?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-a1a22b7c83244831a95b1a950e93722e", "references": ["In what star system does [Pluto] reside in?", "[Pluto] belongs to what star system?", "Which star system contains [Pluto]?", "Which star system can [Pluto] be found in?", "[Pluto]  is contained within which star system?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-83f39b2ee17a45548b51ebef2b504010", "references": ["[AT&T Stadium] was the stadium for which football match?", "[AT&T Stadium] hosted which football match?", "[AT&T Stadium] held what football match?", "The [AT&T Stadium] is home to what football match?", "Which football match did the [AT&T Stadium] hold between two countries?", "In the [AT&T Stadium] what football game was played there?", "What is the football match that was played at [AT&T Stadium]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-9e51b31f0e8c432cb57ccf7bafffbe17", "references": ["What monasteries follow the [Order of Saint Benedict]?", "What are all monasteries that follow the [Order of Saint Benedict]?", "What are the monasteries that follow [Order of Saint Benedict]?", "The [Order of Saint Benedict] is followed by what monasteries?", "[Order of Saint Benedict] was followed by what monasteries?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-9de0f256b11440c1a7b65aa8dcecaa3b", "references": ["What hospital specializes in [Ophthalmology]?", "[Ophthalmology] is in which hospital?", "[Ophthalmology] is practiced in which hospital?", "[Ophthalmology] is performed in which hospital?", "[Ophthalmology] is offered at which hospital?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-c5f2d864a5fd401e9589eb0dd1e9dad2", "references": ["Who is the author of [UML distilled]?", "Who was the author that published [UML distilled]?", "[UML distilled] is published by which author?", "What author was [UML distilled] published by?", "[UML distilled] was published by which author?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-1c538fa4ab504f65866b8d42bf3ae249", "references": ["How many consoles has [Defender of the Crown] been released on?", "[Defender of the Crown] is a series of games made for which platform?", "Which platform for video games has [Defender of the Crown]?", "The games [Defender of the Crown] are played on what video game platform?", "[Defender of the Crown] are games on which video game platform?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-ab8e12adc2914e52b26a8e39ebfed2ba", "references": ["The energy source of [91/98 Avgas] is used by what type of engine?", "The energy source [91/98 Avgas] fuels what engine type?", "Which motors utilize the [91/98 Avgas] energy source?", "[91/98 Avgas] is the energy source of what engine?", "The energy source of [91/98 Avgas] is used by which engine?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-7dc4670b69c94c1db400d9295a5cb9da", "references": ["The instrumentalist of [Sonny Osborne] has which musical instrument?", "The instrumentalist [Sonny Osborne] uses which musical instrument?", "What is the musical instrument of  [Sonny Osborne]?", "What musical instrument has the instrumentalist of [Sonny Osborne]?", "Which musical instrument does the instrumentalist [Sonny Osborne] play?", "[Sonny Osborne] features which musical instrument?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-e62a5dff6d9841229947ab8f7052fb4b", "references": ["In which orientation does the road [E-40] run?", "[E-40] features in which road orientation?", "THE ROAD [E-40] WHAT ROAD ORIENTATION DOES IT HAVE?", "What is the path orientation of [E-40]?", "[E-40] has which road orientation?", "What direction is the road oriented if it is [E-40]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-398f5e7726254575951cc9196c7d2151", "references": ["Which fictional object destroyed caused the destruction of [Pequod]?", "[Pequod] was destroyed by which fictional object destroyer?", "What caused the destruction of the [Pequod]?", "[Pequod] was destroyed by what fictional object destroyer?", "[Pequod] was destroyed by what fictional object?", "What was the name of the fictional object destroyer that destroyed [Pequod]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-bf95d7a861304cad8ee4fb6e64d83b35", "references": ["The religion of [H\u221a\u2264a H\u00b7\u222b\u00a3o] was created by what founding figure?", "Who founded the religion of [H\u221a\u2264a H\u00b7\u222b\u00a3o]?", "The religion of [H\u221a\u2264a H\u00b7\u222b\u00a3o] was founded by which founding figure?", "[H\u221a\u2264a H\u00b7\u222b\u00a3o] was founded by who?", "[H\u221a\u2264a H\u00b7\u222b\u00a3o] has been founded by which founding father?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-bb5eca467f3a4efba2f58dc6d83ac6d5", "references": ["[Explorer 1] is an artificial satellite currently proximate to which celestial object?", "What celestial object has the artificial satellites of [Explorer 1]?", "[Explorer 1] is an artificial satellite of what celestial object?", "[Explorer 1] are the artificial satellites of what celestial object?", "The artificial satellites of [Explorer 1] belong to which celestial object?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-874b4af4ae1e4b57b5876c629522e9b6", "references": ["[Forest Laboratories, Inc.] is the sponsor of which medical trial?", "[Forest Laboratories, Inc.] sponsored which medical trial?", "[Forest Laboratories, Inc.] sponsored what medical trial?", "[Forest Laboratories, Inc.] sponsors what medical trial?", "[Forest Laboratories, Inc.] is a sponsor of what medical trial?", "What's the name of the medical trial that has [Forest Laboratories, Inc.] as a sponsor?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-e040936ff93e4593b5caad307a4481ff", "references": ["Which collection category has [Piggy bank] in its sub-categories?", "Which collection classification has the sub-categories of [Piggy bank]?", "In which collection category can you find the sub-categories of [Piggy bank]?", "[Piggy bank] is a sub-category of which collection category?", "[Piggy bank] is a sub category of what collection?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-fac3a5f36d274776900827acb00b6b54", "references": ["[Saturday Night Live] aired which sequence of TV episode segments?", "[Saturday Night Live] aired segments of which sequence of TV episodes?", "In what order were TV episode segments shown on the TV series [Saturday Night Live]?", "[Saturday Night Live] aired which sequence of tv episode segments?", "In which particular order were the tv episode segments aired on the TV show [Saturday Night Live]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-bedd10bd34f94c6cbd931443a6438b34", "references": ["The [Dodge City Civic Center] is home to which sports team?", "What sports team plays at the [Dodge City Civic Center]?", "The [Dodge City Civic Center] houses which sports team?", "[Dodge City Civic Center] is the home of what sports team?", "[Dodge City Civic Center] is which sports team's home?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-9cfcf7819c1d4a3c8ea37d9a50664781", "references": ["What cricket teams are based out of [Vanuatu]?", "[Vanuatu] is the home of what cricket teams?", "Which cricket teams are located in [Vanuatu]?", "What are all the cricket teams in [Vanuatu]?", "Name all of the cricket teams in [Vanuatu]."], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-58d51711013c4bd3b737076906e0a527", "references": ["Which museum is [Suzanne Delehanty] director of?", "What museum has manager [Suzanne Delehanty] ?", "[Suzanne Delehanty] is which museum's director?", "[Suzanne Delehanty] is the director of what museum?", "what museum is [Suzanne Delehanty] the director of?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-d28921d377e24dc391b669cbd619cea9", "references": ["In which country does [Denmark] originate?", "[Denmark] has which country of origin?", "[Denmark] has what country of origin?", "What has [Denmark] as it's country of origin?", "From what country did [Denmark] originate?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-3089d097249c44c0a2a2a12057a48569", "references": ["[Via Mala] was adapted to make what adaptation?", "What is the name of an adaption that was adapted from [Via Mala]?", "What novel by the Swiss writer John Knittel is an adaptation of [Via Mala]?", "[Via Mala] is the adaptation source of which adaptation?", "[Via Mala] has what adaptation  adapted from it?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-8674e5e2051e47d097b95bb36ceaa532", "references": ["What browser extensions work with [Internet Explorer]?", "[Internet Explorer] is compatible with which browser extensions?", "What browser extension is compatible with [Internet Explorer]?", "[Internet Explorer] is compatible with what browser extension?", "Can you name browser extensions that work on [Internet Explorer]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-fd26aba37f6347df82e81a5280a5aab1", "references": ["[England cricket team] won which cricket match?", "[England cricket team] WON WHAT CRICKET GAME?", "The  [England cricket team] won which match?", "The [England cricket team] won which match?", "[England cricket team] won what cricket match?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-5507b4651bae43e3b7d4c8e3a84718d5", "references": ["The [Federated database system] is an example of which software genre?", "[Federated database system] is part of what software genre?", "What is [Federated database system]'s software genre?", "[Federated database system] is a software of what genre?", "[Federated database system] has what software genre?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-935acbc2d80e4e3783a45864e1aeed7f", "references": ["[Israeli Agoroth] is a sub unit of what currency?", "what is the currency of subunits of [Israeli Agoroth]?", "[Israeli Agoroth] is the sub unit for which currency?", "What money use the sub units of [Israeli Agoroth]?", "[Israeli Agoroth] is the sub unit of which currency?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-3b236100355b44cf9dc2c60f1cb10faa", "references": ["The [M74 Group] is located in what galaxy?", "[M74 Group] is in which galaxy?", "[M74 Group] in which galaxy is this?", "[M74 Group] encompasses what galaxy?", "[M74 Group] contains which galaxy?", "[M74 Group] is found in what galaxy?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-84747cb398484d639900e9ac0712bd1c", "references": ["What organism classifications have a [Subkingdom] rank?", "Name an organism classification of the rank [Subkingdom].", "What organism classifications fall under the [Subkingdom] rank?", "What category of organism has a [Subkingdom] rank?", "[Subkingdom] is the rank of what organism classification?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-d0d2ea91ec9b43b6b026599ffdf6ac97", "references": ["What is the psychological effect of [Cisatracurium besilate] ?", "What is the physiologic effect of [Cisatracurium besilate]?", "[Cisatracurium besilate] has what type of physiological effect?", "[Cisatracurium besilate] has what physiologic effect?", "What physiological effects result from the ingestion of [Cisatracurium besilate]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-98a0c11db75c4f118316c361af92d584", "references": ["What area are in the [Metropolitan statistical area]?", "The [Metropolitan statistical area] contains which metropolitan area?", "what is the metropolitan area that belongs to [Metropolitan statistical area]?", "The [Metropolitan statistical area] consists of which metropolitan areas?", "[Metropolitan statistical area] is the home for which metropolitan area?", "[Metropolitan statistical area] has what metroplitan area?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-1e1c8fa3de9646f6bca5b762ca8c709b", "references": ["In where do people who practice [Ayyavazhi] worship?", "[Ayyavazhi] uses what kind of place of worship?", "What type of place worships [Ayyavazhi]?", "[Ayyavazhi] is known for what type of worship?", "What is the name of the place of worship in which [Ayyavazhi] conduct their religious ceremonies?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-57cdf60caeca403aaa81d91b99fb9b56", "references": ["[Bandaran] is an example of what fictional character type?", "[Bandaran] is which fictional character?", "[Bandaran] is what fictional character?", "Who is a fictional character that is [Bandaran]", "Who is [Bandaran]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-3df2ec2f0457482fb9daba8aa4bd1cb9", "references": ["What company developed the [Deathrow: Underground Team Combat] video game?", "[Deathrow: Underground Team Combat] was created by which video game developer?", "[Deathrow: Underground Team Combat] was developed by whom?", "[Deathrow: Underground Team Combat] was developed by which game developer?", "[Deathrow: Underground Team Combat] was developed by which video game developer?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-7fdd7437667e4c678291c5599b240f56", "references": ["What tournaments have [Wheelchair Tennis, Quad Singles]?", "[Wheelchair Tennis, Quad Singles] are part of what tournments?", "[Wheelchair Tennis, Quad Singles] is an event a what tournaments?", "[Wheelchair Tennis, Quad Singles] are in what tournaments?", "Look for tournaments that have [Wheelchair Tennis, Quad Singles]", "What are some tournaments for [Wheelchair Tennis, Quad Singles]?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-eece2137486047d69e98927c1f786ecd", "references": ["The collection activity [Militaria] belongs to which collection category?", "[Militaria] is a collection activity in which collection category?", "[Militaria] are artifacts that belong to what type of collection?", "The [Militaria] collection activity is contained within which collection category?", "[Militaria] collection activity is in which collection category?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task402-3bff0d2d4f79494aa4a5b8ec88048e81", "references": ["What are the names of the musical albums composed by [Hella Heizmann]?", "Give me a list of albums that were composed by [Hella Heizmann].", "What albums did [Hella Heizmann] compose?", "What albums did [Hella Heizmann] work on?", "[Hella Heizmann] composed which albums?"], "task_id": "task402_grailqa_paraphrase_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task201-ee734c8ca4b2458f81aa912e8aa922c3", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-a72397171d9f4fbdb2b35f740c458d1a", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-f672d214d5e8426cad12dbdb1c59c439", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-92728ea21a60447580dfd65fc94e90a8", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-38d06b0ab97b4561b48ed5b1abdd6e5e", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-26a5937e93c241b6a808383cecda632f", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-868c66ecd4364b4582f86cc244c36fab", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-129e702450a14d3faa3f1d1ff41bae8b", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-ac1f7207a64e42ea9f3da5617b2d545b", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-34c2a772661b443b922a0adf683fbe4f", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-0124a81ffa9a4adcb2578f0b164af292", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-863f9e03e1af4f248e32482fe9d6d4b0", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-92bba1edaf89405dbbbfd6e97efd9b3f", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-53e9914ce7fd491e8446c919a852c812", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-afdb738da6f9442191db9888baa7570a", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-cc65c36f63884f769f6063fea6938376", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-e9c022b8c6704905bb03f758879604b2", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-64d29bb1b03a49cfba502419033c85cc", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-1401b695f5a8403ba5ef9cb2f119598e", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-4ae753f0ee154ebbb90e4acc8b84e605", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-56911679d2914c61bad496744acbc29c", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-e047a21f70ab4f44a036069c9f9669c0", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-84494afbee47431283f9e3d5850ca951", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-b53f0cbb443b486d94941c07f5db001c", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-e49bc246e76a4d0fb3f0f942e3b833f2", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-ca02397d73944d899d6c14165ed9b368", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-77df057ac97f4e078faf05e8a1847e7f", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-bb29c90a73e74ab08f303ddc53b41b27", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-87dd034b27374927a609ace510f6c768", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-46594340c8f04eefbb63197b53a4a9a8", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-6fd1626bdc594818b20d1e86dbefaf3b", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-5edb4415240b4628b6ca3ccb9bad1bd1", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-7184cc54dcd444eba7fde741aa12b0c8", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-fc7255b483ed4510bbac083b430b8836", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-3defa3ec8346459395ff43903b3f8d04", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-8696dd7fa37b413fa963cb2488c5b04f", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-86f38f91fc164ee8933e51e3a1f4c697", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-25a9d79f681a4106a8311d998c22a36f", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-b66ed8c4155443eb9c7c64a2202d6c9b", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-fe8793e15d65443b925a51390590d39e", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-fd07e26a4ed74d61bced6fd8e7acb67d", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-6aa2374e001c415fac82f137920eab08", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-32f3b8d03d1a4960a69500c02b707dcd", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-052923547b384b7eb229246fa7418eeb", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-e48a0dd0ec1248c5b762748ace64b8a1", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-5887d4413caf4d18ada329a7b522161c", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-a97c400f240c46baba5947739257b905", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-a5b9a9e890b646f2958f66acb9ee6b12", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-370749015a8246928f095c872fa8840a", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-fd0deece1a8c4a5fb9236420d7c7b6f7", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-e751866455854760886db81e08a47a00", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-a3493ce882cf43e38c313049d9a27eed", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-903b679dacd04b1c9491f6d247074e53", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-dbd5906aa76f4a288ac57e5433278d4c", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-8d1ebac0a45147b3980d195bdcd1424c", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-808b53377b884ba5b32d85d8caa783e2", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-cc7813eb00974e2589097a7a856fb031", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-e097d77e204e442dac4fa54a10159cca", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-9fe132c2d4664275a0b642afe45153e2", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-35713fd88f3843d8a818f0678ff7342a", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-5ba4eb1c9f154fdf8bb5d966d2869165", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-9585724dbe70493fbc6b0898600c99ff", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-4cc9a04be392477b8a7870d46ce27f7d", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-0489b45907b342118bb7954983b020b9", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-0957b738571745ed8100656aa97849da", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-415075999abf48bd99f2eef515f08014", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-15c6ce87639d461a99e724a8d5099f22", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-74a16d6116e54c4d8acd050926d025c1", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-52d12eaea9ad4e5f91fd7acdfe5ebd13", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-83adf77702ed4ee9921673d135da1ed2", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-1a3426d5e95848428e38bc75f846b33f", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-87d27c0560ca438d8881f50d924632d2", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-2527250396e54dbd8b03b8dbc5f565c0", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-18c544d731564292831332b5453d0978", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-a2bf8383dafd422487fcfdb71c71c7e3", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-3fae0b6d87b84d31b0169ec065804773", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-db8b44b4954c4b3f9e7f0375cf4c9380", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-0126e869517d4086a175adb658f2f30d", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-d49e1e62bce74222bdb7dc76ae589e40", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-bdab7192107c4719a687bbaca4d05b5f", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-bd2328520cea496cb77f6320efde8d5c", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-1aaa6a7bdbc24b83acb8fd7d17ba403f", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-05bdd09221af44709c650b0f3c60309e", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-93549e0873d546b6b7509f9c7fe489a2", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-f48de5856b234336b527857a15aa1c1e", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-7074fd4a171d41f29c5c8cdc8188b40a", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-259deeb9ce9547088307a87b5448a17a", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-689d1ff315c9444f8b63e5136143dd7a", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-6922316730654f3a9a98edc6629d0e46", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-733f3bd478754627914be4778f294e25", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-11234d8eb87646d6898bd7a27fe15f27", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-a1a321f90dc24711bd33cff33d05793e", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-8b7116b40c0943efa88537369d6ad6c5", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-8d88b4a7098d457fbcc15d9817213847", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-6b91401dd2904cfca18e963c4c9d0d32", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-7f3aef11c3db45e19deb17c93b2224d6", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-92855a49bd1746c893fbff1a7d0bef2a", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-89cb284569554d789d96a9e7e3535f20", "references": ["2"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-0a9c3e524e0f410aa4b92798eec33e2f", "references": ["3"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task201-159a483f5f714ab5b1f6f41eec5f6246", "references": ["1"], "task_id": "task201_mnli_neutral_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task520-510f6ed11278403fb32996e2526f8529", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-53b723f5a403412e9b4296fa54fd02e9", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-1ba9571d0a62483a8525aa5094aee0e3", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-6e8253aa9adc4dc7adca56f35690ef53", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-8d14b273fb58425582667f5e1b2fcb52", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-60264b415f744b85abc1f2895db34ab3", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-9a156182eabb42cc9f9a170d7fb07fc4", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-e9be5c1d661044689591e35dec85a374", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-b17cd6095d1e42acbb6158f324c98f89", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-708ecb04674342eeabb2f285338b73aa", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-9533514a24454f899c7a8544d3b79097", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-1219d5fa5913492b9422e6f4a2c14185", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-00d6998afc4b4f6aa13741a55f872210", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-74e0394039264b19b38fd70118b00613", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-33e10af69e364bacb5178af1fbb29151", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-bee9901ed7444f959b569da2b2097ac2", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-f30b3efa028045d5bf4235e0a6910529", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-7b9a37d6eb9d445a9465c66ce888153d", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-7643c8eb5ff8421bae8d89d9184b0d7e", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-a78a9e15fb0e4f6a9f095c3189a25bc3", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-ef9fb56e982340daa8069d7ad4af6901", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-948963bd86e2454b966af103d2956f9a", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-4f5ac206661f4fb489ab21ccf43706d4", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-df7b850f8130466f9f4764c3840c1a53", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-5ea1a3c5c31b4c078a841a056ff7510c", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-037396706cc74b72b012cf567074366c", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-881878875c654c01b5962e96fa008a0b", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-7e068f50fdba4f009f5c5337739a6b5e", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-cc47ad8fe6504ff082fd129caee93ba2", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-c103485acefd487ab1f4ab87823cc1d7", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-7a7eb46ceee34e77aee939fc2f3414df", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-d59516d305ba4ed99b06b86746e3ffe9", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-39ec2bb7f6be41ea9a8a5dfe1220ad4a", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-175c5dc8f5434a7caf6d94e8f6e346dd", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-4465186fca5145f093afd7bdd69fc83a", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-ea19bf62a8dc4919968d36e000585b87", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-1381415c3fb64eecac630843340c362e", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-94d4a09fd5b944228638b2bb652ab112", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-86290b9b7ef54040a2f80c438d981dea", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-cee1f331af834dc78207a213e6899720", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-c36e8037a3224e31a264ea5499840f82", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-af35dbcadbb84223b6df1b72d80ef82e", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-93a3863bdd624de3864ec750625c8f49", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-078a03bd9f9442a58f180ce523863209", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-247f7cafbd784b3f9df1f4da1c1e35d5", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-e1aa207acc7f4a68994db139ea8ce446", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-95d9e39b8cbb4983bf419f04638ce56d", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-2e619e71f0ff4a928460b5daa50f91b4", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-b9f7fd39921d411494e40b34fd63f98d", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-42155f5c2bf143ae8646b1e1293f0e81", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-472d0ab563124f569f160065cfc58aa1", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-297bb030c4824a399803799a266556f1", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-4ec020f90cd84f7da6f1910135a9ed1f", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-b24e4a8fb4e84e8a8fc42ae3cd445d68", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-dd09f43f0c8b4c7fb032a3ee53a6f591", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-e079073c9197442bb7c17756ace15a88", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-6f9abd4b3ce84d26a4c028cfc8c5a439", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-48fd5ca4b4b94aabac6af41ecb710989", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-d540d1962a1d4ec5b1e85810276330e7", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-8033cf0892ca45f1a063737c002b074c", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-38791d9f547e402a89a1e76c4f0b17e1", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-4f82496c5dd740518c5fc96253c67768", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-8755c2ca1f9b4c6c92686a13b833280a", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-2a930b5bdce24943af860632084a5d5d", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-890e380f990b4bae9e5e04056eed852c", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-76d1fc4ca09449a495cf63809fef59c9", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-c405eb11447a4be09e05ca396f66b77c", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-ef5b2d5a451946369c8dc79432c6833c", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-d027832b85aa4163b1f8eff4fbbb97ae", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-5fc8edf8c2484b51b3eaaa2cdb7d3393", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-3ac973c628ad4e8594ef0eaeb8125cf6", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-cf68f16ad95a4b4eb120a606a7eba738", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-ddb5e0cfb520405daf5d4b81d1572edb", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-fcfe312fbe3e416d96365ff53dc9c20a", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-607e8a0358ef4093a6f194322d52193b", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-60d779f7023c48c98f4a9cb3b0cd4743", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-8d9ab112abbd487cbecb9b606246af37", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-f821cd275d4a4c119dddb65bb7f19290", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-ddc8e0e54abd402cba94fe75ae017169", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-4397cedf707447acaa58204b5ee608b7", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-25b3967c6f864c43b303cb4651d1abc0", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-b948894370d649118effe3e14e24bbec", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-51f054f458284808b0302ee9daf96afa", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-87c85d82e8b94078bd2e80ca4e26875b", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-4f8ceed187374dcc93dab72093bfbba8", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-9c2d9e017c1245a9bb3db68fe60c52b5", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-7b57cac23d8d4c9daebe1e1046191c8d", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-541780731c3e441c822e401954752ba3", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-05df5d531bda4f919b3621e0ab3b644c", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-c3cb6b1fea3f4d2c931f3b8f531a2d84", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-312b1877eacb4ee3825f65cb5f1d0f41", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-0f26a6f023d8473cb7573a38d8664ad5", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-bd382eb7d30f419da79739f7dd444289", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-8928a5348b5b4e95a0b21af2e017fe40", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-5d306ec0dad441789cd1d839dff1d57c", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-b5d1136c755c480489e109fd8a1914b9", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-b18f49df164248f6820939ac2289f905", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-c4ca2d3534504805ab99796a1a757d56", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-c4748bf0967844ab8012b44aeb66dab8", "references": ["False"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task520-f59a7ee256d94403b8049c48f2f5a1fc", "references": ["True"], "task_id": "task520_aquamuse_answer_given_in_passage", "task_category": "Answerability Classification", "track": "default"}
{"id": "task892-28108760b7ce481d844f87cb3a9324a6", "references": ["she"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-4046566f5bd945b291c7b1c76c16841a", "references": ["her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-dee618d4850f4cee8cc75aa77d28530a", "references": ["him"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-fe944c1206094ab8b9f65c40cba1a1b3", "references": ["Her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-07f2fe0199834444b6f3e6b4c26192f6", "references": ["his"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-d209c66c8207464ca3439c045f2e0699", "references": ["he"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-8d792b13b5ae4191b60f900f2782a55e", "references": ["He"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-03992974dd684257ad035f3d383ee71d", "references": ["She"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-22e57485e5b24b8ca2efd7470cbbeffb", "references": ["His"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-2705a5687f3048a0bd559c9c43ea7b4c", "references": ["she"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-9b1ba2bda89d4c83b9febf8277ef2f6b", "references": ["her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-acdf58a9415d4beab291919afb8c008e", "references": ["him"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-43896df8e1744480ade94ce3273d7408", "references": ["Her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-edb791c8ad134b2d8595b39abbe4d1ff", "references": ["his"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-202c8ae3a0b94ed9930218d4c00833c0", "references": ["he"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-441c770f579d40ff8e25198103a1866f", "references": ["He"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-8fcaae7da46849528d6e2ce25e40c0cd", "references": ["She"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-db0b96276fd44f0da859d51782e58380", "references": ["His"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-53ec72e299644ea6a4088301e72aba3a", "references": ["she"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-0c1dbdb547fc4b9382b3e9f2ea31507b", "references": ["her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-7c9c025d6e184b69ac4401719228a643", "references": ["him"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-c2f5cdcadd6342d2a4cbe167b7725959", "references": ["Her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-b0a23c55f8b64ee4853e62f7617d8e20", "references": ["his"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-003a16d7da9c4c71a9fa977fc363dcd2", "references": ["he"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-f50136ce2675451f895ed7baf8be3849", "references": ["He"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-231ff4ca39324d4cb6053d9ee905790b", "references": ["She"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-ab304376ec7f409780cd2491ec799955", "references": ["she"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-8b8636ad446a446e9467ffe693793af9", "references": ["her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-125428ca9d2d4d27b7cf3a0f300f2b60", "references": ["him"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-ad592bacf8b346e2acf46d4e1d1648de", "references": ["Her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-975aca73fb094679acac5031933b2576", "references": ["his"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-5b35128c4b4042d48e56d6c83292ffa1", "references": ["he"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-80ef645955cc41ed8f61a1e2a9f34ee7", "references": ["He"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-197d0652b3f4468a84ac23a38a7600f3", "references": ["She"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-36deb4bad03049b88dc8a5a48543184e", "references": ["she"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-9f32bcd222f34fae98f24aaa6ff0456d", "references": ["her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-15296cefc0f64fb7b789f852996437b2", "references": ["him"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-c0617ff0da214ba48b48877b00bfc0e2", "references": ["his"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-f347daeb1604406e8756c523ffbb0c1b", "references": ["he"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-1b33ce48198042a489f5dc454e194d7e", "references": ["He"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-c0f92677046141dfb4dd5b3ae0933fcc", "references": ["She"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-87c91d5b301f4b3bae0cc19cb7d80e64", "references": ["she"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-1b397283b2e64af2b6e86b4964509820", "references": ["her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-6d6f63d0560543c19f6537315e172748", "references": ["him"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-d5a13b7af94349ee80d8514cfe453e90", "references": ["his"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-a103d94ea2f94cadaad0fd4d2c2085f1", "references": ["he"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-118fb1a3396943e8b16fac34725c7ceb", "references": ["He"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-bf04f7dce1b14f1b8857bd56dbbdac26", "references": ["She"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-84cb90b47428447aa2390c228ed10c7c", "references": ["she"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-6570aff6c48d47099480f67a3efed2e0", "references": ["her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-c46d59063893465eacee03edab71e82b", "references": ["him"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-1f42663ce0f640ceac9e8af7caa7a07a", "references": ["his"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-0fdecedc860347cca4af378c56e3a3f8", "references": ["he"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-051cb2dfd5ea460397de206d50b07103", "references": ["He"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-bb3b981eda46479d8e47de7dc5fe9797", "references": ["She"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-8fa30b943f22497fae459eeede416b6b", "references": ["she"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-1d5bdae278af4a1d98ab20182c56ae7f", "references": ["her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-d5d2f7d59ad646d5a6885d54497b9329", "references": ["his"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-67878c6921f04ab08f736203919f4f60", "references": ["he"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-5b3700b33f2b4f19b0387557f66f61cc", "references": ["He"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-747d460f19f2484c8de78ac2bc7d91bb", "references": ["She"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-9c0174196c9443a583355babe1336f25", "references": ["she"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-f82cf51b58c048a4a45a23eeea0a0c8c", "references": ["her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-336a1bb3c40d4ea19d3cff42a688c0be", "references": ["his"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-c21163372dbf4a898a8629968ae8b7b3", "references": ["he"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-e4fec60d594247ff966e41c6ad231b44", "references": ["He"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-1bc22602bc3a43f1afe44b4fb25c10f9", "references": ["She"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-fd2f48e3125f4797aa9a12ef233f0b4d", "references": ["she"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-5ceafa7595a346388bf347374c423dfd", "references": ["her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-e78807416a814c67a6b6b7bcfa31d215", "references": ["his"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-f37421199ae44d3f8c0ccf4d34d06c51", "references": ["he"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-498ddda085ff46a2a75c333462cb5f6d", "references": ["He"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-55bf4549e2714b9e90516a2f5a61779b", "references": ["she"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-7b0879b2348849a4926a9515f0e37db4", "references": ["her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-8dfb1de84347420f827545418e39be95", "references": ["his"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-9087680d7f5243128ff0760dd5061f3b", "references": ["he"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-7456db412a5f4fc1aa2be32d75b25191", "references": ["He"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-b3a89129316b420eb91440bc177b1343", "references": ["she"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-c98dd76ad612445292628f3f082566b7", "references": ["her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-8ddbe27c8fad44e6b42103dec7093f9e", "references": ["his"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-08ab3bd6b4b04bcb868616a6dc997674", "references": ["he"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-6fc206e502d04bc08eda82359a80a648", "references": ["He"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-851ed3d5b24642db9dd90b9eab947c83", "references": ["she"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-994d6d6bc70a48028269e1836db64096", "references": ["her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-dadd77143bea40c99638799c072d6a72", "references": ["his"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-e2b0e90b204f4750a0bf4d56806b6125", "references": ["he"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-18c053f9fd5d4f5c9c35e820887fa260", "references": ["He"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-ec3c5954016a405e8be2c3343d5e651b", "references": ["she"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-5b31bf897ea8463ab4f1c718a4ebbd8b", "references": ["her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-2f55af9871104b07ace2a898723897a5", "references": ["his"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-6cb1dfe39fb24dcdb24035ce233f534b", "references": ["he"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-48db04b23dcb46d09026702d1a65b31d", "references": ["He"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-09571a3dd5d649378662285628f767d7", "references": ["she"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-79dad4589a594f65ae72f8894ececcf0", "references": ["her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-3f76c13f56c4484fb973e2f0761f9c43", "references": ["his"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-728c5af1312a4b85bed4f2cfa4d0205d", "references": ["he"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-4ff78175fe504946a42e8c435d8932bf", "references": ["He"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-02aab42f3d4a40c4979e21fa1393be15", "references": ["she"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-f94e2084459d4201933f798a4c916bcb", "references": ["her"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task892-51fc2e41b5ef463d9b795e92e581e471", "references": ["his"], "task_id": "task892_gap_reverse_coreference_resolution", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task828-ade8cb53e1c44721b96208dfeac42dea", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-3b754d5943c74144b06f1f56bc7efb40", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-aaf6b9723e344972920e517ce7390799", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-225e8348923e471cbeaf9cde88e75c42", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-50ed48e04e7a41eb942e65803c6536b0", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-c89fcfea55a646119be058ca2668afd2", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-30041bb9f36045a7b895d2d283a402b3", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-af83dfece8004e7cb75c903c13c7b510", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-b7a505bff3ee4b61a784b7cd6669527c", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-849b3f90f01649129a5cd38475a48954", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-57149810b25947eb975a07ec296fee08", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-be0b59723e4a4e7595c3caccb0760fe7", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-8ee2a83fe6944418bd9d4c577ed43ece", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-d26822b44e384097b7211fec87d7f8d9", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-9fa8f830026d4133b34d5f32d4577aae", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-c4c2343eeeb44f4bb48b7e68c55408da", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-84bad77af50448a791583b72adde0f68", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-c6dd655639444eb49632de46a946aa0f", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-09cff0674e18436cb67a173adb2b356b", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-c7fd0c3beb4d4251b36a2d1f7d1d7e42", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-566b1571e4914513aea41b1eebd147c9", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-080ee06d04cf4376a72c2521972571f9", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-96c49ceb4d1244029906470934978ee5", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-0c3ccc43f93845cd900f37ce59b7ce36", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-1e67dc0ba5cd4ab3894fc13a858cfcbb", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-55436f7d798f449e851e8310266886a5", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-3af4db2ba5cf40c788e8335d94e0bcee", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-20d2880b7c574d4497be5d8f089b3373", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-b3dba65c8e074ea7a837b2dfcb69f4dd", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-79b2c81ceab24f9e96bec8668316d438", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-bdd96e16fab341f1bdf1c43b08be5ccd", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-f39ba2d4e2044b5a80877e713243d39a", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-d8e88f7e8c8f48faa3011f63f7df8271", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-ad5c44532084455b99319dd49017bcc9", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-a2b7549ff0084a5a9290de7659332aef", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-a62c660d17974c808ba5e07e9cc139c3", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-5ed3d0b478ca4c6a86566ad115683893", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-8162ecfbd5dc4d69af8e7232fca949b0", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-d9baba8fc86e4807bc6d0ccc9bb07b69", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-eac1eca50c57463a9b5093242d3e8e34", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-61df6c89dc204ac7bacbcd4772b91cde", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-d26764d4e83344489f04c645e72c44a8", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-6d5bace5173a463e946b69455d41c447", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-ec72698bec3e4116ba41e8d58a62ce18", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-f81b86d73ceb42d882600c92b2bf90d0", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-e187a7def1c0495482989573ae863770", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-e360be8e8616422db78e99c1fec6b2db", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-579b3a9120bf45ff84872d83a6ba1a9b", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-94cd8acedb534ccdaaa9e6e3d1ff19e8", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-fe9cb9a7d4f8410bb74479e0e7224411", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-6c5bcafc15bd4961a8a0c0627014bbe2", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-c1263f2266b04f65be882dce1b142bba", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-2ccc314e6f004628add9b39daf6fe242", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-8133037b82ec4c1aa83a2b4550d08181", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-ec7e3694a9e7414cad554ecf8593982e", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-591d551f15c74f95a98a2682d03174f5", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-25975799542948c08aab0cd1ed94488b", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-6e1ef880a07249aab68194403a78c13e", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-044ab4181d914c9dbb418d3524f944ad", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-7c72b9ae602c4ffbb9136feea354cf5f", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-9d6cc75151b54e8f8a2787458bf5e4a6", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-4d0ea0039872419c92845986de8e8d55", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-a70a680e644a43eab4befeae4c620376", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-e1a39191327e45448613057e3ea3fe07", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-4f1fb4bc04fd41b58a322e7a8a69bc39", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-ec2e4d41d9574b65a159ab16743dd395", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-ea3ce96076034570998dd462edda4b13", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-a05723902bd34414960f539cd27423f0", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-f410c7d5183340da9dd14f7e09152997", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-4b70b02e9a06466b9b87f4d4216ad9c5", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-6818eca6a44244358882aa566467eade", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-e51d1f536d10434e97cd161d497e68ac", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-9b8737b33a6b43cca770129d50e09e17", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-0fd0a96e468b45f1acd293324cbb184a", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-3b6c78e6df1f48109aae59706759320b", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-2b21cc22fd704a7b8dec0760be95368d", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-093e1d2ac455425ab1eb32a6ff20b067", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-683320f1042f406e96bf0e53a9f90d8c", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-327ac134f6b6473db4e0c003f552bd4f", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-eae8709cf3a54b0882c62bd3800177e9", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-9dfc4a260f584c9b9301ea4e23bd7234", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-62ed555d14fe48a5973f1052e356dfc1", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-39791e07212a4a0b87fea8a1ac054de9", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-890078fd6c99415eb5775be18cf8861c", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-ded3514f04414adbbe5ff07fee228118", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-b7087b360b8047138a3d716784f3ae4e", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-c9cda7c893704da7ba804b33c06a4d3b", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-9c6cd74a34f84d9f9acdeadbb4224edb", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-065ba982ee84461284cc197ec6f0cc17", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-3ef1945c116943ce9545918a11d3dfe1", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-b714e263bae84cf7869e1eae5e8de4f0", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-9e804104ba7d451e814265470df49b2b", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-d75f740233904360a990b0b772db6972", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-9433723915ef4d19bd8425e0e0ad38ce", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-729aa97438ce46df8e79f99d71bce8b0", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-d1e28a278a9e4a1aa24d6ef2a4a5f771", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-ae255b1a245a438a836b4d20a9c7b98f", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-0fdbd1216dec4d95beb3a92337469b83", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-31e39d301101469d8a3e3a8f79cd0e69", "references": ["cause"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task828-384d45d431df4c0aae4eb051898b4f47", "references": ["effect"], "task_id": "task828_copa_commonsense_cause_effect", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task769-f407ae97f1b94c4696ec04b2dc8a8aaa", "references": ["Webbed toes"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-3784e882beae49a4ae1befddf2e45e22", "references": ["Nix v. Hedden"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-39f516b1ba7c4c0eaed4571fbc099ac7", "references": ["Union Public Service Commission"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-aaf9622d13bc4511a5b0e6e01a3d99c1", "references": ["Little Witch Academia"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-d26f452dc11e4d9ab211e208beccb85f", "references": ["Pure Michigan"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-f6fddf035b1a4715b24bccb0eabf0997", "references": ["Cloud Gate"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-bb73e08f0cc345beab841f4de7bfd128", "references": ["Trade winds"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-630f97fe266b489ab34a275e8346803e", "references": ["Earth Angel"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-dbbb5539edc64d69b7941aa19169c936", "references": ["Princess Leia"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-970472e1c7be4d29b8258c2205c1b597", "references": ["Ray Bolger"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-d121f8a9ea5c437a99af77e5a14aa18a", "references": ["ICC World Twenty20"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-c37fb14730a14be6a631019b1b03ce2b", "references": ["Rockefeller Center Christmas Tree"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-09b0e11854004d2ab8c20b964dba5bbd", "references": ["Kaycee Stroh"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-7d2da10a49b7450eac1f2df737e1ff7a", "references": ["Software asset management"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-d81c8ac5d15f46839b7c205ca4d6daee", "references": ["System of a Down"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-1a1569b136cf4abda85fe77a987152d8", "references": ["Milky Way"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-a794f066e7304c75a4e73a830e5d66c2", "references": ["2017 FIFA U-17 World Cup"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-a9c970d1115641a89751ace4492d76a8", "references": ["Turtle Bay Resort"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-354ab1eae459446eae72a0d9133d9b17", "references": ["College World Series"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-1b0e69b782f14288b5f335132f6d3998", "references": ["Agra"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-a27d1090fdb6446d80ec0d3bee016eb6", "references": ["Perth Arena"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-60a6c5239bc94ad09be9fb1a78c1a904", "references": ["Nobel Peace Prize"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-e670271bc8ee4ae0bf97be4ef44d870d", "references": ["Fraser River"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-3d48fc9df8fc4f748ffd2a508a003fcc", "references": ["The Notorious Cherry Bombs"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-326005325a174f8880be375a954dacd7", "references": ["Munmun Dutta"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-b36563b581784fc18f768a92a56d917e", "references": ["All by Myself"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-aa442a3e4c8c49a59e7acf94b8e67d82", "references": ["Capital punishment in New Zealand"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-82c655a8af054130a93fec28f9fb8166", "references": ["Iguazu Falls"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-ce15ef82e2eb40999b14beb2e2a54a4a", "references": ["Diablo III"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-e5a58dec2a814805a2de21b2a6085206", "references": ["Orange Is the New Black"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-fb6369938d284e6d9f0686729aa5ecf5", "references": ["Iroquois"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-c875c9cc9ce34709bcdeed28f0429e42", "references": ["Devonian"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-aebf08a30ab74145a3ec5e4097c6ebf6", "references": ["Kuala Lumpur International Airport"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-84dfde1440be40258f4b6e5a4f3bb428", "references": ["Wonderful Tonight"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-9c778269e4564687b520233b833a6d8f", "references": ["Atrial natriuretic peptide"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-46e345abf4e14172b9e5384400075aea", "references": ["David Bailie"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-4ab4b39ee64a4b15825fb7e1646cbd71", "references": ["Cape of Good Hope SPCA"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-fbbaf956c7e24df79344b3d120e6bdd2", "references": ["Emily Bett Rickards"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-5128b648565f498cba00eee6dcc203e1", "references": ["Christopher Knights"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-c5a741a3f3054f33bfacbf86a93d253c", "references": ["Mono no aware"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-b4ead21e8bbe4d43806362786147a727", "references": ["Rory McCann"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-3b9a752d2fc346ffada2ff6acb206758", "references": ["U.S. Route 42"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-7fb1f0152cdf4d16a10542e453d48c9d", "references": ["The Lion King"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-6ba9790f105b4721aad8b4b2a497deb0", "references": ["Computer animation"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-dbdee26c9e7541eaa61af4bd290a20b6", "references": ["The Vampire Diaries"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-0ac653a92ead4e7f8070904aa35b98f9", "references": ["Balaam"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-bf05c375ae1248dd9489da532bc5dffb", "references": ["Mount Shasta"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-a4ca18ad68e74179b6e0aa326622d536", "references": ["Horsehead Nebula"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-0c9c9a3280084209a62fef6e8bffe48c", "references": ["Charlene Amoia"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-81f1369a1d2d40c7bfb9c1ea250d2a30", "references": ["2018 A-League Grand Final"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-1816aaf52da84f3eb7ad869e9b67c108", "references": ["First Battle of Bull Run"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-cc034fd3c17846188f96d0c654f3a4c4", "references": ["Metonymy"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-a32246f69a3a40908c854c4b4cca632c", "references": ["Lens speed"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-0fbf479342f64972881ddfb506053e9e", "references": ["Dominion Day"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-bbbda02c194641ad815e44d35d54749f", "references": ["Robert Smigel"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-24a93d8fcb2c44e59809090e4fce8c16", "references": ["A Prairie Home Companion"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-7898601805394d2ea559a4b8634d1daf", "references": ["Bring Me to Life"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-4ee11906b04b46fcbf61d820c6286902", "references": ["2017 FA Cup Final"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-c2c1d67f3a9a4c2981855e5fb5bf8da6", "references": ["Amado Carrillo Fuentes"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-ce43889b04ee483195ae7a29b16d4baa", "references": ["Reverse chronology"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-66b7371db2c24802b364c72f55ca87c7", "references": ["Ring of Fire"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-4a71cd40dd8246ebb62d4f81f7dba2a5", "references": ["I Dreamed a Dream"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-a7c0542eed27441daf7c27457e682420", "references": ["United Nations"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-a2acc9e5a3d7453e929b0aa485ebfce4", "references": ["Blood plasma"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-66e0b0892b0b4d84b323a61ccc2b0a30", "references": ["United States Secret Service"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-d5f2bfd627bd4230a8938baaa34e61f4", "references": ["Chicken Little"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-42d3579d507749f6886f85f291f907a5", "references": ["Samsung Galaxy S8"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-2a8de96102f844ba9145a99438af9244", "references": ["I Have a Dream"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-845cf297fdeb42df8046bf25af8a0a99", "references": ["Christopher Daniel Barnes"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-9d8993bc2f9a47c9a3b006704064eefd", "references": ["Middle East"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-bd78f9e3936a4f1d9e426e67cf4af37c", "references": ["International Monetary Fund"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-865af497fb1544df9906b539160637a7", "references": ["Surface tension"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-014e8cd92ad542588e4e4c268cdfbec7", "references": ["Globe Life Field"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-057934ac6d7b4495b8e60807fd95ff87", "references": ["Casino Night"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-686675e57c6e45d89f38d6e5560089bd", "references": ["Cape to Cairo Railway"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-e5a0674488a149218f3a01310cb0b2c9", "references": ["Brand New Key"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-948a8da5e3654868947829727a6e7a1f", "references": ["Haunted Mansion Holiday"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-c9bb371a58a046e485f61e105f28c949", "references": ["Dickerson v. United States"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-4d6c6be2099542d3972daa16f2854d14", "references": ["Orange County Choppers"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-86464b1b747e49b1ac899b8a2f9427f4", "references": ["Icarus"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-2d48f9e1333f4bec881da5ea08972b8e", "references": ["Siege of Yorktown"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-ee540dcd4e9442f3bbf1c9d050c2032c", "references": ["ASCII"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-f17b2f803116402084c957aa0599a509", "references": ["Puerto Rico"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-b10317ce7c75441298cbdc0aa944efc3", "references": ["Day of the Dead"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-7190de8cbaf648fc972409956ba15974", "references": ["Leave It to Beaver"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-c4d9dcf61d044708af49c2ed444f8ecc", "references": ["Haugen"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-b0dcab5d0d8442e096bef58c8443c7a4", "references": ["Marie Curie"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-0b01b44ea4a14b928a027feb95e7b4e1", "references": ["Three Rivers Park"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-118d8876c2d0417dab2dfde8a849218d", "references": ["Book of Daniel"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-b5af60fc2d04465d81c8c59960e4e1ca", "references": ["Edwardian era"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-d1425eead22846da8e3a356d8fc46f68", "references": ["Hunter Tylo"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-772e6d27bcec422d95a7d0c9900d7489", "references": ["Anthropomorphism"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-8ccb1336a7d647dbb6ebde04026b24f8", "references": ["Vegas Golden Knights"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-2bab08b917804282b5a50a2ead4b52c7", "references": ["Field goal percentage"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-f540212e8543409ca3a95e88e48c2f70", "references": ["North American Free Trade Agreement"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-0407fc2fedb44989886f731413311d17", "references": ["Leukonychia"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-fe746c2faa234bb680bd1ed5f7ad588a", "references": ["Bile"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-4cbcc76bbc7142df8a96442c11378bd1", "references": ["Finland"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-360f83aad5284415859b365c39e7615f", "references": ["Shelob"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task769-ab8f7fd7345044dba0f7d83febb9e626", "references": ["Berlin Wall"], "task_id": "task769_qed_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1155-22524e6bcdf44d208f4a0a0e992a59ba", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-5c79b4d2e97d4c16989d0b656f2660f7", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-9272fb97292b438094fae1665c7d5597", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-9b4ab9e1fbf54c52b3491fb525af784d", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-2cdb731f7b8149bcbb91f6fe5abb940a", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-809ebe64085946ccbc1f7b1aebf163be", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-abec801b3e424ba8a9e3d31c3ae9cc5c", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-f4b91f176162453b923f33ea4b389bde", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-6855769d23064d788a63c0001ac7b285", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-f7eda3d4134142669aca4a1800c9d4e9", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-8806970b942241e5b454e7175d4ce465", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-008a479d46c542bdbdc92cf8353b9a26", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-3e39ba85fa134e8e9f83933ecc47355b", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-24cb5d13189a45a4ab1ff56563d034bd", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-f19a19cd790e4cec9b2da975e965416b", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-f032c00296e945bdbdf9396b572f9e89", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-23c45053d2f94799bde7e975111fac67", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-c0b7cae3c55346f1a40ddd38378b7e1c", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-d5fd0abe45d7456c8c0f26b5e8828b43", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-e9333ac607b844148abbda2109a00b6a", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-7f28d12b02d54f21bf571bc59f54fa17", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-25b29384d5084ad799fad725ef24baf3", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-3e3047f3b36a4497805f8abd2c9e0b9a", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-97c5fe7dcec64a62947265e1c6abc229", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-957df1ca9191406f8b3076019ac5af2c", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-ec4198bcb8a740b1a386b9151c772d79", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-2dc6b681ef714029ad61106946294f88", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-f90f11a4af7d40aa90429734750b68ba", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-36420a8f52c4427ebb5b3f6c50b8c1f4", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-7c6a8f7a79cb483ebe0b93371e9fb96b", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-f14baf5baddd410eb15a9ccaff272424", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-626a29699d2d4d1480cdca2704a27b2e", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-7baf760e656748a4a083b5688f6f04a1", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-c703a08f36a24c9fa321bcb4cf296c7e", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-784923de36b8471f96a7f089599ed318", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-1dd7475b7bc7405d89226c06a8d3b85c", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-fd1ac4b8fbdc4fdd8ec1b63a85246cbf", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-1f9a82e8d05d4a788e7147a0f68e3bd9", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-10ddc6201be04195ab4aad348673f301", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-e8baea15230d46ff9bc98fc12d697c45", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-6c4634139ab94bfdb0097063a2b0e684", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-ff2700f5882140ed8122ecce05bf0ba6", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-73c2554cb90e437eae962c8d74740319", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-2b49a6a24eef4ea08a01da18962ce093", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-e8abfbddf1c846a1a67d79a563c65680", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-adc4555a840047e7a77941b110d6e56a", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-c2e7115f6d16487c92fc13e8ac77378d", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-3937eb835f444108878aade5f4742519", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-67bbf2188eb94304b107957a11e85cdd", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-6c960c58adcc409ea5ec741af5f14cd1", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-f0c4bf5072d44402b600573c4eebca92", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-527c659ce46a4092914fd53cba8982eb", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-ff86311544f748d7a715cefce1f5f08a", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-68ad5b25e70346d2a498fa56cd5778a4", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-447c0ba88b8747feab5f8eb8afeaa11d", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-48b9c9edca1745819d8ec78c7a965cf5", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-8c98bffe5e854ed9b4ce4f1c41577646", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-97fda9501be448bc93865d776884c148", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-f2bc0064db0c4f5e8c88a33ad2e8fc3e", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-65398a33207f4b3d8e66a659893a79e5", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-dd2ac62808c7493cb8a3d787554a3c3e", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-54d5895d13af40d289d49c275282064a", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-0cb5069587474f6b9fdff82c1199c7c2", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-31e1c9c0e40b4d5c95142f4635df53aa", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-b0bfdab2a6e0482786e1a1355bbd1804", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-0e9162ee652a40f1be8bc0d50efccfc3", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-88b2b0bab05f4f3997d827a4448060e6", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-fee343c4be654c8580667a8baa534cfd", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-85e1eb34061e4eb19d51343d18fa45ae", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-d202a864c0684d5688acde1db28c9cdf", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-1a9e815daa8c411aa555be3cf9abfc8f", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-0fdaa09615844a5f940fca43f32076b5", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-21192a39c16c4362a2e415a2115d415c", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-6f19a0a05d7c4f828ee08e059ce6087a", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-aa0436202f934a979a5ad5d0f9f81679", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-21972912a7234edf848ef76e138379d1", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-0b6c4975c5aa44e18f8b41fc741ce142", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-5468cac86bad49c59d00717f390838b1", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-8937427678224668b38b99cdc432dd8f", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-4c9ce8924e254e1b88ef3f4d173c97d8", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-0e867e5f5122448f9a6289784be4a69b", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-20455c51009b4666989e29852f516725", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-af64173f54b441c898552f2a2b32c8d5", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-ee7aed6fe4e74917a2c0f9df12b543dc", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-1fead793429f4dd88969a668cf7d4cd0", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-f53938863c6e4b3d9ad6eaeca4a1fc84", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-9b3342ed5c874253aa7995528cf0e00d", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-90e22d3a2e564715ae8801172f8111de", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-cc614e7c84a8407b9dfed947a8cc704b", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-2cd9b52a27784ca9bc4578b9cfd8e849", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-7fcd4d75313e48a58dda1709a55dd7d9", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-c2f0fc2535454e36a41e8758f534aa8d", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-b3fa8d8e2af9470d9ece6a5d247a3574", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-b015eb01acad4f6595e52cb9b1aa1aed", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-e086bbf3b2bf497e9809f8f8d9c72af5", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-d9e53aa6bcb14f2fad6ffb5b0d694c12", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-4e86e314d60243f285958267bd6d6ac0", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-dd1abfeeebc14a9e8d6b677119f8f39e", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-4bfcdd081d1042ca82821e13d501f81d", "references": ["treasure"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1155-00bb7fa8989743b09c5f8603738c86b3", "references": ["trash"], "task_id": "task1155_bard_analogical_reasoning_trash_or_treasure", "task_category": "Word Analogy", "track": "default"}
{"id": "task1385-8055154d7fcc403ca76d16988e37a2d9", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-df5802fea06344f989e0211bd6291a9e", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-a6da376a40cc412c86eed661a810ac1c", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-11731cbf5bac4726a81494fd3d20bd22", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-2f7582c17899479092de8aefc0d9c1d9", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-c9ac63a7cef84849800dbb42443f23a4", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-b81f25b663554d72bdf3e4a197e01025", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-36dff548c2b740668ae9dc51579d0e7d", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-fd278fd0a1704b3786bba1d4f6eaf0ee", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-c89dab659c7c479e837ba98f4d1b77f4", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-4f8608f03853460ea89c963b7da71b0d", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-792282b5de4f446f9b6ebac65a83bb3d", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-02df210ad2dc4bf0a93a4c5556c6a903", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-24197effe1b74f03b21d0f6d3e5ac094", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-21e9799c7a75491cad7981764c5d57f5", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-7aba763fd076460f818840a31519bcb2", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-3b1f947626a64a82a061d1f6b9b8f04f", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-1413c0240f734d58aad97b6c78d74035", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-1c74ae2cb4ff4f1b978406599a5a020a", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-80216898d0e840c2951a5123a68ac312", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-ee9f040329814549b369653a12fd22a9", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-df4ed18f581c4687b407a500e8d756a0", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-fcf2be0e17e744ceae45860759efe807", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-c2b0a969d25b494ab16a552d212aff72", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-8918e38bddaf4413a2183ca51a0a8368", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-4f703a28262d4922af94a367be67ed4c", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-c984c56448414ec0bc192c7d21cdb935", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-e6ab998d176743ad9b65f90751d34c37", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-ac8b28fc17e84a049e4be8d9a99438ad", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-3c61850d59dc469fa48c70cc958df0f1", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-9b67d9a419fa409f98ce928ce5c9b5d9", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-68eb9779a093446f811bd5a68d24594d", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-86a12fff784f4fc585955699993832fc", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-72fefff8b0b44fdcaee331d3045e0516", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-45a3b84c38ca4b169353d139320ea974", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-a6c6a242f9214bf48b164fb5bd890edf", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-f0659e2a581c4e42bb53e88a1f3f9300", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-1042553779984db1b0335ca0aa6676ed", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-85790f8d20324446befaeca25efa0888", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-b8212f2e846f46189719e660c0c3bc2c", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-5efce778e98143078f75f1900d9f95a2", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-381f62848dc3403992850369dd297a30", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-83e398fff9874643958cd1b170284249", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-d19ab294a2bc47e08378d83be526ffdf", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-b6af9abbdba246e8af4038413467f877", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-fcfdfb958d524f4d86c6f97290750e50", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-70b7ea34121047b1ac52763f1ee3a093", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-f0e5ec8567d34a25890299910423d6a8", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-3797bc5f032842c2bae24db8bf25a9d2", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-b2e599ceab5043b2b09a2fd427b2fc8c", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-512a33e562e9417f9b08fd1b5910d1f8", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-ab858aa203d84dffb55f4e404c0b7d0e", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-fa9a43da5e054646869c3f37b78ff065", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-b74de621e8e34718945d0039723b446e", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-9ee13093a2a54b2db7f6d333b210af8f", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-a4dd22498c754b25a45224095f902c5b", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-3c7c3975f7274fc882018e19a7c49878", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-c51a08516fb3468095e7a9e786ba6a92", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-6966ad0f118347bba45c4b1177072960", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-656f125ed3f64a1db7b29e2b51b75870", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-6ae6ee1367ea441fa8b19fb82b5ab9fb", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-f2ed7fdd42914049838aaa00dcb31b11", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-28ee6b26dfab44bc96c5658ee4dc88ed", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-3bc7fa8c1b8d431984c70a89aa9d12ea", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-8256a5a52e1940b196118264cfd0531b", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-b7b851ca74d648b18a4e75d55f89491c", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-cd4633e011dd42769b8c6def50cf1ae6", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-2b66ff560d6c4602b6255609ede96510", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-0c311a5848f044f8904db383bf29b36f", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-985259169a5d4c7b8ec7775a9de99303", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-f9e2d8fbcb39417c97e982a47752b88f", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-4f8c462f42b54ce49c10d8de12871275", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-7a2d0be4850a4b37a7eda3bf8424ab15", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-0200ed0467934e4da49bd9f88f331543", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-f1824d1b5fa84ad3bf1f2904d6245a3f", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-0af1f3977ca741f68b33e85a31839ff0", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-9774f25b91b343a295472af2d3efbdd8", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-86a8db5321594c5189c785befd62b7a7", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-5dac07f596404d87b941c403460683f2", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-6b8f970e9c6640aba7a9fbeb586994ed", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-e23eb854ecfd450788ba1e6b06971c47", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-22ebce1382af4be29c6438788df4493b", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-849b7135e9e74fbcbf6c45b26492a231", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-68d503d2f3d24a5dba248e6e99a98ee8", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-489a924d3df748ae956dee9af39f6e6a", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-17a030e9a8bd4ecaa0d2ac03c20b0d5e", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-32515009635b4b3da8852c175893e0ec", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-e8f02eefb153471bae840f9be2303923", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-24582a4d3abf43f2855286b916bcfd46", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-295c1074f6b5450f9f3c70cce034f362", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-39a032f5987f4a88bf231aa56aafc656", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-55ebae7516b144e7ab262c759ad7f0d8", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-d3977a9861e94220a05b480aff0d30bf", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-8a12c70eab0144e0b181eb478e930350", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-1a957eaf52f748ada41d60ca4e7cb87b", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-b1a15f7f7f294fd7a3b729ceef693706", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-618eec0946b644de85634cba8a719736", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-8874a7f9fff74a5bb1b70bd5d144b839", "references": ["Contradiction"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-713c5c5537bd4130bbc13576582b3348", "references": ["Entailment"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1385-6f7e01239e5b4aecb74cd03b538f4e79", "references": ["Neutral"], "task_id": "task1385_anli_r1_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1531-59441a6efd4b4ee5bd70af5a9d94e2c0", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-0d502287ae47406692217167ba3c92b2", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-1cfd8db689a24411bab3ac1c4ab193ef", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-6a382ff2c7224befa746b66f7067fbde", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-5343c93247a6431bbd3654daf9200f38", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-ec86628689f440c9b97923dfcfbb3eb6", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-b3676d020b884b90808949b08466070d", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-d193977a386e4fd7b0bbb2d24906a170", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-24a3aae25d4a488da513a12dde87bc18", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-5625d82a1c47429ab81265f597ff0c47", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-4b219bc06d7442f1b698b15880bba824", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-76f7e9422c674522adcf64d76b7f79c6", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-0bddce31ae5e437ca0218791e357ee45", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-135dcba713b04605bfd6bb41c7b8b29b", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-1bbd7ed3e2f545ca83a83467ff8aa011", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-8683fa42cf514ff696b3f692889d7102", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-99a9bed5f5f9446eb99484282a520324", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-3484cc6aa73b49248a524e5a8874f29d", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-b0c36fe0dfef42939bcb343875eb4ccf", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-edf34cf5a2cc41e1887c18e59fa88665", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-5271c10d22a14d7fbe6694d8a4865888", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-0560cababd9b471696723e972c8427c2", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-840298809d4b4dfc9a3adec5737ee4ec", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-6822da4c021949f0a5c01bdc423807b4", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-6985355360354182b2e1451d41a9e390", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-42cd5982489e483eb14f9dcb6841bfb9", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-8ab0df6b33cc424fb6cdaf5adc9cd7f5", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-2b8b6abd43124a259ec86a16b05ef398", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-c5dfc529699d4e11962878db2fd8b7f6", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-23db99b7dd814693babd782e9b30e3ce", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-e333322987ba4e6fabd89b150fe8d464", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-02c67943bb654d2d8762325ba3894f6c", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-4e3f5453903d4e1db2acb59d65c05dc4", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-837626f6ee9744bda6482724e22c7e8f", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-c095a5c3fa7e48a097b6d2ddb3bb87fb", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-72d6f1c78797415a8b88f4e1389f443b", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-a251ab45f7d24fc19abc03c0d1eb2ad3", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-697ec4fde38348e981372b4dc81d2a48", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-83e41a116f8e46448e560eced793d563", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-466158d312b04971833062d4f322a17c", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-0bc792c0dce14b3b83cb54aa0404a41f", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-e6a17b0cb2694466b28d2fe46a746022", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-ad609e1999064dfe92bfd6a7e2904da2", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-7c683f22948b4b54a822394541f9a1f5", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-ad6e94e12381412a93a8d756a2487cd0", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-7210b5bf24f0490cbf59174935ff2f7e", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-0f699971edd446cd9fdccbfc7298ea57", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-6d44d4da99944356af33ec7031e5e903", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-9c36c62bd91a4aceba230504729cc4d6", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-92c104ac39c742f3ae79c51cd105d277", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-364fd985fb5d406bbdbff2a62cb12314", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-0e75f8be68fb48f78be0321a823fdee9", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-170600907b3349f4a315cdf36eb24314", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-931e13f5b1de4fd48d27b897f5e92652", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-cadf0198d07e4d5b8f39ee9bbbe8ce19", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-2a39f9a15dde465481f329c7229724e5", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-162aa92428c4422491237056e57019d1", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-aeab6a04d57d4233b0838057e1420f6b", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-a79ccb321b734e5d84b6b495a2b9ac32", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-170f14addd2744fa87d5eb3522287421", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-048fae4fcdd94299a3aa3e98b06a9547", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-7ab712b25e294b3a8ce9285013524d59", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-05570695901f496b9bcf775db558d47e", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-882ab1389b6546eea4f0208ad43d7817", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-66fb89fe73fa451e88654ef239f6f73e", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-300ffcfab3d84519a47bc16b4d3034c7", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-4520a58e3bff487fa821da2daad5c606", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-4566e9077f9646e083d41066ad0382dc", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-c3b3e30f6de0489f8db571740a1887bf", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-99bc580399cf4923b791d5aef2e30490", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-fbd5c779cf824a1c93745b1b0ad944a6", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-349f16d3dd6e4bc9b42a5790a574df65", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-74e462479bcd4fc8b3a6ac2c6bef0dbb", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-4fa502ac872f4a0e88e04a61fff946c1", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-46a4b4d1c2584aadb7e6f5d267b83785", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-2647cedce0a546e9bb7286602907b1cf", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-5d0c6bd7461c451b8d0f58a738218b4b", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-0ebba2f388604dc5a001a39556ce0fd3", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-638c1872fbfe4da7bb6c033eb0c2a73b", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-79b7bc1233c545d09f0943a12609935b", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-f3bda58810834bdcaf8a1cbefaceddb8", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-65bb5b275a064bfeab705ed78f41cda3", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-88adb61a28f344da98351530a41c305c", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-8d033646af5c409dbbba751304c25a68", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-acda79597fc149eeb294bec87ff8516b", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-95ced9ece17d4d60a29209789f95a277", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-fc9de5d7c1d0445a871a6382ad111d64", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-289589c3f5ed452e9bd51ab62843131e", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-eb70b4c4c75247d384870700fd8b5f31", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-5e61c475d84a46cba2f3bdec77a9fa21", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-affe79fd4c4e44608308be606f7181fb", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-ec669342cb1947daaa03b602f061d7cb", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-e3344072fa874df3b09f3a846ec28afa", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-57549b3d28f6468e90b171f165cc439c", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-1afa7936e8954d4595a764b346e88261", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-5931267fa7bd4b72a89a0108fbb3345e", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-1f07f373f0c64756834c1e2b7aa26780", "references": ["directive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-6e6903b1abb04d4f8d1b631189b43b0f", "references": ["information"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-2ee26a3017f84654b80106dd43c398f6", "references": ["question"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1531-4fcebaab20114e2a95a20080e5e6ebf3", "references": ["commissive"], "task_id": "task1531_daily_dialog_type_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1516-78bc211250324f3dafcd9001394c46bc", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-5c01b101361f4d3c9ec5f3cdb72c00ce", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-1be04596b8d144e589027a80d458f077", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-61c8c9a822294d38ab105e0f5a3b6d4b", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-9c265b3862894ea096a2a4e2ee692d82", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-267df980baea4072a35fba02cbe02d7e", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-62306083c7e34189b00fe719f546229e", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-58e738c8eda54284a211eac56ee3dd92", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-c60509fc453247cabde486b72c96c266", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-5c5d21eaca784f8780305c19a1ca8995", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-9039f1b14ad1427aa603d34eb54fefdc", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-ae68e90dee994f46b23d00eb689d3164", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-2a363c8144a24fbdb7a26a15f4a4b477", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-830a45c1f0f4496eaee359c81efb94c8", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-5040f468495a4b7c855d06a85af8c030", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-07a5c985af044bd581864ef8dbe82038", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-41c273774e3e412a8100c8f8bae52220", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-4c98e7cc2d754c74b8275b23cf24f55e", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-68b4d2afbf8c4986a52d96dc1c9d4953", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-73afe50cd5d848f5b9e6075e3eb4df58", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-fe4499a143c048498f5cbebeddd3a851", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-30b6ee5668fb4e9c863f8365627fcbc2", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-2e29f329714b4564a6f382197aa911e2", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-6c8cc27fae7843afa33fddcd4e8d1c78", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-615d59dd49194cc08c297e4891277b48", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-fce8095c85a84addad9678266f9f8a97", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-a8724e7c9e484f25936c8255f32c0d6f", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-640bfa4efa0847789a14e70db6b8ae8d", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-581d879753b7487690b3a02e14adc540", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-e314c13735584b60b29f9d4d2f427f3f", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-d32624a25e8d4ca6bdfce63ac968285b", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-b8af015e65aa4dcca5d66224127e0c9e", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-7ebd855df0cf4039a5a65c1857f3cf0c", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-98eaa1a2b4e04cfdb9a3c5067f571619", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-937d453fe8194dc994ba13c600e62f9d", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-c65d38a45b644ce5981462743ca2fad3", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-0029440b043446e4ac0890e3c7be5c09", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-8dd626a6a17f4ddf9d36641a357b15e9", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-315011545e884ec381ffa03bdada7ea7", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-3aedf4cb1a2c4fe286ae02b12b4859aa", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-11cd0b687db449a59991762760b803c1", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-05d6e83e6e2643f58a62dc7cc713bb0d", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-48e2769160034f0cbe231c84ea4523bb", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-c5dac34bf8c14af8a520825ca09230f4", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-d20c1295e61d4989acbd631de363a5e5", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-86b260f497824b41bb03614ab6b524a3", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-334713e2365b44ec8146b1f9a2d899d9", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-9c99fa01f08f4b52a73b142525929b57", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-4ddb1ec105234f5aa6971271fd43cd64", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-bda262edcea7470b909281bd53d3d453", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-24211f5b3d814634a47370ca16569af3", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-aaac996e70ee419aa7b1b23612c456ea", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-fdd238762fd74f389343a6dba9c7cafb", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-2e5a9b17643a491ea8a4e419f7bb2caf", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-993b590dda3f4408941cbdf2ac03994b", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-779f67062e0e439b99d679a138c821bf", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-d7187a2f5d944864b9fab782753f4d51", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-677fce01230d46ffad3acbe1c3a75347", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-396621c8f5b94c9e89d32bc3447def88", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-b330dd122e8545f5b0bd37f528dd04c9", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-2c55fa35bfbd4b73971a657edf4db46e", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-e9f48c7ede104e7fb613ac02db3f757b", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-ef1a91f3c6684b05aabf2e0f6f8042ce", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-e5cf2647015c4b4ea9e669add4e55c6e", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-36fb7e12d8ce4d28ac5d0e6c751ec1d2", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-001c669b5f794d749d4a49af34d8caee", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-7349561b73454e40858816e895f6c154", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-b82acddc51cc49f789604abcc8dcf6d3", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-5a14788796a14f4fa2015258e5305aad", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-d96209aa21624199a3915d585f6cedd3", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-300a1fb998bf47f09381488773a04fce", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-bccecc0f16ba43dfbc91e3f87737ed0e", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-aa37c500a73c4ef7ba4bbfc603f0e4c3", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-125e520f6f1340a7b9c090d8bad18b41", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-722e63fddc91433e9dabd62eb91ca891", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-b3a0508cdce3477b9c7d53ae71a46143", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-e32d2b812f8a4359bf3ecd47a316edcb", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-f6e95b3f229e430580d312cbbf72afa9", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-fcc1572022724554ab01ada1f9cc3770", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-3224ce66feda49e9a4ad8d95fb815c6f", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-eed9bf5f9c8d4f1487d54d39293db164", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-ca382dd64cd749549fe0f8b6131cf711", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-b868f1b33b6d4f15984b5815f309ff1a", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-8f7fd43e8fd240e399aa8892316a8f3e", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-c00c55a9cf514a8882c540cf8fe137a2", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-c6f2f9e17c6f43a09f6954fbcaaff54a", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-3a03a1fca69a49ef8bf8f4782b2d2607", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-ddc53e86841a43ec980831ec207bc6d8", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-184696ab0052408190c5bf0c827a1f45", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-0846293dff5b4933be877b2335e62b7b", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-eaf656d113134b53bd0aa2ed4e33c5fa", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-998245bc082345b0b14140c6d70f5dd6", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-7966f46a90af40e6be2b6d564ec20725", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-9b522477ae044e33a8e57acbaf8d2986", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-6ecb5332360240f788469649a6861735", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-8b77f3e1b4c744a688dd36780e1bfadd", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-e2b7cc34329245a5b3b676ee04efa7fa", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-891132ed2aed489bbf6d0c64fe3ae9dc", "references": ["positive"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-1a1c41968e47425b9d7838fe3165c094", "references": ["negated"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1516-b6526f916b084d268e174c188aa048a0", "references": ["neutral"], "task_id": "task1516_imppres_naturallanguageinference", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1394-53d2b299acb44f44a087e15de332feab", "references": ["UPDATE_CALENDAR"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-42fd239e2e3741689ba55d2802431e37", "references": ["ORDER_PIZZA"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-9c6100fd65bf4d2abd8242faa20e07fb", "references": ["MOVIE_LISTINGS"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-a3ef5995321e4b63b2c805732689b177", "references": ["EVENT_RESERVE"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-0eeec6666add448e99cd865855f27097", "references": ["WEATHER_CHECK"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-4aa63131638a4efa89aa6a9054e7cf75", "references": ["UPDATE_CONTACT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-1467c40667124c36b66e8aa43fdf10a4", "references": ["MAKE_RESTAURANT_RESERVATIONS"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-62e9026b1be74b088900d1ed270f21da", "references": ["EDIT_PLAYLIST"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-57e9b65b12504ab1993141a1cecb5b90", "references": ["LOOK_UP_INFO"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-94d60173c9334100acf1924328017c45", "references": ["SHOPPING"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-cf8d1a9e43d845d690511bf54a7e84cd", "references": ["STORE_DETAILS"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-837a7df5b5e3400f8ee5b5492918bb1b", "references": ["SPORTS_INFO"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-c9efd0a016be446e99dda2b1e357e546", "references": ["QUOTE_OF_THE_DAY_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-88e1d2a214e949aabd40ffadf75031fd", "references": ["HOW_TO_BASIC"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-cfacdde1a004438fb67c8158ec6d6258", "references": ["PROMPT_GENERATOR"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-7330c2a9a8bb4d579beb8f747b3fe688", "references": ["LIBRARY_REQUEST"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-0d265fb36cae4043b844f4d9595b1dce", "references": ["BANK_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-8a382576f5d74f03bea68d0b99c425fe", "references": ["RESTAURANT_PICKER"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-3a4d899685ca414189c01d0a511ad8a6", "references": ["PHONE_PLAN_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-1aaefbcb729a40bfb4a3563151e4b33f", "references": ["NAME_SUGGESTER"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-b2eb99394470460188b1fd84b57d84e0", "references": ["CITY_INFO"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-e447ff346c484666816ed33f250702f7", "references": ["MUSIC_SUGGESTER"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-d2d8232ebde8423caad97dcec1365e42", "references": ["AGREEMENT_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-b100054ee4ba493db2dad2b7da65d2f2", "references": ["PET_ADVICE"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-ab7a41be17174cd0ac082d3e3681254c", "references": ["GUINESS_CHECK"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-15574ad7005b4551820523392b5b951c", "references": ["GEOGRAPHY"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-06a7708263dc460d93ad96d987448b50", "references": ["ALARM_SET"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-c23366bc560d418e85e3ad049774396c", "references": ["CONTACT_MANAGER"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-0b4ce8aeedca487da7b21cf67dbf2267", "references": ["PHONE_SETTINGS"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-d6bfc32763184417ad12559942c41d58", "references": ["APPOINTMENT_REMINDER"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-bc3db71952994c3284e8b22b7493f210", "references": ["HOME_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-08a55561201144bc8950483f623fdc6d", "references": ["POLICY_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-4545fb254380485da0685771fb64bea2", "references": ["DECIDER_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-960534a23b2e4f728624686cb8966dd8", "references": ["CATALOGUE_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-462f0abcbf66475bbeb8bfc4cc5a6187", "references": ["SKI_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-10a079829891408297b60aa39a336f0d", "references": ["BUS_SCHEDULE_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-5a658f02b7b64e03b5dc5d13af9ab961", "references": ["INSURANCE"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-fb01cb52d7724505850b93f50d18ff5b", "references": ["WHAT_IS_IT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-b4ad05e8f0de40c0b3d28fa8e0d35ac7", "references": ["AUTO_SORT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-ebe2c2a983c348e1b1630845466c78b8", "references": ["SCAM_LOOKUP"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-7537e5aa35f74fc2a494971b40b9ebdb", "references": ["TIME_ZONE"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-3def3b0d868e4082a6c0a77fe25bc2c5", "references": ["PLAY_TIMES"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-9bdfb33457b7485884a3c3eff6ea10b4", "references": ["GAME_RULES"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-02e3077f9dd248069f4e68681a7581c8", "references": ["WEDDING_PLANNER"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-473dabfaf9fa487cb23ec458246ec80d", "references": ["CHECK_STATUS"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-8c3642c294ff473598ddec08ac78261b", "references": ["PRESENT_IDEAS"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-c68736a3d95849f292241bf02a95d2e7", "references": ["APARTMENT_FINDER"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-0adeefd15c3a43569ecd6ce9fa032dd4", "references": ["UPDATE_CALENDAR"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-26411d4f0446461ba421645525542704", "references": ["ORDER_PIZZA"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-d79fc9e8380948e7b756af076e88d72c", "references": ["MOVIE_LISTINGS"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-61de97a571e043c7bfe5ad79df055f08", "references": ["EVENT_RESERVE"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-d1e89fa2dee547ef948d45a916d52c80", "references": ["WEATHER_CHECK"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-9547e1043ed74be59628dc0a37ac3fe4", "references": ["UPDATE_CONTACT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-3675c0a1a0f140568735813194c55c45", "references": ["MAKE_RESTAURANT_RESERVATIONS"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-a90b3454a9c34fc0abd38aef05ced2cf", "references": ["EDIT_PLAYLIST"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-d8f54eab7b7d4038999ff39b74bb7007", "references": ["LOOK_UP_INFO"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-44dd19949f324a79bbe132d7914a7f9c", "references": ["SHOPPING"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-da060293a42b479ab35fbb1c2d53c706", "references": ["STORE_DETAILS"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-6cf1ea2492c04abdab61d7c141b82331", "references": ["SPORTS_INFO"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-85b5c4245a73498789f3c0bdb9571cbf", "references": ["QUOTE_OF_THE_DAY_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-344962cac5d343e48b7fa9217c4c28ed", "references": ["HOW_TO_BASIC"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-a85e5e54ae734bbc90980391520f72b2", "references": ["PROMPT_GENERATOR"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-d3ca4b6b6b9f428ca5d6f439e9ccbea3", "references": ["LIBRARY_REQUEST"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-56ed85fdde5749c9b0dd48738fc7fb8b", "references": ["BANK_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-d107ab32c594484d942017bc418715b4", "references": ["RESTAURANT_PICKER"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-bd048616c5fc4b41b2d7cc30f83da95c", "references": ["PHONE_PLAN_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-d7653081de364857b56e1a9e1cf34a57", "references": ["NAME_SUGGESTER"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-be4094a92c9543e9b01de0e96c8a798b", "references": ["CITY_INFO"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-f13cda409d6e4fadbdb7cb953740ecbc", "references": ["MUSIC_SUGGESTER"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-e2ff89a7feb348f794f683c1b3ac98ae", "references": ["AGREEMENT_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-e64f9ad0aae04eacba7a64b49e3814fb", "references": ["PET_ADVICE"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-17c4457b08264d97ab25ee5bf8507e12", "references": ["GUINESS_CHECK"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-4693526e93924f62889a6dbe85063e64", "references": ["GEOGRAPHY"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-ef6e093ece3c46e4ba90f421d74a4f0e", "references": ["ALARM_SET"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-6dc93de853094581a9ae285d513d63d9", "references": ["CONTACT_MANAGER"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-537e7252998b49219b7a87c22d766471", "references": ["PHONE_SETTINGS"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-b9d76677c63a4701b87a1b2a5e7e2a47", "references": ["APPOINTMENT_REMINDER"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-4a687c1412a94ed7acbb5da8d740686d", "references": ["HOME_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-2ebae81cb3474d0d91b3221ab65323d9", "references": ["POLICY_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-719f25a7df734b0e9aafe44087fb1324", "references": ["DECIDER_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-4ad4b2f201da4794927c872ee900c287", "references": ["CATALOGUE_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-3f82583f4b6f4827bb245bbe2ed84d35", "references": ["SKI_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-f23c57b6b5be4edfaf720501772315ca", "references": ["BUS_SCHEDULE_BOT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-c70a50628a184babbbaeb96ba41936af", "references": ["INSURANCE"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-3adbdaba67e14648a47145faea554101", "references": ["WHAT_IS_IT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-14f6b04875fa45c2a7305370f232b7fc", "references": ["AUTO_SORT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-143d48e0c04e4cc394eecc8d1f009e0b", "references": ["SCAM_LOOKUP"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-109d7c5a3fe64664804814006a0e8670", "references": ["TIME_ZONE"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-f18ed1697b2c4e73be4ed5c375dd8c20", "references": ["PLAY_TIMES"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-481708fc1f2948bdb30c5bc7ec287516", "references": ["GAME_RULES"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-4d4cc3b2bbdf403ba44c13ec58efbb28", "references": ["WEDDING_PLANNER"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-58115101411848409658246abdd5f625", "references": ["CHECK_STATUS"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-f756808b60564140bc1755247c719e6f", "references": ["PRESENT_IDEAS"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-e01ba1ce037441edba075e3338792457", "references": ["APARTMENT_FINDER"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-1c4f524cec9e4db5bcc5d08d7e12e78b", "references": ["UPDATE_CALENDAR"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-992613924ae44652b64d1588be4ca215", "references": ["MOVIE_LISTINGS"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-6aa7ad72b5674f628e316f61b83d7da6", "references": ["EVENT_RESERVE"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-0c012947f72b410299c69a9e326a945d", "references": ["WEATHER_CHECK"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-0e6784aab71a4de0bd6a0d7e97ef25a2", "references": ["UPDATE_CONTACT"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1394-1766d325657847ea89acc6525805db9b", "references": ["MAKE_RESTAURANT_RESERVATIONS"], "task_id": "task1394_meta_woz_task_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task401-00893066ed6445e29fdddf6fb50c73c5", "references": ["slut"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-8b764e1f4599490589800a5d71d7ffb4", "references": ["body"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-18f99575b31e4c62a0895726c1f54cab", "references": ["submarine"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-bc99f843cc5548149d44c9289932b482", "references": ["Run"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-3c8a8b1c4f23449193a6d800f8a6550f", "references": ["plenty"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-1cb96e4f71544edc894f4c275d04cf92", "references": ["disaster"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-ca4f7ad240b143a294ad04cc8c9a6886", "references": ["dad"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-516e0a10e95c44178c8576f5e6c96d7f", "references": ["weapon"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-50f67c2aa43847449ed885ffd4738bc9", "references": ["darts"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-7794bc6935824679a5ce33106db57e70", "references": ["vagina"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-dcedffbdeb594d479df884e01767d360", "references": ["soul"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-60a6423ae6384a2d8e0df28331782703", "references": ["lizard"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-cea3e84af42a44629bd5159c1770f7b9", "references": ["dress"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-3c3f2c6a41b540ecb37d1eeb816f69d0", "references": ["car"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-4d7184cbd914402383f4b09b253a1d91", "references": ["man"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-1b8dd0b3ed784f75b5eba4d1d389b5b3", "references": ["sermon"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-17ac244c2dd5447fa32d4ccf1b8a4936", "references": ["damsel"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-e5cadc8850fb4d6ebe4e1963fbb05422", "references": ["percent"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-5302f05d81844dc6b4ccb0bd9e94173e", "references": ["games"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-269b4f14e3e844cb859019f79cffc017", "references": ["gift"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-d78908a2f5f74f4682e43a80509f29ac", "references": ["calls"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-cffc6883bd934038b0bfea6359887aae", "references": ["clips"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-64fbc0a373d745b0887004156210920a", "references": ["life"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-7a5adb760b394bb6bd0eb17aad95951e", "references": ["scores"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-36814b3c946b407f9b200fbe19b64bbb", "references": ["films"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-9738300e9296488fb6eb92afabcb3879", "references": ["cleaners"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-735a3cb4fc38453eb5bdd699360285e5", "references": ["dame"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-7e8693610e9747ec8dd624db10863d13", "references": ["password"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-54b2d9e7965a4313a70463878411c034", "references": ["Darlene"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-55e3f93a34a2455fb65cfcb528449e1b", "references": ["scale"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-3f07465c823643f8bbd5e1292bc32267", "references": ["endings"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-6e73600e2e7741a5b29f4ce9e48b9f60", "references": ["telephone"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-c1baaeca7aa24094b8fbe7817304a7f5", "references": ["barfs"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-2976fb1dbe864747ae4c58a70b9359b3", "references": ["family"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-c3ed1cc450494381b8ad1708bac1c5b7", "references": ["pillock"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-5c1f1979311b49e9a66ba264448cf942", "references": ["girls"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-4b5f6c4913064ff8aebabbfd77d87531", "references": ["Faith"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-358f8a3287cc4d3483db173a60e72838", "references": ["fire"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-52757346998e4d5db0616f63f53a8e4d", "references": ["quinjet"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-80bcdcd20a2d4a26b0a7d26509dbc071", "references": ["columns"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-e9070887df434a319391a1871d44af6a", "references": ["Africans"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-130771fbd5c846a7a34634b58a2639c5", "references": ["eagles"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-be49908f54ff4a07bab6a5fc07eaaa9e", "references": ["drink"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-3663d16bc9fd422cbfb8f255e436878c", "references": ["status"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-372fa683a16e4f7386e10f5be0e2c878", "references": ["lawyer"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-f10aa610d21943879ac8d8bdcd9e4739", "references": ["photo"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-ba99c1159c9a4b4db403a31f62f03ff5", "references": ["moment"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-96dbc0e8dd7844c5a7f5e0023652afc3", "references": ["gun"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-50c498412605451bad867be900d27538", "references": ["tunes"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-4a7c9db5d32a40a8adb593121904df3e", "references": ["people"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-f39376392fa7428b8586860f55c39b4a", "references": ["bouquet"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-f2234252156f4e98a1cd140f1e660b6e", "references": ["beer"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-901d75a3ef6c4e78a329a60eba84371a", "references": ["dwarf"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-cd2db50e3bfa4ba9a38e2199605e3717", "references": ["weeks"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-4da8f1fadcd5487a924abe47b001ddb5", "references": ["watch"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-76d5ae28d0d84b3aaef1abb166dee875", "references": ["carcasses"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-75b424e09e7542a6b5c44de1b0d1c971", "references": ["dollar"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-e3f03b18312d4f1dbf86eaa546a1f122", "references": ["killer"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-a531444413054881986751b8f21a250c", "references": ["phone"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-80f3768c0f15472d93f1a6a66809feb7", "references": ["week"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-8755a4205e1a4014beac2370d33d4bdb", "references": ["rats"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-755e51e7be0746c5bec445bf85d5e8ee", "references": ["Wayans"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-0023dbd6c3554ecf998e3ba549067714", "references": ["wires"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-17b8a4ef98ff4f3a9ce220ce0c05c326", "references": ["line"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-4ef89899408242fb841670126b143479", "references": ["Hanholders"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-5340b32042dd429ab10eee47d229a338", "references": ["odds"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-fee738506b1a495a864328e70ba8fcad", "references": ["daughter"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-4855b05381ce4ac0b0380b768d384707", "references": ["desk"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-c644349fc74242108c7b1c72014f92e2", "references": ["sales"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-9d40e7a7d1744a43aec2bbfb5a3c77fe", "references": ["tickets"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-fda40256238840f99f5e7ed674a7fe71", "references": ["buttons"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-a64da50999234d279307581cbb112284", "references": ["times"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-45f5fc95fe30472ab91861fe52fcf033", "references": ["number"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-75ea6970400345bc8092e81b2643f2a3", "references": ["weakness"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-2bd2607fde46403cbdde54f3e97e521d", "references": ["Signals"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-283e7792de3447b7a30977743573471d", "references": ["bill"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-f7e0461906b944558b166678aafb679c", "references": ["opponent"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-52d005ec2097456aa22a33480317607f", "references": ["age"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-ff7f4291516442c6b1d2b72dd52a3eb8", "references": ["mistake"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-7693d9adc5ec441182dbfbe43389240c", "references": ["year"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-78c3108f77b448a08670dc60dfe5314a", "references": ["answer"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-e025f59ae878456a83434e694a9be2e6", "references": ["foreigners"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-5abdcc28a53443b3b6d36c152cebaa67", "references": ["story"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-00dedd1dcbc343f1978ee1bf641a593b", "references": ["side"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-86890f31aea541529a22abbb321f04a5", "references": ["tows"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-f6f7481086ba4033b1ae0277d6542998", "references": ["days"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-5bd0ebd9838d493db6e38c235648604f", "references": ["rabbits"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-1e05d0c59ac542f48d5b0c85b87d8382", "references": ["Zofran"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-88e4a94312c24a5195acd7e3e9a14d64", "references": ["problem"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-429edfe134164b3c8a331505e3d4ef15", "references": ["eye"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-8865f795ffa040af9c56396e09c61ef8", "references": ["bodies"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-237a43fab7f04195b9d711c55d4e9dcc", "references": ["mutants"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-5fe6789c7cb24a0cb5ac3b046ea4c978", "references": ["license"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-302d654fad5a484bbafd5839e61f4dd1", "references": ["movie"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-efb3569aa564448fad0021f361844f3e", "references": ["set"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-b93c5d89ed06450380c8cc266a247a41", "references": ["idea"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-18bbe4f102a441468a0c872275976343", "references": ["partners"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-281dacf97f15475fbd7310c962762c15", "references": ["murder"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-46f653e8a6db42a78437d81ca14aa8ce", "references": ["coffin"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task401-0e67b6af9cd44dedbc4accd77f67d37d", "references": ["sister"], "task_id": "task401_numeric_fused_head_reference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1598-22b411d5e58a434fa168d5074b3cbfa1", "references": ["xname is the best restaurant since it is in the Upper East Side with welcoming staff and mediocre decor and xname is an Italian place and expensive."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-a2e371f9df364bd9bab8fbff04b60564", "references": ["xname can be found close to xnear.  It is a pub which serves English food and is child friendly."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-2cfcdb6ddac54914b530fe698973a124", "references": ["The is a cheap fast food restaurant named xname."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-9e7f3e5c556044c8982e09e44ba6aabc", "references": ["A low rated coffee shop in the city centre near xnear is xname, which serves Chinese food."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-9986e41050ae464b9ec14087681831f2", "references": ["In the city centre, close to xnear, there is an Italian restaurant called xname. It has a customer rating of 5 out of 5."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-e35918dfd14d4a38b6d109f2f3c2bf0e", "references": ["xname serves average Chinese food in the city centre."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-4ce95e283205482c8c98130b1898fbca", "references": ["xname is a coffee shop style restaurant, which is kid friendly, and has a customer rating of 3 out of 5.  The price range is on the high side and located near city centre."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-9a8f1bebce554befa748166256830237", "references": ["xname is the best place because it provides decent food. xname is in Midtown East."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-c69b5c6973d94ad5bf8edc317ddfe983", "references": ["xname is a coffee shop serving cheap Italian food.  It has a customer rating of 5 out of 5 but is not family-friendly."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-3bbc1188eaad4914be49ba9d7a1b548d", "references": ["I would recommend xname since it is a New American restaurant and very pricey. it's in Manhattan with adequate staff and decent ambiance."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-cc945bb9be3f475f90c62f81faf4b186", "references": ["xname is a mid priced family friendly Italian restaurant.  It has a low rating."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-e63e8ef64e0745b5a257d36cfcdcdaf3", "references": ["The xname is a quiet establishment near the city center with dishes less than \u00a320."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-5d7c7d5a383d427bac8a662f016966b8", "references": ["xname is a coffee shop providing Indian food in the high price range. It is located in the riverside. It is near xnear."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-228d8a1b9e714efaab0cdb1296dc3459", "references": ["There is an English coffee shop in city centre that is non child friendly with a high customer rating and a higher price range named xname."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-28bb7a86b3484622bd9dd2e3da723a64", "references": ["xname is a coffee shop situated near xnear in the city centre. With a poor customer rating of 1 out of 5, a high price range, and a no for children, it's not the best of options."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-6c8c8af6b8fa434c8791ac1f956176a2", "references": ["There is an average rated coffee shop xname located in the city centre near xnear that provides Italian Food. It is not family-friendly."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-5996ee193ab749d5b4b1a738c0f36889", "references": ["xname provides Indian food It is located in the city centre."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-a01f5685157941de97d6ffe4268d9e4d", "references": ["xname has mediocre ambiance, but it is cheap though. xname is in Chelsea. It is a Southern place. It is the best restaurant."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-0dea85cc10ce485a916886c58841dda5", "references": ["I would suggest xname because it provides decent food. xname is a Chinese place. it's in the Upper East Side."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-e6aa2b54f4cd4888892ee0e87158e5e2", "references": ["The xname is a child friendly Japanese place in the city centre which is moderately priced"], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-7415d69a143e4852836f079486bddbed", "references": ["Located near xnear, xname is a moderately priced restaurant with a kid-friendly environment."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-206fec3617fa4e7a93655b6fc2e5ba5f", "references": ["xname is the best place because it is in Murray Hill with adequate food."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-2d164c0dfa43496cb43db58723b6e401", "references": ["I would consider xname because even if it is cheap, it has mediocre ambiance and bad service. xname is a New American restaurant. it's in Manhattan."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-b155fd524a464146ab418bfa15da8e8a", "references": ["I would suggest xname since xname is affordable and it is in Manhattan with rude staff and bad ambiance. it's an Indian restaurant. it provides acceptable food."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-cf6ddd698df24cf4978ec0f6652177f4", "references": ["xname is an excellent restaurant located in the city centre."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-5dc0effe9ba44faba09f806820d8a23e", "references": ["I would recommend xname because it's in Midtown with acceptable decor and adequate staff and xname is a Japanese restaurant and very pricey."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-f7461c7e1bca4dbb9ead05841e5a75c2", "references": ["xname is an inexpensive, family-friendly Indian restaurant."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-ffc7e5e53b1d434bb9c710a1c90bbcfa", "references": ["Near xnear is a pub called xname. The price range is high and the customer rating is 1 out of 5."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-6e535c6a3c454db480b1c976a3ca700c", "references": ["Since xname is in Chinatown and it is a Chinese restaurant and pricey, it is the best place."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-36ccb10cb96e4fca9e02585333e24474", "references": ["xname offers expensive Chinese food by the river. xname has an average rating."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-359a014598f941c9985a5bbddb36bd35", "references": ["I would suggest xname since it is an Italian place and very pricey. it's in Midtown West with acceptable food, welcoming staff and bad decor."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-18e4367b12ec406ea5ceb394feee94d9", "references": ["I see, because xname is an Italian place with good staff, it is the best restaurant."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-df328fd1a8bb4c33a06d29341fa3accb", "references": ["Located near xnear is a kids-friendly place called xname."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-487a5c97f55c43dabd2132b58da255d7", "references": ["xname is one of my favourite restaurant because it's in Manhattan with good staff. It is cheap."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-f2f1b89854b548ff989e6dbb4cdc4ecc", "references": ["A high end, English grub and coffee shop - xname. Good ratings, near xnear."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-294db19b358940fab6cca76bf219daeb", "references": ["A restaurant which is children friendly would be xname."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-069429a1fa1d426890cd6c9e88ef3779", "references": ["I would recommend xname because it's affordable and a Fast Food restaurant with mediocre food."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-3dca643fbcb04988b1d88385c5d5cc60", "references": ["Located near xnear, xname is a coffee shop with an average customer rating. This coffee shop also sells Indian food and is children friendly."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-b8ed6dd54cf74ae8a531e12eadaffb73", "references": ["xname is a coffee shop providing Chinese food in the cheap price range. It is located in the city centre. It is near xnear. Its customer rating is average."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-cdb1ea2a65b4484ca43181c87af1a090", "references": ["xname is the best place since xname is in Chelsea with acceptable food. xname is a Mexican restaurant."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-3c993a1dbfea472aa622d537a74accdd", "references": ["xname is a fast food restaurant which has a family-friendly atmosphere. The food price range is below \u00a320. xname  is located in the city centre."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-5402a490c88f4d6e85d179cdf088e2c2", "references": ["I believe xname is alright because it is pricey with good ambiance, but it provides flavorful food though. It is in Midtown. it offers great staff."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-4a7bf9e2cda24cbab1dd18c6eda22e32", "references": ["I would suggest xname because it's in Midtown with acceptable decor with good food and it is affordable."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-c2b840950256402d9f8955c009fa1ba4", "references": ["Located near xnear, xname Chinese restaurant is child friendly but be prepared to pay more than 30 British pounds."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-adb1cab76259437db358b739377934c7", "references": ["For Fast food xname coffee shop located  in the city centre, near xnear  and has food under \u00a320 price range.  It has a low customer rating and is family-friendly"], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-88b1a6ab6f6242ee8f7b5d92f8de52ae", "references": ["xname, sited near xnear, Luton, is a family-friendly coffee shop serving Italian food at mid-range prices."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-48878fe17f634adc8cb7070f2b083d15", "references": ["xname offers French food for a price range of less than \u00a320. It has an average customer rating."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-087b7b1538104e07952daafe8898dc1b", "references": ["xname serves Chinese food and is by xnear. It has an average customer rating and is also not family friendly."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-b3c07c822fc94890a5835c848c654e71", "references": ["xname is a coffee shop along the river near xnear. The prices are quite high while the customer ratings are quite low. It is not recommended to take children there."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-3372b3b0c2ea4478987fc1543dac7402", "references": ["xname is an English food coffee shop style eatery  with above average customer ratings near xnear in the riverside area."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-7c5b2fe7332f4e6b88e330e74994f0c1", "references": ["xname is the only restaurant I would suggest since it provides rude staff and bad ambiance. It is an Italian place with satisfying food. it's in Chinatown. xname is affordable."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-d2b323329b104adf8b089a6ebc5bb200", "references": ["xname It is for the whole family"], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-6c99c4585ebc494d9f5900460642df41", "references": ["xname is a French coffee shop located in the city centre. It is family-friendly and has a average customer rating offering prices less than \u00a320."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-649d3d4d6bd64d89a0380eceb8c20369", "references": ["xname offers the best Indian food around and is children friendly. xname  has a customer rating of 1 out of 5 and is in the high price range."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-9403d8250fe9472d80d3e0fa2a4a832a", "references": ["xname is one of my favourite places since it is affordable and a Thai restaurant with bad decor."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-75371c26eb6543da88c580d1c76ce9d2", "references": ["xname is located near xnear. It is a child friendly coffee shop with an average rating. It falls into the high price range."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-e0bf120a756e417fb2ecd9a48cf03d5c", "references": ["xname is a high priced venue near xnear in the centre of the city with a customer rating of 3 out of 5"], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-a354b283d1474966a53ec921b0992888", "references": ["Dishes costing less than \u00a320 are served in the riverside restaurant xname"], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-5a0b6a20cea843b3a11b024278beba29", "references": ["xname is a kid friendly restaurant serving Japanese food. It is an average priced restaurant but has a low customer rating."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-c14e07d2a6c845f29116543599f8c130", "references": ["I know xname is alright since it is affordable and a Vietnamese place. Blue Velvet is in the East Village. it offers good food."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-482e8d1c80be4613a5eb2897abb9dcea", "references": ["xname is a coffee shop that provides Indian food. The price range is \u00a320-25. It has a high customer rating and is kids friendly."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-6386aa32deda4144b3a927d8b1e0e999", "references": ["xname is an Mediterranean restaurant with decent food. this place is in the Upper West Side. it's cheap. I would suggest it."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-aaae5c432ad94f3fae0372683d8de4fe", "references": ["A high priced English coffee shop, xname near the city centre's xnear had achieved a poor rating from its customers and isn't child friendly."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-fbc376c08ec04365a25f37513284af02", "references": ["xname is a coffee shop providing Indian food in the more than \u00a330 price range. It is located in the riverside. It is near xnear. Its customer rating is high."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-f3d46add1c934c00811adebc5c21ffa0", "references": ["xname is one of my favourite restaurant because it is affordable and an Italian place. xname is kind of in TriBeCa/SoHo."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-da388f1eb72145ad822c74070eae7c2d", "references": ["xname is the best place since even if it has good ambiance, it is expensive with welcoming staff. az is in Manhattan. It is a New American restaurant. It provides acceptable food, alright?"], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-a44ae7aa3d1542b18847c55b5a433bf7", "references": ["The price range is high, the customer ratings are 1 out of 5 stars, which is low; nevertheless, the xname pub is a fast food chain enjoyable to everyone."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-d4fad45494304be4a61cf2931aadd91d", "references": ["Because the xname is affordable and a Fast Food restaurant with decent food and satisfying staff, the it is one of my favourite places."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-09345465710143b5a82388a5bd2b7051", "references": ["xname is a cheap coffee shop for individuals near xnear with a customer rating of 5 out of 5."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-fc9f6cf5a1474c718a3317ed37da4c99", "references": ["xname is the best restaurant since it is affordable and a French place. It is in the West Village."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-9efc7cb79aca48fe955e89e33b56560d", "references": ["xname is the best place because it is affordable. It is in Flatiron/Union Square with acceptable decor and adequate food."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-ea5975593c734e00b79dfeb116e144fd", "references": ["xname   which has a customer rating or around 3 out of 5 serves French food.  The food is of a high price , and is not children friendly.  It is situated in the riverside area which is also close to  The  xnear."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-c7d1c11981424b098dc16f4416c98914", "references": ["xname is a moderately priced English restaurant situated in the city centre. It has a low customer rating."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-40b14e07718c4e278002cc52f10a81e8", "references": ["xname has a high price range, and is Children friendly. The customer ratings are average."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-e33f66ad57f24a1bb40d8f93371dde79", "references": ["xname is fast food that has a high rating."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-43e4d26825754de3bcda029dfbc69718", "references": ["I would suggest xname since xname is affordable and a Thai restaurant. It is in Manhattan. it provides acceptable food."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-a0654933b0554594a6fd439ef23dc131", "references": ["xname is an Indian restaurant near riverside with a customer rating of 3 out of 5. It has a moderate price range."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-6aef9e3e1e674664a56eefc3e6fae59b", "references": ["Because xname provides good food and decent service and it is affordable and an Italian restaurant, I would suggest it."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-2bea28ac6edc45619cd5f5fdee8d5f7b", "references": ["Located on the riverside near to xnear, xname is a coffee shop specializing in Italian fare. The price range is within twenty pounds, and it has a low customer rating."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-3096c62a9c1141c8b9e24c74cc0dcd8a", "references": ["xname is one of my favourite places because it is a Latin American restaurant. It is in the West Village with satisfying food."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-d25f1bbcdecb4cf29be56e1b3eb728e4", "references": ["There is a moderately priced Indian restaurant xname, rated 3 out of 5 by customers, near xnear in Riverside."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-ae290849106440ef8bf4ad65198c43a5", "references": ["xname offers Italian food. It has a customer rating of 5 out of 5 and is family friendly."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-ee93e967aae34d2bbd7f10d6a6fc4e4b", "references": ["I would suggest xname because it is a French place and expensive with bad ambiance. it's in Manhattan."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-619285c65ade400bbd9f108f091a0ee8", "references": ["In river side, xname is an Indian pub which is family friendly in a price range of \u00a320."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-7a20a256a99d4ba792f03834701dc881", "references": ["Yeah, xname is the best place because it is an Italian restaurant with adequate staff."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-a721f41d103d4ae6bd44e11b7b39add9", "references": ["The xname pub is an affordable place to dine."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-4e09b567ef9842d1b8ebde2401d4e47a", "references": ["xname is a moderately priced coffee shop serving Chinese food that received a customer rating of 3 out of 5. Kid friendly and located in riverside."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-c860c5c60c2e46d48fe392c73061dd37", "references": ["xname is a fast food. pub with low customer ratings near xnear."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-4c60667e59344d15bbf39d3ca7678fa2", "references": ["xname is an average riverside restaurant serving Japanese food.  its price range is under \u00a320."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-e1ddb30e43eb41b18d89b5b01ad8b37c", "references": ["I would suggest xname since it provides satisfying service and acceptable food. it's in the Upper East Side."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-1f497703b9464976a6c06c6385d61df2", "references": ["xname is the best place since it is a French restaurant with decent decor with good food, you see?"], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-2419ae3b00e94cafb6ee08cfd8974f20", "references": ["xname pub is near the xnear, which serves Japanese food with a cheap price range."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-4ffcd953b17b4ae3ba2d88869f0590af", "references": ["xname is a highly rated coffee shop in the City Centre near xnear. It is aimed at older upper class people serving fantastic coffee and delicious French Food."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-7c4874e7017442f19d45da9c2d580fd0", "references": ["I would recommend xname since it is a Fast Food restaurant with rude service and sort of mediocre ambiance. it's in TriBeCa/SoHo."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-59e36b7beeca4353babc292e2e0d9ceb", "references": ["xname is a cheap coffee shop where you can pick up a picnic lunch and wine and spirits that is great for families and has excellent reviews."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-55d22c13a0e44c2ab2e3929850dacb51", "references": ["An average rated restaurant located in the city centre is xname.  It is a family friendly Indian restaurant"], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-a9fbe8b41d634c32b1c855bc417c454b", "references": ["xname serves French food and is located in the center of the city. It is not family-friendly."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-019c566df6e54e30bcd5d2867e63e1e5", "references": ["xname is a highly rated Japanese eatery located by xnear. It is in the city centre, is not child friendly and has a price range of more than \u00a330."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-1de0283db3134f42aaccefe3e86ca7a3", "references": ["A children friendly restaurant named xname that serves French food. In the city centre area is a coffee shop with a price range of more than 30. It is near xnear."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1598-efef0fab7f964f31b457e9f211ad3eb2", "references": ["xname is a coffee shop that provides Indian food at a higher cost.  They are located in the center of the city near xnear and do not allow children."], "task_id": "task1598_nyc_long_text_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1615-de6cccc396ac435db7312f4d6331d603", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-b92ec02548a041478e1982612dabb402", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-82020768f3f7493b8f1c73b67b548031", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-f7964ffe39fd499a9bcbc9d5c39b6b02", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-4e8aff477e5f41bc8842b775b3847d81", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-a46ddbba4110484f97fc9eb7ec2d7369", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-1d7abd6832d24908b328f79d4f35a8a8", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-63ef076209b0429b959dbe7e4bc3901d", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-5c82e05285ab463e8617196bbf998faa", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-78390c8639304fe7b248bab5dbae0705", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-b5fee60a807644f394be8ea9adb17b90", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-f4eb9792fd6e49ffad7ea5e5b15e3a2d", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-2b9345ee803345bc9907022a37728935", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-4487aedcf0e24f7c8c38053333a9403d", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-de1d6064a924486a89a125e9f23341f3", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-e8771a22c0094267aa73a29374dd59fc", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-cb82f8ea5d194d01b65a5500a8c8e70b", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-5d43a7dc630a4804b76b33fd3123b20e", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-44eb8bc1b90a45c3a69e38d824d59da6", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-90c166765e4f484eadde57d5896501d4", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-04c217ac94c64f66b726091069cdba49", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-11e57fb93c4a4a6d83ff1124cd1297a7", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-ba9c870543964bbc8a75c34e5fe6a976", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-54acc829f98445659b73083dd0e310f1", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-40c11445b85c4205bb1f1646f5ff3bf5", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-bd435effc719460f9d06f538aaf242bc", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-9b9b2a36101d4ecc9e8461f840310eb9", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-0841adf81fcb4cc2a2b5a73d619b2247", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-a29bee85fdbe47da82d118bda0f7f4fa", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-46362740900143bfbf9a692c000b1074", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-14814638c3a5405599ca074b2fc13669", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-fa8cee88ea0640859d77bfa9eb361cf1", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-17e36c15251e44be9d2366f31d32f42d", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-d2bba8b606664f1dbe50fc9618570ca6", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-912e6740cb074342911e7bbd51994162", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-06cd4e3fa5ba457fb83aec6286ed6792", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-52ca50c816c849a4900633ba0a427cf0", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-8f21fdff378d4176b9b028a5b2ccdc01", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-2ad54246a49d4e62b0e7e3d7ae7fa487", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-fa1e8ddee455449988cdd717ca99b217", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-5195ca184f024478b39cff8afd5eb32d", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-f3c31fb2760d46fb8ee3dae58fbb41c5", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-06776607ef56425e8791817fb7c5cbf9", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-17f070024c4440aeaec56bda5ab1ba38", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-4eb81dc33fe04c098f989bb17ea27133", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-5e47bc5d526144e386b6e31959bc66c3", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-34834d1ca5db4560b950db451d2e4e98", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-3cca3dc5bd7a4ab5bb0020ed7a5dee76", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-9ced3d0e2f4f4db58e7d5912befecb28", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-774f4a099b30475c86e42bc4e03a2722", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-90f51520d8594027bbf9d84ed66e9a87", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-7dd634ca1b1e4b4c9608cc81c2248189", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-24f29ee6139c4be1a6036bc3f3e22ffd", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-74fb51b049f24efaa8efb2a22a88ebc1", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-e5cb28cc633f42a7badeb2f8372c9ffa", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-bbf58c5ee1ac4b12baaadbc6eb59b85c", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-45b4c14d5e00463eaab4147764cbd060", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-73b6b24e70ce47f7bd4985b483494945", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-b8d697a3e7414ce6a8f3c50190c7a84c", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-03838fb581ab48c6af2abdc38dc224b6", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-4584884377cb43e5945194d051ddf2ea", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-387512f54b4e459ba116b567bc026a39", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-149bf7a65e6c46c7ac03823dbfdcf31f", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-3b521c44ae9b43a4b960e68e87406e24", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-65afe60bdb8e4244a0e5bd09f11cfa61", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-d600e4327bc8412e879e17eecdc89aa7", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-6cd6b1a11f854036b9b3a6b7d38ffe86", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-69f3a039c54d49409e0da416bfb7f51f", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-ae545b7960dc4ff9a05216a145284595", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-b606c2840ad94f73b9412a480f1c1331", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-544d945fe06f4ee18ad532d4392ffcd8", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-11414ded733149e5ad28de53ef75749e", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-624ed64547af4d95925d346ed74e42b6", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-ae1efe3cd7c741bb8c6a980b1d3d9254", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-b66264605e2d466f921dd7227defc88c", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-117ed8eb1e96457890556e27199e80bc", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-794fe7fe09dd40faa38cb8dae2fa7c2b", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-f943afd344c24bad91a178d2677f286d", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-a2843c2f30eb455eb3af97360723c04b", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-45fa190a6627441f90dd837fd4ce4e7c", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-9612e12fc58e4dc889c4cda5756302fe", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-62072dfbe2514309a1bb878b725a6a20", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-abc80a80481c40fda03119c6eb98646a", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-a09c9ac884ea4367bd47bc7f99053641", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-4e6240aa99e94f5da2815fa1bdf2c989", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-01ee49d00b9941eab2b7e2d4489a3746", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-bd0ec61b66e949428e73338dbf0610d6", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-135dc146880f4d8cbf9bf80381423cc4", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-48ed498b561d4bd0b9877692ae4024be", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-084ded9231ea4ef4ae87f115a23448a4", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-a1c38e8bc0154883910ee4584b36cfbb", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-579112e6586348f98ae108bc9c8eed23", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-509a3190eefe46628ac037d5ed555dab", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-fac0c21950344eddb3ef405818d07dc7", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-5c7602f01b5d4d80ba0944e2fdfb6006", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-0fa7d83ad83b40078fefdaa5c7765ed2", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-f0faa9c62f3742f7b8e45053c6e24347", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-2dbae6252392412f9a0b95321c5f158d", "references": ["B_contradicts_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-59cc69d818fb452983b70251423df05f", "references": ["B_entails_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1615-f2e8047096eb410e85e81f30d009e5ab", "references": ["B_neutral_A"], "task_id": "task1615_sick_tclassify_b_relation_a", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-7437f563b2f84906a823464c13e91f4c", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-ec26d7f2f2064822907415e25e91697d", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-a77747150a0b460ab7f861d87df77a43", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-8f3d2dce184b40f79327b8612241e731", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-820c7e9e41864233aebfaed4cbb2938f", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-35b18435e3434650b4d5f99f5d225c50", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-dbe44cf2e2c24e5aa516fa10f8a2046d", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-c9607047ada84db189ea138c7ff2f87d", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-66c61b77836d473984b3f958cca6f083", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-31eb667ae2444a9fadb7695a6e5e1202", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-952db270233148278143c04320fbcacd", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-a66e9d37cf2d4aa9962f52195a1bd649", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-cf58cf04d2394dba9a1f2073ad351217", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-20e9094402f94d648cfa863f4ba91e83", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-26b937796dd547d79c1a56dbb9b5072c", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-0f11b44415d4409e8f55b661259e2960", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-e4efd9bfa47d45139e459cf12dd962e2", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-6070ce96f9ab4ec99e527c936011110b", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-5cf2c441618042de8056dc18e734bed2", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-a21098c33f9d42f6b15b81f7edc5a42b", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-595852e0fa0541ebbd09577b4d5cf4b2", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-52ff5e21e27742aa9bdcdf07f6445da1", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-9bb7fec84a1344c28535c4f4e1bc60ce", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-4b5effbddd864e6a8f17c81e823b07ee", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-27033d783e6141abb176002ec8232d6c", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-28e414914e804f53ae263c6c3d508351", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-dff4c67cee404cce954737e9e78692c3", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-efea2b26b8d64c8f9a756d50c2b63814", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-a855f9e12fad491aa308beacbb995ca9", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-c6fec9a51e9a4dbc818145772b7f1367", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-a0637cd076504dc191a101516605352d", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-597657178c554fff8c1996b1cfd99b9c", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-7b9d0e279b244bb58e7f1d17cde892b5", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-20696689857f4c8286215b2f1e132e6a", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-b619ea19a06c4876877a1aafbeeff15f", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-d48a992851f5460f84c363ccb7f9ba38", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-195a8a13fbc4479cade3fb7fd3ca2413", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-bdf7b4a399794cd4bf638c0a8f7e6449", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-9778a70e58364daa8109d3345f02f1e8", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-17c8787058c84b05ab694bcba92db298", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-93e9937fb3a4404591bfd21d91d7757a", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-d269582a9df24b5ab2407b13b6ba42ec", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-1a37b161e5884b1a86456287dd834e8b", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-cb348d79023a4b88984b91378873b22d", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-e8265d3d8e1f4ad7af1872b9f8b76f29", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-b6f061bd67aa4e9cb7e2956f9fc10e7f", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-6bc89280a985459da662fdec44f40383", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-e9dac11a91764799bc08d1160fcd0ab3", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-a9565acbbef6450cb4f79c91cbac340f", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-396c94e4ceae4ec1ab72c3ca68570fde", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-6afa615e848f4d4fb3877427fc8464e6", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-f53bd5f808ce499b998747613c7fd958", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-6c5e856a52be4906bbe9b669ad9522eb", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-26f833b392ae4a42baf3006fb6b92866", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-180a397846a144ffbe93aee09db9ec8d", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-cfa3cda99e51492abaaafea39b1b79aa", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-04b2fefc12af4d6aa84030ec5252b0f3", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-ec1300f051e04d40b7f6594616fc87ef", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-a2b0b1586664412ca835939489411a04", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-d119cbbb45174b7fae137caf591e58ce", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-7ff464ed44fc43c6a8056eb55e80a3de", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-d52de22d313e4738b4b1f8f754d750a6", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-fe94851e291a4676a54bfaaec4254c07", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-7d83db9b92a74df4959241b75efa1de4", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-110effc67c074aa7863630a58e121a5c", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-1454ecc24e38401c812f3ff16b31079b", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-8a85e4a85d7641c08490040954e76e5d", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-d82286832c4a46448b7a22ed4d4224d3", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-e88f1ef9b1d044588a324fd2e9fec054", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-3eb79f384ecf428e8157abf198846d67", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-0a93cf67673941ac8aad4361c9f9f7cd", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-b4170b5795784ddea32056e2e0b9523e", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-65034d1f552b4c34a5744e827697ea04", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-a174cd2d451146d0bc0177e8b8cdc813", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-806553acf8d14b4ba3efe2f3e69d934f", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-c4c27830e03e4f5ea5c0fba50eaef2f2", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-41c3622353e94f6a87adfe1f814b4786", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-c4dacf9cd186429f9532a4e527e6224f", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-64a8065f21d2403dae8ac178f7b45559", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-8d513be93bf94874b84437bacb2915b5", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-2a0ec32af71243c6bea3cb6a7026effe", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-514eb8cce4704f96935f6998f1dd6ccc", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-0aff34c44a8a4ffcacd8a8cc1aea9e3a", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-2c96c9f5105f4f96afe738b5b8be16d3", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-0421b6f4fe394d6a843ee83a2e6f7e4e", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-239b16b90265447aada73b66e4ce17cf", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-c8c39650b7d04deea8e4e5a7d483a161", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-f79db7c05ce24858833682ed77e628e2", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-d2704aad155741b383aff2988c2030c6", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-1cd8d6ef1b0e41bf9aa07837805e869d", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-ec0f12bad52740acb2d32772fda275ab", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-bab7f29c192d4e79a3f7f7bf2adb5bf6", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-c345c8e62bdd449eb959d28161a06048", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-3505da1190ad44d7b0ecd419131149bb", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-037d10a448e846e7b08ddcaf354a0d3f", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-9980957f05c64d46acccfc299964be5b", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-f4f8fbab99794a09ae9ffcbb8d482700", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-eadf1ead74ba482ba8827a8b24a8ae2c", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-4b33ec95f7b4460fb9a2eb07b22f73d6", "references": ["yes"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task970-cc0c19e1f2e9421aa1fdff3b86437e30", "references": ["no"], "task_id": "task970_sherliic_causal_relationship", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1390-6b3374868d88411983c038af0ef04a79", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-3734f4c94cc14a8b81cc2613fc43538d", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-5cdf644c3ac14a68b4cdf18d3fcae7a5", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-0a02db2ba1c34140ae576986db29c899", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-dce8a772695b4a6c8e2a6b51e5395489", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-75ff4cfad9a548608b67cbaf168744ed", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-8c92147a08734a55ad492539e03eb42e", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-d521959751e046eda3ada3d021c6d245", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-640a8cf0a1ff43e29d9d98dffb2eaf1b", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-7116de2214bc4d96bc2ce481830fb17e", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-6fabd7742477422581f59db150dce757", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-acadb0cd56ac44e5b6bc400a1d73b0ec", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-e219b4d635354ed49b4986e4911cdc65", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-eff3c038b7b74d8b9d7a74b06eb38474", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-d5bf836e30524be49b2b9a0a53a70d2e", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-9b134219f4b34a9092ba2881e0951194", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-478874dd80594af286e6b197cd07f9cd", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-501a977c2adf47cd8a431acff1e2691d", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-c387fb8931dc427287316291d8682377", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-b1a7b0e9385743dc80ac8fd27428d591", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-1836ada435604b1095b58437feb80d04", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-015832aca1b54342ae5a87f709ea7e2c", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-a9f754d473704bc39971bc4e07b41e20", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-2aa7cdd5ba504c359270dc06740c76f7", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-0eb9e40436514fc4bd499788f65799d6", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-3e05b8490e6742e0a3fe2ab7ff54667d", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-e0d0f8b6c92c4c71aacb0a8254fdfb7d", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-fb27858a7d5d43c387b1dd5362380e40", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-967b2a68265840cbab8d1a70960f49fb", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-11698d4faca64426a2682769514bcbd7", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-2a226048489148abb148571d0b9c14a3", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-0259a8898f0743e198717d3235e588fe", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-828dd91480af4e8f957bbdf3c0e662d1", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-86f532d06c2d4ebcb1295dbe15bec78a", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-5c69b126054b4044a04b157acfc19f35", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-9dcc735b97e741cb9978c3834e4896e3", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-dff7e568d3de4c5482aceff9245a78f8", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-046130827ad448efadc8c4be9bae880f", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-2f8cc827687d442493f5d1a76138d7f4", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-ccbf1647ca6648aaa35ed69a75a8514a", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-572e76f4de434595b0d94e69fa1cf630", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-b0f0a16831984f418dbe435e1c6df3aa", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-a02b018678124896b36b86f3dc2f8de2", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-1039783c3d424b1a89e21b2fc2a448df", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-47bb9257edd34f01bd9589321127c892", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-700b76c53eb9413c97dceeaf4a93c843", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-8ff26bdefb414d7fbc8c19396b672f9d", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-7714a7b22e334f708f8ba86423322725", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-b85c8f05fb40487eada2e3127467812c", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-3d954b39238148ebaf5793a2315ae0cc", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-8779c02c28c04d9c80fccbd1ed3b012b", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-3841931f62b040bdad11eda393b58391", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-47b8ec0457264f03beff54890bc85b27", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-4ba6214b125e4554bf7f9b4f1ec37241", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-81dc5e4617344b288bb9e28621fa8ee7", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-f4ec874eaecf46d29e6e2bff90628c1b", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-ce79eef2f64f4bb680dc53f60fde4acc", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-b5bc0cf05ec1443a91a3d6ed5e5a60e4", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-6b26a3d5942047e381b7c1bbd1766521", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-f14aafd688144685a5fc88c534158c43", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-4c1b834a738c432f9ffe4e3751fb124a", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-5dea026a75f644bbb7bc7ad509429f48", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-2bf70e74a1e446bd9da5a40b5f3a1d1c", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-048413c05c454c4f8b9a8d9e028393ef", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-085afdebeb184c3cbcc5b383ce9681f9", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-e1843e80472041dcaa3cf5b768776676", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-0b3bf6cb593a4777a1ba00e375123f83", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-be43c625d8684525b0c2dd1cba3f774e", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-761589d85e62415f99da09b87cfd3d49", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-831232fcf747443f95c739d61dab0eea", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-af552776c5834501a86f8642910c5094", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-79c977f801974d58848c800e0ce3548a", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-714a2d432d3f4e62b3f1405b8935af70", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-ed10df4e762843e4b7d16fbc0930388d", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-dd001b24288f47beb81bd64ea9222808", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-d96b5bb19d3e4f58b4436dc3c42bb3e5", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-ff3bcd44e96a4e8ca5de0e4bd6889917", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-286c4d0510d7492dbac1e625448b5e6e", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-e8af656fd2bb471094f3a2b76617a7dc", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-7089c9d6cbfc4222a538d80d8c4fb15f", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-7bac7e8a02014ce88cc5acfab160139b", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-9fd6192f11154b2a80b710644c6968b6", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-cc1796ac0b274abe8450e3db09195080", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-e4e6c49736924026b99a4cf86b8df367", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-b6e00d47c71a4e97a2b2aa930e99fe08", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-2882cef97f024217a00c9ce18c507625", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-daf3dafd3cd64218a7584349311eaf0b", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-d6004dcdff604ca0a378eee0b0ab6931", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-f68de20df41f4d7fa9a52e9511f4b171", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-90eb49c7ae4b4dd785c0c302a82d5d61", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-8388ecaa36524b3d847ebf367421eea9", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-cda90756c4714e0fb91ef736c74a0bab", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-a7914224701a4d5a97e80ddd436862b0", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-d7d861cb7cb2478595c0cbc9d574f1a8", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-8e82bef4170b4de683c3f9ce9f1ea944", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-7a9d847850fa496891ffd3392dce088c", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-07980920be1a4a8cac7eb1656825bf53", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-dae1a960e5f141188a54ea8511ad154c", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-5e84373d0dea4b27b618234d359074c9", "references": ["True"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1390-f0f1457ba12f40d493aed294e281d712", "references": ["False"], "task_id": "task1390_wscfixed_coreference", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task199-040869ec588e413dac03e1155bb4cbc8", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-8fef535148e647c3ab7a5380294010d2", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-012c56b4f4314a8696398742554037a8", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-b6e53b42df0f429f8746f04fb101d22b", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-0caa05659d9940368b73e521b47f0774", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-4f535aca238e46aa95c1aa760bcbdb50", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-26e04c2a7bb946929633bd72a34ccc67", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-b9be6730c9b143bcabcb9fde045e15af", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-5752c4994a3a48d78df452982184a94e", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-e12b038189fb4239a9970e9a46244269", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-8fc48f12852843eca8c63e99897524df", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-7faab7e7d6764e50b7f125c2cf39925a", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-4f627bd71bc24acd99c9355ccc8d25a8", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-f986f40c6c2549e5914e82c9b00fc9ad", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-8e183733452b4d41ae32de79931a221a", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-e58e22436be045ba8ec2468607dfa432", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-050707010b334464b9fa3ef7c7e14de2", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-c76cb91bc46f465c98f7bacd9eccb9bb", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-9efcd4f803af471da5d1c6ee672b5019", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-abe5768068824296a91859349645b68e", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-af374af761b04e2bb6cf4798ac83d2ca", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-5ab4602faad44c5d9c8d81aed80d90e9", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-8c1998ec9ef740069489b3f96edd2966", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-99f200c2f61f427ea6b5aaec365d29d4", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-e8c4740598914ee988a0c010b88a4c00", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-f31b075a622b48d681dfc84569d984a9", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-159bb12d6c6740539a36495375b35e1c", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-4874558fe1664c678550997c57fe3ffe", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-e8ff6df722b2407e87a2e7b9c0ac76ad", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-8c6ce6c5fe5c4572b3a865ad0ff82f50", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-f6422377c1984b27bb52767672024d35", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-000b5b5daf16428ea99a1b6a541176a8", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-f0f9333e1dda4e59bb4d0d2968a26579", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-4b2c07777602409aa987149a96e718d9", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-976d1cbd8d8241198e58694466835be2", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-0d5461c1dd2846f9a54b65a58f2a14fa", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-d8db55fb5d304133876ef1bc6baa77ad", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-45a08365f6464774b5cb2f02555d1137", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-7daff0054eac40f09eed102ed7fcdd1b", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-e75895bcdc014231b07b41aa3805968e", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-5c15e9f9c56542d58e27786e50e23fba", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-858e1c46732d4e68bb96b21f1bb5e2ed", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-368dd455e19248a4b353cd2cfa326a9e", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-9f3e5f6241ce41c7ba83bf9a2b6be11d", "references": ["no", "yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-90a604c6b38442908802d18d35f0188a", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-093faa6ac3b5420888c297856dd7f89f", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-c9becb2a54604b1995bb10939ed92cfb", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-ee54ba91d42e4ff089adab3828895cda", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-1e94172cac6c4649a7290b66240c7cfc", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-41987f6ac792494aac86229e183dbb7d", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-a3975e54f4894f99858ba26b1033358c", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-900497e05eca42c481cd1d879d74daae", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-6bd6d44ca2db4827bb3341088cf0f72f", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-cab0cf75010e42c99c4e0970c921b623", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-9ee9cfe163a5445ca0ec579c3315c33b", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-43214da0bfb34989b8c49c6597126c42", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-0e67d6ca26744bf0833460a55f77fd91", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-e17114f052b64f679e0217e2db1c4576", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-5090468b1cf64b01966f305b74bca42f", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-910198ca599c4badb4b3e82c3045d158", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-b34fc897119042379c6229db90dbe59a", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-0abdae6ba31a45279f806ecab2fb13bb", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-984ae8a4826f44ebb2f37271048c68e3", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-a6b85d86cfb04f5db861c6c74f539106", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-a972c34714ac42f7b03ecff915beb5f7", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-a6605c994ff44e2f987fc0090f4b6e2b", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-7954a71c0bfd42159a97ca1b19f4dd9c", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-139608b08665452990a21ba46a859c9d", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-8a330d20c35047539d489b5b63d38189", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-6dcc9183865c4827a2020e6c301096c7", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-1efa87b4992b49318ee24b0f905ea467", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-c69a7a3f46334d4ba7070d9b9a54b3d5", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-c563f283917d4aa89596f3c2e04bcdc8", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-d445bc43c81c41cabfa1ba61d5370df1", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-7ff2335a2c084f59b1cc89243e824cbf", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-00982d22157b4b72b4c389eee72ec6f8", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-9e6635f952a946e59aed093ec25b07d7", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-81d282eab6d04099849ed52a2259212d", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-500cda71c84c472d90b8c50b2f63ae96", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-d413f1f63e85446fbba8090cc7085507", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-6bc1c56a1fef4dfe8964035ff82c1d24", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-014597f9ddda478b921fca3061342d66", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-c9f5e17eb3ae437fb55e8b48e9ee45e8", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-0bbdffc90d7848ba84672e64c56a90f0", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-df0453fbc9624dbb93573d682ef683b6", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-b02f5359b24547398b008a31734f8e3e", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-be3bdd74a96642d584070280a7d96f07", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-60bd4b72e6bc4d75a07e56160a42b181", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-268021834d2a47238289e49986e73792", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-2ca897f5303a43778de4fd71e38bbb7a", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-4971488c770944509301e2d5148436bb", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-4173b1aa8d894f14b56d88b21309c0b6", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-e5d3b89ba41a463c98d549d594b352dc", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-5f116a7543ed43fa9ea4365b3da6c80c", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-fc6d332ea2ac49ec9c8ba7596e466352", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-8ac141596c2743e2a2184e4182829666", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-eb11cc63e3d3415cbd42f1474b0cdfad", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-e29c8ffccacb4ccda5c8662c6c3f7a54", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-343947af36994640a3ccc9c8f1c4ea12", "references": ["yes"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task199-c8d1cc78465d4f85a9540bc1ceba2cd3", "references": ["no"], "task_id": "task199_mnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task034-1b3f3da80f2d4bea83cbe2f03904891b", "references": ["After watching the movie Kelly began to work on her own story. The _ was for her job."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-6f54b45d91f440d4a5380522fc7d8199", "references": ["Because of the large amount of snow that fell, James had to shovel it off his car before he could go to work. The _ was needed for his job."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-2493cfa7783b41e3ae1a0bbaf5a1434b", "references": ["Mike used a new notebook for his notes instead of his notepad, since the _ was full ."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-e95c0e88bc4e48cc830e92169bda5363", "references": ["The rats fled their holes for the open spaces since the _ were roomy for them."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-cb604df58351454c9133c2848b498da7", "references": ["The dish had the peppers removed and replaced by mushrooms to accommodate the bland tastes of the customers, as _ were mild ."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-fc004c85bb9a4e6f8a5ef4d7002f800d", "references": ["Jane chose to write about dolphins instead of bears for her thesis, although _ were her strongest area of expertise."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-068ff77f8f804050b3ee5bbc963d9cc4", "references": ["The swamp was a good home for the alligator because _ needed a watery environment."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-8ff567e1f3584dccb345c5caa9461be8", "references": ["The parent chose the daughter over the son for the inheritance, since _ showed more negligence ."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-b13114cd3fed419d9d75887c4439c6e2", "references": ["The babies' little hands could not fit through the slats in the crib, because _ were too narrow."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-bf4dfb635b8c4eaaa201682fcb4866a1", "references": ["Todd chose the job at the office over the store because _ was harder work."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-2520d05b13f9436892723e1406541f7c", "references": ["The medical records were stored on disk instead of paper because _ was more dodgy ."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-232c5eb3b1694cf6b483c7d5d38bdd5c", "references": ["John could learn math better than art, even though _ was the easier subject for most."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-776e50411b744d1abaf8e206477d96a9", "references": ["The ability to speak is stronger in adults than in children , since _ are more novice with the language."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-1cca74e7afc84b1a8e29d63194663e9a", "references": ["Because steak was so expensive, restaurants ordered less from farms , so _ lost money."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-f35d3a7881a74883b27b1f5e45761d65", "references": ["We wanted to travel by cab rather than wait for the bus , but _ proved to be more dependable for our journey."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-6a4a266f0558453a8f91bdbff680e7a9", "references": ["The manager moved the money from the drawer to the vault because _ was less vulnerable to theft ."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-8663a23ba7624510963d15b406c982b8", "references": ["The wood was still smoking while the grass was not since _ was on doused."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-06f4b4fc8f314f2791e0a4d7ed2cdcc5", "references": ["The jaguar lost the fight to the fox despite its superior muscles, since _ just had superior dexterity."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-2c06fd4a1b374f2eaeab7a243b4e35b2", "references": ["For fitness needs, the elliptical was more popular than the treadmill as _ achieved substandard results."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-cb8cc9bc3527423c874e9ad8eac2f564", "references": ["The steel reinforced the columns that held up the girders, so _ received extra strength and resilience."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-7ccd75bd4d844abc9c5a5d51ee5eef71", "references": ["After the accident, the innocent baby needed another bib but not clothes as the _ was sewn."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-80c557ac3bf84a3b80ad124ab3e05c67", "references": ["The shirt was in the cardboard box and the book was sitting on top of it when it rained, so the _ stayed dry."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-c2e28f94f99548cea5d96eeb2cdb36e6", "references": ["Switching out the car for the truck was a permanent decision, as the _ was perfect for these tasks."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-ebcb76478b9346f38e6b50ad865e6d66", "references": ["Although the legal proceedings were reported on television and on the internet, the _ reports were more read more often."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-3a4c6e6d6ac04fe28e0f55fa76a59b23", "references": ["I need to transfer cleaning solution from a bottle to a cup until the _ is full."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-99bd8f3bcb0b4abbbdc98bf98e5951b2", "references": ["People in the city are less healthy compared to to the people in the countryside, due to the clean air in the _ ."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-23289de6a2a64a5ab703da3c77455e41", "references": ["John wore a glove first before collecting the iron rod from Jill. The _ is cold."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-36ea213efc1b4493b2347f561d1093e9", "references": ["They had a lot of depression that needed to be fixed before the happiness could come, so the _ was loved."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-7186964247b34dc1b9b72b89156d675b", "references": ["The jogger changed the sweatpants into new shorts as the _ were clean before the run."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-4ce18199d0744954a36ab18935b131cc", "references": ["Alice tried to show her sense of fashion through her hair and accessories, but the _ was harder for her to change."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-d6ffeabc895448cda6590caab08304d9", "references": ["He made the cake for the wedding before blowing up the balloon for his friends. The _ was happy and celebratory."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-6b02ce4c665d44ababe63d58152a5c2b", "references": ["Amy had to change her appearance, either by adopting a new personality or buying new clothing, she chose the _ because it requires some effort."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-fe7dec74a0d840ee8724918f31674c4f", "references": ["We cleared the course so the ball and tin can could roll down the path as the _ was rugged."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-dce7485afd0c4032a15fe7d87197ccb3", "references": ["He needed to remove the grass stains and the dirt stains from his jeans, but the _ stains were easier to get out."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-f595b6240eb441d29400b6d6a337710d", "references": ["The credit card wont fit into the tiny slot because the _ is too wide."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-46ce068020d344a1acf537af9306bba6", "references": ["The desktop worked better sitting on the couch than it did sitting on the pillow, because the _ generated too much heat."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-0b0925cd92ad4060905530d265ed5ef3", "references": ["Carol handed the date to Robert, who refused it. He wanted a prune instead. The _ was perfect for his taste buds."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-1515c84db40e4631b3235f1ae463ccff", "references": ["At the hardware store, he bought a key padlock and not a combination lock because he knew he would remember the key for the _ ."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-7267621bc85f4e84bfe1a8220451b254", "references": ["I started the Atkins diet and was told coffee would help with headaches and soda for the nausea.  The _ was soothing."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-16801bc0f2cd42518a1222c0264305e5", "references": ["Jimmy liked broccoli for dinner and chose it over steak because the _ was very leafy."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-9002ab9c6fb74db89533fb61937022d4", "references": ["James had been reading the pages of the book within the break period and he is yet to finish. The _ is short."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-7dc3d52f2c314d1a9d54ecaa6e586831", "references": ["I had to buy a new phone because the old one had a buggy software I like the camera on the new phone. The _ is cool and sharp."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-6e5da49839a245aa952453ee3b9b35d2", "references": ["Jacob thought about deleting WhatsApp from his phone, but he found out the _ needed to be repaired."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-ccf5f05c287643f8a6d19de82a705ffa", "references": ["The girl chose to get a game for her boyfriend instead of flowers because her boyfriend liked _ ."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-380951b9825646169ca55c3561f65804", "references": ["I wanted to start a lawsuit with an attorney for a claim against the polluters but the _ was too expensive."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-718cb03e34fd42a98e84e96a531d4ec9", "references": ["I want to incorporate more tofu in my diet. I could try cooking it more or buying it more. The _ is more expensive over time."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-9034f6a158524f97bb78a7aaec52ba04", "references": ["Tim liked to walk in the garden instead of the park because the _ was farther to his house."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-e289aa6944ee4fbfa5fd6e7507510f2a", "references": ["Jim knew he needed more fibre and less fat in his diet. The _ kept him feeling bloated and sluggish."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-68f459323f654922bb31bcc8c1f9c629", "references": ["The global post office ran smoother than the local mail station since the _ uses luxury packaging."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-16167c1687fc427591d9c47102c1cfb4", "references": ["It is easy to visualize guns than theremins because images of the _ are rarer in the media."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-7e673c6cad2642d4b41f7d1b4535fac7", "references": ["Al got solar panels and a small generator installed at his house for electricity as the _ would be his backup power supply."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-77591aa347bd4ff886172afdbd6bc03d", "references": ["My landlord made more money renting the houses than the apartments because the _ were bigger."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-b38732a4d79a4afd8f1b3b7f8022dc10", "references": ["At the theater last night, the act went on until 10 pm unlike the skit, so the _ was more shallow."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-f07492aebddd4403833d22246ca87b3e", "references": ["On the island the weather was frequently unpredictable. It was supposed to rain this weekend and the _ would be very heavy."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-0b7de2d222654f36acd268f12a2f0992", "references": ["They tried to get the gum out of their jeans by putting it in the freezer, but the _ was too stuck."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-32362500652940cab0063cbd5dbe412a", "references": ["The hunt was cancelled after the snow arrived and ruined the weather conditions because the _ was rough."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-fddd2153018a475fa06036d9438905b3", "references": ["James could not finish writing his essay within the minute the teacher gave him. The _ is short."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-93353772cdc145d983ad8d7df988fdb5", "references": ["Dennis planted strawberries in his garden instead of potatoes because he thought the _ were sour."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-a0691bf75242425f9d05dcaf4182582d", "references": ["The boss was undermining his career but could not affect his salary, as the _ was protected from his manipulations."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-b4f3e301692248aca39b9a0318e8492d", "references": ["The muscle on the arm hurt less than the muscle on the leg because the _ had been warmed up."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-9c82906e872a4c31996131615d07d811", "references": ["The chef recommended sliders to the VIP guests, but they ordered ribeyes instead. They were told the _ were not fresh."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-f8e03e1dd57e4806ba4bce9d9e3240e8", "references": ["Because he needed to make money, Joe spent more time at the workplace than at home because at the _ he made none of his money."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-dca995f58c084725a888cf54bd182341", "references": ["The kids hide the coin in the jar in reaction to their game that we hid. The _ is clearly invisible."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-dbda3aafa24c4c079504e9624242d695", "references": ["Joey needed to take a test so he drove to the testing facility but the _  was too hard."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-90f228cba9344f0b8a60ebac8e1877ef", "references": ["The boyfriend bought the flowers on Valentine's Day, but forgot a card, due to _ being harder to remember."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-b20f9eef4b6c486698b17812ed1a9a21", "references": ["They gave out grades instead of doing ratings because the _ would be too lenient."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-19d313e57412428a845a482ee68bc1d7", "references": ["We picked up surplus wood pallets for cheap.  We wanted to repurpose them as shelves or a tree house.   Necessity won and built the _ ."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-2a3aa2585ddf43f287e39e551ecad202", "references": ["The man developed his leg strength with weights instead of machines, as the _ were useless for muscle training."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-c78631fd5dbf4f75bef24e0ec8751edf", "references": ["For our 5:30 PM flight we took the 5 PM train, and were worried about time.  Fortunately, the _ was early, so we caught our airplane in time."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-6535d67d5b8d4f648eeed534bb7415cf", "references": ["The management at the Fortune 500 company had to be replaced because the _ was too young."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-1e4d23cb47f94a888ec996cc4a5c65c4", "references": ["The man started doing cardio instead of his typical workout at the gym because the _ was focused on losing mass."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-30e29e42d069430e90f03f15fa5b7bd6", "references": ["I was very tired, so I chose the pillow on the bed instead of the couch since the _ was far away."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-a09affac84164fe7b21e4287e4cee52c", "references": ["When John watches TV, he doesn't really like sitcoms, but he'll put on some good cartoons. To him, the _ these days are worthwhile."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-ec419fc2ceee4d16aed8925da18c9fdc", "references": ["Tom put the laundry in the barrel and not the basket because the _ was small."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-1d400e0237c242acae713dcef1432f41", "references": ["Helena used a little bit of chile in her bowl of chili because the _ was flavorful."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-f4f26b1df8ba440aa7eb0de311ff2c27", "references": ["The computer ran slower than the tablet because the files on the _ were larger."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-22af72986a6b466cbdd19d79648b4de1", "references": ["Cora had a urinary tract infection, so her doctor told her to drink cranberry juice and avoid tea, but she couldn't stop drinking a lot of the _ ."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-a8714cf63ebb4f7ebedb7a39aafd66fe", "references": ["The cake spilled out of the cup when baking because the _ cake it too small."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-1f0a74bdb1ee49549d21e1389b8d02f0", "references": ["Jilly tried to put on the costume but couldn't get it around her legs because the _ was too big."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-8930ab9b8b814d66ac81a45d665af869", "references": ["Jack wanted to grow roses and daisies in his garden but the _ have no scent."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-94d3cf8fae1446dfa2b8335f541594a0", "references": ["The reunion committee decided to hold the reunion in the park rather than the commons because there was less room in the _ for those attending."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-1dfb5a5655db440198cb4ca47dbbe34a", "references": ["Since I took the books out of the box and put them into the storage chest, the _ became very heavy."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-da45c72b28064ce58d1fd8b10644305b", "references": ["Ashley's asthma was mainly triggered by pollen so she kept her inhaler with her at all times. The _ helped her breathe easier in the spring."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-e3d3fa59c170455383c841484445f480", "references": ["At the estate sale, John saw the hammer was more affordable than the screwdriver, because the _ was brand new."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-d087c12e6e8544aaa28c64e7beeda5c5", "references": ["The man was able to stay dry better under the tarp than the tent, because the _ was made of impenetrable material."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-2ecbf0ca895c402a80dc3ae0f4c1aff5", "references": ["The cat doesn't like his wand toy, but he likes the laser pen. The _ is entertaining for the cat."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-7bd35e2a0cd2463b87b146576c417b6e", "references": ["His hands were cracked from the air, but were healed up when rubbed with the oil, as the _ was so soothing to them."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-8c6a34c6756f4e389ab9497791812238", "references": ["Donna suffered from acne and needed to cover it with concealer but the _ was too light."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-f1439785d0754cc083e53d6b85b9285b", "references": ["He didn't appreciate the trumpet as much as the violin because he thought the _ made serene music."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-834b349036ce466fab64e3965c3ec773", "references": ["The doctor said to take magnesium instead of asprin because the _ was more chemical based."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-bd88ec9edc65472eaa5e718c49840d18", "references": ["Their talents could not be showcased at the various shows across the country as the _ were normal."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-7fef7ddfbdb34c19be1520d7724beea5", "references": ["We could easily notice the pot shining in the son unlike the rod beside it because the _ is dull."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-898930d195f94614a13387eef86ea161", "references": ["The teacher said the study session was more important then the football game. The good students went to the _ ."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-8a90a12807db49af84eccc2ff5214e03", "references": ["I took everything out of the bowl and put it in the jar since the _ was bigger."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-e9e104cef2bf42dfa480fcd001fc6726", "references": ["Water was served more at the hospital than soda because the _ was worse for dehydration."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-7b134b1c4e40403cac52d26eef0469e7", "references": ["People disliked the boxing training at the gym when compared to the cycling. The _ classes were simpler."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-9d21fb410b2f47b79c7f020790cea5d4", "references": ["John tried to feed his rabbit, she liked eating the carrot but not the candy. The _ was too sweet."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-5a4e3782d1664ca1a9fb11d8615d3cd9", "references": ["Jerry was making pancakes for breakfast but realized he had eggs but he didn\u2019t have milk. It was necessary to go to the store for the _ ."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-dbfcbcd9e4cc410bac87c8017c0097a4", "references": ["The squirrels ditched the acorns and harvested seeds instead, as the _ were abundant in the nutrients they needed for the winter."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task034-d50ab0011f4b4b4aa071979ef06cefd1", "references": ["After much thought, the writer chose to publish their romance novel under a pen name, because the _ provided a disguise."], "task_id": "task034_winogrande_question_modification_object", "task_category": "Question Rewriting", "track": "default"}
{"id": "task133-300fe089ca834eedac1e15b48d3e8cc1", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-8dafb80f965048bb9f7dfed46eb23da8", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-aced785f70e6434c9e15e34e80f694f1", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-9fe420c22cbb437b9b9780fba2df2fff", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-8683a6eb837248dc8b374ad50580e6ad", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-a7e160df58174214b6c68fac2280bf53", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-67169994dd434b6cac1959fc20449c57", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-1f6f260284764959a3aea7c11e37814f", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-f8f7e2bcafb04c109f989cf43ee4be36", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-31d0ba6d42634257a90e2f3de70c4f27", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-3068bc05a7bc4e459adf2ef75baf8d09", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-e3502633550f4a1490904c1706dce9e8", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-9430d5eaeb474f8aa2121ada2ddb9835", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-c452cffa86af4ec1b43b780efa4eebcb", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-587ee59395184cc681badfd6e67809ee", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-bc9763e2d7634fe9840dfd8c8d9c2187", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-90cf4dc5bee9449590a7e3e6cbd91c5e", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-94bed1eb93c04414b49ea49b73d300af", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-73d1b8c697c04f4dae2e02a049780bfa", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-8f24a573dddf4767b725d9fbae30aeb2", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-d2b5abb5505a4ddca74ef43ed9ebabda", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-2bc69d144fa74791be2ebe9d607b1177", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-47c330fadde94ab1898454f6612343ee", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-dd1757f7599d4daba0cadb15fa49865e", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-321ca19cd516489190dadede4b511810", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-ff85923503f843f0962fc90617700697", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-b922248986b14cc0817e9b1b6d8f87e1", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-12cfec6b512649b9a5d8641945075c0e", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-8721bf5d65c3439b88c03618e5220b5b", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-a3be47f22aa64368805b9dc22818bbad", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-a968d6993bb14b80ac234c82aaca9c25", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-7b961cf1d55e4ecb976e4f0cd6c35b8d", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-7e0856a09dae44c3bc20d4f86b8d9dca", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-c14274e24a9748f082a613bfdb75c99a", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-5144eb5589a044b9b5be8541025f20c9", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-48c05e03c8f443858d0eaa1e7becf73a", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-957cf53008bf45629ea6b0f1710e195d", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-37f66122019d4baa88aed5ec86176457", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-f0f953e8e58446c995652358e70e0b14", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-57ed26452c044043bf7c5132895c22d3", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-a424b1e0863c4378b2ef9cf6d955d831", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-388611dbb46c4030925275b1c934b2fd", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-2f09127abf764495962a9dd56a8da066", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-64f2a52992a943cb9712a2851c343a8e", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-71c8f81a9739463f866c5a8fdcf91bac", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-97220050e32f4de780100da1bb3790ab", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-6588ae43aaa441e0bb6d1366ec2a8fbe", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-7b193a449c8645a594ef48127f98ef59", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-302a39142ea14a70a22bb50959c87c62", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-5024bb9d44ee4ed780c302b8cd045faf", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-819f4360df8b4823ba2c1976a4a613e4", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-2ac397a898ec4f03be445e0887d0041c", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-40ccdabc9718442ebb570479c854d9cb", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-a5240fce838641f6a595c0b011c575a3", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-92486bf59fd249b79ecd7613483b4dd4", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-11772a8f7fe847e396514457fa46d86b", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-08a041b77663495cb291090830d74754", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-27f991cdc6644598a18ca8a039ee69c8", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-23cc6ffef9a14f008316d30739a6fa41", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-31515dc11ca24dc8b1a8d12e66cc814c", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-c282ea1a44cb4becb9dcb8a1f72cb70c", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-3e35af545e694538afe5672587fe413c", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-d815c35f8653476986507765e0377629", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-833186d6b5e94127a3fa451416c5413e", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-6bbf7e19740b4d53a7e7062d77463737", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-a37d36bccec847feb94dad68adb67315", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-148e37c1e66f49049890e8fd518bd2e6", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-8be6f48eda26493fac69d6c394b9e7fe", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-57ed5920bb5b439c92d12c34d3f17717", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-567d8714d3cd4e6e93ed2a35f05179eb", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-e9ce65a073014c92926f97a602366308", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-fff4db9f41154b5c96dfe4335267f51b", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-766e52ee4f2a42dc9110465688a98706", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-37cd67dadfdb46578e5f0b0ec0efa88e", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-fef2884505b24bbeb5d69be9cedb4988", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-66a3889cd47d4c618d0d169aaa89cb19", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-cc039ae6160a424989b1d1322dd17898", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-6d6b208763cb48b193f07e5ffa7cffe8", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-18be7c5f3e744fc28ebd6b4fb2d61db8", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-44a678a5f8264ba8a7999cb136e0425a", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-01bc5c81a5604851af7337d4eb4ad009", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-a402ca3116c6405fb147fd87fe74403f", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-3b989e7ba420493d908fa6669b5734bf", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-4fd1d77dde4c431c894b3cf761fc68c4", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-5cd1a5b10c914334a3668871d19b7c31", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-937090b8ba8c4dbf98a84ed5bcf44498", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-01a73228cd04457c89533282a74085cb", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-d2eb7640c608432d8c372db8f516f32f", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-4a366e3cd2574e088e5b6e8fb849712d", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-91837507db56497681ff46593a3a956a", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-6e337ec3590543cfb5135c88bce8a7f6", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-5ee01948f54b4a65be2a1f880f74a35d", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-cdd2a021dcf94195b54d7300a84109d9", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-3b3f3b365ff7422999fac5c30cee6d39", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-0c105e328eb048f3b11d002486da6126", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-728257e9356b4bc7a65e10b44b12e95e", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-e1e0c1487ee24d1ab7bc75390c07c2da", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-86834935c3e34bafa1808320f2c1323c", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-bccc0c16fd104552b2c7b2a1226b026e", "references": ["Correct"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task133-2ce4d5d4f72f4f44b367ef8c3908d4a7", "references": ["Wrong"], "task_id": "task133_winowhy_reason_plausibility_detection", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task226-f8c4e0af6e4a4a03814fb46a66898433", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-66b623a784be4e7786168003bd93a196", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-48975f06cc4d491fb9cf020e85bb6e02", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-f137d495c0a74964a015a1cbc73a6f64", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-481059db4ea849019fd5b0c4fc6fbdc8", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-415b30e2d9394e9c8c5aa7694123a274", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-6ad3bb7f53ab475cbd4efd5cb2c70906", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-9dc51f87c7e54839bb8488a747164bd8", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-c8eb0e36c67440e4a4d75c3fe1a48ee1", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-ee7f54e68def425695d45b0e1b3fab99", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-f479476b921741b3b6ca6e724b69cec9", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-54c97d62880a4267bad58dc35c68ead6", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-6ca2434808a4435bbcc8ab46c8d05328", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-dcfcb64d2d304e5ab8a6d7934d01e6cf", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-021c61d714c64f098edb536ef8d55bb5", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-b0ddac32188643bb9ef7095b590d7b0d", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-a1e97cfaba554ea6a32bf3c5ce379d75", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-ba8579af138644ca810ab0237af9cc35", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-d31664c3028a40689c18215702894dd3", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-8b446af1261a44238c58497aaa329233", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-ef2f28a0855744eb839d29bcabc52e77", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-7526fa1f4a3b4ad5951a774694afa943", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-6109604d80ee4ffdb4f1c818b9dc8fef", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-8be14e93beb94105af10d009c65039d5", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-9cbb25e9ce0b4ed59551826da2d652f6", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-944901bf6c104e51856858d327e3b854", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-d1c55076dac84645aa3158e12cd50487", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-dfe39afbf4ab4dfb91a84f98f511db79", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-3db5eaa4573145ac83da98135fb892f2", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-014c7290ca1e44f3ae7f82dc16b95f14", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-67f8da57cd58423abf00173e55dfe7d2", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-4b4ee2b206594743bf827475a4986653", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-8338bc46523a48ea9b57020a27e0b481", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-0e157706059c4fb9a7b8efded6f19c7e", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-66727c79865d432fa61688173fdf3182", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-d792ce037fb044e3b9931671d8cf3142", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-b465cc041e3d475d844eeb76211e4d96", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-e505f96e946d42aca232c2d3aaf090bf", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-ac1f16223e88450b82d903617dfcaa13", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-f1ec02ec87a649d7bb20fb7cec4c3cc9", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-5a4dd9d82c284fb5a6a38ce00e3397c7", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-eaa927c315fb46baafd81c1b41a07208", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-3ec27759211948008df7e2b17000044e", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-e76bf97a69654eb19a188f10034e5701", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-67755def6e9b4f52ab4c08de54eb6d9a", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-20ec5bcf0ae14b0792cf9783721619a7", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-e8c9d51e5099450c8b2fb38323a3a770", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-0ff7e65d50a44d7da9286fff975bc546", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-86d89c47185a4d3c8654cb30d833c881", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-a5a7f8d8420649ebabf06974226a1623", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-4c994ddc8d744da2b48ae9ea99d2e496", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-11ec02171601419fb7c76ab919f18dbb", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-c747dd658be54d389122a14474582791", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-6c7f226d240a41ecacdbbe03e9828f2a", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-8b0a461cba3742f6b0e43992e8b4adad", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-d8e81f6352594c238f371a4eb7a2980a", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-174b7164614d49b4a2cbaa7f47e6f021", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-33dadd1685294bd6aff615ec72b0523a", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-0c58fa51bbb24b4db51a90a88ee80713", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-d0d335759c78418fba48e0cbd79a3e83", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-6e073655f7514b34a06c771ddc1e8016", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-b5e7245e33b74ba0b81632b337c3d354", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-5e05bd081b0c456f86ec43a01f222e5f", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-3e3828cc9412450c93592fd353a541e6", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-b709d66dc2064023b2c674125bf34644", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-335a5675715a4d0f9bee702a73b11ff0", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-909caff7d6974e00a6084b462f78b783", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-267765d9ab9947e1bb338d0fb27399f8", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-5a9742ad5eff47fab5493cb61ee47381", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-946e27fae53e40a79f1c3c945b1074f5", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-bf8fe07c10e04c099cd721722bb1614d", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-d62b2585307e49e0a4559544e51d625b", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-4919bf0e969b49c88603d7417940f43a", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-a149a0fc32414adbb72f21ec854876d4", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-2918549fe9604e49ae2ca5b520d2ffd9", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-4d763298f1f14a5281ed6811bd2615a5", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-21045d03a85e417e9ce6a866b3247293", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-5d60f060e9554abeab1c1ef3ca09a34f", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-12fcbd1ed16446dc87c3469954e4f4a9", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-3869f294d88846d1b689af28fd48ee41", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-cba29f7d095f4b61b76d35af54b5d5e5", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-9c74c2f7c17041c9b03b33fc92f9eb78", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-8956d9fb891d4445b3d0ddfa52f3691b", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-6f544bd7a9434ce4b2acf995146be6fc", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-a98171a8dd4246a9ae3c44df7cc29c79", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-2cbec16311e8422ba2c1de15993b1b78", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-8603740e74b54218ac390832b560da1f", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-493c49683ba142dd979445e3c973d68a", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-ad81f6548698422b8ae7b68190114590", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-cbdbafce07b644258bfb7c2e20570a31", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-9e66ab90c3f54fcabad4eff3119f6072", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-8d91a5f7a6f642b79638038056541ac4", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-83ee8e3d0d3f47269446523a6ade5ada", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-b71b2ea8919d4c749d1986598f213e39", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-eeef221ac24b48a0ad45ac3d109ca25a", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-83f8792cc75f4cc192dfa98c1e574954", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-6a203da6daf442088879f5c43030bb5d", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-7ac86e9b055640b18cc082b50cf85c81", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-556e43de24ed445bb7a843839b4ed8ad", "references": ["no"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task226-81aa7a6630da49cd86ad34443c15503b", "references": ["yes"], "task_id": "task226_english_language_answer_relevance_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task510-543de887bea54590be4462d6e2068eab", "references": ["Tifu getting too cocky while drunk?"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-0db137d6fec4429292a2d8a8e21fdee7", "references": ["TIFU by wanting to look in the mirror"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-89fefd7247254aabbe39951c3bd98226", "references": ["TIFU by being a complete asshole accidentally"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-90a6483ed9464a40b8d5e41ba2b36ace", "references": ["TIFU by not waking up early enough"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-caa4ff5e9b7d472285e86b8cc219763c", "references": ["TIFU by having a one night stand."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-fd7b2b9870de4c6e871c8e8c846293f5", "references": ["TIFU by dropkicking myself in the balls"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-c56bf1a10f6f4420a760698605a3ca15", "references": ["TIFU by trying to save gas."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-92e2b9bc287549b09da4ea6f740a438e", "references": ["TIFU by walking into a glass window in full view of a packed restaurant."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-4bfc7340b9474727995f9768338ff113", "references": ["TIFU by accidentally party-crashing a NASCAR luxury suite"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-01863e0eea314545a25b06944eedfa76", "references": ["TIFU by taking valium (on prescription) and sleeping through an entire day... twice."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-586aaae56f6343068385636c7e6d7f2a", "references": ["TIFU by using my stump."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-5cd82ef878bf4356be664d0ce6ac180d", "references": ["TIFU on first day working at Mcdonalds"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-0d3d509e84a94a508e7f0cb26a6a7353", "references": ["TIFU by doing a DIY hair mask"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-0a50bc274aca4df8a28c18514d21887e", "references": ["TIFU by telling the truth to a customer"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-f569aef67f4c420890c69ab18801d00d", "references": ["TIFU by losing my son's teddy bear"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-cc0c98ead2f5405699c79a0c535bfdcd", "references": ["TIFU By Drinking Piss Out Of My iPhone 6+"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-0917b47646834fadb07ddfed79fd538e", "references": ["TIFU by indirectly pooping on a stranger's shoes"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-10fa59cd27c242b3a22b6bc07a7b7593", "references": ["TIFU by Attempting to do the Housework"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-44c8276e55c1444fa5576e7fef2ecc07", "references": ["TIFU by burning my nubs."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-50e4e35b3bae4af4aacf663274fb2ab9", "references": ["TIFU by not watching my step..."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-8a40e59b3590407fac59f70735781f80", "references": ["TIFU by naively kissing my boyfriend's friend and complicating everything for no good freaking reason"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-98904f8af5604b228767e69389448468", "references": ["TIFU By losing my PS4"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-568b49f97343402ab5e64b56585635e2", "references": ["TIFU by opening a pack of batteries... [NSFW]"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-b77098c6fd0246c9abf84b3f5bab538e", "references": ["TIFU by running full speed into a kid half my size"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-6864766aca624adcb70cf2f5961bfcf9", "references": ["TIFU by not unplugging the garbage disposal"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-33e93dd62c3946b9a78739fb800de9c6", "references": ["TIFU by going to see Rogue One in 3D and throughing my sunglasses in the 3D glasses return bin."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-07ca0fa03116426cb095340e297906b0", "references": ["TIFU by scratching the shit out of my new car"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-f09fb830736741f2bd714e82e0c84282", "references": ["TIFU by deadlifting a corpse"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-dca83ae68de74d159844aa0bd247ec00", "references": ["TIFU by writing an instagram bot that went on to like and follow lots of porn"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-433bd102100948749b0d9e2faa2ca98b", "references": ["TIFU By getting beat up by friend with benefit's ex husband. It was a set up."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-e4fbd0b37ec1486789d459626a092743", "references": ["TIFU by calling my girlfriend mom"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-8e41ab9e10104cd2932cef9aa95df2d4", "references": ["TIFU: By sharing a campsite with some Irish and not watching where i stand."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-7b989b563e904b7f86fad584b6ef0867", "references": ["TIFU By rejecting a cute girl's request to give her a call."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-16265dc01b354e129c9ede2c80d32dd8", "references": ["TIFU by watching Eyes Wide Shut."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-05f25992301c47a78c5eee993bd9c60a", "references": ["TIFU By Following A Truck A Little Too Close"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-e1c11f34927e4303b675f5836bc2d853", "references": ["TIFU by trying a new flavor of potato chips (TL:DR)"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-86d093ae7d504749ad94bebf2f771351", "references": ["TIFU by accidentally letting a horse run free"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-20796582daa844a8b00bf9609f0f7284", "references": ["TIFU by petting the dog. [short]"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-926431608b6f4836a373219c0b0a73e0", "references": ["TIFU by nearly killing a 6 year old girl"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-04753347c3de41e3adbb9d4cb3ee69d0", "references": ["TIFU by losing my Red Charm Bracelet"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-51fa7c5c18484010a73d1771832a78fb", "references": ["TIFU by eating my own smegma"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-6159961aea964b6f8a037fbb2e8d9804", "references": ["TIFU by watching Game of Thrones"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-2f7886b1316a40de8396612c867a68ab", "references": ["TIFU by doubting myself"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-ef9aac56f95d4a8eb2e7223e89dce5bf", "references": ["TIFU (3 years ago) by getting myself trapped in self bondage"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-aefd9371c3ef4b8dbd94a9ed41cfe5a5", "references": ["TIFU Buying tickets on craigslist"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-d84e40cca3b24956939b1bdbb3bda5df", "references": ["TIFU by sneezing on my period"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-1da5f2e7f3f647f2aac7cb9a3a88f5d2", "references": ["TIFU by telling my best friend's girl I don't like her."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-48aabd688bc646bdb617cd24b20f3bab", "references": ["TIFU by bringing handcuffs and a sensual massage oil in my hand luggage at the airport"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-a5f6f463897c42aaae563de19491fca1", "references": ["TIFU by looking at a germicidal UV light"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-1c52e8ab24cd4c37bdf4d395f20ef549", "references": ["TIFU by walking in on my mom's naked friend"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-5348ce4bf822478d909e0d4deae016de", "references": ["Tifu by asking a border patrol officer if he enjoyed his cigarette"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-64b1e5e91c62469f9653335a288f9e8f", "references": ["TIFU by slicing open my finger on first valentines with my new girlfriend"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-c28cb69f94b941859de71813b5857b36", "references": ["TIFU approached girl in bar, whats the worst that could happen?"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-5bb34e816e924541a5dc756e2ef6a921", "references": ["TIFU by catching feelings for a FWB. Again."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-06d8713ef79d4de5ababa979456dd146", "references": ["TIFU- Binb4"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-0766c3ea70004d09b5b3e34ee179dec0", "references": ["TIFU by finding myself at a murder scene"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-1e86e121782c408396bf39685c41537b", "references": ["TIFU - Asked a girl for her number [xpost from /r/moto360]"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-eca85c4a872440f6922bc202e0ae14bb", "references": ["TIFU by not knowing what a \"wake\" is"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-870aebbec4db4098887350b339eb17ed", "references": ["TIFU by pointing out a doge to someone who didn't know what that meant"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-6d0114dd64b6467dabde414ea4bddbc7", "references": ["Several months ago IFU by making a suicide joke at a kid who committed suicide's funeral..."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-95e9410b143b41548eb59714bad1a2e9", "references": ["TIFU by being kinky in the Middle East."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-91b689a7781c4ffe9ea92ee87caa827a", "references": ["TIFU by approaching my crush after preparing for 3 months."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-48123ad4572e426dac461f72080acf9b", "references": ["TIFU by stealing corn."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-5e53bc7df9694e93ae740a3d5460fa43", "references": ["tifu by leaving my coffee on my bed"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-d12dbe8d7da643eb9604397ea8c08e39", "references": ["TIFU by selling a lawnmower on craigslist"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-042f74dbb373488b9d76aedb89d21f19", "references": ["TIFU by deciding \"Damn, this chicken tender is really hot... I should put it into my mouth to avoid burning my fingers.\""], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-632f1ffefb0d4669a9aed27c2bc6e2dd", "references": ["TIFU by having sex with my brother home."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-db9578f56cf6460dbb227c2f50f86e0b", "references": ["TIFU by embarrassing myself in front of my boyfriend."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-2e39dbdd368a412ea0d71a6809eaa5cf", "references": ["TIFU Washing parents car"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-d17f0fe578d649ffb2ee9f44dac751d7", "references": ["TIFU by ending up in the trunk of a car that flipped into a canal"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-eefedff7fda74e87add35dfaf602336a", "references": ["TIFU by making a ridiculous bet( and losing)."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-238271eb77f345b2875d8bc2c35872a0", "references": ["TIFU by sleeping naked and perioding on my hand"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-29218bda17dc423d8dcfdc34f40878b0", "references": ["TIFU by nearly getting cooked"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-23ed825af0c7468d845c6d8a8fefe861", "references": ["TIFU by getting cross-faded on my anniversary with my boyfriend (NSFW)"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-3fc082d7e75945dba5c96568d00ad275", "references": ["TIFU by putting water where the oil is supposed to go in a car"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-a5b9d99e116841688e65c736ba92d019", "references": ["TIFU Facebook Stalking"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-a8b565260fc5405a9e2b0453b7ae0568", "references": ["TIFU by talking to a neighbour"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-314e7d4781e7440983edb27955f56dba", "references": ["TIFU by buying a wooden toilet seat"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-f4f1226f68f04454b768fc37c5cb3720", "references": ["TIFU by not being used to wear High heels and trusting google"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-9d732e58972f4f88b13580ef56fab3eb", "references": ["TIFU by eating a chocolate that wasn't mine and tripping balls at work."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-c975074ce748478e87e3cf598b00479c", "references": ["TIFU by deep frying my friend's foot"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-02a1d739d88444889e01540b9bc8e1d9", "references": ["TIFU by getting punched by some crazy Russian at my school"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-2d69db257758466c9febf5d26726893f", "references": ["Tifu by using my favorite vegan lemon soap on my whole body"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-4a28e03be2ca4c73b523244cbf2918ae", "references": ["TIFU by not ordering from Best Buy"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-ebfac6b7ac5b4c13ba54aa0bb748f18a", "references": ["TIFU by hosting a basket party"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-e2cade13c1fa47f2be7e1b1daa03605d", "references": ["TIFU by forgetting about my vibrator"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-064c19de4c0549618fa2d3f54c5f812f", "references": ["TIFU by deleting my family hard drive backup."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-667f23ee6d9c4862a0247bfa7cd5b7b8", "references": ["TIFU by trying to stretch the truth"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-3d538f3b9e7242a9ba74560a2e688b44", "references": ["TIFU by giving myself a golden shower"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-890d045298eb496f81b724b8aa53e94a", "references": ["TIFU by farting around my fianc\u00e9e"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-3b3b2ffbfae14a2f8f0a179691d77b95", "references": ["TIFU By Telling Police to Fuck Off"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-f327104e819b4ec1954301a850a89f6a", "references": ["TIFU by orphaning two fawns"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-d0596fe6fef74a9783f8e8606d6d1608", "references": ["TIFU by getting my teeth pulled"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-60050d043d744287a7d7e7041754605f", "references": ["TIFU by showing my friends a picture"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-11a1f7bc6f834103a95305a19f450cc0", "references": ["TIFU By having sex next to a ceiling fan"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-a204804f51d54fdb9afe399ddb087274", "references": ["TIFU by drinking purple drank"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-16c9900b48b54dc0a05ca827d78b5f2f", "references": ["TIFU by being honest with my pregnant wife"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-75b22a62578149a8964b21acf6c9949f", "references": ["tifu by calling my ex a \"horny ass short haired fuck bag\""], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-93540bcdb1714e188756e0e6441ad755", "references": ["TIFU by telling my friend I may have cancer."], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task510-48ec821582cf4fd1b14c80cb25809e64", "references": ["TIFU By getting toothpaste on my asshole"], "task_id": "task510_reddit_tifu_title_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task935-d728bb7b73a04f25b560e95c72e2461f", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-817d3c58c3d444b7a4c12bb773eb377e", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-aa294765b8294401add65f646eefb875", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-b85b5aed97544f038a1935aecec175e0", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-11f2678c6c714cd4969bf762e3577cbd", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-99739d32693a49b89d73aa3d699cbce1", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-e105607f035e4f819b7da26237480d7d", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-34b83956d42b48668d63c1bf7f2485dc", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-e423263700e54d619ef3c5983f1d836e", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-18f4d94e57db4304b3b87f1418a0d78e", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-be61e7c1a34645099769e330e2a5ff0d", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-0f6fddf105b348d5997599c60c4fed26", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-c5b60c56941640c796c69034b3baf8c9", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-e1461070fb774cbe848bfdc86a32968f", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-240cac48eabe4a21afde7bd4d16e9216", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-358071a60987456da8787b6b02c94453", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-e9c537995afe4f82b5566baf56f348d7", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-2799857152404a6686cac02115e2458f", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-540cab67598b4f2e8eb03d4649e56c7c", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-9dd6f823c30b4058bb48afa7329f1c06", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-66ef16d5d3a742f390c39ab741be08a8", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-4d0b5ea493af4cc29583e9572288ea62", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-8ce9dfe974524a92bf61fc2e8acbd800", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-42dd8f204af746848c7ae42799d29165", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-cf47b776564c4ece82a47e947bf6bb42", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-02e5c0886dfe4d199623e52d25a1dfa1", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-006f7fee17164bcdb99b31ffa564cfc2", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-04c716c6b539415eae7684b6f62660c0", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-352aab5c38954e7a99a61d487cb70c27", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-3e5f8069d4304ba3ac84a8a61254dbf9", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-85670e63f0a14ca2a026f426d29e74f5", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-8cbf9efec8844891a0a563009d3a7318", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-23c5b385cec64a88b52ceaf28bedaa51", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-ab8f3cd1b5904bdd9682f4b286c55486", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-a59b723bc7224583aff2cfea977302e5", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-ee6f9e4767bc4ccab934720164a09051", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-13b9641b65c048e4b9aa20f877ae61dd", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-c30755bfd36c4311bf5a8f196aa7ac2c", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-14b10472691841cd8f899ce0de7ef3f0", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-ba38510802c84c06a6fbf264e00f3249", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-fb09a6afdc264419a03e7a3c2fa739d4", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-8df8890cff7a47a69fb3058979583fb1", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-3d2c8ef6495b476083c3688d307b49e1", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-39050c794dd74174a8551664de2572ba", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-b4a59711fd734d9db6e06703412cbf06", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-73a43b20cb3e44179774439f50f5d5c8", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-70b940f49f4c41e7b310209878feb238", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-048b463394704e5e8fc34bf109d00273", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-7f223494ceea46ceaf88ead0f93480ad", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-28dc5137a25b4ef78d23b4a54289f0fe", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-63c624b8a1944f859ab040e5262dfe59", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-50d0ff0535ce42c8a117249bc084c888", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-f784034c05b947fb9d898e87f5dca934", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-17412248e8ed4b3ab8829f0823164ef3", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-82b66fc0ac23463783df6398264bb156", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-e4f26114d2ee4a85ac9faa80243720d5", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-7cc36203fbf94ac78fee0b5be49d86a1", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-f46a3136c4dc4ce69c81621a9b32f4d0", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-ccdf9d5847ad49999d3300c4db5d5849", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-ed883485413147d3b1b3bc29a9f2de14", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-a426da86573b44038d6d92e57f146c6f", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-8f2b3b56416f42fc880348bd2c1361fc", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-a58b406485234eb9872f4d4c1901da2a", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-fe3dcb01525f46c49cefe18ba5bda373", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-0661846e26944b4e9aa314d5b510d9ed", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-1769634ade67439182c3a8701d97e899", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-3bf1aca4e2e74546a742059db3ccbc74", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-b4323d458c864b24963e7765df52c0f7", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-f37e9bf01fb848b1b59f8427d827da98", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-0c761eb9a4694f84891fb44efe44f6ef", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-4fc8a14e7bcc4df2ab74ac194c42aad4", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-ff87b14115d642688168578965e0e9da", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-8521d1d997cb49bea941bb432021b795", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-2643d173ba5c44978da64b99f1d55a48", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-ca71648ac91e4d6887148f74da8afd16", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-24fa985d3fd748ec8ca91e9751fa991e", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-21581108e8d944de9cebd5ccb1b15a45", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-f5db55169e9e468292d7f94fa188bfee", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-ecc483bd0dfd4af19d8799b6cf55158a", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-3a49c5bd5d03493facd8764f848c7f0b", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-4085737306b447109bd521b9c0408e22", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-a36744b790b148d58be9a9845167d60a", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-756140fec1b34a8895b994bcdf2404d3", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-dda97b49b4b74af295ff5adc216137db", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-b19ac43432534d09ba098e08c67f1824", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-e072b655330844d3be3587d6f3b2e430", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-badbc6086ed04ab2a221bb443197ed36", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-f83a6d79b5b840cd97085b92c9b4163c", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-e1036a41424e417abc759699e8d33e60", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-552016ede2804ef390ea2baf4fd9feed", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-efe77777d9bc449891a4782f153fb082", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-bd900e60323a4fda9f28fcd6bffa08e7", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-f6effa2f49ed4375a0d218b504c6a82d", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-ba32b96ee32f45279e0bf1910f660eb0", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-bddc951b1c244b019afe313890d03383", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-c2f67660b0aa40f89d2e8ba72917e4fc", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-8940920bedf44b76b50dd1efd820016b", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-ad698312566e4a83a6bbabf7e4b78ba9", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-8ce04f61c6454acbb3c0b880dc0b529e", "references": ["strengthener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task935-748714ff1eea434497af2a645600fb05", "references": ["weakener"], "task_id": "task935_defeasible_nli_atomic_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task349-46c92fffd6bf4e52a75ae1aa4a713fa9", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-7c8034b1865f4b579210f329fc875553", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-4344331f6b49480ca0e1916174df3ecf", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-40eb32e1dfda4b7d9d4cd32151b3c692", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-0f1c56cb8e20435bb375acad20350137", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-395e1115318f42fab10da36de30ad14a", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-fc32ff3264eb4205bba362700ec61ad4", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-27b4c9a347e44f5aa3a271d655bcaf42", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-dc0b936d959244b0b73f3a467f6d24e5", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-6ac77d0410d54769a3f6f07a9f5aab82", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-0857ae444931454c9417610649f8e939", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-cd99ec5cb63941b5bbf230781e52ae04", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-ebe6030064d04669b571520119f70121", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-8179c0727cf34b5a97cbcb7bc86b75ac", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-dc31fbd6498e4d40b68d9a23e0d68aec", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-1734adf9a72543818f3899c12bce2164", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-eac98d8de9d94dfc90f00aad73658938", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-b7b5a5c9ece34ffaaf6a2161b80b03fe", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-ccc36b44e35e4ae39059474dae923221", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-d9a6310a2b8b4bd98b16d0c2a3c443a1", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-e023fe9afacc4d9c94e3a49e15fcd767", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-46fdccaf534d4344a8361ead55ab0a3d", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-0be84c1d553545c49dad2a1cab6d25cf", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-222fbc3bf97b4fbfa06afe8f3e4ac661", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-6e05ccd1564f4b13bb9457e0b3831b9c", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-37647499d2e249b39ce1bf48c557950a", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-da4c1f5413c14e3bb2c98e341d457894", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-89e95dfceaf84739bdcd7618d939d30b", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-f37c3b0ae7524b47922d03f87451a8ce", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-8fdd2946dd2242a6ada152eed911b178", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-6d3a0d3abd8542719648bf3ff255ac1e", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-5b4e49007313449a84444e9357303fb1", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-1a7170ce605f47ada86cea052c100b35", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-64bae9f80be047df85b5f0f13ff28a4c", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-29b78a2b4ff648db9e7abbfce5828545", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-3738f0a9ff904967a1b3c94079b0efd3", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-ebcad26d8d5649f996b6092459cbf3f6", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-0767917d2c6546a5bbdb504406acbee2", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-d3d3fb00c8b3409798aa0c597a3ab5a0", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-1de58d45d60c40f78e421c76995c2dac", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-8bd17f1e12164f46ad9f400935de03f7", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-9a8524b97c204f3d84fa6bb67f055cb3", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-7d10673fff7e435bab9333b566691a28", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-125230fe7bad4cec92f49363e8b49131", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-069d7839a8bc43e28e220022fbdee023", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-11014be455f64946b9916863f1f5d285", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-dc65b1dc777b4acca0a684dfa0c4d047", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-2d4d061eebd6436bb66fa53f4b2ea99a", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-5eb9a093c0fb4c2396d219fba64bf3e4", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-654a8c744132490b86b9edd7f751cc3a", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-9579a8718c6c4199bef289e46035188f", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-30e24217e81a47a09b44374ca9cffb72", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-a95912a1e897404da9f5f712e215705c", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-af3022a6f2f84b56968d418033631019", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-539e84d801e34adca82e93d97f1e91c8", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-480b2f842c3e44ad82060e86907fd511", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-3f1bce0fb6af4edc9d81aa5c884c026e", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-b51952d6755f4cfdb9d6408c305af5f9", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-4ea62c1c7ffe4076be25af7f986a5201", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-faa2b943e3e44c2b902039e260eb42f6", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-ce7f255da3974eae879c0dce941ab019", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-94e09ad7f69d4beaaea08a6e70fb5174", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-99ff4529dcc6495e9d55696b77dca5bc", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-0d6b204bd3474c45b1906c45629ff2c1", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-173ffa13c8754375a616599dc00b99e2", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-f4c37e1e350c4b4eb33a8c74bfa3e278", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-3f1e069671c249cdb8a9e4265180812d", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-6308f92b0bc244dda24bcde336a19a73", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-a22d6940c0dd497ab34f255c0c3e631f", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-f2597118a6aa444e89f0d705ba63d2ce", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-4da5e3cbae524c9aac13d59dd98e542a", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-97de3faffdfd4ac3a2a0ce837bd7d361", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-5da4f6c456f5463a85347789dbc6a6f6", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-0b5cccefcd034c2986b8a55890e11534", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-3641f843652a4adfba4611f81142011e", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-e94ed9f8249a4c108531a85591986e5d", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-09e92a0abeaf4d66970b696f5d86ce52", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-2d1e9d342ffd432e81b3b32f60cb432e", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-98e0fd69dd80428a96c621db19592efd", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-3f255226abfb49c68afdd6e5f6f7c270", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-0badc2de400f4a8a9271d7b308685f0a", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-f0f2f051a1e349e29006f6fa548b6695", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-ba5113290c194948a8b2e6457fb6e062", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-2e5a631d461a4ed58157a5ecdb0fc674", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-4cdf250ce83e4e559cf73a0f7f63cfc6", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-4c6d6019499b4ee99dc3667499cec8fa", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-85d1aeb387b94979bd0ba4ba3c9649a1", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-bfb3ac4c02b94d968118c33113db4c0a", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-3106fe25f88749eabc81af06ba6ae236", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-71d781f020d14fb1865ace334be77549", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-43a1a7635cc04b9d84fda3c46d252410", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-fb1233d07a2b4974850b51a609cfa093", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-c92e1b6b3360435895e86462412ef847", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-4ba8f201a03943c08ea983317dfbcb6b", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-f9c9ee04ca564595ae4ffbeb704eb99b", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-575755b526344f8d9fab6beb766c183b", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-0885643c6d6b421d96fa2e999febb23b", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-f3e04b048f8a48c09e661fd1c35057ad", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-e639fbb23cfd485b99d78027fbc849b2", "references": ["True"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task349-f51f896a5b104eca8d635e3ae32cf25f", "references": ["False"], "task_id": "task349_squad2.0_answerable_unanswerable_question_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1157-52939868a2bd4733ae2571f10ad0bd6c", "references": ["kitchen", "bathroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-671b0c11ca964e67a4bc4d1f2967b922", "references": ["library", "hallway"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-5473823fc7f74aee8d072697b9a2bb9a", "references": ["office"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-5476d036cdf54f86b454640962cfff4b", "references": ["parlor"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-daff548bf45740aa8c39237dd788cab1", "references": ["hallway", "bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-d06fc8df08e740b88697827621039de6", "references": ["garden"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-d4c630d276c54708a3c61400789ad624", "references": ["den"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-066318ea88d94206990a452fe6549958", "references": ["bathroom", "kitchen"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-3c4b894416504161ba2bbd85d131b3e2", "references": ["attic", "garage"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-d59cdcdd3de644a28fa5a9d20070d03f", "references": ["bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-d997ff9a714e4d4fa7bfdb3bdded374e", "references": ["garage", "driveway"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-23f676df60834c59a689003d9361905f", "references": ["nursery", "bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-0406881e5f7e4a369cc9e676fb5044af", "references": ["kitchen"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-32c14981061e43b9ac0b89168a1433bb", "references": ["library", "hallway"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-901e77d997554ec6a34299b62f6c7ec3", "references": ["office"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-f209ed8c89b74c82b386aad0a8c94830", "references": ["parlor"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-b2368b6a74204322ba1a36f4e0463f70", "references": ["hallway", "bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-99e59c0fa7c4477b89487cd36144e86c", "references": ["garden"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-153a96851c6343efa032a2e1a06c36d5", "references": ["den"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-9f736cca0b8046ed83ea20f96dc2c97b", "references": ["bathroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-b8df4ecf445c4f21adebe977eb88451c", "references": ["attic", "garage"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-eb1bc51df9f342878fc934657485fd57", "references": ["bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-5f2538f24e5f467d9036f54e4c6a067f", "references": ["garage", "driveway"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-89167152c75b412ab46c99fe6bd130bf", "references": ["nursery", "bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-134f27caf64e4c8ba4d340d33236cdaa", "references": ["kitchen"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-44608f27ad9847ac843a654bc6739098", "references": ["library", "hallway"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-6f2fe58d22e843fa8a88105aaca81394", "references": ["office"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-62868d124f5c4eb9a0aee53efee5c813", "references": ["parlor"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-f57ef9f8422444eaaeab781980b51c78", "references": ["hallway", "bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-bcb59a313f624d6db86db4a9e745a74a", "references": ["garden"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-0bd06a77cc6649e88e7b1d6b01f4d4b5", "references": ["den"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-a123621955674b86a8689a31d9784835", "references": ["bathroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-82eb560aa7f04c0e8f4c002d14c50b43", "references": ["attic", "garage"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-9c69dce2e1354a83908ddbbce0530105", "references": ["bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-1b41e62eae494ff3bd04db250105aa11", "references": ["garage", "workshop", "shed"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-681d91f9811b4abf82c8f146671289a9", "references": ["nursery", "bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-f2a1911959304fed89fef7986e0979cd", "references": ["kitchen"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-efa6ee08ef7c4a908e62e8f7e9ef8b81", "references": ["library", "hallway"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-8ed8ff3bcd6a42989c9e5ec008ccde6d", "references": ["office"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-73624cacb2354cb1a4ee6a8fd96a4d3f", "references": ["parlor"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-31ca042692e64867a705f3f5e3ce637b", "references": ["hallway", "bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-a139f14eaeab4c89845266a5ba01938f", "references": ["garden"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-04bc5df91d634e209f8846b68c9c238b", "references": ["den"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-c95a573c139240639d1b4e89d149e2fb", "references": ["bathroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-a7c896230315489b84c3855d0c79e331", "references": ["attic", "garage"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-fb0c70269bb84b73a8a91fd7af4c1a70", "references": ["bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-0813183d75a34ad3b5069b06ed8c7ce6", "references": ["garage", "workshop", "shed"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-2ee670f38667483697a97f024b38c715", "references": ["nursery", "bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-970e454d6ce4494382581463fc35f3b9", "references": ["kitchen"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-1c3b37d30e764576a79e6784240bb545", "references": ["library", "hallway"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-e7d794429aca4f1dbda5c69204805883", "references": ["office"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-286e7035ee8e4df0b0618236c2ebc44f", "references": ["parlor"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-d6a78d7a97e04bc4a53c86735522f6b7", "references": ["hallway", "bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-eb6943672c1748b8bdc29714fc623782", "references": ["garden"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-2e6987bdd05e428e9c83bc7a19c76c83", "references": ["den"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-a817960b916a4257bafcdad5699128da", "references": ["bathroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-02e253d15470460e8c237637cf46f09c", "references": ["attic", "garage"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-667908eb3c3a474499178edfd7524e91", "references": ["bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-39f50a3ad7a84b0f9ea8538a2924cc57", "references": ["garage", "driveway"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-c2b3575f40744ed882d8ca9e08350d5c", "references": ["nursery", "bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-53134a8972344334afccb8a03f5d75e4", "references": ["kitchen", "pantry", "nursury"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-c3721af4a3d841eaa24fc5e46d366b8b", "references": ["library", "hallway"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-142fe0aeda5f4ecb9ad01b743cfbe77a", "references": ["office"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-1e58786e0c644a2abb6db2bd99dac65e", "references": ["parlor"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-fa5d8dc525644fe2b6cc2d66d03acd1d", "references": ["hallway", "bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-ab8485e009b84b9689b54ade5a8e00d2", "references": ["garden"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-6c918637f2b542adaa35019458f2b158", "references": ["den"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-90747c16291544e0bcd8ac450d07045e", "references": ["bathroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-372dc1dbb331496f8c318b77832a6d38", "references": ["attic", "garage"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-deb92ba40fd14359a7c7f38b10de6840", "references": ["bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-fb497e1309734a66b308975c58ed9438", "references": ["garage", "driveway"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-bd3a7a1f01814e1fb3cb6ef4c3a94022", "references": ["nursery", "bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-a7c1d31651f4407cb94a3c5f21551821", "references": ["kitchen"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-9d8854a26ce540a1b70228ad805125a9", "references": ["library", "hallway"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-0f9b99b5d44a41a296c0fadf31fe10d1", "references": ["office"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-97a950d5ab7948f68d6436ababb9343f", "references": ["parlor"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-e78d7de916c44ff49fe9737c3b3438dc", "references": ["hallway", "bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-b75d3c3d7da84d1b8be3603ec97b5bca", "references": ["garden"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-8980d2415f554f97a4d333162e0d9e26", "references": ["den"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-6213a6d1dbe7410e9f0a362d25fdc4c0", "references": ["bathroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-da6cd9a0b2a442498813f5130d934a0a", "references": ["attic", "garage"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-e08c2c8cb0c44afd944f65017aae9ce8", "references": ["bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-eecdca663b9f43f39841c683c092e724", "references": ["garage", "driveway"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-3aec1567e1684214823f72612fbe68f7", "references": ["nursery", "bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-18e45ca0a317422dada5581b9f4ebe04", "references": ["kitchen"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-e3d44a385ee84bc9ac65d42a4c937875", "references": ["library", "hallway"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-0faf9bbc68074aebbe236376cea2d79c", "references": ["office"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-6a1631c870284836bd9684f5a0091a6d", "references": ["parlor"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-35cbf26ff1594b30b7ba509fd060f8c0", "references": ["hallway", "bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-118c6b057f554b91b3d27414f4836b75", "references": ["garden"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-c834a7f235ac4a3982b5e9e6d55d2341", "references": ["den"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-938908d117b143fea50a6f6f63808799", "references": ["bathroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-c1818e92d48b45d9a67a703501148b2e", "references": ["attic", "garage"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-2c50617fae294510905287eda4b07afc", "references": ["bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-a72f4b6be72b4d5593455b064fbaedf4", "references": ["garage", "driveway"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-818304f7e79c424b88e9d2e71d225230", "references": ["nursery", "bedroom"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-f4cbb21b9a5e43ec8125b210cd47c6c6", "references": ["kitchen"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-fc3ecb85f9f646a1ae86006b727e754e", "references": ["library", "hallway"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-ff9c600a720d432baf0b429ac4c95dc4", "references": ["office"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task1157-12c6625990814a7a9b0be4f00460f7f9", "references": ["parlor"], "task_id": "task1157_bard_analogical_reasoning_rooms_for_containers", "task_category": "Word Analogy", "track": "default"}
{"id": "task937-c11c527689934050b3e14491c36bfa0d", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-8f5a3b6f1f8b48d69aef8cd0b03651b7", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-69e6f28547774a4b851adb1f5a03a432", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-53a4894dae414e1e88febb50f48dcec1", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-3e121f140f174d3fbc45ee3642c1a51e", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-ad52df42de12418294a8d0733865b2b7", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-8e3d7279bd7f4687bfa3219e99832b38", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-1fe087171d984039bd7f28482e6b0d8c", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-54789cc5f261467388c6cca9c61311f0", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-162cf3c9ff6341fe98be46f85dc68286", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-a370c047c7eb4366a5e0db173d7d288c", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-2d45c1befd674763850879ef096d8fd8", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-b9302e272163487aa814f7877ef94a2c", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-5f8d0cf73e6d4e87926cb79042d60643", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-7776244dce464f3193c970365e49af5c", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-d559f1f2157d4b3f963f62926f07b507", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-6d5d60a2df3247bcb9acf8728c610b99", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-5eafb4dda4d549c9bab4d4e2ce76a021", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-1401a2bc08244238aa30fe86aa5accae", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-325585f4f38b40b7a85a1816815657ce", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-ae5d524dfba747d38593fd1b915464e3", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-b2c6f4e5306b42e2909dc328c67f5ca8", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-ad8a5ce65fbd48ac8895870de2dc95d7", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-5269e4dcede24322bcad085b6701dc4a", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-52ea42bcda544693859fd1913b4f5af7", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-b948dd1d4d824a32bd49edfe7d13ba91", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-218cc3de9d7842589192db0e5a488f0a", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-ed9b2ba926ea4f9d9700b06c5cb3604c", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-c88739a577154e8bbef79098b5f52cc6", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-64dd091f0b1748e39c09f8bf6d705d85", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-44f099801e2b4c9492ebfea733ace8f6", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-3c0232b2d78a4a869d85989f8326452a", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-39537868237048bd98f4d6f023c16c07", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-989099b139934907b1943d48e7dac34d", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-fd7d24dda5f94bec9ff910eb9cf12093", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-a01f8c997909459c8990088686d4410d", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-fa632437443246c6a1980dddb50345b2", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-cd55d98514b64b83b859c84055a14907", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-63e821960be84374838ef106d58a5268", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-542ba478d99c4a1d99a8f584b8d33bac", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-8f0348b9f0ad4d7fb589eb5dbbc44d3d", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-6ad0464eacb74578a85616b018f77370", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-d62d0a13fcce453aac5fba766d92b34e", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-ed511d1643b841e8962326272cd82057", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-b6df78d5bcd0406e971d4c1bd69bd077", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-6379ea59d97443d2a11f0cb1b8b622ab", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-76e01f1218e24d68bc3d08f2b312baa6", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-bf5e60b3d4394323b3cd43d0d87ff982", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-a3b4e3f1cebb45c092256a7ae9e47c67", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-877f1823c28445089cb28512031cf72c", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-9d0b6f791f3a4d7d84bbb793e5fa2dff", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-9aa86bd08eda49278f7a6474c38b747e", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-e28dc1a07ac442998c3b73a5884dfcba", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-00fb79370ab547cd8b6007d1f029928b", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-df96089b65264ff588a7d0bfb1f109e4", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-19d41aca4cbb4fa6a5abb98cefca136a", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-6cad0a61b3264b8ab81f4c72ad774d8f", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-8db54eefaac141bab3f499f9ccf6de0e", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-b8428cf54f2743a69becf23ed2f3877e", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-3414d4b2b4c242fa9e4ceaf567b4265e", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-e9b78ac4e8b94bfd8231c96db16afeaa", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-d90e40d393404488b908d890c7c5dcdf", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-7f6f8b481077476480b6fb4ab1f93f04", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-287377f0940346ac9e8a727135f4520a", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-08781fa3604544f3890aa738566bed0a", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-a45e34b2c7ce4647a3496039fef24eef", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-58ed6c3d44234bc0923771a5fb00d391", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-de410a80536d4467be7a6af903896e73", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-13df35ace4b443029a9c5dd3632fa186", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-c73e570a0cb940c2bc2853cff53be9d9", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-fbd612e15e844698a63d3d5dfe13f535", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-6ba00c56b25b4405b779476307acc848", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-08a8c0ac940341a1bcf68ee04393c266", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-c1141b9e6088442b8e76edb131959586", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-e2770494dca14bc38e269f5250e86fc6", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-4da4d86c2fa241d39decf5876119d8e7", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-e7930660bf8d493fa0a3331e1e4a3acf", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-53a81102ec5442e3bcf3b38294527d35", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-7faf191f681b4439bafa46e557cc6e76", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-f309817636354580adb965d6fe05d59a", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-8b8e527711ab4960a92fdd23188ea6f6", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-c3935e0c8d6c46b48ac7dae6e1c1659e", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-a11c469f3d32414c993dbb81e6b9c77b", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-87149f918ea7468e9c8b970d6154bb26", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-d05d7df7319b4564a54c4ea52b077142", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-f6e77c6f5dff4db68cf3a2cb08efe486", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-f61c1a9699824004947c8ebefc38c4bb", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-73a3f304c7ed499cac9b1fe292e28840", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-8e4ee09677b241c7ac159d20b33a038c", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-dec138936c754118b3c9f34085d3c68f", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-a57281f5350b4bbd90caaf1a3257f5d2", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-bad7cd9c59b6461587b7c5331c929ca5", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-41e35ab1d270400281061bf655e0f102", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-7ae407462e164640b86e8a6f925baf8e", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-22f97c1080df42d5aa829c128b76ad1d", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-d28f33528e8c4ed88199477c53724475", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-5e4db6b9f9a94fdebd1be400b642b96f", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-94186eb717354c85826922212a5d6704", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-6a8a0aa8796d4a968b7027d10d37b7f8", "references": ["strengthener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task937-ec939c854c0542b29ce78944c92fdb5e", "references": ["weakener"], "task_id": "task937_defeasible_nli_social_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task743-4b71730decf34790abf48f5b3a802029", "references": ["Council Directive implementing in respect of the film industry the provisions of the General Programmes for the abolition of restrictions on freedom of establishment and freedom to provide services"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-be0ff8ad1fe944bfbba131d77e90a10b", "references": ["Council Directive amending the Council Directive on the approximation of the rules of the Member States concerning the colouring matters authorized for use in foodstuffs intended for human consumption"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-ada53844fac74b1ea1c4b7e8c20ae029", "references": ["Regulation of the Council laying down common quality standards for garlic"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-bd023ee2feb14ff089aea8d9099c131c", "references": ["Regulation of the Council on application of Article 85 (3) of the Treaty to certain categories of agreements and concerted practices"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-f750c07070464596b7393e5243467c68", "references": ["Regulation of the Commission amending the common quality standards for certain fruits and vegetables"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-2982b17dd474410b9276ede34c2746fd", "references": ["Regulation of the Commission on the non-fixing of an additional amount for Polish eggs"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-b164ee96f13e49d4b346f4353ca11e0d", "references": ["Regulation of the Council of laying down mortality and disability tables and the assumed salary increases to be used for calculating the actuarial values provided for in the Staff Regulations of Officials of the Communities"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-bbc82fae83d740849a206e442b5d7f11", "references": ["Council Decision setting up a Standing Committee on Seeds and Propagating Material for Agriculture, Horticulture and Forestry"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-d852ca7f643844eface335ca2058d1b3", "references": ["Commission Decision on the aid instituted by the Government of the French Republic for the purchase of aircraft (Only the French text is authentic)"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-ed15839c7f7d4ceb869a78a7f0da9376", "references": ["Council Decision on Community aid for the Italian Republic towards the granting of assistance to sulphur mine workers affected by dismissal and of a number of scholarships for their children"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-4a7ed59847144fad8ecf20d51ebaff14", "references": ["Council Directive concerning the attainment of freedom of establishment and freedom to provide services in respect of activities of self-employed persons engaging in the provision of electricity, gas, water and sanitary services (ISIC Division 5)"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-652cf9d927834c11942e0cf57e21ae0b", "references": ["Council Directive amending the Council Directive of 26 June 1964 on health problems affecting intra-Community trade in fresh meat"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-0815751c04c8461db646f91d01afac82", "references": ["Commission Regulation exempting the transfer of small quantities of ores, source materials and special fissile materials from the Rules of the Chapter on Supplies"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-9c3b784bc956454ca0ae18b2128d83c2", "references": ["Regulation laying down the list of places for which a rent allowance may be granted, the maximum amount of that allowance and the rules for granting it"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-534ecefaf3a646ca8a17a8c506a0eb7a", "references": ["Regulation fixing coefficients for the different varieties and qualities of unrefined olive oil"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-e985a2ac5cce4ac4a0849bbef161cff5", "references": ["Regulation on the non-fixing of an additional amount for South African eggs"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-6bef5f3145e043e3af084358a6d65c20", "references": ["Regulation adding a supplementary quality class to the common quality standards for certain fruits and vegetables"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-2f42b85d3fe544f58f67306d78f0bbfd", "references": ["Council Directive concerning the attainment of freedom of establishment and freedom to provide services in respect of activities of self-employed persons concerned with: 1. Matters of 'real estate' (excluding 6401) (ISIC Group ex 640) 2. The provision of certain 'business services not elsewhere classified' (ISIC Group 839)"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-3e169491bc14424e8f615c8425e603ac", "references": ["Council Directive amending the Council Directive of 26 January 1965 laying down specific criteria of purity for preservatives authorized for use in foodstuffs intended for human consumption"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-28d5258a62d244859ca82f6433fe413b", "references": ["Council Directive concerning the application of the laws of Member States relating to agricultural leases to farmers who are nationals of other Member States"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-b090504af5db43f49097c98c29a0b4c5", "references": ["Council Directive concerning freedom of access to co-operatives for farmers who are nationals of one Member State and established in another Member State"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-32f70e1defd545f79bdc19bbd3d1f735", "references": ["Regulation laying down criteria for determining world market prices for oil seeds and fixing the frontier crossing point"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-d1f131120d8e4960946bec2e3bb8c97f", "references": ["Regulation on fixing the additional amount for imports of poultry-farming products from third countries"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-260df0775dc04ab8aad80a50532ef558", "references": ["Regulation fixing the factors for calculating levies and sluice-gate prices for derived egg products"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-a9cd729878fe408a9be52ec650f99c21", "references": ["Regulation amending the common quality standards for tomatoes"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-48f7a39b03e745a38b48c6ff7b313e34", "references": ["Regulation on detailed rules for the application of the compensatory amount applicable to imports of certain vegetable oils"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-13984ac1a4e94a42b3976f96b1d163aa", "references": ["Regulation determining the emoluments of members of the EEC and EAEC Commissions and of the High Authority who have not been appointed members of the Single Commission of the European Communities"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-547be4c4f6ac4e888470365fb8391ee9", "references": ["Regulation fixing the conversion rates, the processing costs and the value of the by-products for the various stages of rice processing"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-2b947f982385420d91ce552d2ff3116e", "references": ["Regulation on the taking over of paddy rice by intervention agencies, and fixing the corrective amounts, price increases and reductions applied by them"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-a92b53fa248a4127957fc620a55d0f6c", "references": ["Regulation amending Regulation No 202/67/EEC on fixing the additional amount for imports of pigmeat products from third countries"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-8116d15ac4c34129886c12b26f7187d3", "references": ["Regulation laying down conditions for intervention in respect of oil seeds during the last two months of the marketing year and principles for the disposal of seeds bought in by intervention agencies"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-b5b655b5a3cd4d379fb090e9c4461a18", "references": ["Regulation on the non-fixing of an additional amount for Australian eggs"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-4effe0b1e7bb467ba6123cd411a07e18", "references": ["Regulation on export refunds on colza, rape and sunflower seeds"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-4cb60636840f4c03a73941c38c538ce7", "references": ["Council Decision amending the Rules of the Advisory Committee on Vocational Training"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-1e4eb4fef3f64d14a2619cbee7456f8a", "references": ["Council Decision applying Articles 48 and 49 of the Treaty to the French overseas departments"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-bc36643c243647a18a6008706e73e224", "references": ["Council Decision setting up a Standing Veterinary Committee"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-aca62b08fcae4b308934b141d95c401c", "references": ["Commission Decision authorizing the French Republic to take certain protective measures in accordance with Article 108 (3) of the Treaty (Only the French text is authentic)"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-57c4163b896545dd9d7404bf06ad66d7", "references": ["Council Decision on the conclusion and implementation of individual agreements between Governments relating to the obligation of Member States to maintain minimum stocks of crude oil and/or petroleum products"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-942e0523646f4bc1850308085387aacd", "references": ["Council Directive concerning freedom of access to the various forms of credit for farmers who are nationals of one Member State and established in another Member State"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-db7e56da195e4260b0499d12f5d9a8d8", "references": ["Council Directive on a common method for calculating the average rates provided for in Article 97 of the Treaty"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-0afa7f8e6f02455ea63dc5c814a0538a", "references": ["Council Directive on the standardisation of provisions regarding the duty-free admission of fuel contained in the fuel tanks of commercial motor vehicles"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-9e03573c726e4a5cae0e54ba455a8374", "references": ["Council Directive on the abolition of restrictions on movement and residence within the Community for workers of Member States and their families"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-42712c369485436a8582a6295b8c850e", "references": ["Council Directive concerning the attainment of freedom of establishment and freedom to provide services in respect of activities of self-employed persons in the personal services sector (ISIC ex Major Group 85): 1. Restaurants, cafes, taverns and other drinking and eating places (ISIC Group 852), 2. Hotels, rooming houses, camps and other lodging places (ISIC Group 853)"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-facdc03f3c34457cbf862e1bc8aac31a", "references": ["Council Directive laying down detailed provisions concerning transitional measures in respect of activities of self-employed persons in the personal services sector (ISIC ex Major Group 85): 1. Restaurants, cafes, taverns and other drinking and eating places (ISIC Group 852), 2. Hotels, rooming houses, camps and other lodging places (ISIC Group 853)"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-0cabd7c46a14462d87d01845be37432b", "references": ["Council Directive concerning the attainment of freedom of establishment in respect of activities of self-employed persons in film distribution"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-3fa9e1e4b78e4582a805e0353cd6585c", "references": ["Council Directive concerning freedom of access to the various forms of aid for farmers who are nationals of one Member State and established in another Member State"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-2071b330a2fa4ac5ac1fd6e1cb8ae99d", "references": ["Council Directive making a third amendment to the Council Directive on the approximation of the rules of the Member States concerning the colouring matters authorized for use in foodstuffs intended for human consumption"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-77d1441b48cd4fff867305d91633f62b", "references": ["Regulation on the denaturing process for colza and rape seed"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-e0dff18c1c04416ea1f342556e744b97", "references": ["Regulation supplementing Regulations Nos 282/67/EEC and 284/67/EEC on oil seeds"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-a84700bcea0b4c738e20c59191ac7744", "references": ["Regulation laying down detailed rules for differentiating between delivery contracts for beet"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-c80f7ee08dfd48938490379ba3a199ec", "references": ["Regulation laying down the conditions and procedure for applying the tax for the benefit of the European Communities"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-6201f9e987e2406d9d88281ed1c6c6ca", "references": ["Regulation amending Council Regulation No 423/67/EEC, 6/67/Euratom of 25 July 1967 determining the emoluments of members of the EEC and EAEC Commissions and of the High Authority who have not been appointed members of the Single Commission of the European Communities"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-4d45f3a121314f8abe767493a21fd851", "references": ["Regulation fixing quality standards for fresh cut flowers and fresh ornamental foliage"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-b3492f8626974aa8af2b3a4467f3e892", "references": ["Regulation laying down detailed rules for intervention buying- in in pigmeat"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-1a7f6b7f4aec42b9a17724157365f495", "references": ["Regulation determining the standard quality for raw sugar and fixing the Community frontier crossing point for calculating c.i.f. prices for sugar"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-9c31ea9d049e4b18a12793b2da782ddc", "references": ["Regulation laying down general rules for intervention buying of sugar"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-f36ab35d916c4cccaac4e1f0569730fe", "references": ["Regulation on the non-fixing of an additional amount for imports of live swine and pig carcases from Poland"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-7a5067dec4de4ce1bbfc11edbb0a07fd", "references": ["Regulation on the non-fixing of and additional amounts for slaughtered fowls, ducks and geese imported from Poland"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-5d4e897908ea42ed9bfeafa86cf30779", "references": ["Regulation on the definition, applicable to the granting of export refunds, of hulled grains and pearled grains of cereals"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-10fbc84194fb444a914cbffc6984bd82", "references": ["Regulation altering the period of validity of Regulation No 142/67/EEC on export refunds on colza, rape and sunflower seeds"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-3f50de0342bd468f8e078d051fa06266", "references": ["Regulation laying down general rules for granting export refunds on milk and milk products and criteria for fixing the amount of such refunds"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-424b4f12d1fd4a81bece92708d9327f7", "references": ["Regulation laying down general rules for granting export refunds on beef and veal and criteria for fixing the amount of such refunds"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-d0d403081fbf471a9c356f4bdc8d3637", "references": ["Regulation amending Regulation No 470/67/EEC as regards the quality and quantity of paddy rice taken over by intervention agencies"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-7f92099700a74ccb8faa5af31e6a9f3d", "references": ["Regulation laying down general rules for intervention on the market in butter and cream"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-422a90f87ecf4dbbaba0c67cfbc59156", "references": ["Regulation laying down general rules for granting private storage aid for beef and veal"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-f0fee70f351f41339fb889b31dab9bf9", "references": ["Regulation on the non-fixing of additional amounts for imports of pig carcases and of certain cuts of pigmeat from Hungary"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-a78b47075a404fcdb3eeeda69919eb14", "references": ["Regulation laying down general rules for the public storage of skimmed milk powder"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-edc7e2b6622c429cbd46102d07f23a52", "references": ["Regulation laying down detailed rules for determining free-at-frontier prices and for fixing levies in respect of milk and milk products"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-0a51eb60b07644ff88edbf3a78d8d25c", "references": ["Regulation on the export refunds applicable to certain products processed from cereals and from rice"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-c277389dd467464d80bb27b448cb65be", "references": ["Regulation on detailed rules for the application of export refunds on milk and milk products"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-2c088c5ed4144dbd84a43d47586fb8a1", "references": ["Regulation laying down the method for determining the lactose content of compound feeding-stuffs imported from third countries"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-d8ed2ef396c14ce1b0aaface082fc943", "references": ["Regulation amending several Regulations on milk and milk products"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-af5aae611ed34115b91bb2a357938a6f", "references": ["Regulation amending Regulation No 474/67/EEC on the advance fixing of the export refund on rice and broken rice"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-0ef2f51aa40247b4a1f343693b3978cc", "references": ["Regulation supplementing Regulation (EEC) No 1043/67/EEC by defining the expression 'undertaking producing sugar'"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-373c404519424d2fbc5207c1c7db6386", "references": ["Regulation amending Regulations Nos 282/67/EEC, 284/67/EEC and (EEC) No 911/68 on oil seeds"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-e60b99e1ad1c4587b8814c01b1806dab", "references": ["Regulation amending Regulation No 91/66/EEC concerning the selection of returning holdings for the purpose of determining incomes of agricultural holdings"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-f50f6cc0540e4eb392c9be647c0eebc3", "references": ["Regulation on the system of minimum prices for exports to third countries of flowering corms, bulbs and tubers"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-68efee8dd2cf44fc9e6a5a982588ff6e", "references": ["Regulation amending Regulation No 136/66/EEC on the establishment of a common organisation of the market in oils and fats"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-353680698f144ed49b48eebe5757f7aa", "references": ["Commission Decision on the French systems of aid for research and the reorganization of production and distribution in the textile industry (Only the French text is authentic)"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-cb26494077904aa9a462cc1ae593d136", "references": ["Council Decision setting up a Standing Committee for Foodstuffs"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-10783e36827042e081e9c2e02a8d041f", "references": ["Council Directive amending the Council Directive of 14 June 1966 on the marketing of cereal seed"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-d79838c12e694b50b9a60637ae5806f8", "references": ["Council Directive amending the Council Directive of 14 June 1966 on the marketing of seed potatoes"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-0dfc9a522a96494085221896de3b09ad", "references": ["Council Directive amending the Council Directive of 14 June 1966 on the marketing of forest reproductive material"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-6252789d81084be086c471bfb10174ab", "references": ["Council Directive amending the Council Directive of 7 July 1964 laying down detailed provisions concerning transitional measures in respect of activities of self-employed persons in manufacturing and processing industries falling within ISIC Major Groups 23-40 (Industry and small craft industries)"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-37710e3c95fa49828d591c383759caf9", "references": ["Council Directive concerning the attainment of freedom of establishment and freedom to provide services in respect of activities of self- employed persons engaging in exploration (prospecting and drilling) for petroleum and natural gas (ISIC ex Major Group 13)"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-ef0a334b9d334082bd03f32e12278c54", "references": ["Council Directive on the harmonisation of provisions laid down by Law, Regulation or Administrative Action relating to exemption from turnover tax and excise duty on imports in international travel"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-33311a1afa1c4e0d845e06cfcd5d5d24", "references": ["Council Directive on the harmonisation of legislation of Member States concerning turnover taxes - introduction of value added tax in Member States"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-6044b7dec59742f49a01dca154f00dbe", "references": ["Council Directive on control of Potato Wart Disease"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-c9c8057393fd45e3a17515a407abcd1a", "references": ["Council Directive on control of Potato Cyst Eelworm"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-b28ddc1c2e4b4c349138b77c9b741b79", "references": ["Council Directive on control of San Jos\u00e9 Scale"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-d932b3c3c8fd45048d125da815196e1d", "references": ["Council Directive on the approximation of the laws of the Member States relating to crystal glass"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-82a8dbcae7cf440d82491c3f8dc8981d", "references": ["Regulation on the advance fixing of the levy on imports of olive oil"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-c25075c7502b4936bb09aaca946c6839", "references": ["Regulation laying down general rules for the disposal of frozen beef and veal by intervention agencies"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-69443c5b57c44e1895cd954ed57d1d94", "references": ["Regulation on communications between Member States and the Commission with regard to milk and milk products"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-4d1b681b45f24b398c0369ae0ff5b6e1", "references": ["Regulation amending Regulation (EEC) No 315/68 fixing the quality standards for flowering bulbs, corms and tubers"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-48263c7e79c04c498b3cdc9e9173bb20", "references": ["Regulation on the reimbursement of aid granted by Member States to organisations of fruit and vegetable producers"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-ef45f7c9765d4090931f4603fa3f5a4b", "references": ["Regulation determining the categories of officials and other servants of the European Communities to whom the provisions of Article 12, the second paragraph of Article 13 and Article 14 of the Protocol on the Privileges and Immunities of the Communities apply"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-bd45ec3563a844d1aad0edd1517e36de", "references": ["Regulation amending Articles 7 and 8 of Regulation (EEC) No 766/68 laying down general rules for granting export refunds on sugar"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-fa0893aeb3f74fe5946320781daba8c3", "references": ["Regulation amending Regulation (EEC) No 837/68 on detailed rules for the application of levies on sugar"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task743-6988af5e55834b84b961a22f9fa501c0", "references": ["Regulation amending Regulations (EEC) Nos 198/69, 507/69 and 685/69 on intervention on the market in butter and cream"], "task_id": "task743_eurlex_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1388-b242d3b1bc4d48a49cfde235426f0896", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-57dd37c990cf4030a90c8e001f6011eb", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-53f931a6ee41415785df396207630f54", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-67bc97a4bf224730af09efb544543691", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-d4cefba7e8044d7b8e8877ffed28bef9", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-5e902ab797344c2294fc27884d504d86", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-519caeb9e98f492abbe94193bb456efb", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-1708318d329846a5b3eeb8c54e19bc0f", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-a1f747caa68449c583b0b53728791baa", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-197a249f9e1f4117bc3da2c09f040f5a", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-3840236d651a48ef887228ecef2e68e7", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-4bf0507e59e34d4a82304a4a607bbb76", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-deb05bc495364454afaeb63ef1b36789", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-e6a9dbf50f8043aa9edbc1950413c638", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-d7f9a1040b3141c6b7f52076f8f58df1", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-d18a9573c5e344398875ad8b80a95e79", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-3b098b849ff743278dc0514bf076c878", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-c8d8640d79884bb885882b7fe1ac445f", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-cb94d80662914bacb966f8a141cb96f0", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-d2805433eb3c410a880d8eab42553faa", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-3ffeb8e32c794e8ba9adcd52fc73c53f", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-5f17ad50c9c64652a3aadde5af93ef0e", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-dcfe0bebfed841299afb205238cba7e8", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-e54c3eb1442e456b99d8a76e98a9d140", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-cefd239afd244203b611ddea5664fe96", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-45a58a70cc3048f6bf6bb7dd42a2d4db", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-6d8d60da400744f797a797124402eec9", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-b052ae3486ae453abd2156dbc59561bc", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-98ca26378cd44c4886ac5702e27340f2", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-1403787bd69c4ece89ad2b641892fb20", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-5da570cab1674fe2b143172f5487a4ac", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-0580797d39f341958898cf8207bfa889", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-dde0a4780f774cb1ab420ff1eeb74108", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-6f2fc70fd0144b36821bbdbf907af116", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-43f985c4c42e424ebab4e4ac4548b67d", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-b6bd9263505749e89fcd71c91465fe0d", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-aca02a2f6feb4c53828fbd561ac046d7", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-696f54ee2330497e8fba3b1cad690a9b", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-ae5164fe205a423b8bbd7c650dbc5cd0", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-a5343f5f9b424ffcab5d0e454c88de66", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-401a364e73734184b2c4fcef9a6ced9b", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-7d91bfea64fa4b7790fbb5cac9ce737f", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-751b500cd68540afac2f439214e09f47", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-4cfa750e91ae40549c3c4f66f5dd60a1", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-99a407500a8a46edb38f7d2cceabafbf", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-367a930b9bd346eb962836d424e13c4a", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-0b43d96b5b684bea9374cc304004d30f", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-d1cdc86684004c7f9f18b2cab3b3ffe8", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-84f477cf7857413999e2ce1f46e382cf", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-6492c6abc30a4dc68efe4272590fa85d", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-780616a33f9346b4b00822dd6ca0659e", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-e2e8aef99fb840d481f478f4a57715e4", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-afe01bfbc68b40e7a04364ed3cd271ff", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-84501df581bd4f4ba53a449b897221da", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-88b70dd162f34032ad50d55bf1ac319a", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-0474e7b88f754b53a162d87bbfd7d7b5", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-7b5bcc47d2814617a06317b162a9ccfb", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-babcadff18924c6aa0f0872bf3417c06", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-e428ffdaf8ad4e27b852dcb78c4157c6", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-0fbc3fb2def742c8817056599623a14c", "references": ["Neutral"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-54093d0c6d144587a448bb31cc5ed4e9", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-409f6d10212e4217b479a030edfff922", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-9bfadc1ac3e645d19544063f5e38cae4", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-8bd90a568cbc4c8c8d54cffb91bdffd4", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-db49298fb4bd4c12aaf6f16c8ff6ba0e", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-3aa2ffad1e314cde908ea4e51f108e09", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-0c9b9aadeffe4c1fb3752e6eb82c5f77", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-c6b82a4c9feb45dbad6793815a2e8b57", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-68bddaa1608c4d07ace7e362d0ad440c", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-e84104b69b7b4a7d940749565e391508", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-488af08a6e5d41f18ddf7140a9e85f75", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-3049397569aa43939f859997adf45511", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-ea64866f3bf64d2f8a1ae4d5173f6397", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-bb302c034f304051ba1d21433809ed8b", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-ab416cae93e24ebcb7b1927b3928a8e1", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-a40698bc75fe4f20af050212a1895686", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-52129d1ca0504393981ddcad2b2af6b5", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-618af3c2260842e99468c59f422ca830", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-04b55d6cee434195a20f6a7b570d9090", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-acfaf5f755514768ad8fb8fafe03d3fe", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-a137d9e481b84ff8934c6c541ec47aa3", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-b933bc7da1864f7f8f03da1577b7d999", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-c89d75eef4744cf4aa9d4fbf7bc8649b", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-3de4ce939b5444db845039662ba938a7", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-760d3d64b887435286c3bbb192e30516", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-038deea895bd4586acc0c6aca29fd69f", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-4d6c3f1d841a40e7b665eec6545a3939", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-87745458acf748e7ad7ec9ec8aac179f", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-a4d5ee5599f94f41a5411d4ef4885b4c", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-958545c4f6b9481fbb9f9b058c709e68", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-8411b25f1f8a42aa8104b40717078218", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-30007a98f4f24fc4ae23e53ab89dc7a5", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-9ed9b6e5f4a4445c8306637141ae249a", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-4ad844a79abc4abcbc2e4d74e685b866", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-7c01549d0af64fa5a888b7ae2247422f", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-2e3c2e025c4d4779b508979ca23e50fb", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-b8be66de79074c68badc0dd0699e708b", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-c3222b4d56794727b4d7d4ecc6a18976", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-7e5cf952235f4720975749762a371b0a", "references": ["Contradiction"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1388-a73caada952643328477f2fa7b47473d", "references": ["Entailment"], "task_id": "task1388_cb_entailment", "task_category": "Textual Entailment", "track": "default"}
{"id": "task671-8786848e04cf42c1a39efabff9f6e182", "references": ["When did the Simpsons first air on television as an animated short on the Tracey Ullman Show? \n April 19, 1987"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-655df7ecdeca48b78fdf0e4ec4887510", "references": ["What is the legal age of marriage, without parental consent or other authorization, in all but two states in the usa? \n 18 years of age"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-9c4a72404c61434fb784d4640a27b697", "references": ["Who starred in barefoot in the park on broadway as Corie Bratter? \n Elizabeth Ashley"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-d14da11e2a21457db3cb699268dba30c", "references": ["Based on the initial thoughts of the project, when did the Manhattan project begin and end? \n Began 1939, end 1946"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-03d8f7b4d27c40269221130644939222", "references": ["When was the last time UGA won a national football championship? \n 1980"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-06e48961855c4f72a18e552eadab32d9", "references": ["Who sing play that funky music white boy in 1976? \n Rob Parissi"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-a11061116ec74fc683ccfbe0e5a1737f", "references": ["Consubstantial with the father in the creed means what in humanity? \n common humanity which is shared by all human persons."], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-61d01466f2c249069c8aa1b44d18d071", "references": ["Who was the voice of Kaa the snake in the 2016 film The Jungle Book? \n Scarlett Johansson"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-f5b071ddeec84e288394eb8f986c6a54", "references": ["When did the han solo movie first premiered in Los Angeles? \n May 10, 2018"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-fa6383d9eb1a4560990e534457913e27", "references": ["What is the IATA airport code for Abu Dhabi International Airport? \n AUH"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-9e0e4995364749809d515806abfc5f32", "references": ["Who sings the original recording of \"You Don't Mess Around with Jim\"? \n James Joseph Croce"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-ade5de4bae0f451d934fb52118e3a54b", "references": ["How many ligue 1 titles does psg have as of 2017? \n 6"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-0e8fae234ba94d9ea0c57b254281ee59", "references": ["As of 2015, when is the next time Easter falls on April fools day? \n 2018"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-a6d13c83e51b40d8b26daf639cdd2d6a", "references": ["How many seasons are there of Star Wars: The Clone Wars (2008)? \n 7"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-f8d1b545de3f4592a16da73b74b216b2", "references": ["When did construction start on the national World War II memorial? \n September, 2001"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-d167f6d3a3f04a9f9d800fc5aab92d84", "references": ["With what group are the german die br\u00fccke artists associated? \n the Fauves"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-7fa35c1d80c94845ba6420f4e649176b", "references": ["What is the scientific name for all red foxes? \n Vulpes vulpes"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-64386a5fb53b488e9a067b8de89fe99f", "references": ["Who is the \"father of accounting\"? \n Fra Luca Bartolomeo de Pacioli"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-7a248d48d66d402e9af0b6385988c8c4", "references": ["How many jury members deliberate in a criminal trial for serious felonies in most of US? \n usually 12"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-da6d2e140a434e119892c80d884c9d8f", "references": ["Which team has conceded the most goals in the premiership? \n Tottenham Hotspur"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-e063a0a900914bb38b38d56fd5befbd6", "references": ["Which country did the 60's scoop take place? \n Canada"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-c424f91a0728404b859abcf5658cb9c2", "references": ["What Englishman invented the process to remove impurities by blasts of cold air blown through heated iron? \n Henry Bessemer"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-e9ffa5f6a3824df39843f68d500be329", "references": ["Who played lionel in all in the family in two unaired pilots? \n D'Urville Martin"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-e8c5f792dbd74723b91be24fad11360f", "references": ["What setting did the Just do it saying come from? \n The Wieden+Kennedy advertising agency"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-fd2885343d5e4263949abd45f400e2b2", "references": ["When did Brazil bid for its first World Cup? \n 1946 FIFA Congress"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-2c9d1e4c496243a0969b181465922e19", "references": ["What ideas were used to justify u.s. foreign policy during the cold war era? \n deterrence and containment"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-b52bad5d3890413d8a129bdcaef0ebda", "references": ["When did The Sims Mobile release in Brazil? \n May 9, 2017"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-95beb661aa484facb8dbc74eae089a26", "references": ["Who is playing in the peach bowl in 2016? \n Washington"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-d3af5c0e29494a36abe7144467534604", "references": ["When did ireland rugby first win the grand slam? \n 29 March 1948"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-dbc79c67ee0b487cac088a500311ca0e", "references": ["When was the second time eagles were in the superbowl? \n Super Bowl XXXIX"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-950476a8863f40c5a57b5a4ecd234356", "references": ["Who was the first captain of the Indian men's cricket team? \n CK Nayudu"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-371309c0c66a44978696de7c757ad217", "references": ["In what book are the ten commandments first mentioned in the Bible? \n Exodus"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-22535aac60b94a30979d5fe331975a61", "references": ["Who became the minister of agriculture in south africa in 2014? \n Senzeni Zokwana"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-48205dae4a3a40dfa54848760f621921", "references": ["What day is the new star wars movie releasing in 2017? \n December 15, 2017"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-dcfe234392f1476eaa06e4890a11d883", "references": ["When did the original lg g6 phone come out? \n February 26, 2017"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-d9b5aa319ad745c69e2b8347af1c6892", "references": ["Who is singing as Tina Turner in what's love got to do with it movie? \n Anna Mae Bullock"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-96b9741ec83843c08db3fa03d393101a", "references": ["When was the last season the jets won a superbowl? \n 1968 New York Jets season"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-71b8440af2744db9a9ab503b834384d5", "references": ["Who acts and does the voice of cortana in the 2021 halo tv series? \n Natascha McElhone"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-b2ace7ed1a78462f8e5c1a37972da0ca", "references": ["Who plays Obi Wan in Star Wars Episode 3? \n Ewan McGregor"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-123c79abedec404d989bb28d085d2975", "references": ["What are the names of the three prequel hobbit movies? \n An Unexpected Journey, The Desolation of Smaug, and The Battle of the Five Armies"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-c93e364ac83b4f08aca9f7ce850b0657", "references": ["Where do historians believe the black death originated in animal? \n ground rodents, including marmots"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-c67c41cba3e247fd8b12ce8056cf3e18", "references": ["How do they test for most drugs at the Olympics? \n urine testing"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-7ec64d0784a640f1bba6ccd254accbe4", "references": ["Who is the secretary of state in arkansas from 2011-2019? \n Mark Russell Martin"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-c443b946b54040119324257e70e4a081", "references": ["Who wrote the music scores for shrek the musical? \n Jeanine Tesori"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-6adab9fb2d524fc3b0ed1754e9a65ddc", "references": ["What is the length of a California King Matress? \n 212 centimeters"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-24f4abf1681b446abb21efafa2d5d5cd", "references": ["What is the largest province in sri lanka by area? \n North Central"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-19054082c9af498f8748c3bc446f8e00", "references": ["Who plays the Man in the Yellow Hat on the TV series Curious George? \n Jeff Bennett"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-42a67a1c354b4f7f91f5f8f68dc68113", "references": ["Who played the original Amy Pond in Doctor Who? \n Karen Gillan"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-1108e1fdae0b4c93aa59a95c98bfb3ac", "references": ["Who sings lead vocals for you make me feel like dancing, released in 1976? \n Gerard Hugh Sayer"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-c2be5ea5236d41a1a67f761ab2105074", "references": ["What is the legal private drinking age in Russia? \n none"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-cf65bb72b8154fad847563db88c2373e", "references": ["How old do you have to be to get a tattoo in Indiana without parental consent? \n 18"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-fbcff687d47142fc9e9d8200cd7774e1", "references": ["What is the name of the princess in Frozen, who eventually becomes queen? \n Elsa"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-70e6950da2c94ddca43ebdb0685a271b", "references": ["When did the land that is California become part of the united States? \n 1848"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-8e0a97d124e944a5a28dc5448d089ba0", "references": ["Who sings bet on it in the high school musical 2 film? \n Zachary David Alexander Efron"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-e62d33f582774286b59e8f9ddb41c0dd", "references": ["When did ariana grandes new compilation album come out? \n September 27, 2017"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-0827303950bb4c78b9f52c3a550907ff", "references": ["When did the edwardian era start? \n 22 January 1901"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-c408c09577074dc88a83d84742f84102", "references": ["Which dog plays Marley in Marley and Me? \n 22 different dogs"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-7177b22d4e2e4db983b7aa9f3ecc3b03", "references": ["On what geographical features is mass wasting most likely to occur? \n terrestrial slopes"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-f7c85755c3094bb3b9478423f8196e40", "references": ["Who wrote the music for the 2011 conan the barbarian film? \n Thomas Kloss"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-cbc6314a88bb4fa4bcb076cebefd9920", "references": ["What type of book is the fault in our stars when describing intended age group? \n Young adult novel"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-e57ef62ea85e4198b854f74f8666ccb5", "references": ["When did an old age pension start in parts of australia? \n 1900"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-44d4b303b9e24e3c801b5763ecb400d1", "references": ["What is the name of the plant in the 1960 film The Little Shop of Horrors? \n Audrey Jr."], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-aa8b19659e5d4d7eb6d3fcd34345f58d", "references": ["What is the brightest star in the night sky seen from Earth? \n Sirius"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-33d6dfb97812408d8616bf0100b6bf4e", "references": ["What kind of ships did the sea dogs have that led to their defeat of the spanish armada? \n smaller, faster, and more man\u0153uvrable"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-fdb36e00154f4077a0bdca8c15d9e3ed", "references": ["At what point was the forbidden city opened to the public? \n when it was transformed into the Palace Museum"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-6ef86e9a0cb7403897cd0f960033a9c5", "references": ["When was the first larger mattresses that were later standardized as king size beds made? \n mid-1940s"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-d9558f0592654714846ee7012a40ca65", "references": ["What was the cost of the prototype of the Airbus A380? \n \u20ac9.5 billion ($10.7 billion)"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-275a890e14b94ca2b54f20484ba7b376", "references": ["When did rolls royce start making jet engines for World War II? \n 1940"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-c6f23ce5ca70408a9dd7efc0cb448228", "references": ["When did i can't get no satisfaction come out in the US? \n 6 June 1965"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-f5860cc798fa4e3a95861ecbe716b64b", "references": ["Who scored a hat trick in a FIFA men's world cup final? \n Ademir and Geoff Hurst"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-b9dba81dcc23410dbd1650176e89a334", "references": ["Highest paid hollywood actor for a single movie, who deferred salary against a film's gross? \n Bruce Willis, Tom Cruise, and Will Smith"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-f8fbf326f9a2491da0fd75f6be841cbe", "references": ["List of top 10 largest countries in africa by area? \n Algeria, Democratic Republic of the Congo, Sudan, Libya, Chad, Niger, Angola, Mali, South Africa, Ethiopia"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-90ed08a16cfd4cfb9bcdb658e166f062", "references": ["Who sings the only Fools and Horses opening theme? \n John Sullivan"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-3a1911ea10e64fbca0f0db601c21a5de", "references": ["What book does the saying all quiet on the western front come from? \n All Quiet on the Western Front"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-447efbbf990d4d18a01a58bac45a78be", "references": ["When is the sequel for Batman: The Telltale Series coming out? \n August 8, 2017"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-129d743259684f3abb4a60331e936116", "references": ["Who is Ryan's brother in the OC? \n Trey Atwood"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-2a7dc8b68eee450cb2a74bd8c49eb622", "references": ["What was the population of Rochester, New York in 2010? \n 210,565"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-24e0d0c54cb9428b80fbb4ab78030356", "references": ["When did Republic of China become a member of the united nations? \n 24 October 1945"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-75af81d0aa9443df8963a2a4957a4805", "references": ["What character cut down the trees in The Lorax? \n The Once-ler"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-cc6bc833090449279a02798326962bc2", "references": ["In what season of Murder She Wrote does Jessica Fletcher move to New York? \n Season 8, episode 1"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-44f06d15140b40efb3e828a9926c5b9b", "references": ["Who holds the record for cycling from lands end to john o'groats? \n Andy Wilkinson"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-e52c01a0a0ef43e985de09dc6e3444da", "references": ["Who were the declared Republican candidates in the primary for attorney general in Florida, 2018? \n Ashley Moody, Frank White"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-9ef62fdf06b645a3bfddb4feb8f4d598", "references": ["When did university of georgia start playing intercollegiate  football? \n 1892"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-f2c2b30d0fe64a659566ca7f94bed8d9", "references": ["Where were quite a few scenes for the movie charlie st. cloud filmed? \n Gibsons, British Columbia"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-95f551c068564a2f94f23b0441f0d902", "references": ["Who did the Philadelphia Eagles play in the NFC championship in 2001? \n St. Louis Rams"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-b7278a8165294b0d85a3a93a22de1622", "references": ["Where does the cumberland river begin? \n three separate forks that begin in Kentucky"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-97eae61ff50c419aacfc0f7c29e8d97d", "references": ["What are the measurements of a standard full mattress in inches? \n 53.5 \u00d7 74.5"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-27c7c0c0918445199f97742524d37468", "references": ["When was colour tv first showcased in the uk? \n 1961"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-c7aecf46f53e40f192028f6aad44329f", "references": ["When did the packers first play at camp randall? \n 1986"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-2147c8c107ed48df9df23675d2ed6ff4", "references": ["Who has the most passing touchdowns in a career in the  regular season in the NFL? \n Drew Brees"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-dda5f0445e304073b3911a408943d295", "references": ["When does episode 24 of the 2016 berserk series come out? \n June 23, 2017"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-0a1cb8b4d9c54a88b673600eb37ff05f", "references": ["Who actually appoints the member sof state human rights commission in india? \n President of India"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-e4cdbffaff404c33a1ebcf75c376389b", "references": ["Who played the dresser in the animated film Beauty and the Beast? \n Jo Anne Worley"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-56e32b52a50f426bb9089ca8aff3ef6a", "references": ["Who was the majority leader of the senate in 2018? \n Mitch McConnell"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-c3f51dd3f3604ecab63ecdb2a1655967", "references": ["What number season 5 Full house episode was michelle's first day of kindergarten? \n 1"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-a145c77c67794277a9eaf71f97e88660", "references": ["Who played Oscar in the 1970 TV series The Odd Couple? \n Jack Klugman"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-8acb340b92db40f0865f0656d01185d5", "references": ["Who sings nine songs in the movie Walk the Line? \n Joaquin Phoenix"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-c8564b224615413486e62be8fe0d1e7b", "references": ["Who has won the most tennis matches in history as male? \n James Scott Connors"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-a451f528a6974c71846c5f1ea5f67264", "references": ["Original singer of the chorus for rock me mama like a wagon wheel? \n Bob Dylan"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task671-7145ea00c3af493b872542a2a2149f21", "references": ["When is the season 1 of telltale walking dead coming out? \n April 24, 2012"], "task_id": "task671_ambigqa_text_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-eaf1a8010cf24fbfadd22cf4eda44524", "references": ["Are bear sightings common at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-73f4c13f8f544a9ea37f916318c63a3e", "references": ["Are both the average litter size and life expectancy of this dog breed greater than 7?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-280840d79e8e473baee1dee1b680cc37", "references": ["Are fires ever restricted at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-cce43d61badb4fc5a2fdf92530fd69d0", "references": ["Are the bears active at this national park year round?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-a5ce4c14039c4aed8420f6efe919bf0c", "references": ["Are the docked tails of this dog breed longer than an inch?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-1aa6107591734400988828b055113e92", "references": ["Are there brown bears near a waterfall at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-17182ab7f0d74b7e950fb0122bb6274c", "references": ["Are there more than two places to eat in this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-70266afbdd824cff814ba4f4b6a65e5d", "references": ["Are there places to launch a boat on a lake at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-e5241641272b4ec68b533ca6871a3a5a", "references": ["Are there shops at this national park that sell fuel?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-97f7797d0a644bfb9f011e53071c752c", "references": ["Are there times when campsites are closed at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-af3f11ccd2044cf58cf96156a7c22e98", "references": ["Are there trees whose leaves stay year round at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-843883801ccb4da6bce843c7c436449e", "references": ["Can i buy food or go boating in this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-9fb999966f1046a0898c320b75300065", "references": ["Can i drive in this national park and see mammals?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-212f1159a8b34611b8e5296e0146ca24", "references": ["Can i use or rent a boat to visit the waterfalls in this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-9908bb6519144c60a7b9bce679fc3670", "references": ["Can the long fur of this dog breed be more than one color?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-5daa4486b10c45a181c0691b59a81c9a", "references": ["Can this dog breed be a white dog that has problems with entropion?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-868517cceacd4a31b2134a81291cd899", "references": ["Can this dog breed be white, black, or brown?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-f483d8a13a99488eb2108e428c1de0ba", "references": ["Can this dog breed have a white coat when domesticated?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-230fb87b10be429ea36cbed16f2bdc05", "references": ["Can this dog breed have only one color fur or black or brown spots?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-52f82c7933554505bafb16194e5a9d6f", "references": ["Can you boat and grill at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-c50174c5e7824a3b90dac9de54537c95", "references": ["Can you go boating at this national park all year?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-6887f42733cc49ab93685b33f9c1584c", "references": ["Can you go spelunking in more than one cave at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-2de991b7dd9a4259a2da306ab0a042aa", "references": ["Can you start fires in the summer at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-9fa630421b424082903dd65588c21dde", "references": ["Did stock market performance increase after this president was sworn in?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-8c30de5eb9bb480ea0057ff45519958f", "references": ["Did this president ever have an approval rating over 40 percent?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-437e9583410e42e3be02525be107c955", "references": ["Did this president ever lose an election after being vice president?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-2c538ce9428947b182bdee18501c630b", "references": ["Did this president go to college in the state he was born in?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-96ccd624eb3f4fb28fe426d106dd3d2c", "references": ["Did this president grow up in the united states?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-a0e36bc3c7e946bc88ab454794aa4c23", "references": ["Did this president have children with their first wife?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-d5eaf47878fe49b398b760a5bf91619c", "references": ["Did this president increase the budget deficit to help the economy with stimulus bills?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-2a8f63fb95404763b791502c808a4e9c", "references": ["Did this president lose his home state in an election?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-82fc4b4500494420808a971eb197aeea", "references": ["Did this president major in economics or business?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-592e963ca58341d98362f416a4ac0629", "references": ["Did this president sign any health reform legislation or see a stock market increase during his presidency?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-ee854dc5a1a8447f9756aca709e289e9", "references": ["Do the common diseases for this dog breed affect average life expectancy?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-429b4398fb7f413ea61f1e85cbc4c451", "references": ["Do this dog breed get large enough to be used as a watch dog for farm animals?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-60b06333b24c4d0b9136ab3dee85bc3f", "references": ["Does this dog breed always have black or brown spots?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-b42ae47f4b894d13886628ac7c64bc49", "references": ["Does this dog breed have long and straight fur that is larger than an inch?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-40b5850f1580473a88e6d1d4a102fd46", "references": ["Does this dog breed have long hair or bark a lot?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-17d1cbd4b92b474495f896166a87d92f", "references": ["Does this dog breed have long hair suitable for colder climates?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-5868e47cd4e5408caf2b4762938aa90b", "references": ["Does this dog breed have spots of two different colors?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-21bb789a638a4490989e207ab105d3bf", "references": ["Does this dog breed prefer the cold or get along with other dogs?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-7a86003d907d470d80ec6962c779d07e", "references": ["Does this dog breed shed frequently or have spots on their coat?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-3ba1532e9f5b4400b09613c75fd52b89", "references": ["Have wars caused this dog breed to almost become extinct?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-1a06908cd9cc43d88ffb36f3066f34b1", "references": ["How did the economy perform as this president took office?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-2cb4a7c9047d4dacadfa0c2e7db6aad4", "references": ["How far away should you stay from glaciers while kayaking in this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-34620d47189f4a56b0c060c7ac5940fd", "references": ["How long should the tails of this dog breed be naturally or when docked?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-c99e7e60ecfc4ba58660dbe022e7e6ea", "references": ["How many plants living inside this national park are endangered?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-e5b41217c11446298ac042ce45f5c9bc", "references": ["Is 25 pounds within the breed standards for the weight of this dog breed?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-9ffa40edc9424ca4bd891276d3fd2caf", "references": ["Is it acceptable for this dog breed to have white spots?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-aa51fcf0b1474e8cbf5a064b68d49fcb", "references": ["Is river kayaking or canoeing permitted at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-fed553f892f7483784efc072f0c071fb", "references": ["Is there a restaurant to eat at or a hotel you can sleep in at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-df6f17f55f254ae4b8728ba42206ec15", "references": ["Is this dog breed a friendly dog who weighs more than 25 lbs?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-90d49aa62b1945f79d87c47d01212e4f", "references": ["Is this dog breed a large dog that has ownership restrictions in some countries?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-e6245ae4a04e4584b2d512992aa9beb5", "references": ["Is this dog breed a large dog that is playful with other dogs?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-e5aa918325fe49369a03633050f29417", "references": ["Is this dog breed aggressive around other dogs?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-ffa560a8bad2403bb5b639a610dc42cc", "references": ["Is this dog breed an american breed used as a watch dog?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-70f6ce809e9b43c49bdb7188b8c0f9f4", "references": ["Is this dog breed usually covered with white and at least one other color?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-c08af1c2eafc46bd905630b808e7254f", "references": ["Is this dog breed usually used as a watch dog and are they usually more than 25 pounds as adults?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-607a676440ac463e8e58d18c153889ca", "references": ["Is this president ranked in the top half of all presidents despite having previously lost an election?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-2234a577accd483fbf403d148b975be9", "references": ["Is white a typical coat color of this dog breed?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-05f0f85e86e34378a6b90384397d2e3e", "references": ["Is white the only acceptable color for this dog breed?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-01bcd010024a48a3b35ce59886e24207", "references": ["On what day of the month was this president sworn in?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-5a846a29427f46e7a8fba8fbec8aa01c", "references": ["Was this president a democrat who increased the budget deficit?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-2898f41bf6a147128f4883e280880a53", "references": ["Was this president a democrat who supported abortion rights?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-a0c76ba351b247a084e1774aa109a509", "references": ["Was this president a governor and a vice president?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-c0b655d1af2a4eee9b3226b6175f1b11", "references": ["Was this president ever married to someone born in the united states?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-f04c74091f5b4196a80e50717c7c1b2d", "references": ["Were both the mother and father of this president born in the united states?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-8ccdbbd0d9124c8ab486695a423361c8", "references": ["What american movies with 'dog' in the title has this dog breed appeared in?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-23a1fad651bd456b887632b0dad7b44e", "references": ["What animals at this national park come near your campsite?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-48e1804cbb444449a4aefabf25832231", "references": ["What are place that tourists eat in this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-d9e331d655fe4b749932def1f2ef8044", "references": ["What are the first names of the pets or children of this president?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-48d453f2d75148e18f7d3e5954e05101", "references": ["What are the medical names for diseases this dog breed gets that are not entropion?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-daf0ff308b8b4c528c0d7ab43cb43bd5", "references": ["What are the names of famous trees in this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-7f496a8fe142413f9b8b8c50b24932a3", "references": ["What are the names of the children of this president?", "What are the first names of the daughters of this president?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-b94b896be42d4863bb5d4723e8aae2c3", "references": ["What are the popular activities to do in the rivers at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-c8c92f55253a412196da3b119421bafd", "references": ["What are the typical coat colors for this dog breed and what types of coat get them disqualified from competition?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-c341ffca64224429ac6e6725cb8fa23d", "references": ["What attractions at this national park can you have a summer tour of?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-6f2e1f1b60e6481eba6570107ef45c4e", "references": ["What birds can you see when you are bird watching at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-ac09dcd6ceb7422292082fd383ebd3e3", "references": ["What birds nest in trees at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-98afb3d18fae4c2ebf934fdba585674e", "references": ["What color and tail length meet the breed standards for this dog breed?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-de27d19457bb412f88f708b47760accf", "references": ["What colors is the tail of this dog breed?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-835344a0fafa4f489f195195f5c981ef", "references": ["What did this president study in college and what degree did they obtain?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-5283d91bfbcf47bdb9b4c31958da3667", "references": ["What hiking trails go to waterfalls at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-7aadfba25edb48899ccc50c7e9afdca1", "references": ["What is the average full grown weight and life expectancy for this dog breed?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-23b134386ebf4976a103992174622b27", "references": ["What is the first name of the father of this president?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-d6d08e37ed1d4655b5cb0871bc82f5af", "references": ["What is the first name of the mother of this president?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-c80586758dfb4ebf93c299e2e2805e92", "references": ["What is the maiden name of the wife of this president?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-3794b30c69424666b4d4f63bb9153c09", "references": ["What kinds of bears and endangered animals live inside this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-16a36b234f9947e9810257474e040cf7", "references": ["What lakes are not natural in this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-2c3cd7478d824c4f8d6cb30b59afeed0", "references": ["What lakes are the most popular at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-5df6e4a780af4a6db01960519313e108", "references": ["What lakes have bird watching spots in this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-e3acc7debe8648aa8233ce0c0bbb4ab0", "references": ["What major river flows through the park and creates waterfalls at this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-2cf3b35ddf48451999f8aed447a7c98a", "references": ["What names for this dog breed come from their original country?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-094b601bd2b64923ae2b6ea55fbd31d9", "references": ["What party did this president belong to and who did they defeat in the election?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-3b9ac0ea6eae4757b1f91a26a5eb4a64", "references": ["What party was the vice president of this president?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-1bd9cceaff0d4685852d30ff17959145", "references": ["What states did this president live in before he died?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-d89b85e478734df481e11e6de3f0e61a", "references": ["What time of year is best to see the popular waterfalls in this national park?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-23d886f64db14c6f838b04642058474b", "references": ["What was the maiden name of the mother of this president and what state did she raise him in?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-0b4f476d802c45b49e1eed3b6a53a508", "references": ["What were the approval ratings of this president the year he left office?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task121-925e99d1c19d460c8fd2df05e183d8d2", "references": ["What were the budget deficits any year that this president was in office?"], "task_id": "task121_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-21643870cf4f4a5a8ef3724cfe108e71", "references": ["Which level of prepration is enough for the exam jlpt5?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-cfa11486417d412580444df546f3da8d", "references": ["What can cause stool to come out as little balls?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-bafab96810c045e5881438f738891379", "references": ["Would a second airport in Sydney, Australia be needed if a high-speed rail link was created between Melbourne and Sydney?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-87e4653844294415b0b7062b372cec26", "references": ["I don't beleive I am bulimic, but I force throw up atleast once a day after I eat something and feel guilty. Should I tell somebody, and if so who?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-d09cfc736bcc4d2a80d8e94fc074f898", "references": ["How do you become an air traffic controller?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-7e4cd11c392449758c21a7357320d473", "references": ["University of the Philippines: If I take a second BFA in the UP College of Fine Arts, can I be exempted from gen. ed. or core subjects?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-fa52c5441c1d41cea07cd38602ce55e1", "references": ["How can I move to Jamaica?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-2b4726a476104dd0b97e047961a6c26c", "references": ["What is the county of Edgware and how does the lifestyle compare to the London Borough of Enfield?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-1aec3eeae20e40ecafde4234d41f8eaf", "references": ["What is a qualified SAP ERP key user?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-58298bbdfccf49b594962bbf46bfd69c", "references": ["Why do a lot of theists and agnostics confuse mainstream atheistic thought with \"positive atheism\"?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-df767da43f934e7ba843b482e40cfb8d", "references": ["How can I go to Disneyland with little money?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-61330fd19984444da0fc1006ee52295f", "references": ["How can I contact Donald Knuth?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-6a46db49367e451aa5b0d774436f62dd", "references": ["If a die is rolled. what is the probability that the number on top is a 3?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-49d13c0ec3044c0aadc68b99556737a5", "references": ["What is Morse code?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-37618b45753b4674baafa66a5c81a2bd", "references": ["What is the best backend for my app?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-6d3c1f39614e4597893297c74ae8bd25", "references": ["What type of government does France currently have and how has it benefited the country?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-3da463def78d4e1087d980de867aa6aa", "references": ["What are some creative ideas for arranging a freshers' party?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-80fae1dabccb408cba8c4ea2fd4ff213", "references": ["Why are the people on Staten Island are racist?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-3dda8917a5d64140a7bf4f1bd0369bfb", "references": ["What are some good characteristics of the American culture?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-a033347fb9a046aa90729cafb6fce6f8", "references": ["Why are the HTTPS sites not working?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-6ee92206dcb942308e641b14c7fa71ae", "references": ["Has Ancient History been scientifically tested? Is it all real? Did it happen differently than we were told it did? Did it even happen at all?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-2ca8293fc79644ed811ecd82bf1f18d7", "references": ["Who is won indutal medal?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-6dbae44600264fa28ef4314345beeb32", "references": ["Which are the best recruiters for technology executives in the san diego Area?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-417e37d02fea486b9c85c0d54f37dda7", "references": ["What is the best answer for 'Hmmm'?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-491088671ac7454982586b55c5f4d145", "references": ["I got selected in Infosys via campus placement in September 2015 and received my letter of intent in June 2016. When can I expect the offer letter?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-b1b5c19691844d778c563a6af43a8c40", "references": ["There is a good looking guy that acts like he is by boyfriend and that we have a thing. What does it mean?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-3f48f80dedd6458887562479bc7fdfc7", "references": ["What would happen if ants disappeared from the Earth?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-520dc070cfac4af1a3e7b3a99a365486", "references": ["Who are some artists with interesting or inspiring childhoods?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-48816f0d68184d2da78a98108bee48e9", "references": ["Does the end justify the means?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-cb41d6046b334fff81336c7986626204", "references": ["Will Narendra Modi win 400+ seat in L S 2019 ?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-b92e83b4fe4742e59957a0b30231630c", "references": ["What is the lead time for SSN4EGS411 board?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-404141bdf75d43ea93787777a25ecf71", "references": ["Do any popular Quorans gain financially through Quora?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-907708b480cd4d51ae6bcc87f11335b7", "references": ["What is the length of rebars on beams between slabs?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-70f242d5a8dc49498651a22ce5c361e9", "references": ["What types of government did Aristotle want?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-d1c52cc3c92a45a1a9dc8ca6c6560194", "references": ["What is the happiest thing about you? What is the saddest thing about you?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-e66f475883054b14980c9e945b2c028d", "references": ["Are there any parallel universes?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-9dd92e0af7604f74afe312a8411c50e1", "references": ["What is the difference between a virtual circuit and a circuit switch?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-38182d4582a84b8d9b11f0a9e89b2088", "references": ["What are some things people believe about Ireland but are laughably far from the truth?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-03ca0c73f99d4929b9bd8ec25640caf8", "references": ["What are the different types of nuclear families? How do they all differ?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-e202efe8143448e38d765bf193cbeef0", "references": ["What is Nuru massage?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-d02ec5202ce84404879533479a6d46d8", "references": ["I created a Telegram group but I could not find the option to Add Admins. Why?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-c92439ad4ab6415cb962472f374ddf34", "references": ["How do you reset your Yahoo! password?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-edaa86a0036a4394b780c96f0c7a2dae", "references": ["Do Tamils usually watch Malayalam movies?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-d17816f0fe5a4a5492534bcbda2ba135", "references": ["Why did you lose your virginity?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-cd144e0bfc58470d8b37bc8db97b5ea1", "references": ["What is Donald Trump's IQ?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-6249a33cf522475c85a5d45441f20598", "references": ["How did most creatures develop noses? What was the starting point?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-24f9e904da2a4c0b88185ce23fc86365", "references": ["Where do you want to spend your last days of life?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-aa2224d442874ab993637cac1f1cc7bf", "references": ["Which technology will win the OLED vs LCD battle?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-ec27561802924314b3a4c4bd99afbd1f", "references": ["How do I accept that I will always be alone?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-f61af2d3ef3241359c28ea2615b887e4", "references": ["What are some examples of typical bacteria?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-740c14a386ca4d0189102bd337f5c34d", "references": ["Do employees at Pennsylvania REIT have a good work-life balance? Does this differ across positions and departments?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-0ff36c0fbbe04eeaa141ad04af9af25a", "references": ["Which is better Honda City or Maruti Ciaz?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-8c18705287a44db580aeec67ea7173ab", "references": ["Can dreams come true?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-762e29bdc5464198999305a99138b365", "references": ["What are the pros and cons of GitHub versus Bitbucket?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-fbec19440e6f44c6af9c5f2b7d2f0fc4", "references": ["What are startups?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-0f0d476d02fb4223ae3f859ecafa7e6d", "references": ["A polynomial leaves remainder [math]2[/math] when divided by [math]x-1[/math] and remainder [math]1[/math] when divided by [math]x-2[/math]. If the polynomial is divided by [math](x-1)(x-2)[/math], then what would be the remainder?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-7a4a154059a9436499efb28ad57aa136", "references": ["Can gun control prevent a robbry?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-d0b1f5ebc62a4de59f88fc01a23d64e4", "references": ["What country has the most attractive women -- either in absolute terms or in density?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-06132aca152a40d88ddd21431bca49c2", "references": ["What is your review of www.buttermyresume.com?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-160e0700ebc64336ba4701667b76b609", "references": ["How do I install Windows 10 on new Hard drive?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-0931b1915b994c01a54722498770c863", "references": ["Did Hitler underestimate the jewish mafia-nation?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-6a003b4342484da694d14da5f86ccd87", "references": ["Why are Laxmi, Saraswati, and Ganesha depicted together?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-5eba960f79db4ad5a81aaea59c873921", "references": ["What is the perception of Mikhail Gorbachev among Russians today?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-99bbcee074be47a9a4c32e2003086483", "references": ["What is your review of South Indians?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-e3cfd04e139345768e339a22fe09fd2d", "references": ["If I liked Skyrim, what other games would I like?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-19950c4a0edd47429af95a64c7f05c8f", "references": ["What is the best way to meet new people?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-b50618c1bfb74b649fcf19ad0bdddccb", "references": ["What is it like to live in the Chinatown or Nob Hill areas of San Francisco?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-93ae073d7515402b943e094cec96cd65", "references": ["What is Barack Obama doing now?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-5569d80b1a5f480c9723e6382d667c03", "references": ["What is the business model for wooroll.com?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-1e243738999e448d95041a3572a1fff1", "references": ["How do you get a job with a criminal record?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-f318b8624e41419ea93cf486ac9c955d", "references": ["What is the best way to meet women?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-999189b3bce44703b40caa06e1337d85", "references": ["What is the first reaction of a girl when a boy proposes?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-a0b68ccdf59449a38545e0524caf4d6f", "references": ["What is the difference between Javascript, JSP, Node.js and EJS? From where should I begin?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-5cac7a5cfba94a56b4bbf5c539aaf736", "references": ["Why is pewter so expensive, and how does its properties compare to those of aluminum?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-d19cbe257f95484ea40a4189ed4a0725", "references": ["What are the best gifts ideas for sisters to give on this Raksha Bandhan?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-5aa61608a151403baf5277e69e687600", "references": ["What will be my in Hand salary if I have?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-3864c760f9934e9586f0458f615e33b0", "references": ["What steps can be taken by Indian Ministry of Tourism to improve foreign visitors coming to our country?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-769794677e8d4643b39899d8451f077e", "references": ["What companies are similar to IDEO?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-4f05a66fe894439cb11d4ec8d5944902", "references": ["Which are the most modern courses in engineering?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-9aaea7487a4d4777880945187c3e525b", "references": ["Is nearbuy shut down?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-168c35ccb97b495e84855317573333c4", "references": ["How do foreigners who have settled in India feel about India?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-3d2765fa122d474f83df06077f52bd54", "references": ["How do I recover my Gmail password when I don't remember my recovery mail ID?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-b76aa517603548dca615f06a2764aee4", "references": ["What is an explanation of G\u00f6del's Incompleteness Theorems suitable for a 10 year-old?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-23ebfa4dbfe44d5f986d7fcb68b8d9db", "references": ["Is it not normal to go out with friends?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-726032e56e04490997643f47804aa406", "references": ["What is your thoughts on the theory that all of us living, is just the Earth being aware of itself?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-bbde635925e847bcabd1be5dc5ead2d5", "references": ["Where can I find some cool home decor products in USA?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-85a26697e0284d008c60cba722d2819f", "references": ["How is BIM trichy as compared to IIM Kashipur?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-4dbac55cd45246d0895859bc3c7e54bb", "references": ["What does it mean when I dream about somebody dying?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-01fcf9eb9a4e4a79a32d0018e7256408", "references": ["\u03bcTorrent: How does VPN keep someone's identity anonymous when downloading torrents?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-292cc9d52291435ab49bbdbd9aab0f73", "references": ["How do I know if I can trust my business partner?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-a6ddf15d8ebe405ca2423e5241bb9a56", "references": ["How are peanut butter and jelly sandwiches good for you?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-f7c9ef38d1314a7eb0f6f3c8c6810892", "references": ["What are the best stocks to invest in 2015 for the Indian market?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-d87f10d40ef847a488535f883315ee6c", "references": ["What are the different types of physics and what is and example of it in astronomy"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-6e04aa0a42874371b9f263b1af98c550", "references": ["What are the most important things you should teach a kid?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-3569fc274d5247d2a6d6c02bef608aa4", "references": ["What is the difference between England, Wales, Scotland, Ireland, Britain, Great Britain, United Kingdom, the British Islands and the British Isles?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-7f1190296eb94f588239dab39cd3ffd5", "references": ["Does Facebook charge content sites to display a \"Post to Facebook\" button on their pages?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-897f84502b41468b9577dbf8dbbb1c48", "references": ["I got a python Kerberos module installation error. How can I fix it?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-9f60574674e048cfb62653f102115b7c", "references": ["How would you explain your stance on the TPP to the Prime Minister of Singapore who is in the USA now?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-adde51a28e81470c9f53d758c54ebec7", "references": ["What is 45 converted to a fraction?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1345-79b2a8262a504426b79fbe85edd6eca3", "references": ["What is a DAG (Directed Acyclic Graph)?"], "task_id": "task1345_glue_qqp_question_paraprashing", "task_category": "Question Rewriting", "track": "default"}
{"id": "task330-e6a8ca791bc9490fac078d4bc7bc93d0", "references": ["Reucassel"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-5ce1742fdbb94d73967e6aca215e7e9e", "references": ["Beryl Markham"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-48c1dc71d6b34e0c9300682ae86cb854", "references": ["Jos* Alvarez"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-40d8e3e89c69488a99bf978ca30dcdec", "references": ["Faik Pasha"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-4cfa47ab2099416f86c61717e9672426", "references": ["Jake Burns"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-454e340035a246a3b84655abf7b02d75", "references": ["Cowan"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-395b880a9b3c4acdbc17ef7bb2ac367d", "references": ["Beverley Callard"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-0dfd67d2531a459b8762c595429f5ad9", "references": ["Kallergis"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-7e115984f1ca448f8ddf40983fd2888c", "references": ["Nicole"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-8accb603d0984730b503be276c357044", "references": ["Queen"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-95ae3770cf5c462aa2bbc385ddd1c6c5", "references": ["Michael Kidd"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-c3d09dddcb6d42ac9c69c4f4e8b1a156", "references": ["Herring"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-dc9685f59d6f4513adc34700d3b3342e", "references": ["Wright"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-9f6133bcd10149368e544ae8ed5141b1", "references": ["Robert Fripp"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-d8b9c761970640649898f4e1639c5734", "references": ["Lenin"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-14c3edce25144d56b8edb6d97b0545fa", "references": ["Andy"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-81c5b4e540854c65ad9e9247c684c295", "references": ["David W. Taylor"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-3308496383294dda807a0816526f21a0", "references": ["Joe Christmas"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-44c9a2c4db064603988c9d8935def770", "references": ["Hicks"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-da2a609ea5964ecdb855555d20eec9ac", "references": ["Bonavia"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-289af4f0b9494b42a5a0f217477454e7", "references": ["Marcia"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-64d9a82a31e649428d61fa4deabf470a", "references": ["Martin O*Malley"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-b7fe8fb9efb441199204b734aa3cc303", "references": ["Shahjahan"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-e5384f9e53874a0ba2bd6fd10cac6d62", "references": ["Sheikh Isa Qassim"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-1db07db9a3064bc1849344fb4e0ae6fd", "references": ["Wade"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-3013897c22cf466faec1969e6e474d3d", "references": ["Haqqani"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-7d50dad36e794a52bf1148c22763f174", "references": ["Wozniak"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-4e64281eb7b0442a8efa34e4ea654f84", "references": ["Thomas Coats"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-81ca79e9a8924379a8405227892a405d", "references": ["Mark Wright"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-30bb6657007447eeb73257e1f8ce0bc0", "references": ["Clarence Doust"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-d6619881887841b8991582ffcc71f90c", "references": ["Albert Blithe"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-d81e64a575a34d0aa8ccb8cb49ce286c", "references": ["Arun"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-c159f0e859c348fa94051db0163f7e43", "references": ["Novak"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-82d46755858f499095ab64cf295256c9", "references": ["Bawa Ardalan"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-b4cf01aa134948ce9446fe25256df203", "references": ["Hamza Aziz"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-65ab9c40593649c7b7ecb2cc998971ce", "references": ["Kelder"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-8ef11b777c554cd4b5acf706144f4521", "references": ["Paul"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-062d76a3a7c04e69a0caf66ba2e612bf", "references": ["George William"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-de16002a43664ff89b37ebee3b369def", "references": ["Sadiq Khan"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-2e58d9a60c904b21a6f9acc0f4c6be32", "references": ["Walter Freeman"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-0ecb648f9cb4418ba311b098469f3c17", "references": ["Beryl Agnes Farry"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-e0f5dcff810a40b8a4f57ac1834163ee", "references": ["Ilves"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-69a70b1f565841bbb2d58b41deabe1dd", "references": ["Lateesha"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-7a910140c3b249a8abc7685e8ec151b3", "references": ["Luigi Infantino"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-d83e3928663f4b9584924f94689a1328", "references": ["Elizabeth"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-2c6ff61452a647ff890222fa81b84e74", "references": ["Nash Turner"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-e9388bdd54604bbdaac229e464d82fb4", "references": ["Philbrook"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-ee9d8db1ae46401e99926add4abf8717", "references": ["Duke"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-e98c5b6485f64020a5023cbdad44570e", "references": ["Stepinac"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-c1c86be34cd94bf6be5b65af05492ae6", "references": ["Simon"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-d21ed172a6d0420d850b7a43494cf12f", "references": ["Kristy Puchko"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-bef381c729b6491389bb29f07c74d140", "references": ["Brian Bosworth"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-1fe8a2e056d844db96b3362f0df40d5a", "references": ["Clyde Lovellette"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-d8efc6dc716042238f2d8ef6b0c31803", "references": ["Colin"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-5e9e923de35740208904bfb0b34495a1", "references": ["Patrick Bridgwater"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-b43450b1e4264b7795fc9a9d47fad141", "references": ["Ricardo Rocha"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-704597787e9245f3afa37d48fcd16b4d", "references": ["Jacob"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-676b51f84d5b4710b9c7ab25c7ee7a23", "references": ["Aiko"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-7addaf0915a848c689c3a7b1f045ecc9", "references": ["Houllier"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-7f96134e166f41af93ffcf0f3bf9a906", "references": ["Edith Jessie Graydon"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-e4965efde018479e9660076781b51d09", "references": ["Alan"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-3902e340c7cf447ebe28a3f7bef3f836", "references": ["Bill Forsyth"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-a72efe34b54a45dbbeab41d77b7d2314", "references": ["Ezra"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-4a910d90c1e541769db14164cd39c990", "references": ["John James"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-95e0fc762b1d48b8864c137edbe1892b", "references": ["Austria"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-8d6ce801ee154e4288a28758bf2aac61", "references": ["Peter King"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-54e7324e58064985b369fb834b3ee536", "references": ["Matilda"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-fac1e1eba36948199fc94c0ca9e89122", "references": ["Aparna"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-4b99732648eb4d7e993f667e3bcac9ac", "references": ["Bynum"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-196314961de44d249f8632c0304188a7", "references": ["Chris Jericho"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-525d420ce6bf438194376fc9db01b6a0", "references": ["Franklin D. Roosevelt"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-e7c402e696354fddbdd5b45e6f186229", "references": ["Hung Vanngo"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-382c0ae23d534094a07866fec1cb36ee", "references": ["Chernow"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-22f6fe9336ec4d1dab0806c9538e06d1", "references": ["Diaz-Twine"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-cfbdd3fee25943f19b3edd5affa26f99", "references": ["Downey"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-218f9273c3d84e7d9aecd806380c2a39", "references": ["Jonathan Foyle"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-aaf669b864ae48a8872da2dded0b4713", "references": ["Hall"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-08f7d0185be44e12b5d34cf7551a98e6", "references": ["Wilkins"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-1bde470ca99f476a89f5c64b9d41aaa9", "references": ["Brown"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-00cc27ef7ec343bd836466e6821fdd6f", "references": ["Jenkins"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-e84efbc4237d49909bf477c3fe8f1046", "references": ["Chenier"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-53de1a45fad4427a8280e1e151033693", "references": ["John Huntington"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-e3759f9d963e45f8ade2e8bcf6cb4d30", "references": ["Jim Dowson"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-b4fe2aaf601c4560bad4ec954dc966ae", "references": ["Reshammiya"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-9dbe4bde90c74fa59b3603c270520e60", "references": ["Mulder"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-263d21b53b6d4ce38f5479d213e20ed3", "references": ["Voltz"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-3774ea2885284e51adee0e92b852fcd2", "references": ["Drucker"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-1fd4ea97455341ac9af5eba59b95b201", "references": ["Hibbert"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-dde113ddb5e7407eb92325bea06ca6f3", "references": ["Billy Hancock"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-ff0e68fbe915404f912ec3b1949f5fa4", "references": ["Dunlap"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-32e5d8248bc14b0895e386a2b1051e5d", "references": ["Swami Brahmanand ji Bhuriwale"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-bf517cc9f7fb4bfa8b7988f3852eea0b", "references": ["Owings"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-2d4b1fca330f423f8db28c64d8bf0b81", "references": ["Schr*der"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-5f6706e06b894d9c9e9270869290d923", "references": ["Newton"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-3058aaa05253415294f9dd3dedf68a6b", "references": ["Thierry Henry"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-110aa9b7eace4817bbea66ba0ab97d51", "references": ["Morrison"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-db5927849350434788d19ecda37214bb", "references": ["Larson"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-88c6bbd865ba47db887147817281f297", "references": ["Elizabeth Bennet"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-c4423f142a084ecf98116b70115910d4", "references": ["Davis"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task330-d3edd07a5949480ab52a1aec116d010c", "references": ["Simmons"], "task_id": "task330_gap_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1342-a31ef060cc154ac186e81b7fd84b5d6a", "references": ["Compaq replacemt battery"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-ba266ee0d4b54d758b4068f6567fa5a5", "references": ["Otherwise it would it be a perfect replacement for my old modem that broke"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-ac60d7b52eac4c2b8f70d24b4ce1391e", "references": ["They love it!"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-985e3b380f374226b176f8b9de1acb6f", "references": ["Look Elsewhere"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-f2baf69a9e4146958279cfde544aa0d5", "references": ["Kidded Myself"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-0c0532979a9748c289968f3b6696ee4f", "references": ["Sufficient"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-140dae600bc64248bd0cf077e554bb82", "references": ["Cool case!"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-c7aff9c32e7744f0b3ba9cbea06076df", "references": ["need quiet buttons"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-29aa1bc9efcd4907aadb616a83af112e", "references": ["good mouse"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-d6dd008db65f43de9f516e02450da589", "references": ["Happy with TFY car headrest mount"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-350d5c42318244b7857e9e1deeb77e1f", "references": ["Quality Q88 version tablet! had it before from some ..."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-a1ac1f4ab6e24568b0513dd2bd1cc67f", "references": ["Verry Poor Wireless Keyboard"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-4f6cc423e0ba4ddb96c136749da7af73", "references": ["Worked and Then..."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-8328b6cd28fb44c9ba8a1a3f3f4af081", "references": ["Five Stars"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-593aac09c20f4ff39de70a5a2134c17a", "references": ["Likes it"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-a11c1474c5654280ba9c4fc22b6f091d", "references": ["I love it on my iPhone but trying to get this ..."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-c62b5103336a4a5da4b80c93554d9bbc", "references": ["Outstanding!"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-4ac7d292d50f4d0da1262068bbc315b8", "references": ["Love Gumdrop Cases"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-08d3190e60884b958068630c580422a8", "references": ["Where have you been all my life?"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-47d3a8ee810b4b3c8b726f485d3b9008", "references": ["Garbage - DON'T BUY"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-be11044aa0004e3492befe0d68f7ee2b", "references": ["Great"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-6d10f4abfd7a4eab86384d15df98d8f9", "references": ["Perfect space saver for your desk"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-74a6a59bca8a4e24a60d032ff598f9d5", "references": ["dissatisfied"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-ce8d5cdb9dff4a51819699223a2d116a", "references": ["One Star"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-b2bab7616e234c8bafc826e047a89aeb", "references": ["eh............."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-1f86859e1c2841fdb63f80d6d64621fb", "references": ["Met the requirements of my new server"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-1ed3ce221f1f4654b19c6038efc1c2d8", "references": ["Saves power"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-0c07e9cfddd045068eba90edd5d88d7d", "references": ["Would recommend the power cord by PWR+ instead"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-cee93b1c6f984096b6b413cea1d9252e", "references": ["great"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-da1e8ab1035f4124ab08bea18b5c9fc6", "references": ["A must for everybody wanting a backup drive."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-8eca08b776054d33b59a164a9c64c45d", "references": ["A good thing, but it didn't last!"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-2f8ba05d6fe845eda1079f64cf002b69", "references": ["good"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-29390c8c846843a59930450747f76963", "references": ["great product, love it"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-cb6e18a333bb42e3a711ea5c522d37f4", "references": ["Nice aluminum construction"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-5a75bdacaa4447d3bcbecdb1cf6eb7de", "references": ["Cool + Useful!!"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-075284617da74d8ab0caa493c0fcb7a7", "references": ["Awesome"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-a130e645d9c1462dbbe086dff7faaf18", "references": ["Magic Pad works like an iPad for gestures."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-35233302d7414731bad9011122ddb78e", "references": ["Works great."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-237dc608a2214e198552c9161332f262", "references": ["Great keyboard!!"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-cbcff3995b4a47138dc0d77bde5b1382", "references": ["Superb customer service, nice product..."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-033d4fe5249d4883aff34f1fba0a455f", "references": ["Good quality."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-650d5d32cf814caba2bd81318a48436e", "references": ["my new kindle"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-c353f2bf29f345f4a6ed121fadba8192", "references": ["love the back light"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-940bf08db2474b8eb0cc865e9e037f9b", "references": ["An Awesome Ebook."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-64946b219ccb42bea43d0045c2f0a268", "references": ["Great Camera Memory Chip"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-dbb366bb7fdd45148942a791b64247b3", "references": ["Fantastic"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-cc18f15287fd4fd9a8d708d45175f50f", "references": ["Perixx PERIMICE-711, Wireless Ergonomic Mouse"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-0a56f5538b684d63b47370ca6857bd6c", "references": ["Case Logic Laptop roller bag"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-e6b0c4e7850c4b28a1287cfbfc798e50", "references": ["Quick and Easy Solution!!"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-6e48d6c5dac449088bc9854137c19a89", "references": ["laptop battery charger"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-8909f0c6a0354f9888dec31f315b932b", "references": ["Extremely pleased"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-19cf8b845c7049caa16af420273da8b3", "references": ["Great product, great price"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-6f183b5da32b47729c5109331a203320", "references": ["but it's not super nice. That's about all I can say about ..."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-5784e328e30646aa96200e0543396c83", "references": ["Died a month and a half after I ordered it."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-0d8099df54fe4968ac109c372fbd7994", "references": ["Love it!"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-eee70ef9db7d43559fd2f1350d29ffe4", "references": ["came on time"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-4297aa454d464e34ba97471637cfee99", "references": ["Doesn't Work! No Wait...  It Does Work!"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-215f9d88b94946f7a6350ff6952c4d86", "references": ["highly Recommended. Don't think to hard on it. just hit Buy It Now!! You won't regret it!!!"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-864a75f9ac9d4b3ab9cb1d5a60e8b046", "references": ["Mediabridge USB Cable"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-44603e65176348769413dd97d879f8ec", "references": ["Perfect for Nexus 7"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-8bca08d4262f466c9cee933cf7d86c6e", "references": ["My monitor has an excellent picture and no buzzing"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-9f71b8f5f125413da50ac2037c1613eb", "references": ["Love it!!!"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-eef49f989f2a47a9bcb5500b1467f9b5", "references": ["Paul Sasser"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-5ca5c338f5db40638ff4f993d92857b7", "references": ["Good present for dad! He'll Facebook that to death!"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-b5d1f12455e84d6e9bf4c5a3efe435e6", "references": ["Perfect for 7 inch Samsung tablet"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-a0e63277152447088f8e133702e0159d", "references": ["No more dents"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-9c39895b68f647bfb32b081a45db420a", "references": ["Great product"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-6dd82f69322949feb352dc5ce1915bcf", "references": ["Well, I'm stuck with this so I guess I will use it"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-5a37598f89774df8b2b94c26d28d358b", "references": ["Three Stars"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-66dca9c6f19342bea54360986e7ac4d7", "references": ["Great addition to Tablet"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-2d8ba499f56e468190cfd081e8af0665", "references": ["Great Purchase"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-9411cbf5f51141e9afb15aa68177c24b", "references": ["Big Case, Small Price"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-44ca3b858a654dbf81748ffde84828e1", "references": ["Get this one for Xbox 360 Live"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-2d4a10de4d634adcb54d847240fc3751", "references": ["Bad QA for a 2200$ Product. LG display model problems"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-47e76473516d4d3c8a7ae29451dc0361", "references": ["Save your money"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-77be4b8a0a9e40398e55612e957e1f8b", "references": ["THE WORST LAPTOP EVER MADE"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-74933d529354403b8653e522e480b0f4", "references": ["Great Case"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-0a20aded4bd540738fd007f342e2d00c", "references": ["Perfect Solution"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-f14d0626e7f342e49ad833b496493a57", "references": ["Exactly what I wanted"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-c9826f8323d74848859a3aaba52b0dfe", "references": ["iRulu Playbook"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-fc10725b728c490f9cff8af8abba53b9", "references": ["Better then good!"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-25b037971e92461386c458ac7339765c", "references": ["i like it"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-3b8dcc17141a4d5fb5a1d802d1dde49a", "references": ["Saved money!"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-b1bfe342bc1d4fddab5143a13ae9dda3", "references": ["Don't let the low price fool you."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-a16e927ad1794dd5b4b30f693a0d2163", "references": ["great picture"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-b95ab199152846eb878689f47fe17b48", "references": ["Notebook review"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-e876d1f622e44941ae3919d5c70afdea", "references": ["Not a perfect fit but it does work."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-5cc4bbb1de3e441f9a44c986672ea6a1", "references": ["Great battery, wrong photograph"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-091e0754e4d347d4a1b5fd368ef781c4", "references": ["Overall I am satisfied."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-ecd5422bf9b84057a3c67df0356983d7", "references": ["Like using a track ball? This is the best choice for most."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-c162b21e0b7f46efb49d7a6a2768838e", "references": ["great product!"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-d7abf29f33c8488dadf03089fa2cd38f", "references": ["Works Great, But Rubber Feet Fell Off"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-36bffc2d20784f4a81935e8329d2adef", "references": ["Ended up having CS walk me thought the set up and they were great."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-5423713c3cbe4a9491f041f4f3ad317e", "references": ["Replacement battery"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-d24463067fa646a497db13007b640771", "references": ["my transceiver got stuck in it"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-75474e3b57004212a6e7f857b909ab07", "references": ["battery charger for kindle and ipad"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-a53094aba5bf4325a28efb32391af147", "references": ["Worked for over a year now. Thanks Buffalo! ..."], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-839c17780df448e3b0a9db8af4046825", "references": ["Fantastic! (review for Kensignton KeyFolio Pro2 in light brown)"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-9a0827d67a4148168d748eb05f0bb6ee", "references": ["Fast and good quality"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task1342-a13e03377e1d4bc19a2838697602e636", "references": ["Four Stars"], "task_id": "task1342_amazon_us_reviews_title", "task_category": "Title Generation", "track": "default"}
{"id": "task329-3177482d4fa24a0bb289c83687a732d7", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-45947b7ccc6d4ecea4013a3a81ad7d98", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-65cc482443b64f5ea96ec5b197dca92f", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-189d87e8668e4f35a596084a9ee42719", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-4156e79598944cd680139ac071c2d16f", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-bb2f3824ba8b479794f10f4e6640636b", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-10c444989a8745ccab5ca5e2c3b3ec28", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-7cfe135ed673475b99233e00df563f7a", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-1b15c339340a440c8e61bf8080cec00e", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-347f9f59d2ad4590b20e176be0a4477c", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-a1d4f969fae247a380e1c15a07615b2f", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-91e08da6d6144981b8c0b195121ae9fe", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-b8220ce1913c42248911dec9aaa4dfa2", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-2a450b8d87b74f01b1f732acc8565fb9", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-50367ee7338d4b5f80b5d6b95892c573", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-c25eeb32961d4cc98383f66349688696", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-129a42586bcd4d3bae176fe9a976db09", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-f84898af4c8045aeb13ac612865c3f9a", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-189f0e6408bd4e6b8a8ccaf889ff0774", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-f07f9ebf8413463bac208129350f08a1", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-24cb2da66783493dab3294cacd875496", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-f241c2f855234c589e7281866627da68", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-efe6fc3bf5d34dceb8e29c0ff7c1384a", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-e40ea4c1939147d7828ed8656d57d7c2", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-d5edb10e13db4107ace91487b34ef00e", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-de769e8648cb45c288eda9a804bed614", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-1514294dc9174dd3bf56939920c6a470", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-c95369838aaa471da6d55aa149857cc0", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-00f493149e204ce182014ad2116b142f", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-e4b93c13e7634b9db91e135d2408d8c7", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-22adf8af42f3421c8cc8408791c24338", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-262b3111acf74fbf839b6f9065d74e6f", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-8a825003f6624b9eb79c3459a904ba2b", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-70c7f03df5834f73acd304db7ad6b5a0", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-b8e755b93ef340409b7c181a5eb6b2a3", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-72d54e039324409d94cb5e3285d0b148", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-94628084036b4e85a443f041304a15a1", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-e839c912ff264e1b99283b6b7525a13b", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-53e164c84ef846bda377923203e104ea", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-5f583b967af34d5088d8bd907954ce36", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-c20318a2cfc24af4a1fad051cce769f8", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-8b5d1f65cbf5461a98797fe21bb36ff2", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-29bdaaca52f3482fa20bbd68eb26e41b", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-1f72b43ce4a84f739a5a2915fcd82922", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-f122691c912f4dfebc469a22b004f4fe", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-20b1e7c38b30411fbdb8755383802020", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-c89fd25982eb42499e104365ec34bb99", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-aa4c99ea52af4db18c591a78a57bb105", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-0aa889aac86040609bc8485da8a43811", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-f0d155adf7024e66915047caf48f1e9e", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-c75f6b713bb844b9929d835c78d916b2", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-4adb4003f01149f4adb3a32888df5aa4", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-8ee93e19006346048a6905eb3510f1e9", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-521e6be8e5574e7297ca8fd6d5070a0a", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-e344d21bed974335ad3f57ddfd6ffe1f", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-b0ef51818ca944fda3cb12be77867974", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-61fb9cb490f04f149f00df9abd682fd2", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-b5d7f9fa8d89480082ac4f578816a8f0", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-68ed938c7aa14a978b56db5185db4e25", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-9a9abb05b9df42d6a90fe79364c876c6", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-7a61c12a9150439c9ef4c1ed395640b2", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-85cdce01467b445eaf7455547275136c", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-fc970edfae2341aeb9dd348bda20fa7f", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-ccc8dbd011a14a569fde7fa355e7520a", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-c1d331790d0b48828d5e03145e7a655b", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-c5bd5a53ae0b491291a170cb9864c867", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-6f54e853f6c04527b5f422675381342b", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-0cffdfb07d97477bb239a82bc4d8953f", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-d434e506bb9042b3ae763b5728bbb94f", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-524a781df7d4411caf2f08912b533e06", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-458f3638338340fdaea9c62752302f43", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-45aa2ec4a077473ba11cfa1665071237", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-d395d6253fa046efad05ac5b9a1f93c3", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-6d9e58df51784e05945788c114eba9cb", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-5c4a8ff8586c4eb189fa5c306c21beea", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-08bd705d290b4b76b9032223c910914c", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-fa54a10e7adc41cf9d495c8fcb2fff77", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-bcb16082123642638840964cba8ee252", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-70bc6698056d49a2a46a7e3d0b011bf0", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-cb69279f86a744839a01061f3e6283f5", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-638aac91bfe34de6ab8ea657220e0431", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-e37385b13ecc40588200008dd32b11ce", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-6cafb3559d764089af2c5ab6fa8ed904", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-407e7ee3504c4f32b980ba68a94213d7", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-f8dc0820a11c4e17bc04a3d0ddceb4e8", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-28dfffcd8d8e429cbea1c3672f5f80da", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-e2a6232539314538878a52f54e7abcc6", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-91f906bdcb0a4cb4888ac444a538e1e0", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-e1a8808999554624a4b1ea0ac2c02481", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-87009dd5307a4523849ac257c840b2ea", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-0bf38f63661d428589683c3f8d17cd19", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-1b5535a4da9c4497b30a741095c9046b", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-d10094df48eb4977a57a422f6762c49f", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-464c4deccdef469ab51fc8dcca3f68cd", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-e3c770cebd2a4f1181e4e7ceb5f8ee5b", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-d5a8dc87fc554acdb686be9500b4da18", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-5225259e183f4a5da97c0ca31063fd2c", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-864b04a4fd994766a62ddc22e079471b", "references": ["B"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-7e99381645024639996a33c66258ed3c", "references": ["A"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task329-bf0f46a0634948dc891d517a7b3590d1", "references": ["Neither"], "task_id": "task329_gap_classification", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task281-f96280228f9b4a0c9651c697e3a8ddc8", "references": ["1: Cuban leader Raul Castro 2: the Castros 3: Raul Castro", "1: face-to-face meeting 2: this meeting 3: meet"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-45a90dc1845446b0a5452fafb32c7819", "references": ["1: sexual battery 2: the sex 3: sexual battery"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-3847cead70ce4002b7f23e45e3b070e4", "references": ["1: rebels 2: Yemeni youths 3: rebels", "1: Hadi 2: He 3: Saudi military official"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-3b2c8c14b57a441aaabf54dbef37caa9", "references": ["1: we 2: We 3: We", "1: surrender 2: cooperate 3: cooperate", "1: other powers 2: the world 3: the world"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-66097fb1ca2e4296ba183e76438ef6a7", "references": ["1: Hillary Clinton 2: president 3: candidate"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-bfe1c7b64b7145da8e95b57945dfb96c", "references": ["1: discrimination 2: against African-Americans 3: racist"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-a2d41270ff0a475c8d7ef1160480896f", "references": ["1: we 2: Patriots 3: Patriots"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-1d69b9ee64f84a1ca90479f5b4efb4c1", "references": ["1: preaches 2: promises 3: preaches"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-df6a25692b25484aa415009b62568454", "references": ["1: monitored by giant telescopes there 2: Andres Figueroa 3: Photographer Andres Figueroa"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-021eb366f78940df9344aff535e2117c", "references": ["1: repeatedly pulling her ponytail 2: The behavior 3: pulling a waitress ' hair"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-c9af4f9e3e3b4222ba415af6b2ce8d5f", "references": ["1: people 2: people 3: People"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-7317d4e5417a4b12b76ca0c419959f8d", "references": ["1: Seth 2: sons of heaven 3: creation stories , Seth"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-ed48b76c763b4a888c5b3409f50f6de7", "references": ["1: religious freedom law 2: the right call 3: fix Indiana 's religious freedom law"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-c083098fd90349808e700d2a6ed8953a", "references": ["1: a tornado may have touched down 2: damage to roofs 3: wind damage to roofs"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-0c373ccff9d04f2faa32b5b0540b2c2f", "references": ["1: an aircraft 2: the Germanwings flight 3: Germanwings flight"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-0d7bc654030e4eb79ed57b4912dcdb2c", "references": ["1: section of the law 2: better informed about police shootings 3: Federal law on reporting of such shootings goes unenforced"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-d52e67b97a8949639e02a3e4d0d6101b", "references": ["1: Israelis 2: Israel 3: Israelis"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-2f1f3902fce449fc9e0bc4f0a6dbd6c8", "references": ["1: One victim 2: A 20-year-old woman remains in serious condition 3: a 20-year-old woman is in serious condition"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-6e365df24b3045fabf63808352e63525", "references": ["1: Thale 2: immigrants 3: Immigrants"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-cce382d7f7624269a8f6e424f61a618e", "references": ["1: Five militants from the Kurdistan Workers ' Party were killed and another was wounded in clashes with Turkish armed forces in eastern Turkey 2: the first incidents more 3: Violence between Kurds and the Turkish military"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-e8fe82f6c603444d81ae97b7604fd127", "references": ["1: The U.S . landmass 2: federal territory 3: the U.S."], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-936c6407ecc14bd2a998cac04cacfdbe", "references": ["1: Moynihan 2: His 3: police"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-f99bd0ac10a443669b4190019a4ecfde", "references": ["1: act 2: disrespect 3: responded", "1: Britt McHenry 2: rich and powerful 3: celebrities"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-b0ea2e06981a4cfdb3086b6a4571788a", "references": ["1: Soltan 2: Your 3: Your face"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-605796c9d4cc4694ab411cfd808e6288", "references": ["1: the minute creation 2: the liquid resin 3: They 're"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-be027331a7684c40b840bb6643d03cbf", "references": ["1: Ray Gricar 2: the 3: Prosecutor Ray Gricar"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-88a4a6f5328e4c52bbb3cb83ac85e22e", "references": ["1: Yellowstone 's previous eruptions 2: 2,000 to 3,000 small earthquakes per year 3: small earthquakes in Yellowstone"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-97dd4c07aecb42399b4f56db502803bb", "references": ["1: Then 2: Strong 3: He"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-bcc98b01878e44fea50abd78466c81a5", "references": ["1: died 2: genocide 3: genocide"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-f8a4d6c224f64d289fd4f96d94237e37", "references": ["1: Duke officials 2: Larry Moneta 3: Duke official"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-0d81c5c064f346e987c44292ee1609f1", "references": ["1: Boston 2: Boston 3: Boston", "1: runners , spectators and those who tried to come to their rescue 2: Bostonians 3: Citizens"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-7322d1852f534278906bc84c4cb0e585", "references": ["1: anti-domestic violence law 2: women 's rights are human rights 3: anti-domestic violence law", "1: It 2: Beijing 3: Beijing"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-0b9629a2ff434d2d960ed60f4da52955", "references": ["1: Ousted Yemen President Abdu Rabu Mansour Hadi 2: Hadi 3: Ousted leader Abdu Rabu Mansour Hadi"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-21dc89a9b0934d86833b6174a2611630", "references": ["1: two of the highest-profile court trials in recent memory -- those of former New England Patriot Aaron Hernandez and Boston bombing suspect Dzhokhar Tsarnaev 2: Both lengthy trials 3: The trials of Dzhokhar Tsarnaev and Aaron Hernandez"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-407960d2e3434b9ea292dbc0da7edddd", "references": ["1: a young girl who may have been dead for weeks 2: the body of the 3-year-old girl , who had apparently been dead `` for at least a few weeks 3: 3-year-old", "1: police in North Las Vegas , Nevada 2: police 3: North Las Vegas police"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-d4754ff49cd2401ab857105091a01354", "references": ["1: four of the dead 2: workers 3: the staff members"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-e58de62bdd0848f3a26696551b8a12b3", "references": ["1: bans 2: such bans 3: bans", "1: appellate court decision 2: judicial rulings 3: appellate rulings"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-3f1af176f53f47c3b75a87e55017c329", "references": ["1: charges of assault and mistreatment of a prisoner 2: all charges from the arrest 3: all charges against the motorist", "1: charges 2: charges 3: charged"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-a8810c04a1294044a557914cb93ecab2", "references": ["1: the Thunder 2: the vessel 3: the vessel", "1: encountered 2: found 3: found"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-b75f71ab8d17469f8ebda375356bfefa", "references": ["1: A 19-year-old man 2: Yahya Rashid , a UK national from northwest London 3: the man", "1: arrested 2: detained 3: arrested", "1: returned to Britain from Turkey 2: arrived on a flight from Istanbul 3: landing on a flight from Istanbul", "1: London 's Metropolitan Police 2: police 3: London 's Metropolitan Police"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-e8b3a0597e24492295e24da99622ead3", "references": ["1: the matter 2: stole 3: stole", "1: Alondra 's mother -- Susana Nu??ez 2: my child 3: The mother of Alondra Luna Nu\u00f1ez"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-498e60378ea84771a739a41f551ca72b", "references": ["1: He 2: Gadahn 3: He"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-a964ee6aac9c4482ad02db18a1384cec", "references": ["1: such activities 2: in lethal injection cases 3: lethal injection", "1: pharmacists 2: pharmacists 3: Pharmacists", "1: the group 2: the association 3: Pharmacists"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-bec8786b3fdd415fa931bb98f30da950", "references": ["1: more than one million Armenians were killed 2: the mass killings 3: slaughter of Armenians by Ottoman Empire"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-a123d26c207a4546b3b6949311060b60", "references": ["1: earthquakes 2: disaster we 're seeing now 3: damage", "1: size and density of Kathmandu 2: a vast population 3: Population density"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-ae614135875943a685e42455fd73aa9c", "references": ["1: Suicide Squad , '' Warner Bros . ' all-star action movie featuring DC Entertainment super-villains 2: the feature 3: Suicide Squad"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-e4f64ada2a134d808ff90e2bce5ad3f8", "references": ["1: A Polish Prince 2: Prince Jan Zylinski 3: Polish Prince Jan Zylinski"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-073f3d0a237947ab8960bd9ddf261824", "references": ["1: The city 2: New Orleans 3: New Orleans", "1: went smoke-free 2: ban 3: are smoke-free"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-580bd259d29f4603b6370d499b9d4fb1", "references": ["1: campus 2: Duke University 3: Duke University campus"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-696e28135c484f9ab5abf68353c8dc5c", "references": ["1: raising the minimum legal age -- currently 18 2: raising the legal age 3: raising the legal age", "1: The state 2: Hawaii 3: Hawaii 's", "1: raising 2: raising 3: raising"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-2418da355ca54a838ee933a02170afaa", "references": ["1: Ben Affleck 2: Affleck 3: Ben Affleck", "1: his great-great-great grandfather Benjamin Cole , a Georgia slave owner in the mid-1800 2: his slave-owning relative 3: his slave-owning ancestor", "1: remove 2: excise 3: avoid"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-42f0d180989d422ba5be5f7a7ba94d34", "references": ["1: The Late Show with David Letterman 2: Late Show 3: The Late Show with David Letterman", "1: final month 2: May 3: May"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-e58f08e6e811430fac7cebd67f5deb8e", "references": ["1: Actress Linda Thompson , Bruce Jenner 's second wife 2: Thompson 3: Bruce Jenner 's second wife Linda Thompson", "1: Bruce Jenner 2: Jenner 3: Bruce Jenner 's"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-206f462a352c45248cfc9393ccade036", "references": ["1: Indiana pizzeria 2: Memories Pizza 3: one in Indiana", "1: debate 2: firestorm 3: harassed"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-ffe14236f3d34d329876984fd5d502b1", "references": ["1: Ahmed Farouq 2: Farouq 3: Farouq", "1: al Qaeda 2: al Qaeda 3: al Qaeda"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-4ece7b0b063f4e199a34bafe17a05940", "references": ["1: Her 2: Genevieve Kelley 3: Mother Genevieve Kelley 's"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-29914a635f3340009017363141dead1f", "references": ["1: Mohammad Javad Zarif 2: he 3: Mohammad Javad Zarif"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-6b363154f2bb45aeb596a3de378a3a31", "references": ["1: of `` Zoolander 2 2: the upcoming sequel to the popular 2001 film 3: Zoolander 2", "1: another really , really , ridiculously good-looking person 2: Actress Penelope Cruz 3: Penelope Cruz"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-1541ddf19660498a909dfb5c30260516", "references": ["1: He 2: this giraffe 3: the giraffe", "1: she 2: me 3: Francis , a hunter"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-828a4dc1cf054bd4a907a4cf3d9fa813", "references": ["1: economic sanctions 2: sanctions 3: sanctions", "1: Iran 2: his government 3: Iran", "1: President Hassan Rouhani 2: Rouhani 3: President Hassan Rouhani"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-1856f4ff4f964fbdb715929e990ac1b8", "references": ["1: of Assyrians , a Middle Eastern minority with a history reaching back more than 4,000 years 2: Assyrians 3: Assyrians"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-d146fca8ee7e460d8d08ff6fee211cc5", "references": ["1: Salvador 2: Salvador 3: Gabriel Salvador", "1: Freddie Roach , Pacquiao 's trainer 2: Roach 3: Manny Pacquiao 's trainer", "1: Moonves 2: Moonves 3: a TV exec"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-be0c8d94f05548dba74c022b09bb3f01", "references": ["1: criminal solicitation , conspiracy , burglary , arson , criminal prescription sale and weapons charges in 2: sold the officers two semiautomatic assault weapons as well as ammunition 3: selling drugs and weapons", "1: Dr . Anthony Moschetto , 54 2: Moschetto 3: Dr. Anthony Moschetto , 54", "1: prosecutors 2: prosecutors 3: prosecutors"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-c6903282e7504e3db700b207c1f81068", "references": ["1: a no-man 's land 2: the territory 3: an area between Croatia and Serbia", "1: The victim of a border dispute 2: the Free Republic of Liberland 3: the Free Republic of Liberland"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-b78746a01b15403da3eccce96769132a", "references": ["1: foreigners 2: immigrants from other African nations 3: foreigners", "1: deadly attacks 2: clashes 3: xenophobic violence"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-a035f83363ac4948beeae6156bd4d8d4", "references": ["1: a rare genetic disorder called Werdnig-Hoffman disease 2: a muscle wasting disease 3: a rare , genetic muscle wasting disease", "1: The 30-year-old Russian man 2: the first patients 3: a 30-year-old Russian man"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-6f862a063a3b4942bd844e7713b386b0", "references": ["1: Andrew Chan 2: Chan 3: Andrew Chan", "1: Myuran Sukumaran 2: Sukumaran 3: Myuran Sukumaran", "1: Australians Andrew Chan and Myuran Sukumaran 2: Chan and Sukumaran , members of the so-called `` Bali Nine '' convicted for their role in a failed 2005 heroin smuggling plot 3: Australian `` Bali Nine '' members Andrew Chan and Myuran Sukumaran"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-00810b176d7f4e8289fb8786c5c158df", "references": ["1: one brother 2: him 3: Brother Tamerlan , who was killed during police showdown", "1: The defense 2: They 3: Defense"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-a00240453e6a454f9baa0eb594a0790d", "references": ["1: He 2: he 3: He"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-b3ddb1e660f44ed29d722dc8b016e477", "references": ["1: the show 2: Grey 's Anatomy 3: Grey 's Anatomy"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-3806470480c34bf189698e95a129e5e9", "references": ["1: The lawsuit 2: the suit 3: the lawsuit"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-a1e3a1baf7b24111b5508e7d02c9bbaa", "references": ["1: Robert Downey Jr . is 2: the actor 3: Robert Downey Jr.", "1: British journalist 2: The journalist 3: Peggy Drexler"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-7dd06d7959f04f4bac7235946f5b9335", "references": ["1: CNN ) `` Real Housewives of Beverly Hills '' star and former child actress Kim Richards 2: Richards 3: Richards", "1: a police officer 2: A police representative 3: police"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-223021c48a5b48eaab1649ccfe68a739", "references": ["1: Arrested Development 2: the series 3: Fan favorite series `` Arrested Development"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-339fd1a147ae47d5890b34a49f0be085", "references": ["1: A militant group called Jaish al Adal 2: Jaish al Adal 3: A group believed to be based in Pakistan 's Balochistan province"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-408a177905f740db9229a40ddeaee3d3", "references": ["1: Durst , 71 2: He 3: He", "1: possessing a.38 caliber revolver 2: that charge 3: having a .38 caliber revolver"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-884ecb3427d64020ac6ebc075a00b4ad", "references": ["1: put an entire head on a new body , a human body 2: the first human whole head transplant 3: the first total human head transplant", "1: Italian physician Dr . Sergio Canavero 2: He 3: Dr. Sergio Canavero"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-745e4b6307b3476ea6a12dff21ce956d", "references": ["1: The officer wounded , John Moynihan 2: Moynihan 3: Boston Police Officer John Moynihan"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-325a24668a5b4d05ab0d101520384e95", "references": ["1: older brother Vitali 2: Vitali , who retired from boxing in 2013 3: Klitschko 's older brother Vitali"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-d6b0c5a2bc604e7386c9c7284dfe6f4e", "references": ["1: best seats in the house 2: A ticket at ringside 3: Tickets"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-25b55a4404b54707a084923e837c23d3", "references": ["1: his phenomenal 8 1/2 minute allegory 2: American Pie 3: The song", "1: millions of Americans 2: listeners 3: people"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-724e2750834e4f619d7faa8471b7c127", "references": ["1: a ramp agent from Menzies Aviation , a contractor for Alaska Airlines 2: The man 3: Ramp agent", "1: The 2: The 3: he"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-4906c41f5b3946d580db87dd5f6c485d", "references": ["1: Dr . Derek Shepherd -- the hunky character played by Patrick Dempsey 2: he 3: Derek Shepherd , played by Patrick Dempsey", "1: died 2: died 3: died"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-aefeb95421ac43f38459ae63716242fb", "references": ["1: Slovenian archaeologist Ivan Sprajc 2: he 3: Slovenian archaeologist Ivan \u0160prajc"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-5735301b2ef442e2b91dd5dd27ff2a6d", "references": ["1: The California Public Utilities Commission 2: the commission 3: Utility commission"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-4b6abd805ef948c78ab9889437cce650", "references": ["1: one of the Pope 's newly appointed cardinals 2: Cardinal Gerald Lacroix 3: Cardinal Gerald Lacroix of Quebec"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-312da5a52f434703ba51eb51f389149f", "references": ["1: hit 2: did exactly that 3: hit", "1: him 2: Venezuelan President Nicolas Maduro 3: Venezuelan President Nicolas Maduro", "1: hit him in the head 2: that 3: hit Venezuelan President Nicolas Maduro in the head"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-e9e6460872b649e3a2ef0820f692e741", "references": ["1: the company 2: the company 3: A British company"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-ee4380fa0c5148e58f889aa73b65ee26", "references": ["1: She 2: I 3: Winehouse"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-75dbfe5aa16e4cd7915ec4cc0067db15", "references": ["1: the West Coast-based ship 2: The Infinity 3: The ship", "1: San Diego 2: San Diego 3: San Diego", "1: Monday 2: March 29 3: March"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-74aa023a5a4149f998f22edc2e9eaec5", "references": ["1: Boston Marathon bombing 2: extremism 3: terrorist bombing of Boston Marathon", "1: Dzhokhar Tsarnaev 2: young Muslims 3: Tsarnaev"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-b9db89dc65e14b60b069d4fc2b32a122", "references": ["1: two Australian drug smugglers -- members of the `` Bali Nine 2: Andrew Chan and Myuran Sukumaran 3: Two Australian drug traffickers on death row in Indonesia", "1: executions 2: death 3: death"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-e51e75e5ba674fceac368a602ca76253", "references": ["1: the woman 2: the woman 3: The mom", "1: her masked son 2: her son 3: her son", "1: CNN affiliate WMAR 2: WMAR 3: CNN affiliate", "1: son 2: son 3: son"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-511913fe0a90433d96fde1143cae23c1", "references": ["1: Three British citizens 2: the men 3: Three British men", "1: their lawyer , Nasser al-Hashem 2: there 3: their lawyer"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-5ed608413d25453086780c25bef82896", "references": ["1: Two deputies 2: they 3: The two deputies", "1: Eric Harris 2: Harris ' head 3: Eric Harris", "1: fatal attempt to arrest 2: pinning Harris ' head to the ground 3: pinned Eric Harris to the ground"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-11b66e9db97641f7920c345b9ef42fcb", "references": ["1: Ukraine 2: Ukraine 3: Ukraine"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-038599b860c042da8573e6a929609625", "references": ["1: The presentation 2: the Muppets revival 3: the variety show", "1: the original Muppet performers 2: the Muppets 3: The old Muppet gang"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-bc734ba97d83403dbab7efb2829c543b", "references": ["1: will follow 2: will follow 3: will follow", "1: I 2: I 3: I", "1: she 2: she 3: she"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-375905fd29f946189bee8d68ce4a8892", "references": ["1: People 2: most people 3: Terrified residents"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task281-3a5c4187f15d4c3ab5ebfb703a91aac6", "references": ["1: the U.S . tax system 2: the tax laws 3: U.S. tax laws"], "task_id": "task281_points_of_correspondence", "task_category": "Overlap Extraction", "track": "default"}
{"id": "task036-e69df649e3434d7789023005535b3b59", "references": ["epidermis.", "fever skin hot.", "nerves are made of.", "nerves electrical.", "nerves heat pressure.", "sensory nerves.", "skin epiderm.", "skin touch pressure."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-2b2f994985d241428be945b71736eb02", "references": ["components of vinegar.", "eyes are part of the face.", "eyes retina.", "humans need eyes to see.", "pickles vinegar.", "vinegar acid.", "vinegar in eyes harm.", "vinegar is an acid.", "vinegar is made from."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-a57dc40c976545c0b342b2d8a9456451", "references": ["braking cars can fishtail.", "car vehicle.", "friction brakes.", "friction motion rough.", "ice less friction.", "vehicle brakes.", "vehicle car."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-1bcb302bc8654f8aa7a76e28b0f1db27", "references": ["glac.", "glacier form lake.", "glacier weathering mechanical.", "mechanical weathering causes damage.", "mechanical weathering creates dust.", "mechanical weathering."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-ef9e41e88ae142dca9eb7e26bce68699", "references": ["light contains photons.", "light is energy.", "nuclear reactions produce light.", "nuclear reactions.", "produce light bright.", "stars earth sun.", "stars produce light sky see.", "sun is a star."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-4ee57bed954c44809efb04bdb20b4fb2", "references": ["Flower nectar.", "bees nectar.", "convert honey.", "nectar flowers bee.", "nectar flowers.", "nectar.", "what is nectar flowers."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-75a33fb15f5542b9a3491f5a07c45fb6", "references": ["fluoride drinking water.", "fluoride in water tap.", "fluoride water.", "fluoride.", "tooth decay can be painful.", "tooth decay cavity.", "tooth decay."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-18e5748e7d7f4f9f913859c65d8da04f", "references": ["genetic diversity creates evolution.", "genetic diversity increases.", "genetic diversity leads to.", "genetic diversity.", "reproduction increases diversity a good thing.", "sexual reproduction genetic diversity.", "sexual reproduction mammals.", "sexual reproduction procreation.", "sexual reproduction."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-cf602b5d09a341eea454b4faa44f0c82", "references": ["animal birth young.", "animal mate.", "animal needs mate.", "animals attract reproduce scent.", "mate reproduce animal.", "reproduce sex.", "reproducing.", "reproduction is needed for survival."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-67ff65e894e34c89af159a0b4065dc93", "references": ["create electricity.", "electric heating energy.", "electricity electrons.", "electricity heat.", "electricity is energy.", "heating helps humans live in cold climates.", "nuclear fission electricity.", "types of buildings."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-1f1ce0f36ada44d9a575453ebd3273d2", "references": ["day and night cycles sleep.", "earth day night.", "how do planets rotate?.", "humans sleep at night.", "mars planet.", "planet elliptical orbit.", "planet rotating.", "planet rotation cause.", "rotating planet time.", "temperature changes day and night."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-d0869279682e4608a14eb3eb8e7f9d03", "references": ["claw paw.", "claws owls.", "claws paw sharp nails.", "claws prey.", "mice prey.", "salmon prey predator.", "what are prey."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-13c9f1fab12247849b6e31e16965bbf6", "references": ["freeze.", "freezing water expanding causes.", "frozen water takes up more space.", "ice is frozen.", "pipes freeze burst.", "water density.", "water expands.", "water freeze degrees.", "water freeze.", "what happens when water freezes."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-33208cf1b0ff42559c04e40cb5ddbf26", "references": ["bacteria spoil.", "bacteria.", "coliform.", "food spoil.", "spoil.", "spoiled food.", "temperature bacteria."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-2df1080d30484b458aaac528984a224c", "references": ["Himalayas located.", "folding tectonics.", "himalayas mountains.", "rock folding earthquake.", "rock folding."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-ae1f7a3b4b9c476faac168f9de878dd9", "references": ["breathing respiration.", "breathing water.", "fish gill.", "fish gills.", "gill breathe.", "gill breathing.", "oxygen in blood."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-be679ce566fb422f8575765a5fccb1b1", "references": ["hurricane is a.", "hurricane saffir-simpson.", "hurricanes damage buildings.", "hurricanes.", "hurricans.", "mechanical energy and heat energy.", "mechanical energy."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-701e4df1bc464cb09fbea6585b683c08", "references": ["conserving energy helps animals survive.", "energy is heat.", "hibernation energy.", "hibernation sleep.", "hibernation winter sleep.", "hibernation.", "squirrels animals."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-bca3d737fb354203b04a94d1cf670598", "references": ["across bodies of water.", "boat people transport.", "bodies of water oceans.", "bodies of water.", "cross bodies water.", "ocean ship large.", "ship across.", "transportation moving people."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-a31a62e1dac54798a3e7cafdb2193d51", "references": ["causes weathering.", "mechanical weathering.", "sediment dirt.", "sediment is made from.", "sediment weathering.", "weathering rocks.", "what is sediment rocks."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-1fc15c4192c34784b3e46285c4e0e262", "references": ["cells tissue organs liver.", "cells tissues.", "liver organ.", "organ kidney.", "organ skin.", "organ stomach.", "organs skin.", "tissues form organs.", "tissues function form."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-fcb1706f71934d34a8d3d82b29ae17f0", "references": ["folding an object.", "folding chair.", "folding objects.", "folding origami.", "folding paper origami.", "origami fold.", "solid objects."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-0a23792de4ff498f9a6bdf341ff34483", "references": ["materials.", "microscope  is an optical tool.", "microscope observe materials.", "microscope used.", "observed up close.", "observing materials is used in science.", "optical close up.", "optical tool glass.", "optical tool microscope.", "optical tools see.", "optical."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-2a8c003b657741249c7177407aaa98e1", "references": ["Light is radiation.", "black absorbs light.", "black coal.", "black light.", "black raven.", "light are photons.", "light energy.", "light is energy.", "light rays.", "melanistic animal black.", "object item.", "sunlight light.", "visible light spectrum.", "visible light."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-781d85d5877943788d37ca0c579c2edf", "references": ["Gene flow.", "gene flow diversity.", "gene flow.", "individuals people person.", "migration move called.", "population group community."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-b780230c278144828826e7dc32c5c633", "references": ["black widow web.", "food.", "spider arachnid.", "spider capture food.", "spider web cobweb.", "spider web.", "spider webs are made of silk.", "spiders are arachnids.", "spiders food.", "spiders have eight legs.", "what do spiders eat."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-7a7fa3487b494c06805aab2044267d0b", "references": ["sessile immobile.", "sessile means.", "sessile organisms immobile.", "sessile polyps.", "sessile sedentary.", "sessile sponge.", "sessile."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-58a76504d6f04694b36c3be181de0e87", "references": ["antennae ants.", "antennae insects.", "antennae.", "insect.", "insects antennae.", "sound is vibration.", "sound waves."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-34e8b2237c404030b0aff618c861199a", "references": ["blood vessel vein.", "constriction blood vessels walls.", "constriction blood.", "constriction contract.", "constriction squeeze.", "constriction.", "muscular walls of blood vessels contract."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-ef0387c3de8242fe8073e95d6c2a7c5c", "references": ["Translation genetic code.", "Translation mRNA.", "protein acid.", "protein compound.", "protein mrna.", "translation protein."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-2f95f72007ad406ca87425485d33fe48", "references": ["adult tunicates.", "tunicates squirt.", "tunicates."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-e44ff221ac044bceab4e173c98b74672", "references": ["\"light bulb\" \"traffic signal\".", "converting electrical energy into light energy.", "energy skin cancer.", "light bulb electrical.", "light bulb energy.", "light bulb photon.", "light bulb.", "light energy."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-c5a434f3e07c446f882ffc8aeb4e0120", "references": ["cold front.", "cold fronts air masses.", "cold fronts.", "thunderstorm fear.", "thunderstorm lightning.", "thunderstorms create loud sounds.", "thunderstorms lightning.", "thunderstorms."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-b428ba8895ff4c388f81873e9e1c00c6", "references": ["Algae.", "algae nonvascular.", "algae plants.", "algae produce.", "algae.", "aquatic ecosystem ocean.", "aquatic ecosystems.", "ecosystems."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-6bfabac32ce24293b18a22bade0c302e", "references": ["atoms are small.", "electron microscope lens magnify.", "electron microscope.", "small object naked eye."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-f4af5cb148d34ab49c31bad04cfc72a1", "references": ["car brand.", "car pollution damage.", "car toyota.", "pollution causes.", "pollution illness.", "pollution kills.", "pollution."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-f44efbdd1d704fbeab1c74cd0ce6218b", "references": ["measuring pounds.", "measuring weight health.", "weight grams.", "weight is pounds.", "weight kilograms.", "weight measurements.", "weight pounds."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-276c2982505540489145ab44882961af", "references": ["Plants producers.", "earth our planet.", "everywhere on earth.", "photosynthesis plants all.", "plant earth.", "plants live everywhere flower.", "plants live."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-3f94a24ab9fe4df7a1d17ae0b1340985", "references": ["a root to emerge from a seed.", "force of water wave.", "force.", "osmosis force.", "root seed water.", "root.", "seed acorn.", "water h2o.", "water seed root."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-51b25c27f6b74bf8bac349916720b53d", "references": ["earthquake cause.", "earthquake earth.", "earthquake seismic.", "earthquake shake.", "earthquake tectonic plates.", "earthquake tectonic.", "earthquake.", "earthquakes damage buildings.", "ground shake destruction.", "shaking ground damage."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-1292bf1eea9b418aac2704a5167fda5c", "references": ["refraction of light.", "refraction speed of light.", "water hydrogen oxygen.", "water is liquid.", "water liquid.", "water refracting light rainbow."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-ef0a861be2af451b8b74e35e2019872f", "references": ["current electric heat.", "current heat.", "electrical current electrons.", "electrical current heat wire.", "electrical current is kinetic energy.", "heated wire.", "magnetic field wire current.", "wire copper.", "wire heat.", "wire heats up."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-c97b1a231d564fe3a33f40acfcba8408", "references": ["female monotremes.", "monotreme.", "monotremes.", "uterus and vagina procreate."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-726bb202e1f24c759838cf98242acab7", "references": ["black hole.", "collapsed stars.", "high mass star.", "mass star.", "star celestial body.", "star collapse.", "star sun.", "why a star collapses."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-d5ff12e911a94c489cfabbf1016b73b1", "references": ["Length of object.", "length.", "measure length.", "object length size.", "ruler inches.", "ruler length.", "ruler measuring inches length."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-eaf668a6ee6d42a8bc653aa244fd3c62", "references": ["erosion creates mountains.", "erosion rain.", "erosion rock.", "erosion rocks.", "erosion soil movement.", "erosion."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-56bfd03b33a14aa39e40e78def04f509", "references": ["backbone is a spinal.", "chordates backbone.", "chordates.", "reptiles are vertabrates.", "vertebrates."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-33e3a1f6958b4d1b99ee9fa7da89d460", "references": ["cancer can cause death.", "cancer.", "carcinogen dna.", "carcinogen smoke.", "carcinogen.", "dna dexyribonucleic acid.", "dna.", "mutations dna."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-dfd12d51883d449385c3391a56b7a9c0", "references": ["brainwaves during sleep.", "healthy effects.", "naps sleep.", "sleep dreams.", "sleep healthy be lifestyle strong.", "sleep process.", "sleep."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-559d1fea4ea34e50b60153cb471af263", "references": ["Evolution.", "characteristic genetic traits.", "characteristics traits.", "environment causes characteristics of living things change by.", "evolution ability fly.", "living things trees.", "living things.", "mutations cause evolution."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-952b8d7f9f154ea2b193e720fc9f34c3", "references": ["a balance measure weight.", "balance mass.", "mass grams.", "mass kilograms.", "mass of an object.", "measuring mass units.", "object mass grams."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-5db89fd20a994ef186eef15a3cf59f3a", "references": ["Cells divide.", "cell division.", "cells divide.", "cells programmed.", "divide means.", "divide mitosis.", "programmed cell death."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-ab22f93f11e84dc79975b8f887f7af5f", "references": ["a pollinator is a bee.", "bee pollinator.", "nectar bees attraction.", "nectar plants.", "nectar.", "pollinator bee.", "pollinators.", "what is a pollinator."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-b39f4c57c7cf451d8e04ed41252e4585", "references": ["Pressure receptors.", "animals skin.", "cold sensors in the skin.", "pressure receptors nerve.", "pressure receptors.", "skin dermis.", "skin epidermis.", "skin largests organ.", "skin receptors."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-f305c7d6c0da408b91f814298d594f8f", "references": ["Air pollution caused by.", "air pollution cars.", "air pollution circulatory.", "air pollution dust.", "air pollution smoke.", "air pollution.", "pollution breathe.", "pollution smog.", "pollution.", "respiratory lungs."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-4315b3d99cd448078da45644812bb133", "references": ["hardness minerals.", "hardness of minerals.", "measure hardness of minerals.", "mineral calcite.", "mineral hardness moh.", "mineral hardness.", "pearl is a mineral.", "requires necessary.", "scratching diamond.", "scratching minerals damages them.", "scratching minerals hard."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-79f54512234a4e248537e95917cacc16", "references": ["Disease is illness.", "disease flu.", "disease ill.", "disease negative.", "disease organism.", "disease sick.", "disease.", "diseases can be treated.", "organism animal.", "viruses can cause disease."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-f220cbed0c6d47b48347515904b74eb8", "references": ["genitals reproductive.", "reproductive organs are gonads.", "reproductive organs worms.", "worm development.", "worm platyhelminthes.", "worm reproductive."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-eca1f1793d84499baa97f8c223e212c8", "references": ["Bird eggs have hard, calcium carbonate shells.", "bird beaks egg.", "bird eggs contain baby birds.", "bird eggs shells.", "bird hawk.", "calcium carbonate.", "eagle bird.", "gulls."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-f7e6208717a540ec9313d385210c6a40", "references": ["bird nest.", "nest twigs.", "nest young birds.", "nest young protection.", "nest.", "nests are made from twigs.", "nests cotton."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-a9328252a30f49678e4e8ba2c4dbe750", "references": ["\"body heat\" reduces sweating.", "Sweating is.", "body heat necessary.", "body heat.", "sweat evaporation cooling.", "sweating body heat.", "sweating body.", "sweating perspiration.", "sweating perspire.", "sweating water.", "sweating."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-0d8941e0c28945df9dbfdbd0108df569", "references": ["ecosystem habitat.", "ecosystem lake.", "ecosystems habitat.", "energy from sunlight plants.", "energy sunlight.", "plants energy sunlight.", "sunlight energy.", "sunlight."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-9101dc65d1b24325b0e34ca3813d32a1", "references": ["cause sound.", "humans can hear sound.", "matter is made of.", "sound decibel.", "sound ear.", "sound is detected ears.", "sound strike object matter music.", "sound vibrating guitar.", "sound waves.", "vibrating sound."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-6d0fa330fca34057854ab9dd55419137", "references": ["erosion soil quality.", "soil erosion causes.", "soil erosion soil productivity.", "soil erosion wind.", "soil erosion.", "wind air.", "wind hurricanes."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-e3eeef2985e242ecaab34a02d3541a3b", "references": ["Viruses reproduce.", "influenza virus.", "reproduce multiply.", "viruses disease.", "viruses herpes.", "viruses hiv.", "viruses influenza.", "viruses make people sick.", "viruses rabies.", "viruses smallpox.", "viruses."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-874655414ddb436797278deba8c164fb", "references": ["Ethanol drunk.", "alcoholic fermentation.", "ethanol fuel cars.", "ethanol fuel.", "ethanol is a fuel.", "ethanol is alcohol.", "ethanol run.", "ethanol.", "glucose sugar."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-f846e78a1e9245a88a535ba715656106", "references": ["blood.", "deoxygenated blood.", "deoxygenated.", "veins are arteries.", "veins blood needle.", "veins blood.", "veins carry.", "veins heart.", "veins towards.", "veins."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-6b5544e0a1504f8b854dce75cf0c6c53", "references": ["consume eat.", "insects organisms.", "organism species food.", "organisms animals.", "organisms consume predator.", "organisms living things.", "organisms plants animals.", "species consume."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-08c4850f8b60444e913ea8bdb9170e60", "references": ["human organisms.", "offspring are needed to continue a species.", "offspring are young.", "offspring next generation.", "offspring.", "organisms animals.", "reproduction fertilization.", "reproduction organism.", "reproduction.", "rise to offspring birth gestate."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-7ed8bd39048d40a0a2b53f9d6d1f0f75", "references": ["Plants have specialized reproductive organs.", "organs.", "plant bush.", "plants bamboo.", "plants daisies.", "plants reproductive.", "plants trees.", "plants type.", "reproductive organs gonads.", "reproductive plants organs."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-0e6533a8353e411d99cecb2ea91b1dd3", "references": ["freshwater biome.", "freshwater biomes.", "freshwater lakes.", "freshwater.", "salinity salt.", "salt sodium."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-d577de7a49214420a1a8dc89e00cbcbb", "references": ["healing wounds.", "wound healing.", "wounds cut.", "wounds.", "zinc good source.", "zinc wounds.", "zinc."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-85b7eee36f5a4ccdb0360cb7e7392b0e", "references": ["dinosaur extinction asteroid.", "dinosaur extinction event.", "dinosaurs extinction.", "dinosaurs.", "humans are mammals.", "mammals ancient.", "mammals dinosaurs.", "mammals dogs.", "mammals human.", "meteor dinosaur."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-940228173385417393c89898b251bda2", "references": ["Environment.", "environment.", "landfill dump.", "landfill garbage.", "landfill trash.", "landfill waste.", "landfills contain garbage.", "landfills garbage.", "landfills hold garbage.", "landfills trash.", "landfills.", "no recycle more landfills."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-f2476674f80c432fb2fc8e8689f4e5e1", "references": ["Friction is the force resisting.", "fish predators.", "fish scales.", "fish trout.", "reduce friction speed.", "scales keratine.", "scales protect."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-5b38c25d174945e3822c1838259e3fbc", "references": ["animal  mitosis.", "animal fission.", "fission nucleus.", "fission.", "fungi fission."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-8c75f5829a874ff4b1df79c045ae1c11", "references": ["arsenic poison.", "birds living thing.", "living things human.", "poison arsenic.", "poison cyanide.", "poison living.", "poison organs.", "poison toxic.", "poison venom.", "poison."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-3926996fa8414e77a481aa6264045d78", "references": ["Sugar is raw cane.", "food taste sweet tooth.", "fruit sweet.", "sugar cola.", "sugar glucose.", "sugar honey.", "sugar saccharide.", "sugar sucrose.", "sugar sweet taste.", "sugar sweet.", "sweet food causes.", "sweet food is addictive.", "sweet food is unhealthy.", "sweet food sugar."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-1707264fb2b34a9b8094fddc13c5b71f", "references": ["Roundworms.", "parasites harm hosts.", "parasitic.", "roundworm.", "roundworms dogs.", "roundworms."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-a0378cc8ea1644eca16ae8a0a855e17e", "references": ["a compass is used for.", "compass magnet.", "compass navigation.", "compass north magnet.", "compass north.", "compass point.", "magnetism field."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-328b3a4265984d13a3d33f3b66b31a7e", "references": ["animals migrate.", "magnetic patterns.", "migrate animals north fly.", "migrate bird.", "migrate birds.", "migrating animals.", "migrating ensures species survival."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-6d7d880cd0b04d71a5b44d23b4756b56", "references": ["DNA offspring.", "DNA.", "dna encoded.", "genetic trait example.", "genetic trait eye color.", "genetic traits eye color.", "genetic traits.", "phenotype characteristics."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-ae651a2077614c4eb384d30671e8eaad", "references": ["marine habitats biomes.", "mollusks are snails.", "mollusks."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-7b684396d90a4e7cb0cd5e8b7746fa03", "references": ["allergen.", "allergens pollen.", "allergy causes.", "allergy.", "antigen allergy.", "antigen."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-53dd88203c59422c9cd995b972fcc32a", "references": ["activation energy enzyme.", "activation energy.", "chemical reaction fire.", "chemical reaction.", "chemical reactions.", "energy to get started.", "heat is energy."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-471ba8285e914beea312a2b21e8585f5", "references": ["Seasons are annual.", "autumn season.", "season summer.", "seasons cause temperature change.", "seasons cause.", "seasons daylight.", "seasons spring fall winter summer.", "seasons spring summer fall winter.", "seasons spring summer winter.", "seasons summer winter.", "seasons weather.", "seasons.", "summer winter seasons.", "tilted leaning."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-2bcf5e3e71f9474091dae4434d96058d", "references": ["fats lipids.", "lipids fats.", "lipids.", "phospholipids contain fat.", "steroid hormones.", "steroid lipid.", "steroid lipids."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-2ed032f4eee5446c96d589ae32e63462", "references": ["pelycosaurs.", "therapsid.", "therapsids animals.", "therapsids."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-b48b84aa860d426495d665eb1e0470c5", "references": ["Marine means.", "coastal waters.", "marine fish.", "marine ocean life.", "marine species aquatic.", "marine species minnow.", "marine species shallow.", "marine species.", "marine water.", "most organisms are marine.", "shallow coasta.", "shallow coastal waters.", "shallow waters are warm.", "water habitat."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-916df9a7cb0145888f41a36ada13f4ef", "references": ["example of gametes are.", "gametes sex.", "gametes sperm egg.", "gametes sperm.", "gametes.", "meiosis stages.", "meiosis."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-46cc31ed01f94beabf907937e9ee3fa4", "references": ["an animal that is hunted and killed by another for food.", "animals hunting.", "blind seeing.", "eyes see.", "hunting predator.", "hunting vision.", "owl sees at night.", "prey.", "seeing vision."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-471e8b744af14c87ad60bb0cdbf50274", "references": ["animals move quickly to catch prey.", "aquatic animals ducks.", "aquatic animals.", "ducks webbed feet.", "webbed feet."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-7f371dd0414c43dea22d03eca6c4367f", "references": ["energy atp.", "energy food.", "growth expand.", "growth.", "organism energy growth food.", "organism energy growth.", "organism fish.", "organism growth.", "organism plant.", "organisms get energy."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-29ed15f6b5cc4358abc7a02c6523e911", "references": ["migration Population size.", "species bees.", "species human.", "species plants.", "species survival.", "species surviving not extinct.", "species surviving."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-674d02554f474deab509491066f07880", "references": ["What  is Light.", "light beams.", "light illuminates.", "light is energy.", "light ray.", "light visible energy.", "producing light."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-ac99c624572545788b15696f2b0f0886", "references": ["animal competition.", "animals alligators.", "animals birds.", "animals compete.", "animals creatures.", "animals lions.", "animals pigs.", "mates partners."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-59deb5f758a04677bd2c153de99212f7", "references": ["Jaws make cartilaginous fish excellent predators.", "cartilaginous fish great white shark hunting.", "cartilaginous fish.", "cartilaginous shark.", "cartilaginous.", "jaws allow chewing.", "jaws fish.", "jaws teeth.", "predators eat animals."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-bd1b21f69e1b42a89f9796b5eff7ba05", "references": ["erosion alters.", "erosion is weathering.", "erosion river.", "erosion rivers.", "erosion time.", "erosion water.", "erosion.", "nile river.", "river nile.", "river thames."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-57e9b65c9af84e85a1f72056ba455441", "references": ["Rocks are formed.", "limestone is a rock.", "river rocks smooth.", "rock granite.", "rock type shale.", "rock type.", "rocks friction.", "rocks smooth erosion.", "rocks.", "smooth rocks slippery.", "smooth rocks.", "solid mineral rock."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task036-545b6b37f2b44850ac027cd3da0f621d", "references": ["allele gene.", "gene hereditary.", "gene protein.", "genes heredity.", "genes protein.", "genes."], "task_id": "task036_qasc_topic_word_to_generate_related_fact", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task1554-51f66136aaea4cf69971a3d67b57dc69", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-bba2b5c01f3d4a14823939b539a5f459", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-66d23fc2e8784a7e849e8db7374d6dee", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-80319c48e61f46aebdb14f8fe9899569", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-48c12773b1204a81aad071f2e5debb71", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-174202691e4a40f4951437d1ff036ecf", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-77f6443ce7fc4e6f9416b41c093bb100", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-d65d99566ad3482e8b08666541426e23", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-cb517e86b66647d0944fa51d23653ab7", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-4cfb340c50824dc5a6c2e56bb241b976", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-e5bc97841d25472b8b536625693c3941", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-12334051d5f2494882dbaa9bb9b59a7a", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-6bbea97f3694478781716028a928f655", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-68b863f62cb749bba9300cd40ce024f7", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-4c6eaf6f5fa84f219df82ce2e0a2e155", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-25b1d6c279f84aee95f5cea50a28cb27", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-398c6b3743a84ceaaa3067567b6b2d94", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-78bfd3cec00147af949859630606b49c", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-68887d6fc9fa4f63ab1d623ab9265b32", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-e99a16cd2d1b40b99648644e42f16648", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-13840a711212421fa613286a7286f82c", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-1398238a733441398f12b5ee795e05fe", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-da9eb36271bf4300817396d372511177", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-ef6bb1fd9d2b497aba78af1a61f949a9", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-fdd8f26b48f34349af70c4c6b6785f40", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-808b7c797943484fa2efc3f535db26d5", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-aaf0e214926649f5a37c6a311e7939f5", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-c3aac225ef6941678589768010527844", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-a0a3b15c79ca4e8da5945a435a2aeb3e", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-accebbeb01874fbebca1ccf00e4fc3b4", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-625f40db827d4b009c07b6a655e47cf5", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-f09955692cf144e79244636e2553e99a", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-44305c044f8a49de9f2b41e63ee38ee0", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-0f6d0d5f07ba418fad7c34d0d6cf5840", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-5c0606d195de4a7b9b7e39e0ee3cca48", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-804bc71ba4d54092a56a9a0baccccd2d", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-57d6562bce8e46e68b83d9f62376c548", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-7b4e8bba451842888c2fc203c7c53081", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-52f97f18310c41d380ba2b6253768788", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-76bd730c652746e98f27bb2d396c700d", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-795f7ad210ca4bf2a83c650132a267dd", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-b823ed0d96b74a38ab013c5d75d81a0b", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-7d18f6111dc74fa3b4a4d82976fe5f76", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-d004b4c91a784306ad7385e7b868e471", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-24a658b792b94b77ad76e5184399a59b", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-110734d18da443b1b32ff64118ee57d8", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-194c97be879343788a56d421652c24d0", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-395ab5b4e9a1455da8e1d6083002e87c", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-f5f7d38e0f3b42ed8383fd13cf540b20", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-0739088635664ffca13a58fd1a6df4bf", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-e27ba6f94a1d49c5a3cc1e529e443d57", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-ec30919ee6484639b823514bc767edfc", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-a19be7345ed2485391b46e1a73d7b44f", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-9c3b69b914b54b44a6140074d8c3f25b", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-e5857aaf8384455ca69f60656399178a", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-5d1cc75b372b47458cfa24396d63351b", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-20535b9cb0244640a9f320a905cd7392", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-21ac60c570fe45b9947f5aef3fae5bf7", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-5d62c2a79a674f4fb875af2c61961277", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-9ed7106ac0db4afbb141f042e162ef22", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-2ed050fe000d40cd8f0b60e15f66838f", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-19ef114c1cac40bb84fce90cca251d88", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-b6da9fb419734800986d2c8a6fd11ee9", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-7a35a1d1507c4b1eafaaa4a63c66eeb6", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-ac36b9e47b1f4652aa87bc5efcd42ad6", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-6526d81d061c41fcb95470cfea380b97", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-f2cea3b6748f4ba4a10b15b3964086b3", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-41f9efc7028549438d490f792fb871d1", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-cccc7f652fcf480daf1af3ae1b3ac1ee", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-2253db59854445df9fda62ce619d652e", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-75bfdc8b1fb54788a5d5cd95d108f67b", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-640d60b9b5e645acad515208ec720e47", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-1480a543cf24410bb981b8a2e83fac2c", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-f535700a2e6249f0a960c0f54f17dffc", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-7813eb58fc6b4b10ae5b550e1070e737", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-58d545a0ed9a4a62ab26e76d58da54a1", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-fc9081c0cb7f432d8a05b132d727e9d0", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-fbb09fcae758486cbd2efaa3c3830521", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-d1e46fa22cf84cffbbed7e1499ac5223", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-6638bbbc6d16479a84505c9488cb6e51", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-d1d7242729f345269d0a8de325446c9b", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-2497cb652f6c49239effb71b22d75771", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-b857982e6be3434e9f266c19e2e36bc3", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-3c4a0803646b457f95ef659f918f8c4d", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-82ea870aa7494ff19e4caa66c11122ef", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-ab0f055b99ab4ff9bf8e5d703b1ad463", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-fcf68389a65e4236ab2812e9941553a0", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-5e72012c25df45a685f98c4d774706b3", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-255c543a256f4207a9c97d3ff193fcd5", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-881a1bbeb3594781b8db119c53236fa6", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-f38cf97713394a81a5b73ca649227b27", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-9103a4be227b47c69ff395b2ded2e184", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-599b3cdfaec8448f985469db0c62dca7", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-564c8c3628674cc4aaf0ed917e0fa2f2", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-c99ce808fff24d5d9fa122ab72b4111c", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-606d94c450cf4ed383ddbf6049b3170f", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-b2fd257c5d694d68a924131f56bcd909", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-907ad2817d5748e9802d73fbdc7f401d", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-f593363a2d1b40d49ca2da28a66af401", "references": ["neutral"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1554-9ca80dd688fe4c23a2440beeb1b39dbc", "references": ["entails"], "task_id": "task1554_scitail_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task050-4dfe2578742b467b904bc635585623b2", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-e2f65f58be2846609faf185b07e5e3a6", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-731b5c95347044bba73541c136783bd1", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-869e5e6002cd47a69d819453fe5e33c1", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-7baa5ac9bf3a427eb05cba89d04528bb", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-f8fb02c0c14b4c25b363eae915260b34", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-60b271c4b2154683a84bbd41c84a8215", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-cb163ebc2327480ca4c173ba2d1828a5", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-93a0339666074210b9e625ebb29e2a95", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-501be2868e2b4ad78474f40b7ca086c9", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-e3a067e227d54b6f96240a78a232a9c3", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-856fca4f835949a0812962b9fbe9f5bf", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-3799c874b17842dba89bb0a357b86185", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-4baef6220f61475a8e5e1c2e5fe24ded", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-5234181d89204602b2247dd51aa61517", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-e3e60fb798384370b08f0418750516af", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-078e69a0a4024dfc8126a435d7f4c75d", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-b4cb1defde1d445d89c266c598a2c31b", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-76580f9d65954f058710a86d3e00eb54", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-eeb17fa50223432bb94bc76b8c39b30d", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-15c4f24bbdba4dffb7f120c6470ca861", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-dfc4a7889da54e398033e75ac4fbc6fb", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-33addad40f8f4baa9cc2b6360070b7a0", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-435d143df7df4f078a3c8ce038d44bea", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-61020949c1d5474687a93ab3f1e668a0", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-867cc269631348979ccb2e487b71c96c", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-d60586174daa498a877b214b399dd19f", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-3abe5484fe9e4bfbbb342771ab5552f9", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-88a18298c6804602bea44cd7d0807541", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-55f37eee2b5d46048ed83ece21c23f3b", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-e63bf415eda849778e044a36be94d099", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-82f653cb9b1e4204b15b153fe05b04a1", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-40229069619c41f484e526b5bf421365", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-09d7b50447554b7baa2772f3bac0e2c6", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-2dfe82ec187941818154a37e7c6a1dd2", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-96b020b341214df786448440e701df86", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-54f00dff0b07469692309d690b69de9e", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-a180d917b1634da5a1ffd106181fe08f", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-4c5e2b3b58ec4d01b6848b3ab1c713df", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-ac2bd022f041444185a6a40aa607ec3f", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-7133bd39f4bd495eac90d8ff8f480402", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-32cac46c33054704a7bb95311a937d1f", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-fa94866b06db4c3399b769093b344dd0", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-bb25eeda67b749be973f765fe350badb", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-ad8995afeb094174a9554b9d92b5b2b8", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-835602640410415aa7bcca73e79cf44b", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-db62c196f5c7485fba622e23d15a798b", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-aacf1113f591465cb3064549559f0b05", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-56462958e4a94a578d8370bc311070cb", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-0083340c5ec94d598415b673b7633d9d", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-aa3e0831142743d5b7a9faa1dc46d93a", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-257785b0f4c6466487e063207077a310", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-b4c65ec5b8a34f92ac9d524bc64849b4", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-50c2e467e2d04588b6c0cb61c5cdca29", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-dd3c02830c17437084e1002c48c141d0", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-a922b9f9bf354a91b16460b807df04c1", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-2afbe866e61241dcbbf7a236d172293f", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-ed657b905c3e42c39f8bfeb544fa0294", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-dd0beed9bd8a44c0917ea448a1198b6b", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-7c7d0124090248a5b4c222e3b4a5ce83", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-9a54695b449746a7a516504bed0e285c", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-0c8d9430c1f14acd801c0c515f2350b0", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-29ebcfdf55e143ff9dd332370227b065", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-27211bf50ddb440eaf0da982d3125f53", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-7d46b7067289436780339f0b9560ecf1", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-c70f7cf5fa3d4e628a8489c58793df6d", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-c85b9f1d129941dd86ea3b03fd061969", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-043609b317ef408ba490b543868a53f1", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-a91bdbd33bdf4d63bca7acf455d29273", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-602bd111d2844380a552c3d294724f19", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-6fc6e37c87d047ecadc45a30a3c26368", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-5eee2bf566ec4746b1505f32bd3771d2", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-64f840c25b3b44b78aed02606021748e", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-495160182e2e4b7cafbed751a071d590", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-47247f08d3774f6e8e2a096d925d45e2", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-af05154f53e84ec9a8abf248234c249a", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-526de3e5f862451bbd701d6d5a143d6b", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-6b6ff88333ee458ba2f006758ba8368e", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-4d3cf3af6c754fba88021d611aa88695", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-958319bf5dbd4859a1cd872169b7a876", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-2618d2f2986b4e999ff728c82802f9a7", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-e054180625a54292b63be46db023d212", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-2c650e72245e44d6ab4a292c8ef99d93", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-7bb853302b774204b9fbcf3137bd88ce", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-3d8f6f7fdb2642daaefd6853f11e0d9d", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-106637e2fc88401cadfe061b9612cdd2", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-5484ff64a43940dd9ec0901ac718d573", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-4d231011e5554bffb82b0a33f08702d8", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-32f5c8844f73442d8be1cfd2aecaa696", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-960f44d2be7d4df6bc1b776c80ec606a", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-7cab4bba09cc4b4890d6622abdccbe7f", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-2640ddae583e4f3dbd218672c5457f17", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-d7fc6d6e7c9a4d728a80d975f094a766", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-2cadc82813e44ecdb1bf23a8c3ee781c", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-6dca9c737e804fa6a61d45fa9c0de287", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-dac57083fcb547f297c0416933eeb60e", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-9501acb5b5dc4c87b51ae1069766cdc8", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-2ebd45fde8444c92927e0bc6e8d395e2", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-86b3f96d2ef2434ca0ccd6f89a4aedfd", "references": ["No."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task050-e3eafad27ce846a9ae1bfe08aa3455c6", "references": ["Yes."], "task_id": "task050_multirc_answerability", "task_category": "Answerability Classification", "track": "default"}
{"id": "task362-0e429342794747918143b4a9d5d49f6b", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-dc47120e0be0402abd50bd745dbbb790", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-784b39451a6e4456a99872de213ff778", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-e5d0b9db96a34835b600c6eb3adfaa29", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-4f82976d432147d38427c9d3c603bf04", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-6d192a788f1e486288024d1bf22681ac", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-e240e0810ace423d9848807b103af8e9", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-6c46298d047646f8b411578d67f7aad0", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-257ddebba2f04e17b8194b350a53a265", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-c3d524bdf0ce4c4e956639188ab23576", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-55b3206576374c20a27e9cb64bb3b14a", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-e1065a1dd92f4639b0fda6dda50c4553", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-e2754130033948bcb2bf8cb2d696ab7c", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-9d05f9d418b94b4f9d2be887f79de518", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-1dadc339c11146428a2695ab68c6757d", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-533716e3ecee4c0399611a0399cfb005", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-21dfc4e278684bf790d61a8220ce5685", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-23ff14b3046b4e2faec518bca80b5bda", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-7489089bb4bb47e8a6245ceeda6be520", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-ca394912b84c4862b81741965e14cee1", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-615a95dc5610407d98a2ac4fce11bf5d", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-da22c519ce2140cc82a3a3170ccc9d19", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-0b836e3624984d43bcdd560a6d20e6a0", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-3a6b29ff4e5440179b671aac7b94a0ce", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-40b0b85e7e1740438116792e4b099ca0", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-b14d61f044e44e94ab9e778072e7e68d", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-d4c53b8935bc4576aba9426eaad29001", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-3267095fc16c4d61b56f97b6e4f213c2", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-0b48a873deb149d7901349fc909c9408", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-7f84dd47c7ad491ab5f9e3e7208c3fe6", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-d08ec3ba4d38422991fb9c0bf7c72eb1", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-c4fe88b5229f4429af308eab1a0a6280", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-40296cdcdac548a199b5e6de5e7a6e5d", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-e2adcc19a59c4047a6aa04af219f1953", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-4465fb70adad4c49904fd109a9a015d5", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-47db4cb7edb740218246967ac6e5c802", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-28fe397bd0aa44439a5c1e629211007d", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-34175e5e956047f295b97480ab73ec4c", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-aab64dc1aeef4f85a692a8641e24aa54", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-69fdd5d3226a48019ce64231562e06e4", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-d23bea76247548528b1146baac5ade17", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-859856b7d0684376b71bc86028ac2799", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-82b4c81d177742e29ae652c42e941363", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-f46e9fb6cf2c416381067489bba5805a", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-6f6338aff4364e62b83a519ff53ea09b", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-7c3c1a9689954e5c9e4ab75718655880", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-0a4767d98bc1496c8c1db5c964e6eb6b", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-6bd74ef9e62942c2be041b2a6e72af66", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-84223441e1f0486585bf0f5c407135d9", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-37eae16239ff45b78241407134e8cd15", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-e46d61f4805d4a65818351801e085ac8", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-885a0788c9c0478a80536e0820ad6923", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-63dcd3caef0449039f29e52314f39bc2", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-b16ccbce9b4745bca2c49726b4546c45", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-1224c909e50b4841a65f7e481965c394", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-ed268f2cc61048ddb2507da4ae18022b", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-52c60d537752408db3cf69a3fb7df95b", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-7a94e17f8fee40a1b10a3a12005250b2", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-efe5b3bd998c4af58411b46a7184bcb8", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-22b1d284874746ccbb89158b01ec01fc", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-703c8005bf974e7591aa084384466e8d", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-8e0724b7ec5a459f9a2a4df94dffae76", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-02c5070bface4618a028c9bd3b7c929d", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-52e4839a3f9b452ab9a6ab112b7afb9b", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-7c2c8dfd3bde44fe97275158200fc97f", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-ee8ff70e17ba4c5cb4a51cf10bfd02b8", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-6de27ab16768415ea4e80033af1b1b4a", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-b8358f43e1a54a889f6d1fd5d95b49b6", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-aebfc240948d4b878dd606486ef25b51", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-ec0d2c1f616d4142bc2278bdbc5d1aad", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-04c00e1f89c34b9a96d77a25f4089d0d", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-bd52b597c09d4b7cad06a0f8c71036f6", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-43a821dbd06f409b805422eb184c23c9", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-6bc77b62064940c3aa2188645b822abb", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-aa17afc3af3546a0ba0bdd3dfe7f3696", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-87687d0659b8446fb7255da473ca1242", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-a4fa9a53847849c1a83b300eb5f2014e", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-1865b53c50ea46fd9c8cd90e2b87d4aa", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-4e15773d36ee45e0a441d560225aeb08", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-3dda625edf694439b4f1911c1bf3003c", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-d33dc4ee3f6948d0809cf0bb213d1fe2", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-d356b650fc044c80bdbfed48162b5706", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-57532eef1f4b4e27adef17bcb9d717be", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-9980b1e1f25d40398ed628b78fe306c2", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-925c26f581184dae8d82c213c80439fb", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-90477168c14945f6924feff6fca361d7", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-3b01199c5357419ba304ad630a06c828", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-e6cbde4b2d454399bb7748c547c8ffa9", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-920a663ad1984a7aba2696fe7382eb54", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-e746b8c10d404044a92f7659a0768c50", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-7f0c5f41b40a4b8d93ceaa8e32cbb251", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-4bfd52b766ab4aeab785af8c86d2bcba", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-154fee94e5904434835aa1fc7a2aaaa0", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-d33f2530af284382b0694fc760d23047", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-183a6f3a76e84534b16a8e81b0ca6424", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-d14b90b22336425f9e98610ff65aff55", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-b547181e576e4d38afc1e9ebb24ba64b", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-c047b3b7f0b440b5906219fdf010d868", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-0c3cc3f718fc42779f3dec4bf6ed19c8", "references": ["Response 1"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task362-d87c7467c35e43e199e0e9af7b2c9eb2", "references": ["Response 2"], "task_id": "task362_spolin_yesand_prompt_response_sub_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1557-a6d58316088847ec8686dd8ed72668e9", "references": ["Furthermore , the professor denies the reading passage in which it states that treasure does not exist and it is just fiction . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-968301d14ea546738a9de9d84c6ab717", "references": ["Then define your goals and work hard to achieve them . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-462e95d7cd1146139d0f6c82d34cca94", "references": ["However , people do n't realize they do n't know how to do it better or what they ca n't do . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-bc681e4114ec4edbaa265dffc096946f", "references": ["I think it 's harder for a successful person to risk something becuase they could lose much more than others . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-b9cae74339f143fcbfae61bfa15dd09b", "references": ["The student must be capable of understanding ideas and concepts , and , at the same time , know the way how they must be developed . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-984020b92a5543d7bcd8eb52dcb91f43", "references": ["For example , nobody is going to ask his personal doctor , which he sees when he has a flu , if he can also do a heart surgery or transplant organs . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-0867ba080d58488f811d739e2eea29e0", "references": ["I think that it is not always fair because not all students work in the same way and with the same responsibilities . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-8ff1269848f1444787dae419b99018ed", "references": ["However , there are a lot of explanations about what the placebo effect does . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-0c4603dc26744a82a7a94399eb0ef649", "references": ["Finally , the third piece of evidence that birds use a type of internal compass is that birds have crystals of the mineral magnetite embedded in their break . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-4cefc465a4ab4244b8d26f5b90bc775b", "references": ["In today 's world , we have just developed the first aspect . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-48a0ba6240ff4cf1b30d5608842a2ed4", "references": ["However , are these things all good to people ? "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-a349639b73434f30950ceeaf7060254e", "references": ["That is to say , not all old things lead to the failure . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-a387e89c1e0e4392bf0b83123059be2d", "references": ["This will reduce the pollution caused by the carbons from the cars . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-4dad4e5b749147bc9b18552df8c5ec51", "references": ["Recently scientists have been working on a new generation lie detector that can perform brain scanning to find out if a person is telling the truth . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-e8f2aca872d446b089d6fd15613cad72", "references": ["He actually changed the way he played in the latter part of his career , when he won all those championships . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-a03288cacf8b4d0f9bf98c1875e71033", "references": ["This will affect exams . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-8c43c9d7d33b49a0a28b94201e8ece86", "references": ["Fish farming uses lots of special products such as fish meal . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-c636ecd6c21041c38abeea98dbf3bcc9", "references": ["Third , the professor agrees that the gas prices in the United States are cheap , and they should raise them to save the environment and people 's health . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-46a518ac5590424bb57a2801909b2114", "references": ["And you stand nowhere on any subject . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-1d8376aae51342dd92cd83deb9544647", "references": ["This has to do with the signs of our nature that we encounter nowadays . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-74c5560092294cf69860e82dd8290dc1", "references": ["we can have glance on , what is happening in the world through accessing internet in mobile also . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-b86c45563ff7453a927e4e6da9ec901a", "references": ["However , all of them are limited . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-6d45fb4ab7cc4eb6b49a4fa6546df935", "references": ["His success is because of his new endeavor . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-c15d75e4e105461da21318b2c5b4337e", "references": ["He mainly concentrated on the questions that were asked of him at the time of the interview . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-1f409a77b0c54f05867dd8c733d0c090", "references": ["But as we are young , our body generates the new cell . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-54fa46d5c3ea45749231e26376d91c1d", "references": ["I will explain my point of views in the following paragraphs . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-920cd11fa36140a8a9acaa8334fed1bf", "references": ["Therefore , I brought a car immediately . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-5001333b21f0495294184db3d0325b5b", "references": ["Many people are willing to buy these cars . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-68d63b847db341548d3cea7097c7a4ab", "references": ["I never have stopped myself to think this , but this is a real possibility for the future . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-d5edaf1079ae4a17a124d9de9833c17b", "references": ["He thinks differently than others and he has succeeded . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-be6daa650f1c4d03beb8f80a05d51195", "references": ["For example , it happened to me when I studied the concept of inflation . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-2f12c0a448904353a68874cf3b6df147", "references": ["So let 's not lose hope and let the scientists do the job , but I think being in a traffic jam is a stressful situation , it requires time and patience and there are many more disadvantages about the large number of cars . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-6668e8822fe6487881111e90559e3546", "references": ["I have several reasons that support my choice . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-aa12a4558f6441768a691f9eb4841a70", "references": ["After I consumed that drink , then I came to know that we should not believe the advertisements . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-9e616e81d5394b5db8aef64770e6ffe3", "references": ["The students must have improved to get the correct view of the grades . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-3990a4a662a641d7ac68c8ef557b0518", "references": ["The lecture mentions , Mongol court records of the time . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-d222ac83afa743318a12134005528f89", "references": ["In the commercials they are all good . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-669ac99324e145e38eaf7c2522dadb46", "references": ["Movies and other television shows provide a lot of information about real life . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-0a4e0f8960974b6c878ff2ac9027f207", "references": ["According to me , in order to start the carrier through its ' success , it`s important to have a solid base , that means knowledge and experience . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-17015d5125f1452d86c72d7144ea820b", "references": ["We are almost unable to show the feelings of people in writing , but in action on TV we can understand feelings better than in books . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-b099bcb5c75c4e1285ec288680097804", "references": ["Furthermore , my friends who are nineteen years old , drive their own car , but they always would like to buy another one . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-c482386f5cce4a1c80ae2cac227e1948", "references": ["People feel more secure and comfortable to travel with their own car rather than travel using several public transportation vehicles to reach their destination far later than they would desire . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-8978d2d4ec7540deb54edee2bd1bea31", "references": ["Menhadens are the primary source of protein for livestock and poultry . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-ff55c90fd2084ac790d0ccbad2f6bc9a", "references": ["For example , one man is a football club fan . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-f92d4fdd1b91481986cee9ccfff14807", "references": ["For example , they can play football whenever they want , but the elders cannot . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-a133854f04064636876906bb10596904", "references": ["This is how we are transforming our society in the present day. The consequence of disagreeing with this statement is that we are looking at what we want in the moment in order to have a great time . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-e546b2f35769457987bcfebb3df3dd08", "references": ["You often find people investing money in inventions only to sustain their own business . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-37692d5e6239466da77d2f953fcef054", "references": ["She know just what her teacher has told her but no more . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-992c270a684a42c98fbcdf5e7a1375bd", "references": ["Some like music and some like science , and some are talented at ballet while others are not . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-8b117bf7ef524cab8ddfa17eebc8fc93", "references": ["Parents have now less time to raise their children , so grandparents might therefore help them more often . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-9f595665ef224677902449295113f7b6", "references": ["However , there were n't any particles . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-3e2625a9de184295937397365303aec1", "references": ["When you understand the concepts and ideas it is up to you to prove them , and see if they are really what you have been told ; I think this is the main reason why students prefer facts rather than just understanding ideas and concepts . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-c6950aa150254188a34c0f1c31b9fc27", "references": ["The new things bring the brainstorms which other people cannot get . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-d798f952524943419cb61b609fa84bf4", "references": ["These useful skills that I have learned from real life are going to be the greatest gift to my future career , and they are skills that cannot be found in a textbook . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-a2caab94397349f7ac4d8e4cef3bc5c6", "references": ["So I think we can not live if old people can not find the science and technology that has not been developed . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-f58e8bb7ff484397aa45cd8bcc7cb671", "references": ["He/she wants to know everything about the universe , everything about God , death etc. ; these same concepts that encourage a lot of ideas but the facts are not found yet . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-7ef3ed96853a4289890353502a39bab7", "references": ["For example , if you are attempting to study arts and sciences , and are trying to get qualifications in both , you are an extraordinary creation . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-96f9c172c2f6418d91b0ef757a0c848d", "references": ["Maybe at the beginning , but time after timer other animals in the environment will choose them as their new food . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-64cc8dcecb274e5fa81af51ac6872c11", "references": ["They tend to restrict traffic on certain days , hours , and increase parking fees , etc . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-85fa0826eeb444678b8637e7b3cfd89a", "references": ["By avoiding this it will lead to a much more pure and natural world for our future generations to live in . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-00d36742ce044b73af2ef2924a7fd5d4", "references": ["Specializing in one particular subject does not suit our lives in this era which is characterized by diversity and innovation . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-fb5445e7b0ef452da5252d8e21752340", "references": ["In my opinion , if they start one subject , they never give up . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-c7615e3adf2d4e55935875d64801a050", "references": ["One car should be used by three people , hopefully reducing the number of cars during rush hour . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-ddb60d588cd34cc6a4c99ec1e03f9f65", "references": ["Scenes of violence can have an affect on them . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-d2e23492846c48209ef088ea534e4d41", "references": ["This economics system is the best . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-d2f0b78e8d4d40b39b4ab0a21e6aa503", "references": ["Briefly , sharing the cost of a vacation trip is a very good advantage for travel . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-419565fa1674484ba28cac2a4788ba26", "references": ["If the status of cars is still at the present level , something else will naturally take the place of cars in our lives , but history tells us that advancement will never stop . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-be760c4500cf49a7a17c98b82f6891b7", "references": ["I did n't have self-confidence . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-847f16e3947245718c436c178a6e33b5", "references": ["Every time when we meet , I will look forward to the new things . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-24dc9bfc876748e39f23391d57a3793b", "references": ["You can become smarter than other people . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-b6c10ec8759946cab1212a5447d183c9", "references": ["Also , most people prefer athletics to other arts . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-cbf26ac3769e4a0aa88de0320ea103e2", "references": ["One of them is the long term wastefulness of the process . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-65ad862b76534fea87169d1b9fc9be4c", "references": ["Once you get success why would you not get it next time ? "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-0664c7c668b14b10b2182215e16f7b95", "references": ["However , I would prefer to travel by myself because with good preparation it can be filled with joy . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-9a8c3f1a077b460d8ad30c73841297f0", "references": ["The way I see it is that the only thing that guides do is limit you , and even more when your in a group . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-a102bb7c723642e2b00ef02ad0642a34", "references": ["However , afterward I learned all sides of what happens in a company , such as marketing , accounting , leadership , financial accounting , business communication , etc . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-ac1fa3adc0c04fbe96b86f6dd9e4d5b3", "references": ["But that is a different situation , because at this stage , the graduate will have already guaranteed his/her bachelor 's degree . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-a2dfef9c475144e59e356cb39c3dd20e", "references": ["Besides , young people usually like new-fashion things , like iPods or mp 3 players , I can see that the majority of young people ca n't live happily without music in their ears and movies at home . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-5c5eb8aa8ae84486bded516acaf5bca0", "references": ["That is why it gives us a lot of opportunity to think . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-b2151e11ca7144acaa0b248ceb62414e", "references": ["So , Ho Chi Minh city will develop . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-8a70da5fbf4249c196f2eb58cf6f6a39", "references": ["The older man would prefer to enjoy all the time he has , remembering that we live only once . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-0edcc69264bb425398074a2d36b80784", "references": ["In fact , old buildings are costly to maintain . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-c9af9baf00d244708ce34e9958ae4b32", "references": ["The professor said that this can be changed to the different environments . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-3efb4cad10204c59829889569e0566f6", "references": ["Compared to older people , they do n't even have enough time to enjoy their lives . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-2abaac46dc6a43b1bce29806176d6e39", "references": ["In the case of young people , the best way to get better grades and thus have a chance to find better jobs in the future is to study as hard as possible . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-7f65ea9b626e4687a491c886cc7512e3", "references": ["Maybe doing what they already know how to do well is better than being annoying by something new . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-59fb54f7126f4f478db024b2247f454b", "references": ["There will be lots of members that have gone to the surface of the moon and they will land safely to the earth . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-d3acb84f6e924e08af1af0d068e802aa", "references": ["We have a grip on many subjects , we get ample exposure greatly and easily . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-73ff830416f84a93b38c91ffd2edab4c", "references": ["Second , the difference between the encyclopedia is that there a lot of viruses and hackers on the internet and one of the hackers could easily change the information about any topic . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-c28477de7784480f8c13303335e45fbe", "references": ["People who are trying to get into a good university study by themselves everyday while I play around without reviewing and studying . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-0a7fa5ff651a459799df2d42d12c6831", "references": ["I think that young people are not able to think as deeply about things than older people . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-f0fbaeb3ab1a4a7ea63f99f4b91410e9", "references": ["They explain specific points by using examples of the dinosaurs behavior and physical appearance . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-48443e80f9654ee7a1e35753f1c6e81a", "references": ["This saves valuable time so that one can invest in a more important activity . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-5e8be6342dca42fb99fe3c896623bfe8", "references": ["I understand people who think that funny movies are more interesting than others , especially when you watch a movie with friends because , it allows you to have fun with them . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-ff6fbea62b4543b3b56244773b00ca75", "references": ["If he satisfied his situation and did not make efforts to overcome himself , not only could he be remembered as a great artist but we also never meet many great music . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-83330ad4dfa74811bc2a6fc2912e0c89", "references": ["Raising taxes on gas may be the most effective and likely way to decrease the use of gas , for three reasons . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-2ad1d44fe070441b8f0a88d3a654f935", "references": ["There will be technological development in the future , but that will only promote the use of public transportation systems and not cars . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-d0d90339fc8549d1b58e52fd458c0f6e", "references": ["I would like to be the one to initiate whatever I do on my trip to get the most out of my trip . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-0587ef3dd82f41b38a49bc9908c09602", "references": ["Because you do not need to be a hero in order to try new things , you just need to want them and know that any result will be a success in things that you want to learn . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task1557-38e2560555214e0d9d83c60bee0e56cd", "references": ["And there are a lot of critics concerned that the required testing is too long to be valuable for patients . "], "task_id": "task1557_jfleg_answer_generation", "task_category": "Grammar Error Correction", "track": "default"}
{"id": "task249-1bad37a954af490c99aba65f90e4ec27", "references": ["jake"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-f248452e2065449b816010a07a2061e5", "references": ["jane"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-0450df3205b94ef3a644d85c701d647a", "references": ["the rodent"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-bf408a604b134ea5b3e1a7ce295bca80", "references": ["mary"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-3c85e478cb4540ddb7f771a25e570c54", "references": ["the arena"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-4e57248936d64c6682e539662f12bcf6", "references": ["the articles"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-52815bfec42b4a3891854bf73c876958", "references": ["arthur's drawing"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-b0ceb733007f44d399778b5f6c7a6a4b", "references": ["hershel"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-f4d5db8b8ea54c4895bb74513fb8faa8", "references": ["the cops"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-1d1061f78e1f47e285c66e8eb535dcad", "references": ["bob and steve"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-e065daa11f1247fa93d2a883215ecb7f", "references": ["the trainers"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-dbe532a4a9e048b38cffa997fa63922a", "references": ["the door"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-4675528a765441c89043ca3d9b0807e6", "references": ["aphrodite"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-07f15d0bd2fe494c9e9f6372b5d4c638", "references": ["ann and polly"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-e0a80a2df95f49de9bad0f0051da40f6", "references": ["peter"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-c32ab37aac1f425d9222d2df41db3b30", "references": ["jenna's mother"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-0ff580848bf24026a7a360aabe5451e1", "references": ["the rack"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-430933560d204a3e9bedacd8f59f08ef", "references": ["henry"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-6aaaaeac54fc4ea9a36be5cb3c20d448", "references": ["charlie and william"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-80dc19aa0cc34f989ea9e232ea084337", "references": ["bailey"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-b97d1ab3533f4514a38d144bd8817584", "references": ["the refrigerators"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-a325178dd5b84177a8cfde4aeba27075", "references": ["emma and julia"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-54ebe4b2102442f1adcc68499dc97ca6", "references": ["the trophies"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-e5cfdb966be44f6291efb61dc02bd856", "references": ["dan and julian"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-24708b5f858f4f04b47b866bc7f619f1", "references": ["lily and emma"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-0b96fccd880749f9a96e8c1e5d2e8782", "references": ["hsfhfsc34r3e"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-883856840564497684c5602ea082ed3d", "references": ["susan and jane"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-02c30d1437e94fd08feb760401cc5b58", "references": ["lara"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-40319216d02a4f92bac171aa40fa6884", "references": ["the celebreties"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-69d6170aff1e40419f01426638d17322", "references": ["the beach"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-2f916a6453e444a49eb5ef2270692bd8", "references": ["the barmen"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-57151b15b0594595ab681cff18a2a3c2", "references": ["andy"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-a03da2dbb53c4b64a9ad96821585b8d5", "references": ["lola"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-c2dfea533dc348e082028eb9a108f042", "references": ["john"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-2d9993049ffb474fad9c9e13b92b6cf5", "references": ["jack"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-68435252af254820bbb8d6d50d3bdcb2", "references": ["the son"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-353f5d9258534f73bdeffea374647781", "references": ["the fox"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-af6a9e26362b41019ea0ee209ed814ab", "references": ["anna"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-c39a063ffec14e63bd291ec015eea2d0", "references": ["the pool"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-28a0094d40ab43e5a2fec694310ab912", "references": ["george and john"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-bc28591aa6784c77b4926fa3198850f4", "references": ["the metro"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-0fb05a191e014df5badffde42b311823", "references": ["the bench"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-b6ca10fcd77f41cb8517957b1727f097", "references": ["dan and henry"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-2665f00f94c44e01b545f734e71efda8", "references": ["grover"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-72983961bea84247b30034fbc84aa53c", "references": ["irma"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-70f64e603f7f4ec4b667480c7a12a9d5", "references": ["dirk"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-bbb824bf46804d9eb4252da5d9ed6625", "references": ["the ceiling"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-258f79ce303645f2b117a8323abcd533", "references": ["lucas"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-a34e00838ba748368eeb391617225966", "references": ["the older student"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-1e65c9b7019042908abd32a58a672bfd", "references": ["Constantine"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-b545c540e3814ff0b896b29b509573b8", "references": ["the chairs"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-f6ad4a0401264f7d8d54c289a8273e2a", "references": ["mary and joan"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-fe4d3ac0844144d7a826f593f9db89b0", "references": ["adam"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-04e270b650af4f87a2c4d4694887a9bb", "references": ["gareth"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-baa5f69d2a52465fbd1c300def28fa2c", "references": ["the picture"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-7cc59fbfe47347b8b6dc00b1fa68f461", "references": ["the tables"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-a01b4456667246a08942cf9c2cfa2f62", "references": ["the hair and dirt"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-aac247a9aa2b4de88f88c2f10e1dee5c", "references": ["joan"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-7cbd295884dc45528e7a7a80adfe38be", "references": ["the demonstrator"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-92c3a805838e49f5b32ac523736a8f71", "references": ["susan and joan"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-8d1c81418c984c70bc7bc0c226a0c6b7", "references": ["frank and steve"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-106daee646c84bff844e961ebed0264d", "references": ["andrew"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-0dce24e922a0414fae4b949e8002f618", "references": ["molly's drawing"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-b45369e7f76949ce9e49c08c5e9bd4b6", "references": ["the water bottles"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-289c916d34154eb6b596adba3cd4b134", "references": ["rob"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-63cae3151f8c446e92e1054de8b7fa25", "references": ["the private investigator"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-46e198d93eb74eac8f6b179a91ee6ef2", "references": ["susan and lucy"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-311a0b08032d4d51a6a9608e1189ec61", "references": ["the protesters"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-36a3243f424f42b3b73355e3a8590968", "references": ["the school buses"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-f07d3aa1243e4617957b6a579f39ae5c", "references": ["ralph and henry"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-f22d3a7397904619b13787cdce3fb41a", "references": ["tara"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-25fcecf0cce04e1e8fa73dcff97a23e9", "references": ["susan and hannah"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-71c803b8ebec4957929b97497151f880", "references": ["the pillars"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-c5b957fad8fd47528a174b39972d69ee", "references": ["dr .  adams and dr . jones"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-29e6fe0917d1465a88e5a9362adc945e", "references": ["Jon"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-c53303d21d144c11a4f6b3edc69d542e", "references": ["the fog"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-28a0bb8f85c94836bb0584956d18880e", "references": ["the star"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-b85a7ecc70e04c8f9a95434d87fc8c46", "references": ["the gig"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-da7cfae48b2f42fca16fd55e8f3114f7", "references": ["george and peter"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-fc8850922fa4487fb414bf9e518f0906", "references": ["john milton"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-67875d939a4d4abebb29b3e83cad6ac5", "references": ["rick"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-293b4fb2b7f5400d8d6b883b528e3e77", "references": ["anne"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-69d35fcf8aab4f39a6933c3a0220ed01", "references": ["persephone"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-0f3bd6c71adb4ee4858997b33e059064", "references": ["the lady in uniform"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-0497a0b44a2c40e887e0aace0adff863", "references": ["the road"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-4293d137b569466dbe6650c7c2436bdb", "references": ["todd"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-d28c1c25e6d44b3d84f1f0d5a32a2574", "references": ["the woman"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-8548b0a5eb0b4a28b760c22524ba3435", "references": ["the hole"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-3563a952e7624dd283a32cbd81911013", "references": ["the canine"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-df738d9bf5064182a329b633703d2b6f", "references": ["woolf"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-0018831662b045648f614aa9e2233130", "references": ["emma and nora's mothers"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-c2c1020aea704dedb01e190deeebb11f", "references": ["the maps"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-a0de5503ba304557940236615209acad", "references": ["brent"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-766d05796f8742298025674030604415", "references": ["the potato"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-6854861618f94168975d359f136d2581", "references": ["Dariyah"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-d3e6b84b1f8a4f96858c946063608971", "references": ["jad"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-881f0fb64af1446c946092dc925b5d52", "references": ["nina"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-4e77a7936ae6428d9e030b484df25125", "references": ["the fish"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-2d8f45bc8298432896c5c3b702ce5191", "references": ["the lakes"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task249-00bcf05182c646e5acf3c21eaf743f27", "references": ["the butterfly wings"], "task_id": "task249_enhanced_wsc_pronoun_disambiguation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task957-38934d14eeaa4e4fb78d022516405ca8", "references": ["Located near Caf\u00e9 Rouge Midsummer House is an Indian restaurant with a poor customer rating."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-9f5aae7d3cb44faba2d50984e32b16fe", "references": ["Taste of Cambridge is an average rated and cheap restaurant option."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-5bc9c1a2a70e4c858796c4c7a368c068", "references": ["In the city centre, near Burger King, you will find The Eagle. It is a non children-friendly coffee shop that serves Indian food. While its price range is quite high, more than \u00a330, it has a high customer ratings."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-4c31118310b743219600a3929eef1a50", "references": ["Browns Cambridge has a high customer rating with a high price range."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-d09014acace34d9e90dfae68fd50619e", "references": ["Yippee Noodle Bar, near the Alimentum city centre is a fast food place with high customer rating, and has a price range of 20-25 euros"], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-1cba7ec6cf484058b3967d3934619a92", "references": ["The Golden Curry offers English food at a high price. It is near The Bakers. It should be noted that it is not kids friendly."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-ff290568942743108b5d291b2134a8b8", "references": ["There is a kids friendly restaurant called The Plough near Express by Holiday Inn."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-5f7d4a43aa1b47f9b122f67406e2bcba", "references": ["In the city centre the Blue Spice has a customer rating of 5 out of 5 price ranging more than \u00a330. It however is a none children friendly place."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-7f635cf44c5841b0bc0c7a63e39a344d", "references": ["Near The Bakers is The Golden Curry, which is a fast food place with a price range of less than \u00a320 and is not family-friendly."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-9fdb4bd315424fe5a8cefbeae3a639de", "references": ["The Mill is a English pub located in the city centre area with a high price range"], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-bb029bcd253a433e8a966797905b6e68", "references": ["The Golden Curry is a five star establishment with a family friendly atmosphere."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-2fc79dc86148452ba466c1c36a8ec3bf", "references": ["Near the Rainbow Vegetarian Caf\u00e9, there is a place called The Rice Boat. It is high-priced, and has a 3 out of 5 customer rating."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-bf8f66c2836b40e986c87767227b2282", "references": ["A pub that is not children friendly is Cocum. The customer rating is 5 out of 5 and the price range is more than 30."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-2fae72ce30ac416da60e3d32182e2542", "references": ["Highly rated and mid priced, Travellers Rest Beefeater is located in the city centre near Caf\u00e9 Adriatic."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-e1f9273253ac45138216fa6293121726", "references": ["The Vaults is a low rated pub located near Caf\u00e9 Adriatic. It has a price range under \u00a320."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-63c7803f900c458e8aa92a347b5c7574", "references": ["The Bakers which has a price range of less than \u00a320 was given an average rating by the Giraffe for a non family-friendly coffee shop"], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-86e2be5f64ef4be88bd6a4826f1c46d4", "references": ["There is a restaurant near All Bar One called Midsummer House that is expensive."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-21e1dfb4d704407ea8bcc38f6f943006", "references": ["High priced The Waterman rates average, serving Indian foods located in the city center is not kid friendly."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-da2617a79a434cb7b77b665cdff80ebd", "references": ["The one star The Twenty Two is near The Rice Boat. It is cheap but not family friendly."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-333431c19e914a83b778ed5d1391b43a", "references": ["There is a place that serves English food in the city centre called Alimentum. The price range is less than \u00a320."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-de856157919b4d0eb7d1bf7aff14ec54", "references": ["In riverside, near Crowne Plaza Hotel, Browns Cambridge coffee shop has a 3 out of 5 customer rating, serves fast food and is kid friendly."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-aed1fc627aaa499895974355dd89bf3c", "references": ["Zizzi is a cheap restaurant in the riverside area."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-8d1d43f2d5ee4fbdad902125c7df53ef", "references": ["Wildwood is an English pub in the low price range with high customer ratings."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-005d0b9f9d3b4c8ea453a6098ca258ff", "references": ["Strada is a Japanese  pub, near Yippee Noodle Bar. Strada has a great customer Rating of 5 out of 5."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-36f03b342a964fbf9aeeb76deeb43086", "references": ["For a Japanese coffee Shop which is family friendly go to The Cricketers. They are near The Portland Arms and have a 5 out of 5 rating"], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-53ce787b5fc44e02a60af81b9f2c04da", "references": ["When thinking of taking the children out for French cuisine, The Golden Curry is highly recommended with a 5 out of a possible 5 rating."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-b4a82e4a264d4e669c91d454fcbaf49b", "references": ["Looking for a cheap fast coffee shop in the riverside area near The Sorrento, check out The Mill"], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-7179050011c44c8baa055a8ff652ddf8", "references": ["The Rice Boat is an Italian establishment located near Express by Holiday Inn in the city centre. The rice Boat is not family-friendly and has a rating of average."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-b69f9dd346af422881b29dd39f315860", "references": ["The Cricketers is a French restaurant near the All Bar One."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-6ce6eacf622e40f29288cdd02900ebe1", "references": ["Strada Sushi is a moderately priced, family friendly restaurant near the Rainbow Vegetarian Caf\u00e9."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-73d21febca284ce2b7de0410b39d821e", "references": ["Fitzbillies is a moderately priced restaurant near to Rainbow Vegetarian Caf\u00e9."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-1e56074462a34e3591bf2e28cc76e599", "references": ["The Punter is a family restaurant with low prices and delicious sushi, located near the Caf\u00e9 Sicilia"], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-259deaffa0e14b7aafdfc61cfdc7a38b", "references": ["Italian pub called Giraffe by the riverside. Is children friendly."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-025bc537854e47059d93eb3f72ef33cc", "references": ["The Eagle, a moderately priced, kid friendly coffee shop in the city centre area near Burger King has a customer rating of 1 out of 5."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-ab6129f8ff5942ba99025727bf68df34", "references": ["A restaurant in the riverside area, The Phoenix, offers English food in the lower price range but has a low customer rating."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-6675b69bcabc40b29eb3ca361a0281be", "references": ["With a cheap price range and average customer rating, Fitzbillies  is located near Express by Holiday Inn. It is also family friendly."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-8240d21c73774e7085e6e3034cbb3bdd", "references": ["wine and spirits 3 star pub relax by the river at Clowns pub."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-9cd872c78bd34a579ccb40e53f6ec20e", "references": ["The moderately priced, kids friendly establishment The Punter can be found near The Portland Arms in the riverside area."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-686e44b7ebb84ba998cf720e2ec301e1", "references": ["Located near The Bakers, The Golden Curry is a Chinese restaurant that is moderately priced.  It is not kid friendly."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-588f621fc89f4f00a2828baa98d657a4", "references": ["If you are looking for a moderately priced restaurant try The Golden Palace."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-7a22c42055d44d73949387f5fa83b0b7", "references": ["The Golden Curry is a fast food restaurant near The Bakers. Price's range from \u00a320-\u00a325. Families are welcomed."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-947bcf580fc84ee78ebbf19644444ad6", "references": ["For English food at high prices that isn't children-friendly go to The Olive Grove that is a riverside pub"], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-08e99b64540647a59d314d675720bfc3", "references": ["The Travellers Rest Beefeater is a restaurant with meals going over \u00a330."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-b877c721990546eca8bf56bfc48ba142", "references": ["The Eagle is in the riverside area near Burger King.  It serves cheap Indian food and has a customer rating of 5 out of 5.  It is family friendly."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-87356b3e68344d39a097d4a02c052942", "references": ["The Golden Palace restaurant offers prices less than \u00a320."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-bafb2db1022e44448ba97c5605749097", "references": ["If you're looking for Indian food in the riverside area, Bibimbap House is near The Rice Boat."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-2dd49f7c025547b7b82a71c39d6ad992", "references": ["Browns Cambridge is a fast food family-friendly restaurant in the city centre near The Sorrento."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-ab684e68705245bd9d7036e6adfed5b5", "references": ["Near the Caf\u00e9 Sicilia, there is a child friendly, moderately priced coffee shop called The Punter which sells fast food. It has a customer Rating of 3 out of 5."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-ccfc145e5013485aabbde0771491d8a5", "references": ["With a customer rating of 1 out of 5, Zizzi is a children friendly pub that serves Italian food."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-e66ca906a6da47cd96301afd16876634", "references": ["The Punter is a kid-friendly Italian coffee shop located near Caf\u00e9 Sicilia that was ranked high by customers"], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-4b6d2b2162c743648253c07e96c27576", "references": ["There is a child friendly French restaurant with a price range of more than \u00a330 called The Vaults."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-82321e358d1b4028a92352a80d682d1b", "references": ["If you want Italian, The Dumpling Tree restaurant is cheap."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-39c95be5c7dc41339eff28d09002e51e", "references": ["The Waterman, with a 3 out of 5 rating, serves Fast food by the riverside. It has a moderate price range but isn't child friendly."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-b44cc629f1ac4fd984e66ddcaeae1b3a", "references": ["The Phoenix serves English food on the riverside. This venue has a high customer rating."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-c893df4737944ded87805ab3ee449da2", "references": ["The Rice Boat public house is in the city centre close to Express by Holiday Inn.  It is moderately priced, has 1 out of 5 stars and is family friendly."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-fa345cf096bd4767937ac9c0afc32850", "references": ["Fitzbillies is a coffee shop that serves Indian food within a high price range but it is not children friendly and has a customer rating of 1 out of 5.  It is in the riverside area."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-da47348e5d4143b49b3a5b033531e651", "references": ["Loch Fyne is a fast food restaurant that is child friendly."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-b0976df4abbe474984ae3da483a6b367", "references": ["In the riverside area, The Golden Palace restaurant is rated average."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-fc6b02b9274c4d509abba765ec1df664", "references": ["Browns Cambridge, budget food and drink. 3 star"], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-eaded70ce10141a69eac67a56b9a9020", "references": ["The Waterman is a cheap Indian place on the riverside.  They have an average customer rating and are family friendly."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-6993553ec46346b6bf962e7fe6813d15", "references": ["The Olive Grove is a child-friendly pub, serving French food and is suitable for those with a budget of more than \u00a330."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-1263c77787134a16a717306df87ad3e2", "references": ["The Olive Grove is a pub that serves Indian food in the twenty to twenty five pound price range near the riverside that is kid friendly."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-4831e65046c14e7499b3e6b4a0cde7bc", "references": ["The restaurant Wildwood that is near Caf\u00e9 Rouge has ratings of 5 out of 5."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-ad626d5b37c3462ab24a928ab64faba2", "references": ["Indian restaurant Strada offers excellent service with affordable prices.   Located by Rainbow Vegetarian Caf\u00e9"], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-044894ae65d5485487f99df2552993c1", "references": ["For cheap food there is The Wrestlers public house.  It is not family friendly and has one out of five stars."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-0033e2df2f204b4198994a3773b585d1", "references": ["The Eagle is a moderately priced low rated Japanese coffee shop near Burger King in city centre."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-81483f28251e47229ad96f41c7c927b3", "references": ["There is a coffee shop that serves French food named Fitzbillies. They have a high price range and it is located at the riverside. It is a child friendly establishment with a customer rating of 3 out of 5."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-c8440bcc72bf41cc98d7ee19ac91bba5", "references": ["Cotto is in riverside near All Bar One with a high price range."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-fd0f3450cd264b0fb752193f4c321ecf", "references": ["Loch Fyne has Indian food with a customer rating of 5 out of 5. It is located in city centre, near The Rice Boat."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-947283dfd2484f88befc7fba326a742c", "references": ["Bibimbap House is a restaurant in riverside near the Clare Hall. For a high price you can eat Chinese food."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-856862d87240478594c95748e68b6511", "references": ["There is a family friendly English restaurant within the  \u00a320 price range in riverside near All Bar One by the name of Green Man."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-fc711aeceac14d328a92f6be533cd026", "references": ["The Waterman is not family-friendly, offering French food at a price range less than \u00a320; it is located near the Riverside area and has a low customer rating."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-582deb8e05be4ac5aa4224b68ad250be", "references": ["In city centre near Yippee Noodle Bar  is Alimentum, a fast food store."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-ffce112d1d4442d094492196106134b3", "references": ["There is a good quality restaurant  The Wrestlers . It provides food for the family in average price."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-0fb37d6a33794230828c6134ecb98c8a", "references": ["With a high price range and low customer service rating of 1 out of 5, a family-friendly Italian coffee shop named The Eagle has opened in the city centre near Burger King."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-ed16020036564a32a5622d6d136865be", "references": ["Blue Spice is a family friendly restaurant located by the riverside.  Price ranges are moderate and it is rated 1 out of 5."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-f83da1b60dbd44bea518a73ad29f1dac", "references": ["On the riverside, there is a restaurant with a low customer rating named The Rice Boat.  They serve French food in a non-family-friendly environment."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-ac27bddb6f124524bf04e6d0c1da83aa", "references": ["The 5 out of 5 rated The Cricketers coffee shop is located near The Portland Arms. This shop serves French food in a child friendly atmosphere."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-9c8d1ef2199545cf8ee94ec4caf62fce", "references": ["The kids-friendly coffee shop called The Wrestlers near riverside next to Raja Indian Cuisine also serve Indian foods with high price."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-d087f98308af4926bdb573ffdbee69e2", "references": ["The Phoenix is a French food restaurant in riverside. It has a 3 out of 5 customer rating."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-f552199ff43845d6908d3bcc4ca55a08", "references": ["The Olive Grove is a Italian pub in riverside with a high price range and it is not a child friendly zone."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-58c521fa3a364d33a6046054a9f9799d", "references": ["There is a restaurant that servers Italian food, its name is The Vaults."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-84b8e2302e134eb99ecc30e73536b78f", "references": ["I love the food at Wildwood pub. They have the best burgers."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-564dcf56775e46048701cf810455ced9", "references": ["The Mill is a high priced pub where we can eat and play billiard. It' s located outside the city."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-08909c0010b644fd998ccf83daf7a9f6", "references": ["On the riverside, The Rice Boat is located. It is a cheap Fast food. It is family friendly with a customer rating of 5 out of 5. It is near Express by Holiday Inn."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-45b25ee80bd74b3085a911a8db7676d9", "references": ["Offering Japanese food, The Olive Grove is a pub in the city centre that is adult only with high prices."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-fcd2cfbcbc0b460fad0d55dbe595a787", "references": ["The Olive Grove is located in the city centre serving French food. It is a family-friendly pub with a price range less than \u00a320."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-0b6e8fa0dbc74a26a3ab4c4d609242fd", "references": ["Cheap Italian food for adults can be found at The Eagle coffee shop, near Burger King on the riverside. Average ratings, and cheap prices."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-b621f50c2aab440d8e0dac83a13bbb6d", "references": ["Strada is a child friendly Indian near to Rainbow Vegetarian Caf\u00e9. It is in the high price range but is suitable for children."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-42c634cbf8274f45b0adb9e648f9c713", "references": ["The Punter is a coffee shop located near Caf\u00e9 Sicilia. They serve traditional Japanese food in the medium price range. It is rated one star and has no public restroom."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-a048d457295444808799cd6c3aabe752", "references": ["The Golden Curry is an English children friendly restaurant , located near The Bakers. Prices range to more than \u00a330."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-4bd1c7af688b4c8b9fb1fd2a1f29ac46", "references": ["The Wrestlers is a place that is for the whole family where fast food can be eaten. It is a bit expensive for the quality."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-1cb9d071357948ec91af3401ff07b9ea", "references": ["The Wrestlers is a cheap family friendly French pub and a coffee shop located near Raja Indian Cuisine near riverside"], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-497bd08e2b6741a3b97dfa4c4977fa64", "references": ["Near the Caf\u00e9 Adriatic, in the riverside area, there is a cheap restaurant called the Travellers Rest Beefeater it has a customer rating of 5 out of 5"], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-bf29e0264f9d406b90479f3c79f967d0", "references": ["The Vaults is a high price Range children Friendly Fast food restaurant."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-88ea93ab827e45559e98b669e8c79f58", "references": ["With a price range of less than \u00a320, Green Man is a cheap place to buy Chinese food. It is not family friendly and is located in the riverside area, near All Bar One."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-e29ad6dc41684a97b2f5b1c15a878801", "references": ["Clowns is a French coffee shop that is located near Clare Hall in the city center. It is rated 5 out of 5 by its customers."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-5532c6df6b43439cbcaf19d231b1e806", "references": ["Zizzi is a non-child friendly coffee shop located near the city centre. While it has high customer reviews, it also has prices ranging from 20-25 pounds."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-21f30c192a4e403db80cb6f3f57657cd", "references": ["The Mill is a river side pub with English food with high prices."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task957-563dfe6156574ce8b5b2dcb08784206a", "references": ["Taste of Cambridge is a centrally located family pub near The Sorrento."], "task_id": "task957_e2e_nlg_text_generation_generate", "task_category": "Data to Text", "track": "default"}
{"id": "task418-b9dc98040d3446dea5f8c0220b2e3fe1", "references": ["Nancy Pelosi Declines to Say If Rep. Conyers Should Resign"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-9ec221fe2695463fbd3f812f28354d1f", "references": ["Citing liver damage Pfizer withdraws Thelin"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-414b94a5b0534b0da3ea0af89147a7cc", "references": ["Hillary Clinton calls for gun control after Las Vegas shooting . "], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-ba29e40a4b684d72bafceaed0a2bdcb8", "references": ["INSANE: Women's March ATTACKS #MeToo Leader Rose McGowan Over Her 'Transphobia'"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-cd767bc72b874ee6866de0cad62bf9a1", "references": ["U.S. snowboarder Chloe Kim tweets about ice cream in between qualifying runs"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-b1b3d3692c884313ac3980543030eaae", "references": ["Matt Lauer Fired By NBC News After Complaint Of 'Inappropriate Sexual Behavior' : The Two"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-829d3069b5b444239c3c574bd39ff506", "references": ["College Football Playoff: Baker Mayfield and Oklahoma sprint out to a 17"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-ce5253a4b23c4ecb906f4fa5f04814c5", "references": ["Dutch politician on trial on hate speech charges"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-236f347cf03a4c1c88838f8bb0fb8523", "references": ["Scott Boras continues to hold up the MLB free"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-0344ba33ec594ce0bc35598e0afa840a", "references": ["US lawyer imprisoned in Belarus freed after pardon"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-637f999213fb43df96eb21380413fbbe", "references": ["Help your kids sleep better and stop waking you up"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-5f3169ea506b45a785049489cd44d7b4", "references": ["New NY Times Publisher A.G. Sulzberger Has an Unusual Admirer: Wisconsin\u00e2\u0080\u0099s Far"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-6c3ef2bb358a469bb9a041241a18fb1a", "references": ["Lunch with Celebrity Chef Lidia Bastianich"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-6b089b5279804532be3b9b173f3a7cc5", "references": ["First lady Melania Trump vs. Michelle Obama fashion cost of clothing"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-0ac0c6b45f6c4231816704b439eb7c89", "references": ["Storytelling Ads May Be Journalism\u00e2\u0080\u0099s New Peril"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-1de0f9aff1144813950b163273e466dd", "references": ["The abuse victim who has protested in the same spot for 20 years"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-fcaae05f3ab446688654209184f5fbe7", "references": ["Clijsters wins 1st US Open match in straight sets"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-daa34b0f645440f7a5954dbd93c478d4", "references": ["Ted Cruz is now likeable , The New York Times wants you to believe . "], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-86ed620214bc4ce6935a598612fcbbe2", "references": ["Team Penske fills out sports car lineup"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-09dfcce1b936444e8dde8a4dabe96f58", "references": ["U.S. Border Patrol agent died of head injuries autopsy says"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-630d5c1990da4787a5a9311ae958c586", "references": ["John Mayer Hospitalized for Emergency Appendectomy"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-9f8dc85573f546dda8ac6fb24f77ef0c", "references": ["Modernizers launch a coup within the House of Saud"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-c98d4272d1b348c9ad381dc08f50acb5", "references": ["Laura Ingraham\u00e2\u0080\u0099s \u00e2\u0080\u0098Billionaire at the Barricades\u00e2\u0080\u0099 Gives Conservative"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-871bd4558d0044ce97a02b43eff45bbb", "references": ["Jury submits questions in Anna Nicole Smith case"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-bba97ba4e2074c6b827c58e3911543ea", "references": ["Were the courtroom challenges worth it for Ezekiel Elliott  the Cowboys and the NFLPA?"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-5cd8a3f59e1141cda91f39344faa8b98", "references": ["Standing Up In The World Of Stand"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-09d3740d84f64cb5871bd8d01f88fab6", "references": ["BREAKING: FCC Overturns Net Neutrality"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-ead2e0874ae541ac93db855343f1a6f6", "references": ["Leftist Mexican presidential candidate leads but gap closing: poll"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-bc5000ac04b14b6b8fdd86930eb6fcfa", "references": ["White House Lawyer Represented Arrested Saudi Billionaire Trump Called \"Dopey\""], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-110e17092cbc4f9389678653c1f05aab", "references": ["Pence Makes His First Trip To Afghanistan As Vice President : NPR"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-4d447178cd7946f495390ed828066881", "references": ["Carrie Fisher wrote some of her own lines in 'Star Wars: The Last Jedi'"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-6da5fd061a8c40c3a2be9e88a3e055e7", "references": ["The Man Starting Europe's Largest Legal Weed Farm in a Nuclear Bunker"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-45ceb946c87349e2bdf7c0c1024798a7", "references": ["Would arming teachers make schools safer? 'To be honest I don't know ' Ivanka Trump says."], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-e276c5079d974845a4576cc1d9a804aa", "references": ["Fired transgender model calls for L'Oreal boycott"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-4bffd12caac84e7683cb25ec76132031", "references": ["Razer's first smartphone specs have leaked and it looks pretty mighty"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-84c698c179c34c5e8207dc8ea4346156", "references": ["Tropical Storm Philippe heads toward southern Florida"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-a36d24d479154cc7b0e31bd364449892", "references": ["New playhouse planned for Shakespeare theater site"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-8a8f40b3d10941ddad74c5a8e2dc19fb", "references": ["Chicago Bears great Joe Fortunato dies at 87"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-3e6ac49826ba4fbdba2929b9b2159328", "references": ["Trump\u00e2\u0080\u0099s voter fraud commission is dead. But the GOP voter suppression effort is just getting started."], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-2214af51b64b4486878933fc756366df", "references": ["Cambodian opposition lawmakers stripped of immunity"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-a2ce6592c5b2473c98faf38bd72c6f5c", "references": ["CFTC to Take 'Do No Harm' Approach to Cryptocurrency Regulation"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-9738ed97a42a46789710acce332a000c", "references": ["Dash cam sales rising fast due to safety"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-565129639f804ff7925e94cedb048d58", "references": ["Texas man with mental disability dies after police struggle"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-3790d48a99034f8b8de77b362e822d39", "references": ["Brandon Peters will 'likely' start at QB for Michigan vs. Minnesota"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-84157ec865e941c6b4eb150b462cad7e", "references": ["Bernie Sanders scores big wins with Democratic platform . "], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-33fd88f452094e6cb76684e8d6cf2886", "references": ["Actor Lou Diamond Phillips apologizes for DWI in Texas"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-79c348a6d87d42a3b4c499769d3081d2", "references": ["Arizona man charged in Christmas Day murders of estranged wife kids"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-75f195ee492c41bb8f9b8ca367a3cb83", "references": ["Japan U.S. to discuss trade 'framework' U.S. envoy says: NHK"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-a944f840dc2e4d0a87c9242baa0e25ca", "references": ["Why the right is terrified of a Hillary Clinton victory . "], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-17114dbadb7644c2863f8a28736a1eac", "references": ["'Ball in your court:' Britain EU clash over next Brexit move"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-eb9846da5af04293a3b08f426db46e7e", "references": ["YouTube star Logan Paul wants \"second chance\" after \" suicide forest\" video"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-6aac0c3ec90046c6acb84ed33efc6ce0", "references": ["McDonald's manager to receive entire $110K reward for tip that led to Tampa murder suspect"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-be954fb256ff412e8ab1957afea4bf7a", "references": ["'Big Bang Theory' star Kaley Cuoco is engaged to Karl Cook"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-f5fb8c3caa6045f18ffb0928a8c62320", "references": ["Trump Uses Twitter To Criticize FBI Deputy Director Andrew McCabe : NPR"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-28899f052ee048f6bb0a620015deeb43", "references": ["The week in politics"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-cd209625b3904b1c8aab2698d3d9c280", "references": ["Anthony Rizzo wins Marvin Miller Man of the Year Award"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-06ffeeb4cf934f92b91f78d81444a912", "references": ["Knicks' Michael Beasley 's NBA career flickered never went out"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-a3a461726417444fa5a806ae17dd1939", "references": ["Taylor Swift reacts to CMAs win"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-2961c56470b3469383ef8e7e32c8be0d", "references": ["'We're not quite there honestly': John Fox's last"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-a52c6b66b3d24c118a3e40178dbd4e6f", "references": ["LeBron James is a Bully . "], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-15186779caff4c729de76646e880d4f9", "references": ["White House snub of South Carolina basketball 'a slap in the face'"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-f6a8f50b075d4760917e6735ed0040c0", "references": ["George W. Bush to Honor Bono . "], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-319ac260bc624380b8c1dd0d7babb65d", "references": ["Brazil court upholds Lula conviction"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-64fedd74884948369d7442a57d965f6b", "references": ["Serena Williams Talks About The Terrifying Complications She Faced After Giving Birth"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-10bdaecb8605434eaa7bb40686c1d89e", "references": ["We still don't know how many Model 3 cars Tesla is making"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-2d1a85b5ea2d483c9121fb16798047b6", "references": ["De Blasio \u00e2\u0080\u0099s legal fees are going to cost taxpayers millions"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-8466dafce7f64b3abbed0396832860a0", "references": ["Everyone Is Confused About What Trump Is Doing With DACA"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-608126e7f95f4eefbbce2b292e8a7b42", "references": ["Las Vegas massacre: Gunman Stephen Paddock  amassing small arsenal kills at least 58 and injures more than 500"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-dc132e2e6dae4cb3ad83f64affe03056", "references": ["Oscars 2018 best dressed: Our favorite looks from the Academy Awards red carpet"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-ad1b3798de24439190f7319d4578eb8f", "references": ["Kylie Jenner reveals she 's given birth to a baby girl"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-03be54c3b2a44727b23e157d4e707572", "references": ["WaPo Allows Torrent of Racist Comments in Article on Blacks Latinos Being Underrepresented at Thomas Jefferson HS"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-10d6e7eddd634c4daa180637782af802", "references": ["California lawmaker taking flak over plastic straw bill"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-80541ad006194217836f9398f1af2daf", "references": ["This Author Is Under Attack for Making Santa Black and Gay"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-643bd5c331b14a2992801a4920950138", "references": ["Ireland welcomes home student who spent four years in an Egyptian jail"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-0a8cdef029454b6d9362c36d0d00b820", "references": ["Chris Pratt Attacked after Sending 'Healing Power of Prayer' to Kevin Smith"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-6f5c9045f78d41e1b85780a783749974", "references": ["'My whole body was on fire': Man recalls being set aflame"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-84a85255ac18406e94f8d061c0b6c355", "references": ["Inventor of 'bump stock' spent years fighting for device and lost"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-b76e125246974502b95ce74b4e3aea9b", "references": ["Government braces for shutdown as Senate fails to meet deadline for spending deal"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-10baaac744684e95967097f757731693", "references": ["Disgraced ex"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-5b4855c5a01e40e9a1977b1f89290d2b", "references": ["Royal wedding guess list: Who gets a nod from Harry  Meghan?"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-d0dbbd3bec794c8c8616a68e384d889f", "references": ["Can American Society Overcome the Fake News Phenomenon? \u00e2\u0080\u0093 Mother Jones"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-c37b4d0f096a4939860fb5ec528c1334", "references": ["Syrian troops break into last IS"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-0136d82442ee4474a7eb195644ee644c", "references": ["WALTER PIDGEON ACTOR DIES AT 87"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-d754eef5f1b04962aae1fd81cdf24870", "references": ["What's the Republicans' Game Plan for 2018 Elections?"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-590c109a2ccb4f08850f8034b25abcf2", "references": ["Deborah Rutter wants you to know that the Kennedy Center is much more than \u00e2\u0080\u0098the Honors\u00e2\u0080\u0099"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-6f82059fbdb54472ad634a78c75986b5", "references": ["Rep. Franks proposed sex to female staffers he had offered money"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-e4bf6b094a4d48a1af4d1777e6e41f04", "references": ["Nolte: CNN's Town Hall of Hate"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-e5a5558870f34df7b8cb6a62c53ea638", "references": ["Ford brings 'Bullitt' Mustang to Detroit auto show"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-847e9e9cf4b94c4da29c988847256f42", "references": ["Justices ponder need for warrant for cellphone tower data"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-385acb77e2c64fc0bce88b485e8fae64", "references": ["Susan Collins had senators use talking stick in bipartisan meetings"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-860e5c5ec86c45f9ac04b3ec888b0287", "references": ["Grand jury seeks testimony about Bernie Sanders' wife $10M college loan: report"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-0332a4faf63b42079bdc061ded7ffb09", "references": ["Matt Lauer\u00e2\u0080\u0099s Incredibly Creepy Anne Hathaway Interview Will Give You Chills"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-c6f5b204c1e04d05ac0529b78b5338bc", "references": ["IKEA founder Ingvar Kamprad dead at 91"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-5d2925d83e2f48ec9850c9517975f60d", "references": ["John Skipper Steps Down As CEO Of ESPN Citing Substance Addiction"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-46b2ab80c8214a6a83456885798a737f", "references": ["Trial delay for teen accused of trying to join Islamic State"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-868c1043e76a46d2a6fdaf9df6ade250", "references": ["Bringing ISIS to justice for war crimes against Yazidis"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-62770861617f4915bc45a166036fa4cc", "references": ["Pipe Bomb Detonates in New York in ISIS"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-72aed2f8037645f9aa76849c63b4e762", "references": ["Richard Spencer: Protesters heckle alt"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-29189fa8130b400b9ed54455a8fc6b72", "references": ["Docuseries 'Undercover High' sheds light on modern struggles for US teens"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task418-cc1e5e3913bb47de9124858b7c9e73fc", "references": ["House Science Committee Wants to Investigate a Government Scientist for Doing Science"], "task_id": "task418_persent_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task614-59f5c644f34541b18f3ea144504d9292", "references": ["R and G don't want to go to the party >Causes/Enables> R and G don't go to the party"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-857cb094569c43f79cc0d5b552db5c76", "references": ["My dog eats my homework >Causes/Enables> I find out about the eating", "I look for my homework >Causes/Enables> I find out my dog ate my homework"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-1b27fda9c6f64bff95f197c79e127ad6", "references": ["The parents invite Jacob to the Bris >Causes/Enables> Jacob comes to his first bris"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-82cb4d8322524946b0400cd688e379d8", "references": ["Allison's class decides to go >Causes/Enables> Allison's class goes"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-52b2853ad93944db9b8ac338540409d2", "references": ["Jeff decides to walk Cindy >Causes/Enables> Jeff walks Cindy"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-e792051351f44e4a9d2dae6d2ec15d9d", "references": ["My wife notices we need a new rug >Causes/Enables> My wife decides to buy a new rug"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-ec0c2f6cf99844f9aff8711b4ffb1200", "references": ["Lauren changed the tire >Causes/Enables> Lauren cancelled the AAA call"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-bd381902136e4d6fb3e087d1811e3db1", "references": ["Fred leans in >Causes/Enables> Fred kisses Susie"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-0676dcf836c941e1ae20de6c3d0ffb67", "references": ["The cops pull Tom over >Causes/Enables> The cops tell Tom his tail light is out"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-5e78f4cd8ffd47c68971b24b0c0ea9c9", "references": ["Carla's mom gave her a vase >Causes/Enables> The vase ", "I get a vase >Causes/Enables> The vase is on my table"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-e97646fc5c5f4362bfec2a1155ff9627", "references": ["A boy played out in the mud >Causes/Enables> The boy's mom tells the boy to clean up"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-86450fcb0dbc406680eb359a623285c3", "references": ["I forget to brush my teeth >Causes/Enables> My friends tell me my breath stinks"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-2f7e10975f814c028ab94bc6aa6a0ef3", "references": ["Steven walks to the field >Causes/Enables> Steven is by a flock of geese", "Steven goes by geese >Causes/Enables> Steven is by geese"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-ba994798b9804ce2846841ab787c0df4", "references": ["He wakes up in a good mood >Causes/Enables> He feels the day will be good", "He wakes up smiling >Causes/Enables> He feels that it is going to be a great day"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-19d6e8c02d3b43cb9c57cfd755c8656f", "references": ["Esther gets lost >Causes/Enables> Esther is too stubborn to ask for directions"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-8d0fb2a76441477f8519f0a4af093f96", "references": ["Hunter goes to grab >Causes/Enables> Hunter grabs"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-4720962e9e5e472e9f334335f511a627", "references": ["Tina doesn't pick up her phone >Causes/Enables> My mom drives to TIna", "My mom borrows my car >Causes/Enables> My mom drives"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-c3e9e6877951435ba958bd52a9eeae7a", "references": ["Sally goes to Italy >Causes/Enables> Sally visits Rome", "Sally went to Italy >Causes/Enables> Sally visited Florence, Rome, and Milan", "Sally goes to Italy >Causes/Enables> She visits Florence, Rome, and Milan"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-325df8d8b59642acac88cf165c21db84", "references": ["Gina doesn't eat for awhile >Causes/Enables> Gina is hungry"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-7c743616a7694b298507dd565d6b591c", "references": ["Suzy forgets about the cookies in the oven >Causes/Enables> Suzy leaves in the cookies for too long"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-645c9a55d8c347fe85c13de448a030b9", "references": ["Gina gets upset >Causes/Enables> Rita wants to know why", "Rita sees that Gina is upset >Causes/Enables> Rita wants to know why Gina is upset"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-cece91159e5f4ff1848a52c3e58d0ee3", "references": ["Tina sees clouds >Causes/Enables> Tina thinks it will rain"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-0452f7da882747d6afd1d65370e5f37f", "references": ["Andrea is asked to get firewood >Causes/Enables> Andrea brings logs inside"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-654ec601989041fd840d101c9172a477", "references": ["Jan has a big party >Causes/Enables> All her friends came"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-83a0c3dc786b4f57ab24f33fcf8e40a5", "references": ["Olivia's baby cries >Causes/Enables> Olivia feels sad", "He cries >Causes/Enables> Olivia feels sad"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-3c1fb4a203a14bba8fd603bcc7251292", "references": ["I am going swimming >Causes/Enables> I have a bathing suit on"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-ea27f527e81a4870ac635e5796e135d7", "references": ["I ask the little girl her dog's name >Causes/Enables> The little girl says the dog's name is Precious"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-64f63e9996d048bebbfcfd7315e5a781", "references": ["Becca asks her parents for a kitten >Causes/Enables> Becca gets a kitten for Christmas "], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-42aeec124a174d26aa05f6a31dbbfff6", "references": ["Richard gets in and out of his truck >Causes/Enables> Richard drops packages", "Richard carries packages >Causes/Enables> Richard drops packages"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-a6c28ec366d54dac846e4f2f6ff8bd34", "references": ["A tiger is born >Causes/Enables> The tiger lives in the wild", "The tiger is born in the wild >Causes/Enables> The tiger lives in the wild"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-b510c48c23ac4849aba0c8a3db30e3ca", "references": ["Her school uniform requires a cloak >Causes/Enables> She needs a cloak", "Anna is told to get a cloak >Causes/Enables> Anna needs a cloak", "Anna enrolls in a school with a uniform requirement >Causes/Enables> Anna needs a cloak"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-37fd1fb91281465095f6298bfd20343d", "references": ["I see a lot of trash in my kitchen >Causes/Enables> I take a bag of trash to the trash can", "The trash bag gets full >Causes/Enables> I pick up the trash bag"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-faab2c7f6f9040c0a5215319fc87238a", "references": ["Max dresses for work >Causes/Enables> Max rides his bike to work", "Max is late >Causes/Enables> Max quickly rides his bike to work", "He gets on his bike >Causes/Enables> He rides his bike to work"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-09eb4ec04c7145808117944e638771ba", "references": ["Heidi doesn't drive >Causes/Enables> Heidi needs to take the bus"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-22beaa42bfca4f8aa1fb9f67186bea2d", "references": ["Maria orders food >Causes/Enables> Maria waits for food"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-830a376831804603945e7284c73791f4", "references": ["The watering can falls over >Causes/Enables> Cindy picks it up"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-c8c24dc5d0ca46eaacda9fa01db1089b", "references": ["I ask my mom to go to the beach >Causes/Enables> She agrees"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-bb526f3121684ff88defa4bf2dbf6d28", "references": ["Brad & Emma went on roadtrip >Causes/Enables> Brad & Emma made memories"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-05f371e0ce51466c917e575c036f79a7", "references": ["We went to the waterpark >Causes/Enables> We went among every ride"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-6f9c3c17631a4c40995599d5b27b8dd0", "references": ["Carmen drives >Causes/Enables> Carmen gets a flat tire", "Carmen was driving >Causes/Enables> Carmen's tire went flat"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-095fd02ff6a34de9a2486e1bd49df37d", "references": ["I shave my legs >Causes/Enables> I promise myself to never shave my legs again"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-0b80812d2c3e4d4fbaa26427b453c401", "references": ["Ali met Jamie >Causes/Enables> Ali went on a date with Jamie"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-890cf21b8eec4a6d8ed6aa539eb0803d", "references": ["Brian bullies Tim in school >Causes/Enables> The bullying stops when Tim finishes school"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-f9122ebdf11c4c5b96bc2380248113d3", "references": ["Billy goes to the zoo >Causes/Enables> Billy taps on the glass"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-593d922c44974ca59d2836753867777f", "references": ["Beth goes to the room >Causes/Enables> Beth is at the room"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-50425b99eb204fe893df034fb6400020", "references": ["Kathy's friend walks to Kathy's home >Causes/Enables> Kathy's friend rings the doorbell", "Someone_Approaches the door >Causes/Enables> The doorbell rings"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-321af254765646f2886a746ffdf3aa9e", "references": ["Robby drives a car >Causes/Enables> Robby stopped at a gas station", "Robby's car is out of gas >Causes/Enables> Robby stops at a gas station"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-2bc553b162a14d47b0619beff691da0a", "references": ["I find my boyfriend in bed with his ex-girlfriend >Causes/Enables> I scream at my boyfriend"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-0047761ad627454393e9033b322a57d6", "references": ["Joey fell off the jungle gym >Causes/Enables> Joey needed a cast"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-177ad53c8abb4047b377fd9969336b32", "references": ["Jim went to bed last night >Causes/Enables> Jim did not sleep well last night ", "Jim did not sleep well at last night >Causes/Enables> He still felt tired."], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-71604f320dd54d508ac2f0485a576f1c", "references": ["Nessa doesn't make the cut >Causes/Enables> Nessa is disappointed"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-2dd4643b882146dcb30bf430071db107", "references": ["The fair is in town >Causes/Enables> Franny and her family attend the fair"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-6810e9284eb8462e9806383c29ce83b8", "references": ["Sandy was laughing very hard >Causes/Enables> Sandy was hiccuping nonstop"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-e8fd335ebdf14cb080f5742aa2af13f2", "references": ["Georgia moves away from Phil >Causes/Enables> She lives in another town"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-9116319abc8b404ca99c41d7972b3bc7", "references": ["Ed takes his dog to the dog park >Causes/Enables> Ed watches his dog play"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-396f98db5e2542c2941e59ad22ddc710", "references": ["John buys a violin >Causes/Enables> John starts practicing", "John bought a violin >Causes/Enables> Two strings broke from the violin when John played it "], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-b1590a9da3df46e088bd330d2b2bf2d7", "references": ["Greg and his dog meet >Causes/Enables> Greg and his dog become close"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-19a1b430977141eaa354b36682e9bd3a", "references": ["He chases the geese away >Causes/Enables> They fly away", "Steven chases the geese >Causes/Enables> The geese fly away"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-b2479ef69b3b4f34b0ba744aa92ff796", "references": ["Sarah gets a fish >Causes/Enables> Sarah names the fish"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-22fe1dc594b44090bdf863648682ed13", "references": ["Mandy decided to give Billy a present >Causes/Enables> Mandy put a tarantula in a box of chocolates"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-8e107f3483684f22aca510016a472971", "references": ["Edna's siblings die before Edna >Causes/Enables> She is the last sibling to die"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-9ec1b06fc9074387bca20977581b0c5b", "references": ["I get hungry >Causes/Enables> I make a snack"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-d4a44d133bee4ca1b1e914bbf43d9fe6", "references": ["Tim gets a night light >Causes/Enables> Tim forgets to turn on the night light", "Tim goes to sleep >Causes/Enables> Tim forgets to turn the night light on"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-6b21bd640f7f404e9d4ce1a769783a7f", "references": ["I leave >Causes/Enables> I come back"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-fc8ae262fa744bd89f05df821716e77a", "references": ["Jake buys a book >Causes/Enables> Jake reads the book"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-10b708bc9c7946ddbeb584326396cc9b", "references": ["Jon goes to the store >Causes/Enables> Jon pays for a phone"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-438e0f00d5234f67b5a1ca046057d353", "references": ["Rene is on the swings >Causes/Enables> Rene goes very high"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-afaf8bfdf7154d3bb70c77d0f40f8063", "references": ["My daughter's first baseball game is a no-hitter >Causes/Enables> The game is boring for her to watch"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-23fe7b9c6c2b4d96858b01bd742cdf63", "references": ["Adam starts preschool >Causes/Enables> Adam wears his favorite hat", "Adam is excited for preschool >Causes/Enables> Adam wears his favorite spiderman hat to preschool"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-53fd15a4176f4a8b959f141c48581271", "references": ["It is perfect weather for a ride >Causes/Enables> We go on a ride"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-68fbacbccad14cfbafb0bc5e45e8f527", "references": ["Bill goes to the store >Causes/Enables> Bill buys jeans", "He goes to the store >Causes/Enables> He buys a $30 pair"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-c265f57bac444904ad4d6759d2f4d115", "references": ["Sarah goes to the store >Causes/Enables> Sarah goes home"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-16d4f786221344b28f730719c8e11d2a", "references": ["Something gets spilled on the seat >Causes/Enables> Her seat is wet"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-aac7446e003c40e391ac9c3d21cf462c", "references": ["I get a problem >Causes/Enables> I solve the problem", "I don't make friends >Causes/Enables> I have imaginary friends"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-285ac258a15441f4b9e260a810fbef0d", "references": ["Nell always wanted to go to Spain >Causes/Enables> Nell stayed in Madrid"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-95f43f0baf1248a3848bf920cc36d2fd", "references": ["Jill doesn't see her friends for a while >Causes/Enables> She invites her friends for coffee"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-c7687471c5f54a348f9635ba6536a9f2", "references": ["The students are friendly >Causes/Enables> The students are kind to Lia"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-8fada84f2d774cf8962208c8e06a49ed", "references": ["My cousin tells me she's pregnant >Causes/Enables> I leave work early to go comfort her", "My cousin called to tell me she was pregnant. >Causes/Enables> I left work early to comfort her", "My cousin tells me she's pregnant >Causes/Enables> I go to comfort her"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-c2005df1a6ba4a669724278c26608674", "references": ["Marcus is hungry >Causes/Enables> Marcus makes a bowl of cereal", "Marcus decides to make >Causes/Enables> Marcus makes"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-14ccca7a8969485fbe956da2855e866c", "references": ["My teacher comes across me >Causes/Enables> My teacher gives me a ride", "My teacher drives >Causes/Enables> My teacher stops her car"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-a3642d3a60dd4f018fb5b789ca00b99e", "references": ["The little boy bought a new blue jacket >Causes/Enables> The little boy is wearing his new blue jacket"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-9687264cf43345b387ce0aa5f18c8f80", "references": ["Sally sleeps >Causes/Enables> Sally gets woken up"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-cf7a429dab354e098372a6194586ccac", "references": ["Peter and Jake eat pizza >Causes/Enables> Peter leaves without paying"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-861da2bfbd3249eebab1ebff26c24d7d", "references": ["Jan sees the pond is frozen >Causes/Enables> Jan grabs her ice skates"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-b49fc9409fc34966ac4cc01e548d8448", "references": ["Peggy goes to the pet store >Causes/Enables> Peggy buys a parakeet"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-86b933a1cf2b4568a7750171a93e5215", "references": ["Nancy goes to a beauty shop for a new hair style >Causes/Enables> Nancy's new hair style is great"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-4f7c511c4fa84853983627a54eb44fe9", "references": ["Tim was playing >Causes/Enables> His mother told him to stop playing in the living room."], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-19aa9e6462b24c1180cf30a94b03cd9f", "references": ["They watch the first act >Causes/Enables> The first act is dancing dogs", "The couple goes to the circus >Causes/Enables> The couple sees the first act"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-d10dbe4cf52249fe9bfa68794080583c", "references": ["We made homemade soap >Causes/Enables> We used homemade soap"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-a1995efea0cc497dbb51182773adbb0c", "references": ["Juan tries drawing >Causes/Enables> Juan likes drawing", "He draws scenes about boats >Causes/Enables> He likes drawing scenes about boats", "Juan likes boats >Causes/Enables> Juan likes to draw scenes about boats"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-8caa2df65ffa47488648ec5255645bc0", "references": ["I get pizza >Causes/Enables> I eat"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-30b8e21314a54303bb8d818fa8a5dcf7", "references": ["Samantha and her family were at a restaurant >Causes/Enables> Another family were at the next table over"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-f429a253c0ed4f1cba45ab3d133810ec", "references": ["Tom's parents enter their car >Causes/Enables> Tom's parents drive"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-794b437f09ee420198086bfb46a08bae", "references": ["I see Something my son wants for christmas while at the mall with my son >Causes/Enables> I buy the item while my wife takes care of my son", "I see what my son wants at the mall >Causes/Enables> I send my son to his mom"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-d0cda118b0cf49d29f31a1318d172bbf", "references": ["Kim sees a commercial for Garbage Pail Kids >Causes/Enables> Kim wants to get some Garbage Pail Kids"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-b3b476879ee24184824410b7c0dceb4e", "references": ["My sister and I tell our mom about the bullies who take our bikes >Causes/Enables> We get our bikes back"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-5afdffadf0ae4a708abebc83627961e7", "references": ["I jokes with Fred's wife >Causes/Enables> She jokes with me", "I tell Fred's wife he said she is spoiled >Causes/Enables> Fred's wife says he is spoiled"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-92aa23e93cdb4c07ac0f3152a40d7e9e", "references": ["The Rodriguez family buys a new house >Causes/Enables> The Rodriguez family moves"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-574151e4af4147048ca1864cd5ff8fa8", "references": ["Gary hangs with new friends >Causes/Enables> They ask him to tag a train car"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task614-50174897700b46dcbdfbcb7094463a60", "references": ["Bob's daugther helps Bob's son open the gate >Causes/Enables> Bob's daughter and son are able to open the gate"], "task_id": "task614_glucose_cause_event_detection", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task677-537b42e41b774880abf62ab0c8bd4ec2", "references": ["THE HOTEL HAS A FRESHWATER SWIMMING POOL ON SITE."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-e9b4e1fc58b04db685dde0fa5eee4897", "references": ["The DVD and Blu-Ray is scheduled for release on October 14th , 2008 ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-1fed99ab642c488eb6cebc2071bff33d", "references": ["Ananda Village and The Expanding Light Retreat were founded 40 years ago by Swami Kriyananda , a close direct disciple of the great master Paramhansa Yogananda ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-2a93f7cc577e4b7ba8beb506df8f4642", "references": ["Since Steve Jobs returned to Apple , Apple has catered to the individual purchaser , people who appreciate what they get for their money ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-6e00a36fb4b94e2880a4e75635579627", "references": ["The Japanese attack on Pearl Harbor on Dec. 7 , 1941 led soldiers of the 28th to remain on active for the duration of the war ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-d5d7621d3b9349b7a5fcb67db32020e1", "references": ["I know that Jesus Christ is the Son of God and my literal Savior , and that when He suffered in Gethsemane and died on the cross , He did it partially for me ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-c79ae2556e694b11be4c63df2c00a1da", "references": ["The Chairmen of each National Committee are ex-officio Vice Presidents of ALA ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-073ca68c58c74e34926079d616d89338", "references": ["Chu became Chuck , Bu became Buck ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-51f4cd56c6d44c0bbde2afe6d4691ef0", "references": ["It will likely be filmed next year in the northern Indian city of Agra , which is home to the Taj Mahal , he told the newspaper ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-9add49a4e04544b595e1384be7fa04e8", "references": ["Michael Shermer is the publisher of Skeptic magazine , the Director of the Skeptic Society , the host of the Skeptics ' Lecture Series at the California Institute of Technology , and the author of a regular column in Scientific American called \"Skeptic\" ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-c8316c329fa44ba18c20d20035124570", "references": ["Schofields is a suburb in the shire of Blacktown in western Sydney , New South Wales , Australia ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-3a910c4bd6d74539868a019d1b01f5b1", "references": ["If the Software contained in this package is being provided to Licensee as an update or upgrade to software which Licensee has previously licensed such software referred to as the days after opening this package except for one backup copy of the Prior Software ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-c15aba4a87924e12873f93b9bb5566fd", "references": ["The University of Colorado at Boulder is also the home of the founding chapter of the Engineers Without Borders , a group that allows students and faculty to help disadvantaged communities improve their quality of life through environmentally and economically sustainable engineering projects ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-c8a424162985488d88724ceefc40191f", "references": ["French speaking Belgium : October 24 , 1998 , presided over by the Councillor of the Presidency Wilhelmina Visser-Pelsma ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-52aef96db7384e46afc5dcaff1acdc46", "references": ["The Residence Flat Hotel Is Located 200 Meters Far From A Fine Sandy Beach Within The Tourist Site Of Agadir In Morocco ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-f9a0112851b54101be744a6f2079a383", "references": ["The Sun does not Revolve around the Earth ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-834d7719f309491295e6d3d1be24b658", "references": ["He wants to make Victoria empress of India ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-3966bdd8eac04b619cc3ccbf02946e6d", "references": ["They think one is saying that the Bible did not come from God , but really what one is saying is that the Bible came from God but the mind of man got in the way in a number of places ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-021c25a6b21c4c9ab19ad6cb58402df5", "references": ["Zurich is the largest city in Switzerland , is a major rail hub and a common starting point for rail excursions into the rest of the country ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-932a02d1e3844c268a61459e6e384946", "references": ["WE BELIEVE that only the sixty-six books of the Bible are the inspired , and therefore inerrant , Word of God ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-d59cb71e86f0487eada2fc63e25f61da", "references": ["Lotte in December expressed interest in acquiring Iguchi , who won a World Series ring with the Chicago White Sox in his rookie year in 2005 , later joining the Philadelphia Phillies ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-47f80bf3c7174813bca431c5da1fc493", "references": ["Karsh returned to Canada four years later , eager to make his mark ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-cab3d05265874201a54e4590fbb459db", "references": ["Rachel Field is the Assistant Vice President of AXA Equitable 's Office of Diversity and Inclusion , reporting to the Chief Diversity Officer ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-1e4855de9c67444cb42fe4d7fe2c718f", "references": ["Kansas City Inspector General Investigations Division , 323 W. 8th St. , Room 305 , Kansas City , MO 64105 ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-f66f2fb41b9545208029b3b0563883c5", "references": ["The new Aurora Medical Center opened in June 2000 ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-f19459de7ba246fb9737ef1a536ba063", "references": ["Branches are located in several Califonia cities including Los Angeles , Irvine , San Diego , Rancho Cucamonga , San Francisco and Santa Clara ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-74eb9363a0d74d7c95bdb7398717fdf5", "references": ["Pop Goes the Weasel by James Patterson *HB* CROSS! !"], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-2853509db6d4443fab6a54b6c0ffe504", "references": ["The Board of Immigration Appeals is part of the Executive Office for Immigration Review located in Falls Church , Va. , and is responsible for hearing appeals of decisions rendered by immigration judges or certain Department of Homeland Security officers ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-64936e225e234316ad7c78fb45d42a77", "references": ["Wellesley Inn Naples Hotel is located in the City Center area of Naples ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-cc3c572a1bd248f889c88cf3580991d2", "references": ["The McCain mailer attacking Obama for allegedly favoring dialogue with \"terrorists \" also takes liberties with the \" just \" word ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-ff96b75a40864cfe88e16599cf2c3137", "references": ["Woodrow Wilson , former governor of New Jersey and president from 1912 to 1920 , was a member of the Democratic Party ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-eec9afa0c0c64eb5b7b90dad0ec1063e", "references": ["Thank God for Jesus Christ who always gives us the victory ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-5f8f5fd2288843f0922432174d3cd607", "references": ["001:008 And Solomon said unto God , Thou hast shewed great mercy unto David my father , and hast made me to reign in his stead ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-bbaceffbeb864a99a0b9473fc4a636bc", "references": ["Associated Press 2008 Republican presidential hopeful and former Arkansas governor Mike Huckabee speaks Monday at the University of Arkansas Clinton School of Public Service in Little Rock ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-4c70612d6407490cb0a4cbcbe36afd40", "references": ["The Philippines is an island nation located in Southeast Asia ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-47219a1a4f8b48379c8a9a08fc5874d9", "references": ["John and Mattie Ethington Ray lived and died in East Glenn Indiana just east of Terre Haute ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-ac7b79fcb67b4f6f875be79596de04a5", "references": ["Japan to-night declared war on Britain and the United States after launching full-scale naval and air attacks on two of America 's main bases in the Pacific - Pearl Harbor , in Hawaii and Guam , between Hawaii and the Philippine Islands ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-9b03ab772217459d8965fb870f35b8e1", "references": ["While most national polls show Obama with a modest lead over McCain , few have shown him winning a majority ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-970e93f404f34683aa8b1f2fc527dfad", "references": ["Preview : they look like big round ravioles stuffed with into snoozy little balls or cats sitting on the movies soundtrack included jazz fusion legend Pat Metheny , Tori Amos , Thomas Dolby , saraland community center Grace Jones , Seal , Wendy and Lisa , Jane Siberry , Enya , and Frankie Goes to Hollywood ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-d1a8396e53914082a3a2a8cd04cff944", "references": ["Bluesea CDA MPC RM to M3U Ripping - You need to convert your audio files to Mp3 , Wma and Wave format , but you have tired to start audio conversion utility every time ?"], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-def6558521df489b89e6546daa486cfa", "references": ["Two viagra online buy i .us the Paradoxical to ( also in Health in normal and offer of central or , the pills of that in bad effects side viagra 50 require the afterwards ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-315d983200c8489eb9df0523958b1d78", "references": ["A faction supporting Lenora B. Fulani for President walked out and elected Kwaku Duren of Los Angeles County as State Chairperson ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-7603805f54dd410c81aff38115d9f000", "references": ["Funding was provided by the U-M Department of Dermatology Cosmetic Research Fund , the Babcock Research Endowment at U-M and grants from the National Institutes of Health ........."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-6c55780f08404bc09ac7902daaea3d3e", "references": ["Koenig gently squeezed her shoulder in silent support and turned to stand by Helena ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-b991290a274e452395aeb744563d44ef", "references": ["Night stay in Delhi. ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-5fa20191c3e0494f84ce295b87e1a509", "references": ["Not to mention some serious bravado considering Microsoft is currently suing Google to prevent an employee from jumping ship from Microsoft to Google ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-d26f95526bc74db6823e12ea62ef0689", "references": ["The President is the Head of State and Government and is indirectly elected by the National Assembly after each parliamentary election ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-3e0b086672c247e18f61f5f2bf99a5cb", "references": ["The Chamber is supported by the Ambassadors , a non-elected committee of 16 members ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-200bed8be43a45be84a4f500dbf634e3", "references": ["Exxon Mobil is based in Irving , Texas ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-26b3ee690286499298075c72bd710ad3", "references": ["Joan Baez , was born on Staten Island , New York , to a Quaker family of Mexican , English and Scottish descent ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-8ea83a5275014fca9ba95095cd5ff50c", "references": ["Cotton is known to have been grown in India as early as 3,000 BC , but was almost ceratinly grown much earlier ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-2ff0c82a14614d74ba25571b636e84b7", "references": ["The Red Sox won their second World Series Title by sweeping the Phillies ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-3240920de8e2404ba94072ba420dd602", "references": ["I am a graduate of California Lutheran University , located just outside Los Angeles ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-8225cf6e26d547979a48fd7abbb42a65", "references": ["Born in London , England , Peter Robson moved to Canada in 1966 ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-bbc1961daead4b9a899c6316db7fa512", "references": ["SMX Advanced will be held June 3-5 , 2007 at the Bell Harbor International Conference Center in Seattle , WA ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-61c4717bbac54e4fae72e2314d96dda4", "references": ["Others came from South America countries like Argentina and Uruguay following the rise of mobile warfare in the bunker ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-8766625c551e4b7dbe5df3768de90b9d", "references": ["So when Cul de Sac were commissioned by Boston 's Cityscape Motion Pictures in 2001 to compose the original score for The Strangler 's Wife - a low budget , feminist-leaning slasher film made for New Concorde , the production/distribution company owned by \" King of the B's \" Roger Corman - their cinematic music became positively widescreen ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-5e4173e143da44cfaa2573928a9657bb", "references": ["Also nearby are the Tuscany cities of Arezzo and Val di Chiana In the eastern region of Marche , you can visit the medieval town of Urbino as well as the many beaches and picturesque towns such , as Fano and Senigallia , on the Adriatic coast ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-d0fbd455659444f18fcb604c86820046", "references": ["This passage tells us about how God ultimate revelation of Himself comes through His Son Jesus Christ ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-a3216b7912e047498d10dd51fc999f1a", "references": ["This is the travel forum for the continent of South America with countries like Argentina , Bolivia , Brazil , Chile , Colombia , Ecuador , Paraguay , Peru and Venezuela plus many more ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-f9739663ff294836baadb4aaa16f9558", "references": ["David Commons was born 7/18/1800 in Virginia ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-4dd1252ae5e841c7972420429d1b9769", "references": ["Find an exclusive and luxurious holiday villa on the Caribbean island of Jamaica through Palmer and ..."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-e0b135dabe89413ca0514a0ee3ace9db", "references": ["NIA is one of the 25 institutes and centers of the National Institutes of Health ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-f382d061d0f7450ca7269ed4d43ee41e", "references": ["Palin , a lifetime member of the National Rifle Association , is also popular among gun rights activists ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-5a151e9ce770446db716eaeea4be0872", "references": ["Nicholas Wallerstein has been promoted to full professor in the Department of English at Black Hills State University , located in the scenic northern Black Hills of South Dakota ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-096a146be4424a7d9d4000cae2dde896", "references": ["Herman was born in Mobile , Alabama , in 1947 and earned her bachelor 's degree ..."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-a6464b624aa74576a6f0b35ae4112c4e", "references": ["PDF documents on this site require a program such as Adobe 's free Acrobat Reader ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-5e3a65c3b8874a76901bd0d9f8f71600", "references": ["Montana Online Services - Includes all online services for Montana ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-e7f3f1a47b3f44c590eb6a16199f0dac", "references": ["Wake Forest University Baptist Medical Center is located in Winston-Salem , the cultural and industrial hub of one of the fastest growing areas in the United States ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-94ebcede5d754ef2ab265311ab0c1bc8", "references": ["What God has revealed in Christ Jesus is a mystery ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-390fc3f7c5cd42e4b6ac802cc7962a41", "references": ["By 2007 there will be 27 PACS sites in Newfoundland and Labrador making it the first province in Canada to have a province-wide system ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-72655b1bba99418ab8d9aa6fe3f43f14", "references": ["France declares war on Spain ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-289d4296790f404b84d9b0cc96ad117d", "references": ["Now that Barack Obama has moved into the White House , politicians here at home are rushing to take advantage of the overwhelming ..."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-c0f4f92498cf4620a560983805ec5af1", "references": ["Never mind that Christmas does not celebrate the actual birth date of Jesus , that \" the Christians stole Christmas \" from the pagans , as Leonard Peikoff points out in his classic essay , \" Why Christmas Should Be More Commercial . \""], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-a24fc40977214ee6afdbc293a42abcf2", "references": ["Huron University is a small international university offering programs in Business Administration , Communications , Humanities , International Relations , and Studio Art ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-3f998f7eb743451790cc62f002d8465b", "references": ["4 Then Peter said to Jesus in reply , \" Lord , it is good that we are here ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-6094a65e736243639cd76624767643ee", "references": ["On Jan . 8 , The New York Times reported that the Bush administration has withdrawn from Iraq a 400-member military team whose job was to search for and destroy weapons of mass destruction ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-43ca766463334ed68b8a025f886aa926", "references": ["The NAFTA at 8 Report was released in May 2002 summarizes accomplishments in the eight years of the agreement 's lifespan ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-8b52973b146846039f92ad9f7988b501", "references": ["This short Video clip includes THE LATE TIM RUSSERT OF NBC NEWS directly calling out Obama on Obama 's Playing of the RACE CARD DURING OBAMA 'S CAMPAIGNING IN SOUTH CAROLINE THIS YEAR in the Primary Campaign - in a Divisive and Extremely Shameful and Negative Manner ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-4fdce5ca115d4523a7cef1dddf210809", "references": ["Skylark School of English is a young and friendly language school in Msida on the sunny island of Malta ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-c0d9c39decb3492ea31740628053ac4d", "references": ["World War I begins in Europe on July 28 , 1914 when Austria-Hungary declares war on Serbia ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-92abd4a7ae7b4dc69a8707c355f25231", "references": ["Paris is much more than the capital city of France ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-cdeb4626c909417195a343bf3001f219", "references": ["Andrew Johnson was the first President to be impeached by the House of Representatives ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-d8aeee43372d408caafe3e66ed2863ab", "references": ["The Dark Knight , the sequel to Batman Begins comes out July 18 , 2008 ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-11ce09a8d5254a2e9a821be46f3e93aa", "references": ["And the Zionists kept their end of the bargain by working through Jews close to the Democratic President of the United States , Woodrow Wilson ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-dcf62fe8cff74499904325466f95b3ba", "references": ["OTTAWA - Defence Minister Peter MacKay says Canada will pull its combat troops out of Afghanistan no matter how nicely U.S. President Barack Obama asks us to keep them there ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-976df2e069d34cc38926edf74ad8a407", "references": ["Buy Viagra Online Cheap High Rollers Buy Online Phentermine Valium Viagra Xanax , often Buys amoxicillin , a porn spammers more than Buy Online Phentermine Valium Viagra Xanax ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-ae113665f7de4094819bce9324b12758", "references": ["After Queen Victoria of England married Prince Albert , the tradition of the Christmas tree was adopted in England and gained popularity ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-66b31abf47684964a7f3539272fff36a", "references": ["On Friday , November 22nd 1963 at 12:30 P.M. the 35th president of the United States of America , John Fitzgerald Kennedy assassinated while he rode in an open limousine thoug..."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-5c93b08135664c4fac80acaa522b57eb", "references": ["Mass he celebrated in New Delhi on Sunday , 7 November , the Holy Father led the recitation of the Angelus , which he introduced with a brief meditation on Our Lady ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-7f6d295a27094c30a44a48ed7b15054b", "references": ["To President elect Obama : congratulations ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-396779d3bb204a6e9433292bb44bc2b7", "references": ["This paper is written about one of the greatest works of the greatest playwright of all time , King Lear by William Shakespeare , is an unforgettably disturbing story of unbearable injustice and cruelty as well as of unseen loyalty and love ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-cacbc381eede4ffaa1f20ef6df38d0a6", "references": ["Japanese symbols translation of John 3:16 is written behind the Women 's Light Pink Christian T-shirt and Jesus is written in Japanese kanji symbols at the front ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-6cdd9d3fee574640b59480536d5aaa70", "references": ["Find Made in China fertilizer Products !"], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-3ab9c92f0d7d45af8ccd582313d90d22", "references": ["Paper presented at : The Annual International Industrial Ergonomics and Safety Conference , June 8 - 10 1988 , New 0rleans ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-b01e836c8c1a4cb5958082a8aca4cda0", "references": ["Fran?ois-Marie Arouet de Voltaire was born in Paris in 1694 , the son of Fran?ois Arouet , a notary who was a minor treasury official , and his wife , Marie Marguerite d'Aumart , from a noble family of the Poitou ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-28d4bb4d2eae47ef849af895b098ea06", "references": ["He was essentially forced out by New York 's now disgraced governor Eliot Spitzer ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-13b76e00072148fc8d20ef5997083481", "references": ["The 12th National Conference of the American Association for School Librarians will be held in Pittsburgh October 5-9 , 2005 ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-c25fcf0e7c524b54ac1bd47bf24710b4", "references": ["The unit and research station are part of the Forest Service , U.S. Department of Agriculture ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task677-af2265ee372e416c88d771499f355531", "references": ["The Board shall appoint one of the members of the Credentials Committee as the Chair of the Committee ."], "task_id": "task677_ollie_sentence_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task220-eee0488f20ec4f9d87baa4cb150cc4bb", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-bc4bcfae1a5e43bbb1a5d043fc4a5aa5", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-e267fd30564b440cb3d4742bfa1f6d33", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-89dc5a9b4a6448b4ae2ad753eda85a13", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-b5289262bd1341879d4c9410fdf6977c", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-11a873ed47bc4acea1b65b6fcc288faa", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-a492fb6f1550483cbe4bc83983a27b3c", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-f7280bd8b2f9463dbea3549a3ad486c1", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-ffb71ba987724d67840c54af49d2b03e", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-eaab9f1a2d1b4aafab705d4732c0c72d", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-165f549966ff46a9979647c2a4b7b2a5", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-d6a1fe0c64744cba8d8fa6b7e1133a63", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-3950ad1e3f194a3fb5d310c3c56e33d3", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-6a4ba8341cb045289d133770abfd7548", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-e41604a8ccfc4573a13c2537dd6ce94f", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-eb53e2aa74a541ddb0e7920f31ba42e8", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-4893717dd3ff424a9b022071f13847e6", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-00902213c31d45fd9934cb083b56ee4b", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-afc0579389a54bf5884c4faf36640778", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-4244cb6dd3344c05854287494893a101", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-74caa67e82274b78be402959696f94e2", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-f88b55aa5d2842ce8f8c53d0d69d23a6", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-727acef83aa64e8bb28beb1b99ab5689", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-f2a593f4dde041e3bbd1b893220200a8", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-e3bf4b93e5dd46b9ab897bef4b49e884", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-a8a97ec3639f4e2997d88fd5bcbee893", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-f4ac3fcc9e2f448bb2b14132e1568803", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-2e2c6bfb7cc043b0bbbf68d1e89ec3ed", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-d90bd1c7d1bf4d7dbb5fa5455120a5c3", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-370124d8189b4dbda1ccf7ab70fba1c0", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-bf12b4de40504adfaaf5025c6acf5673", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-515d24b7987b4de2a8d70c9b2cd8ea36", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-a1d96a103697433d91a2e611871f4af2", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-6b87b522ee034fbbaa6151b5d5aa82cf", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-f60d598f00f8439e9210fc5c3a112edb", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-1260229c09e14348ac6b27e5f66b6967", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-3472828ba47a4131aec5f21ca90d417c", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-df1ace85eb714a77952c31bf7d967652", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-4ab33251c2344a3fadb7c3571948acd1", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-7cdaf6ed7b5144749df4f4693af6b4da", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-517b4f39e5214792a8f623ef34ce824c", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-83155f26123e40e79aa8de7a98cd6d9e", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-dc936e8825564e539204d80edcedf85a", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-5847bcf293164899bcbec591a4ed4a80", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-def61634026f4c7da7e50cd5aaffd667", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-e0ad485893d94fcf914eb843f9468919", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-c636c672b9c8488ab0547d628571483c", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-cc028428ea994a5184e1bbea371a8890", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-9097269483554d6fbad64a9185b841d5", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-1c85a2dce9eb48f490cab9384512f946", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-2e83707f746a4231a65af8c587f1fda8", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-3c1893f17ba4428897832d9e19e65f61", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-3a068132b3204f3e88d5428ef748af60", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-3ce18581bf4049afa853ea2552f39f69", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-00720cdcbd26446ab4dedef996f4fcfd", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-7e34e708ff1546468ec54eed36edb5a4", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-326606cc28e34047ac33de0a0073997e", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-41a48c259b1245edbaf48f4426ac5287", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-2a562a39768c4facadf28c8db0bd4ca5", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-b7fe08b14a0741298ee4a797b3f130e5", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-91af85e2b1b54a8a8097d11ab2dfa766", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-356d765c72b34d4aaf929f1f6eb9a824", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-0ce0e1afe20b43f4a39d38de50575b78", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-7ac419d7bdba49bb8ca0b64ab836f098", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-0663f3e50be241c7a5d2c880de7cb91e", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-058af65c2b6c45fd9b775a7fc4070a06", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-9ae658469bb44044b06cf86d2f9efaab", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-1f59a4c8fc4d4186a304b0209d0b032f", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-9ced27f60847420f8ef1f5117fe517ce", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-7c53eb5899004d868ac40d5dfc2af9d0", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-3dfea1dcba584667b7d163bc30e95963", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-88cefba2996048fb9e721cca64c12ae1", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-d2c8e7da479a4386b9a042157789ea13", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-8da9b5c7860e4fb3ac62c08ca1399a9c", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-a491cbd3d50e4241bb4fcf2ff9d8b954", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-3606976e180a4cc9925c66cec132e34d", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-6abb788228a64d0e83017f4802db0e6b", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-6a98529f2c1641688f51930bdbf39e4d", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-33c7132df4814714b025b9f9e7623395", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-ae52fd485fac4489a201f836ab3299a0", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-4d3366e6a85b4b649a9d94a709b9760d", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-f39fb40a10404820b2106d63ede49174", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-fc06b7e1d2814fe994648cf6b815c018", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-613c99dd8e564f1e9eb4e46f89f60eb7", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-47a97dd20bb8448f8e3a4d5c8de92c1a", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-00ea8c4a2c064e07bd76e2c9c0153463", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-f1ec08714a1a4da4904963e96f4e1bfe", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-0660af00935946d8b98089eec879fc1f", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-246be526d572415a84a7ce4dbe6953c8", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-7a3a30c0d47046bb96221a9a06458d67", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-944422bdc5a3402faf39960f7b426197", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-e06fa3bd86dd453c98bf467bc4dcd95f", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-c18dd02387994da7878f1f992e78615a", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-891fb166d4074c1792d9608870f36fe3", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-ed28c66212b84f1597ec79949631ad16", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-e4138c2115be4e8fb44705ad8c293108", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-bf0bf375301b4aac8336a60618cc1365", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-36b9627567ba4a2485528e0db230b073", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-a01afbb2195e4733b3380175cb89e8d5", "references": ["a"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task220-ae7a00a345084007b2a659a28af9266a", "references": ["b"], "task_id": "task220_rocstories_title_classification", "task_category": "Title Generation", "track": "default"}
{"id": "task1631-8738748ebc0542348627d73fb09a5a5a", "references": ["position of a normal position was taken before and now changed to a fighting position afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-a28a280d67524e9f9a1a9149c79a7020", "references": ["location of person was in air before and on ground afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-15d73097740d491bb008c06f5bd8b55d", "references": ["location of trash bag was in a container before and on the ground afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-6cce4c3608224e97af140979edf74279", "references": ["location of newspaper was on floor before, and in trash afterwards."], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-9d2b4d0b39d24566964c211583852f43", "references": ["location of plastic bag was on the floor before and in the trash afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-494f24bd49744e41875b31844930af6a", "references": ["location of newspaper was on the floor before and in the trash bag afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-9b7d19e011e14adfb3ec6e89d97f4d4d", "references": ["wearing of other clothes was being worn more often before and being worn less often afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-b9322db589124385a02d94f5feff13cd", "references": ["awareness of necklaces was unattended too before and being attended too afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-62f06c1fedc1416dbeaab0d7c9cad73f", "references": ["wearing of extra layers was being worn before and now removed afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-6f35afb5351040cb83f6d6db72d37a64", "references": ["wetness of ink was wet before and dry afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-184285dd14dc448ba2a753901e534bd6", "references": ["capacity of display case was empty before and full afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-b010466dd2bb4e85bf210508ac767a0c", "references": ["grovehoop placement of grovehoop was on floor before and placed against small of back afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-81b0b783e9494c4abe5f74ff33510716", "references": ["movement of hips was still before and moving afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-e0abceec64d344baa2b2f7361d2a0ee1", "references": ["movement of legs was still before and moving afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-ee479b57151c427290319fda910c8a46", "references": ["head of head was left alone before and measured front to back afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-019f33a88bd54c65a254f293753131a9", "references": ["head of head was left alone before and measured from side to side afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-2d674a365dd0449b83f295375d6bfdd6", "references": ["complexity of project was started before and finished afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-e446435938ed4c32aeead30905b561a2", "references": ["freeness of fish was swimming freely before and caught on hook afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-eec2ac93f3824cd9ae45dd1614c30985", "references": ["energy level of fish was fighting before and tired afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-f7f523291ce84b32af41551f3e4c871d", "references": ["emotional state of fish was calm before and panicked afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-2ff2758352ff4c888c61ae97211d911a", "references": ["location of fish was in water before and in net afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-2c38610efce54d2ba81f84e638c8a6bd", "references": ["state of binder clip was in original place before and found afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-f9fdc80eacf1470f96d52ec2951ed8d2", "references": ["length of toothpaste tube was longer before and shorter afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-e7b955ff4a4542a7babf071e41614c94", "references": ["fullness of toothpaste tube was fuller before and emptier afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-22638fad866945af9bb346a4a195980a", "references": ["completion of task was incomplete before, and finished afterwards."], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-0f59a47d40294bb2b107db4941d06950", "references": ["ownership of liquid paper was at the store before and in consumers possession afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-3c9660f8af7d44e4b4c2633275f56ecc", "references": ["location of foot was in front of the eye of the needle before and behind the eye of the needle afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-adb6a1e8b5e64696bbd1999fad543d66", "references": ["state of area was wet before and dry afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-32a4c66670e448dd950c38a6c900e128", "references": ["location of metal plate was unattached before and attached afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-b8993132e3764c458c16a03636225eb5", "references": ["holding of they was being held normally before and now being held in a hug hold afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-8d4fbef7a47a46beae6da2d26c3b6de9", "references": ["holding of they was in hug hold before and now in face-to-face hold afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-9957e69144bc4c95ac6ae47f5e6729bc", "references": ["holding of they was in face-to-face hold before and now in modified face-to-face hold afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-010b4f055d20460f8a7f2c1e672b05de", "references": ["holding of they was in modified face-to-face hold before and now in the pass hold afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-401c845f425543f3b0d84e352e8fd795", "references": ["size of mango, lemon and garlic was whole before and cut into pieces afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-8ee50060355a42f8ba4b86eea9780469", "references": ["composition of ingredients was separated before and mixed in blender afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-a21f69e9b7ec4d20bce09fa847a95042", "references": ["taste of meat was bland before and seasoned afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-6538b9de64114be1a3a923a0cd6dace1", "references": ["skill of gymnast was practicing level 5 skills before and practicing level 6 skills afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-55619af1bc6a4e159ba7b72271df5b5a", "references": ["education of gymnast was practicing alone before and being taught by a coach afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-cf71bdadae7842b085b969bef67f71a4", "references": ["state of gymnast was standing still before, and dancing routine afterwards."], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-fc30ef1e128e4c1bbe8da7e3c7b90952", "references": ["composition of baking powder and cream of tartar was separate before and mixed afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-8c84ced642b74c63af59d20f58b7ecc2", "references": ["location of paste was in container before and on floor afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-77d7b4323fda4a02a3ad2b18a762f329", "references": ["state of stain was dark before and light afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-c59102b2752a47fab3ff6a0e78434f0c", "references": ["location of paste was on tile before and removed afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-c1f6b3caa9d84ed88eb179c2b3cfb682", "references": ["wetness of the laundry caps was wet before and dry afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-158619a5d45943aa903625f4c40651b2", "references": ["location of the rice or kernels was in a container before and in the cap afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-a682c244533043a5a2c702a09ca3fbfb", "references": ["wetness of the rims was dry before and wet afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-6a890cb5da19440dbe801b83c936617e", "references": ["state of the caps was separate before and glued together afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-29ab332d7bff4e819b88ff9f06f9b26e", "references": ["tension of rope was loose before and taut afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-52c01459533f4598ae0815339731d6d6", "references": ["location of hand was in front of chest before and chin height afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-75d9ec1469d540d59878a832846c75c8", "references": ["location of arm was at chin height before and nipple height afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-eb9dedaff2234cb7b1d13e6fc0d7389b", "references": ["position of arm was at side before and chin height afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-3f68fcb43e0145a989cae63061dc3bc7", "references": ["availability of pvc pipe was unavailable before and available afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-4c19f9a3ed8846409b631d4fa379f67f", "references": ["composition of pvc was uncut before and cut afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-86edab49d1624e8c8b70dd525896e476", "references": ["composition of pieces was unassembled before and assembled afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-4e8ec253d903475ca6615fce021a688c", "references": ["composition of paralette was unassembled before and assembled afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-2bedd66f56e14f0b95a98bc1a78900d2", "references": ["cost of you was shopping for tamarillos before and purchasing tamarillos afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-d266d24f9e6c4df1962311414d9cc4b4", "references": ["temp of tamarillos was warm before and cooled in fridge afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-70e7dfc473504dd7ad8aa5423264d93c", "references": ["wetness of the tamarillos was dry before and wet afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-cacafca19f3f409298c999fea009c019", "references": ["state of the tamarillo was whole before and cut afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-2bc88625c09640088146eb27130f193f", "references": ["position of person was standing before and laying afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-fde98abce4174842a83bb486f5c69e2e", "references": ["state of person was sitting on surfboard before and standing on surfboard afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-52b2a5cf48444c26b663303a5281cb1f", "references": ["orientation of surfer was seated before and standing afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-f19f36e282ae42f581c81be8026f85a0", "references": ["location of surfer was standing on surfboard before and floating in water afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-3e67cbc9553e4dd984e729d372290df4", "references": ["distance of feet was close together before and shoulder-width apart afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-82fead212d1846a1b5183eeb89377aa2", "references": ["orientation of feet was flat on ground before and raised on toes afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-41b6fdc32d6b4b0a98e132c810da64b1", "references": ["orientation of feet was raised before and lowasd afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-d52f17b4be2c43a68ce537a71b329b7d", "references": ["power of vacuum was off before and on afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-80708675a4db4a109275d4fd2d464899", "references": ["location of talkum was in the bottle before and on the pillow afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-db2581a18e3544dfb36ef4f3448bab9b", "references": ["coverage of pillow was covered before and uncovered afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-ae095413beb947668646e3d4df02f79a", "references": ["cleanness of pillow was dirty before and clean afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-a8f633e569e04577926660d5dae44d98", "references": ["shape of rope was straight before and knotted afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-494e5417831845759a7a88198e01bf4d", "references": ["position of loop was on the farthest side before and pulled through other loop afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-1e9283695429432aa02c2bed09cd715b", "references": ["thoughts of thoughts was on other things before and on marital fighting style afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-43d9c218f0cb4d47bd89de9008d290d5", "references": ["group size of learner was alone before and with another person afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-3b4dd1788a754fff85f421c85ed565fe", "references": ["knowledge of learner was ignorant of fighting style before and educated afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-af97de65ae7648b3b156bec8ba09cf32", "references": ["mental state of learner was secular before and spiritual afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-48a244753daa41b08f7ef93d302dab29", "references": ["state of fennel was green before and smooth and white afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-9a859b53c96247bea135f1ea8430ad40", "references": ["state of hand was empty before and squeezing the fennel afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-e5c8bdf86e3e4b5784b85c0d38685db2", "references": ["state of fennel was whole before and trimmed afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-cfe7a42cdb5d47188f33770306a32b69", "references": ["state of fennel was bare before and wrapped afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-07d80eff8f3f49619263e7eea437d6a7", "references": ["knowledge of baseball familiarity was absent before and present afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-e144df153a504a7d9c7e9c5950f91acf", "references": ["location of rope was separate from ball before and through a slot in the ball afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-96ed15b2eef143ad96cb8c99fa39b138", "references": ["location of pole was absent before and present afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-e2d970f5e9f84407a232b785b6ea23b6", "references": ["location of bat was on ground before and in hand afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-612c63723d4d45d68a4621e09f0843f8", "references": ["shape of fingers was straight before and curved afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-8ae414fdcd47491fa027b2e54e682f68", "references": ["location of ball was on ground before and in basket of lacrosse stick afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-e487d654db4043ce900a6be58cfe5b66", "references": ["location of dominant hand was farther from basket of lacrosse stick before and closer to basket of lacrosse stick afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-b483dffc39b64b218a5a06a958c1fe5b", "references": ["motion of ball was stationary before and moving afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-5e32775444cd4fd3b2bef54a05ffeb4a", "references": ["state of a watch was needed before and set afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-3579942f58f7403e88d5d448ad5e7b11", "references": ["state of your kit was without food before and now with food afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-b7ae71bf7ae049da870ad51440290d6a", "references": ["state of your kit was without a phone before and with a phone afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-646d263ed464406ab13634ee0d2baefb", "references": ["state of you phone was needing emergency numbers stored before and now containing emergency numbers afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-9461ef7c8329416cbe1ed7b1d1dacf5d", "references": ["size of flowers was attached to plant before and cut from stem afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-db543eba7fd7459aa45155a8cf43a73e", "references": ["location of flowers was in garden before and on table afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-ad6a98b496624605823ce258c0abbe9a", "references": ["elasticity of rubber band was small before and stretched out afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-7106222b8e0946ac84db6b93776865e0", "references": ["shape of ribbon was straight before and tied in bow afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-c1e2f7f2d77843a1be70ecd13bec5c67", "references": ["power of stove was off before and on afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-a2c797336e8c4a99a4bd3329db38e8e6", "references": ["temperature of sugar was cool before and warm afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-78a8579005374f858622c98e3fbbd666", "references": ["cleanness of serving dish was clean before and dirty afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task1631-0df8b3625a604b828dd47bf9a223ecea", "references": ["cleanness of spoons was clean before and dirty afterwards"], "task_id": "task1631_openpi_answer_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task232-89210941e3e44cb3a039b1ccd7bbba1d", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-71ac5df4e4ab4de8bb3d10ce7cf44bd3", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-cd58df15dd4443a4a4ca06e7086100ba", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-4b9a0cd709314021a88165d5d39c7973", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-cd528162eef243cf822b8ed6e349f8aa", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-8c01c635f81647c49cb93ebefb09bf23", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-a0dd7ccb3aee42299c701347bc5f7c2f", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-91a64cef8e26477a98f7a164124df910", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-9557230460c8467d86240d44c0c1972a", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-6f47a88766e84539b2b2b3ebd1a4e268", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-59ae74b3d4e94803a994ac0ebad95421", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-8d32ee1207c04d62b8e8b53a709815f7", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-ba138cab15144ba6ac67302e791e0d7d", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-d0de8f6c04ce4d85bef8465ccbc7e623", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-f89f09806dbf40af8481e358e89fa0b8", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-01cd0165f25d4c42a6efc61992cb1558", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-fad8a8639e4a4229a398a72aca44b2d2", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-a869ddf5b9c34905976d9448ba56a266", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-ea059560ee9f4f89bea4a9e74ed7d3ce", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-daf4728a65374dc9ae37e448a1a1bc73", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-510f34304cfa48b0b50338e84c9cde1c", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-125f1cf098894b5b8534826f07c750a5", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-ec9eca6328ab4b08964e7e0718cf9559", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-f584f0e52b0f4c1cad642e296265bd2e", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-0e922e4e44b74f9c8f06fbb2a314d9ac", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-a35534e3d5e0454a84ec1c3ff4aeb4df", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-0ba31437ca3d4c4487a64cc5e0380315", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-14da8b984a8a4329b6a63eca6ecbc35a", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-8f3f4e6e0bdd476187d820af044d72fc", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-86386ccdda06473c873ffc36da7ddbe5", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-8e580f0b8af247608e9cab2ce6de4b56", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-c21bf623d54747d69ff1a79be4b3d4d3", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-5db4e5db67ff4f19bb0f35284f7628e1", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-079f51ae53e644bcbf08de333d2a4d9a", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-ac1e657580174e0eae7c26dd0923b99a", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-a894e4399ecd4368875f80879ccbfbc5", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-ceac216887da4a95986cba8531190016", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-6fc1e3bdb04046d9bd158cd283c3cc1f", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-949d42645f4e43da8bba68538e3ea3db", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-16362b93d13445dbb67d97d1171bcb00", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-f4b47dd3804a4051b4b018aa00867b4c", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-9f2201a4c16c48f39243eeba1a457991", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-8959440d8719460292cacfe2c1b653a9", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-2b11973b1c874493b9bb429903034b2d", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-fe259e3923334750b6ff9f874579f66a", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-1d012609ed5549608402bd444feec7b2", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-d9b85ab2b1a74298aedefb4b2c3192e9", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-30f86e0c579045a5942b32f5d8cc3f14", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-4e5d7af06dbb43f9a991293a4cd0b475", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-2acb46fa3ef840be8bf9167c8b507942", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-b3c50d2b54114af780edc1f481198b34", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-5e2a3c5cd44143fcbc93b29b27abd672", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-bba56bc836b24501a5f0999ea92a9c00", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-460816b3637740dfb6fe9c54120b64b1", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-b11dfaa6c23441189cfade539fdf6a87", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-70ffc2e7f8d649d3895d746cdb0085ca", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-b6f90e9c938c410f9c1efd7b811331be", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-241c15fea0504477886462480255394a", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-b40b605de96b4229b3a76e9485c0c5df", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-b0d67bb81a17495e9d59827e4d063e23", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-75f9e9d1e35d423da90b99459ca6f494", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-df0b60d338d04361be237fda836fd12e", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-141ab23ac3c941af9a6399f5dd4e9ab0", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-0de80819183f4e0aa56f5b1ad1dc5961", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-eac6f01debaf4fa99223cc896d82f977", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-28847ef9d5e24194bc9b44104aa7fff7", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-3a5251fa2b1647d4ad336030549d7d6b", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-ba4903571db34ff88a7f78f2ac1d8967", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-ee682955b9524e4d86f07618d4aadad5", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-9b6c002d97454503a4772a857ef9cee1", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-36e9d7476c494ec1bdd751d9c24370fd", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-436f99be4d5b4293814b2eed2f4ef1e1", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-43b2d03b34004578879b77b049ab425a", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-d783b82b4d6b405baa4ca24c1bd92101", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-30809cc401dc4d579b2f8d1fd2967a44", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-0cab72976d45411481739ecb95edb425", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-fab5af4f1c4a406d8f853c5fe0b25f79", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-4b8506d0190542edb81fb6c7d2cbecb5", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-ad957145b6c04674ba063719fd5be7f1", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-7cbab4fe11744108878559be9a1ecf12", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-1d07e9c1d7db4372a0aafb6889ca16a8", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-d332d011fa4f4bdf86ec127d9fb01154", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-2b6222e6131c4cdd9be377bd26d366d9", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-b45a3099ee724e05ab9eef482c5f5ad3", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-47ea1205d0a940cb9993eefacfee0468", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-9df303f84fa24bf489e01e91e037b683", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-853aebce43c644a3acdf1ae2951b9a27", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-f039780d1d4e40d98a46896e55044adb", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-8473daee2e314e559cbfc4a00d3818ed", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-9e90d1e244e4430f888cd7e62e96cb18", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-a83cb6c9b66b44ccbd94fa9cbebd72a7", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-434bb7a6af754d4b943a3f464499a381", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-62401dd4e6a748028a9169a328d16b79", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-ab3d748db00540bb8ea6134d177323f8", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-c32a3cf78bd74806ba582813a9a44c09", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-1e52fb90fd6c453bb281f464f7fe0242", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-e9a4fd7b54c841348a49f18339f6565e", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-ce268848947f4365a869c0bd1b7f2395", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-5d083649b339424ab45cd70037fb166f", "references": ["b"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task232-72afd7ee1bc94ee19bf92f6f81918e82", "references": ["a"], "task_id": "task232_iirc_link_number_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1391-70e76e2458c34b6e8f645b51d54b0748", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-94f70cbe065f4705ad82cda463626464", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-c71bf773c9a14adb95eaa03e8b7f0d48", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-8fc3c54efd4345f88ac03f6d37b3843d", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-b1b358364e0f41ad8230683f9f3fe570", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-314d11359477470a9bab0ca100432a59", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-0797f3f385514069ae1e5e90f68293b1", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-21bec8f294d64af68dc26333a8c63d8a", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-90e19dfc27ec43cc8d772facb2d87f7b", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-fad586e58270431b83be1db5eda79bab", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-b8d113079dfb4f8dada0b288549f57be", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-5b585f5323024bc78f4c6e63a4baea0c", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-ed0d0faf30d249bda76252033306839a", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-bf6912cb4bfe47088ce5e2811e95b579", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-b72e949ec52b44df924f9761eadebb46", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-cde4160fd41f44f2bd2e2f3d919154a4", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-1115169a4e4d41ec9a7c5e89bd33d805", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-c9f6f978ba954dbeaa2825e86ffb745b", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-54afb8df3da848228a58db640305633d", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-d33ab79aa42342b5a5d66b7d6ee8edc0", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-f2fb7c4dfc8040779094a6307b0bf0db", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-0436bad782b6443582d037aec6b4b96e", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-e1ef2889b46144c08bc01e0730a7cc04", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-fb653d8425df4e6f9424e76652d9dd66", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-bef9969d88d947fd8fc55ae09a17acb7", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-24e0bb5b57da47b8a242996aa687207a", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-a2cc5c5ad7904f739e0745139c58ac70", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-b9af1f2b636e4f0e993cfdabe44babac", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-6d003c57c885419387687d8102026120", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-eddde61b4def475c92703d11c1045939", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-897e12827e7d4b64847209637eb3e666", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-9c04ec41b04e4195a13b3914a1a9589d", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-f0e5410037dc4a86b667bf1dc44d46f2", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-d2908abbf2ce43cda51d2f4f5ac1cc23", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-814d4b44c8e8498eacd127de18fa3038", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-a566d7c44afa417fb7b48e96418bc833", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-f591da861146412fb166d76478f8da8a", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-35580231403c4db18b781d18266f21f4", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-fdec9fb022ad46438ddc5c91e6c2f8ee", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-23e58b12d18b4c658ffa654ba8ce2ae4", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-adf9183c36bc4e3db5e3dc7ff3989dfa", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-666e4a0aacf34e0cb16dc2fbb5790eda", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-b0ebbecdcb7547afb69a2acfe8271c41", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-411cdb2c57484d869699857c7f8ce415", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-4812f53b52ef4de2a835b384281a269f", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-7e3ca042cc6a4d6294f2b3e24ccb9ab0", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-0fa057aa61744667afb7362e64b6471d", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-826e9592fc164cbfbd75fb084fee3e98", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-fa8e5af9a65b4caa9afa1224087a5ec0", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-8c233e20627f4ec78681ed2752be3483", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-67d42151ec564c4dbed6a829f1592ea1", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-5172eb14bcd440369353ec70b735c993", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-22ae05a91d3241ebbf96003cbf6edddd", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-9c5d1e8b780d4090b6cba32633269e5d", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-a3c7081e0db64f4c9b656dfc1c8600e5", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-3e181d001a4d49079b9652befc52889e", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-3d93340c12a34da69c8e773cae0cfbff", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-008b241243a345789718fbcb1d980885", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-28f150c66e0840ad82e477793fc0c64b", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-e5c73bad2226480d9953a3d73f9b26cc", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-f4e9653bdd6e41f2ba797d336eb240b1", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-e520ae66cff24f349e6aa4314276e03c", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-b9068a9f6cdc4551969a9fb9134e887c", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-fc6a3d6473224a31936c4a912b2efb87", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-6857ab49347e4b5ba2997f1e29b7875c", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-5c54244dcfe547b4baea1f2838f3ba30", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-a8c9a7a0547843388c6e94d5c36b0df4", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-cc97727ae7114fc7b74dd44f59b0f131", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-7921ae7712594613a1ace7071161d448", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-9126e81162f9490b8893fac989b0e76a", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-08b2d4476b024ed8a6b2b6f84f84b95d", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-6a3e0812294b42d3abf90ee6b4fbb621", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-af2d44534de842919f03075b2ef0fe87", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-8f950bf3e33b46beaae61992de08d7b5", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-2bcb0812bf734b1d9d0a9ba3dcc4fe1e", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-b0e21c0b9fe84259ae06de49fad1c1f3", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-9cef772b0fb84d58afd66f103fc1898a", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-4e33b39d99ed4fe7b6df4f8255044af5", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-27044b961bdb4f038e9c011ebb033da2", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-c775debdbafa4f6dadae27fcbcde235e", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-4f62314b28b543faa78bcd62de82be32", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-756c686f4195443fbe76615094ddccc2", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-7779bbc542594c0bb48985660469380d", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-07ac89c092cb4004a65e5fa43e745913", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-f7a5cdc0393243a78dca14cb97068b90", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-8722f2c7b74243418aea05cc07b79357", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-641afa76af40430da4be39dc5a26d4bd", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-4588c83e37034be0a2622b7242c4ba46", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-d9366c6374a442738c2a9edebd1322d5", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-69f1473fb17745e5bf24c3623e303175", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-a6ccdf5d5b084ad68124a06550d6a6d7", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-dd63ba63cc2c4769b7d039e45f531fba", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-4227aa61ecba4e999c064ec34bfcb408", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-7f3cdc53145f4df28b903fc60052849b", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-f1fb39ed8d7c4da79d66287088fa6f19", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-d56ad36404b14790aa83d65e2a22a9fa", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-a397164a2a0440c7b35af19f47e938d5", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-239d47df3a9d473b919fa1d970ce61a5", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-9c8c8a187391415c847ea1d7ff693137", "references": ["A"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1391-8a881e292c5745229146d351e46f0841", "references": ["B"], "task_id": "task1391_winogrande_easy_answer_generation", "task_category": "Coreference Resolution", "track": "default"}
{"id": "task1358-1fe4c6d43583405586415feb81674ca1", "references": ["Cuts fear for 'most vulnerable' students in East Sussex"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-9a29eb0b4c87461093965cb60297a35a", "references": ["'Crazy funding cuts put music education at risk'"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-089b8dc3f27548a2a9ebf761d057ca35", "references": ["Is \"made in Britain\" still a brand to be proud of?"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-00fc69cee8e948e88ecd89089775da4b", "references": ["Rev Keith Hanson banned for life over affair"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-241d7c4a4511456d86df1c953d844e8a", "references": ["Rhondda Cynon Taff to introduce \u00a3100 fine for not recycling"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-c77435a813924462b67a6c0b18598b68", "references": ["Three A&Es 'not an option' for east Kent"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-ed77e417717647c0bc07a2a52288e3ee", "references": ["Loyd Grossman 'devastated' by sauce botulism"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-2ed1db27bfee4d149e087ea49b68c403", "references": ["Shipman reforms 'could have saved Furness General Hospital babies'"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-4a1094cad9864ca48602e22c96f4804e", "references": ["Big school decisions waiting for green light from Greening"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-67e20a9e5008426db198bf53a2ae085d", "references": ["UK to name part of Antarctica Queen Elizabeth Land"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-a145afe3d87f4b60bc63b6af462f5fe1", "references": ["Coronavirus: No Russia lockdown as Putin puts on show of calm"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-5dd41a3ecc5645468c1880c141fbc553", "references": ["First Bus appeals against wheelchair court ruling"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-e468d8a5744d47d2bb17429adbca906a", "references": ["Athlete Sarah Wright dies of cancer before US treatment can begin"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-93802bc52edc4112920a34920b3fd66c", "references": ["Is government too scared of Google, Amazon and Starbucks?"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-c992faf0692842af8211ce08f2f7637c", "references": ["'Pandora's Box' opened over UK-EU vaccine row, says Gove"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-fc2c2928d5d648bc9f76e5c3ce253e82", "references": ["The impact of Jeremy Corbyn's foreign policy"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-3fc620b8565b4a7395cb5a4a7252a07f", "references": ["The fallout from a social media bombshell"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-7b3dd481422e46269d957978e2415c3d", "references": ["Rented computers secretly photographed users having sex"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-81f70c5fe5a5434ea6cfb3f6205c7120", "references": ["G4S Medway report: Painful restraint of children challenged"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-0c66a12284b4408fb6c1acb662b5022b", "references": ["Twitter changes policy after Biden article block"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-9057b6e4544147e780d9831a1fbfeef7", "references": ["Music boss Len Blavatnik named as Britain's richest man"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-ad90b65c76bc43a8a9f336dcbb2a066d", "references": ["Scottish fishing industry 'deeply aggrieved' by Brexit deal"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-a0162ee6c69b45a5aa6521ab790a9888", "references": ["Brain surgery boost for children with severe epilepsy"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-6f28b76e936047f1a8c88742b6d0324a", "references": ["What to look out for on TV in 2018"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-140262ef26414d20bcfb80010fbd4695", "references": ["Murdered daughter inspires Derby mum's period poverty work"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-d1d37dc907e041b6ae6b41a855d39d24", "references": ["Bloodhound diary: Making tracks in 2013"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-439ac1c8befb4f318d32d0062d76fc1e", "references": ["US immigration proposals: What's in the Raise Act?"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-d855a6acbfd44e9e82e85c1a3f9361e9", "references": ["Stafford hospital troubleshooter Manjit Obhrai set to leave"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-9a8caa8bdddf43be9e21ac508451d7cb", "references": ["India child gang rape trial moved out of of Kashmir"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-196228204d3f403ea00f2663b85434aa", "references": ["Merseyside Police apologise over incorrect 'offensive' claim"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-c9cc6c1207034e77bfc4415aeff8ca8b", "references": ["Prince of Wales hails Britain's postal workers during pandemic"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-75601fc1e2b44de694af869938be1ff4", "references": ["Jack Grealish: Aston Villa captain pleads guilty to careless driving"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-404253ff49f246ee88cbd9f1dc06bf7a", "references": ["Featherstone Prison crime 'raises home insurance price'"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-c65e463e10014a06a940b9c46ba7406e", "references": ["Oxford archaeology society's Blackbird Leys dig"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-0a1260ae928d4d4f9df8255e9ad1ec07", "references": ["Llanelli-born artist wins \u00a330,000 Hepworth prize"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-6500bd890efb4c8e803be21a9272ae65", "references": ["Victoria Derbyshire: My father was violent - I understand the terror of lockdown"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-cc7a6d4867254dc3a5d7c7c2695f63c3", "references": ["The man who transformed bookshop chain Foyles"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-db26b19fac354114b2b909a110f36dff", "references": ["Blocking a gene stops cancer cells spreading"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-246e99c6a3934745960a49d7e492dfbd", "references": ["Brexit: 31 politicians - mostly Labour - call for vote"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-eb45bf57878940e590801927e0bb2b25", "references": ["Missing students: Mexico's violent reality"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-7a68de013d6c471f8c7ac4793c81bfc0", "references": ["MCA sea bird pollution investigation closed"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-168615ce049c47eab2bf6fd406e2a3be", "references": ["Wales lockdown to start in bid to 'save lives not Christmas'"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-ebdf85dc659c46569ba682f6910cbe7e", "references": ["Stars give their verdict on working with Madonna"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-018f52be78814968b79f14ab97a9d67b", "references": ["All Cannings 'Neolithic' long barrow takes shape"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-31f91a1b554444fb84d33321d566486d", "references": ["Weezer singer hurt in bus crash"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-7f1de685a9684fb99e09a4467d18435d", "references": ["Duchess of Sussex thanks Halifax MP for letter of 'solidarity'"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-3a897d68b287474cbc1ab501ebfea839", "references": ["Police car petrol-bombed near MP Naomi Long's office"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-fb5a1d9eee064cd2b187de244e9e4e85", "references": ["Coronavirus: Gyms' concerns over Bradford lockdown closure"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-4ad973cdebec40598e5d8e7d76da8256", "references": ["Niger army rescues 92 migrants in Sahara Desert"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-efd952738557426192957f0863d2c5f3", "references": ["Budget 2017: Your questions answered"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-b4567593f05541eca394b3b864562a8f", "references": ["Lenovo: researchers find 'massive security risk'"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-57cfa557143641d38d09ea419fa57320", "references": ["European elections: What pay can UK MEPs expect?"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-de93b6263fa54cbdb44e67801ab05dee", "references": ["Cocaine injecting and homelessness 'behind Glasgow HIV rise'"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-41766bb2210a4ea3b4fdc34b421bed06", "references": ["Church insurer installs roof alarms on 'at-risk' churches"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-a8fc0bd69d9544388c55a108eda8ec31", "references": ["Prime Minister's Questions: Cameron and Corbyn in final 2015 clash"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-518350433c7b44a8a569ec6fbd240074", "references": ["New Scottish technology could end trains' wi-fi 'notspots'"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-694306c4576744c7b60993837bb93e05", "references": ["Kanye West cancels entire Saint Pablo tour following stage rant"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-95727e33576441b1a797dfc0d69a96ca", "references": ["Pope Francis visit: Turkey's Christians face tense times"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-1fc8ef0ad6b34d6ca9deecfd6bc98748", "references": ["Election 2021: Mark Drakeford to form Welsh Labour government alone"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-7eb5492eb903466d816dd40a334f42c4", "references": ["737-Max hits American Airlines forecast"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-08222191baa44250a28c89bbb4581270", "references": ["Bristol mayor Marvin Rees makes clean air pledges for city"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-58782caeabd74d2d9a1d30d1e7ac2dec", "references": ["Covid-19: 'Why a family Christmas would mean the world to me this year'"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-c31cb2505148426fb1f6e8aa9a0d1871", "references": ["Coronavirus: Staff in Greencore outbreak 'using food banks'"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-3cddb6a36cf74fef94d0db3fc7387ae4", "references": ["EU parliament votes to punish Hungary over 'breaches' of core values"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-de37a6a8c2a34a859abe1e7459f71122", "references": ["Cannes 'turned away amputee in flat shoes'"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-83d9b5c1dfa24f73899d6692079729be", "references": ["Historians urge Swansea to cash in on copper legacy"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-9604d41e7c814f39b73d902f22531313", "references": ["Kenya's Tana River clashes: MP charged with incitement"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-7c2817fef6d54c82a1d6ec0ba0e26c40", "references": ["Virat Kohli: MS Dhoni will always be my captain"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-7967e1a6cb224821a99de5ee0cbfb731", "references": ["Leicester City FC pledges \u00a32m to children's hospital"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-9d00bef28d3c42d78a340e5c3310ce41", "references": ["Three striking weather events photographed in Glen Coe"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-bbc31735d918490c83a99b05242c7cec", "references": ["New scanning technique reveals secrets behind great paintings"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-75a4d253f72b4a62a506e574089f677f", "references": ["Fire destroys caravans and motor homes at Blidworth site"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-e1764b5355694a36a72d96b423076798", "references": ["Wolverhampton park murder: Boy, 16 not to give evidence"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-b74e14af563e42c69522570385ad15cc", "references": ["Reconciliation key to peace -US"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-1f31df68a50643ffa6312f6376f81c8a", "references": ["Edinburgh tram boss Jeffrey calls for political courage"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-dbbc5343373045478fe6db3627b006f9", "references": ["Coronavirus: Two new cases identified in Isle of Man cluster"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-1980344bae594a77b7b8ab20ac0ddd7f", "references": ["Baidu in 60% income rise as advertising revenues surge"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-312034c7dfe047d5930ca4638f81c6f6", "references": ["Coronavirus: Stena Line reduces Belfast ferry services amid crisis"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-719529ee573c4f0b8c42c0ba6b11651f", "references": ["Man charged after woman fatally stabbed in Enfield"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-583545a969b1414497bca26179b53787", "references": ["What do we know about Brexit?"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-23a2e0342b184743827afd9f0dba4e79", "references": ["University of Ulster offers extra places for engineering courses"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-8d5b66f7e77b49e69ad08d3b94ef7611", "references": ["Glasgow art school fire: Sturgeon says blaze is 'heartbreaking'"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-9a3f33cb43f14221ab5b78ee1b8c4eab", "references": ["Ireland to ban new petrol and diesel vehicles from 2030"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-5d34a063ce85452ab658dae55af2ad07", "references": ["Mexican reporter Luz Sosa's dangerous job in Juarez"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-3b1d8f65be4f479b934f1220fea5eac2", "references": ["Paramedic abused by driver in Leicester for blocking road"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-88245bb75ee6444d8044a0e1f62e9d56", "references": ["New Adventure Travel takes over Cardiff Airport bus route"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-afb25a7e54884c50a5a9328bb821cfa8", "references": ["ITV production arms boosts overall revenue"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-9ce8aac55e2e46e380cae8d8f0bf68df", "references": ["Weymouth's \u00a39m Olympic transport works under way"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-a65e2103afa54c408b007583ddc121fd", "references": ["Has South Africa's 'racist university' truly changed?"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-2bf2837abaab4027aa5b419585f298db", "references": ["Hospital A&E staffing levels 'unsafe' says consultant"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-9e75de28d68f4be79e071cb9029ebb63", "references": ["Gunman Raoul Moat's final stand-off caught on film"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-8a0574a27449425c9ff325bef25c2aaf", "references": ["Blackhall Bowls Club crash: Service recalls 1969 disaster"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-e9e6c5e553764217991805b06f00b06d", "references": ["Man jailed for Tesco store bomb hoax call in Great Yarmouth"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-de9644e0fb394d0b8e4ceb1058072f1f", "references": ["Cleveland Police apologises to missing Donna Keogh's family"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-fbf220412763408cae20b5b689abc926", "references": ["Workplace maths challenge aims to boost numeracy"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-a9faf609824641bf98ba82bc935c2324", "references": ["Cardiff Noah's Ark children's hospital play garden opens"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-9aee9b5fb8e541fca64e8c15aa1abc2a", "references": ["Staffordshire mental health unit Eldertree Lodge in special measures"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-8808e613a304490cbf73138e05c1d2c1", "references": ["Glencore-Xstrata merger under pressure"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-fc7baf7405d54ed492b09d34a4e49331", "references": ["Boris Johnson v Jeremy Hunt: a week in the Tory leadership contest"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1358-f8dd2db800ed4e4493b04c950382a700", "references": ["Bangladesh PM Sheikh Hasina rejects blasphemy law"], "task_id": "task1358_xlsum_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1533-b8bc6b81633d4af6ae36bf9a4bdfb878", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-61df9ae8a4544a8fb6a4d036ab03681b", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-6597654b78274edfbac9af3b26342c35", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-07fc5e0574854651ae9f6c7a1cdfc8c5", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-669a1d8d93c740c5892597d32f1a5cb4", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-b0f4390146184464a3b71c516023bfb3", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-e1d6e2a605cd45188e62a347947266c7", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-a150192cfc5f4d1aadd76cd4aa5db4b6", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-27c429f63746468791c89e8cfc97604a", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-74d610e0cbde415189e1d94f20e1ef5a", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-9d64d5348ffa42d98c1725231a47fdee", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-6170c30686724a06a413a8877d3731bc", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-129b8639dfa84414b9b461feb12d082b", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-7e2b1365ccc44d4ab970eaf27b9745f1", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-a832c10a1f7d4d4e9353c79e40094746", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-8a88099b93c242ea9cc0bf6f236d1ffe", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-043ca63700b54dc5a3d1db59b52ccef7", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-9c8cd53d1191487da75ae5b74bfa9984", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-09c4d1dcc767423f82e269bd2e1c8004", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-dd5ebed3b0e64dbfa039295b4b80d8ce", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-6ef6357e362348e48ed793bc71aa322f", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-f8088b7ea96c4b0b92fb99ffba00767c", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-3d3d64e03b3b43c1a6eae5e0c94d6d83", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-b0d59d456baa4c1aac331d480abf5a14", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-97fb9c139c8b412ebbfd06107d40fff2", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-4cf0ab5ecde0493bbf9e43b20bb922a4", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-45ef7c1d72dd4d10897149e76fa4b027", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-e4f753b1557949ca8bee133866056c99", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-3232a52c79734836bddf7828f10a7dc0", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-928ee7700ff542fab1c0a86d7bebccce", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-92a8d96362fe411cad0e789ad1ffd1f5", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-60fe97cfa96244b49ef5a6b48744d40c", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-9934accc70c54cb68fc9e1060d648272", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-00676628684f4c44ba491f486315fd7a", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-132d196141464a64a71b43916f39e9de", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-c61711f33f914e208f31aec2ef03c084", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-d3214a73db3c45749eb7c5b256f7c33c", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-5c35eb9bed7c4432884656f3fd49b49d", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-6457f77ec961488fac105aa3bb1f2455", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-294449a1685c4619bd82086a2b8f93b3", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-310baf02709f48748cdff55a2973ab31", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-566814ba4ee44559bb4dc6faaaba1237", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-fc8bcec454534a7a85b964df08fbffc3", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-93402e6db28b428da06c5d792469c6c6", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-9c4a0b9bcd874f2eb73d3db21fe55260", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-c02efc853ae54aaa9c079aba4164a5f3", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-54daddcdc3294952b4a3e0575fbced37", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-fc2f5bab6a0b4fbfaf7f8116808e9fa0", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-9021386386d64d7fa95f2f66fb8c7023", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-f1fa69679c584feb91538a5e3afe1fc2", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-fb1a36db68254787bd73c6e41317f314", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-1e9f0896952248ec84f6347ac36a7e3c", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-88ed2433bccf4d4b8569fe464beb8a8f", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-cd0684f505da49aa9230b19c5c7b6aa1", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-bfc9b8ecddfd4d3292160b45357a2dab", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-a00e7463a6d94e0f9cfb56f1a4950712", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-5b19d6e628ce4353a805967d9703eabd", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-d5dfaf9007bc46879eaecfad8d682868", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-bdd9c03b80d9491399a40bf95a20b8ef", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-f2f96108fc8c4c7883a47f04b8ac6aa2", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-cc717af3c69440c797c182cc8be61a5b", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-2667025521f74108a46dea37acd0c754", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-9791017e54bc4662874ad6e941d99ec4", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-3348aebeb7d848d38321c893d1c92246", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-c3d30b8a8e6b4369b6d5b9a7b7b478c6", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-88ae05ea63124e499d352b7a11b4218f", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-fe1cf2a9855d418f89dad2fab4845ac5", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-bc80c4c995dd45339e03209db2d0d329", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-2215daae74ec46be96b10c663ea794da", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-757efad23b434b7a8b4822565e6a2c68", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-952d633fdd8d4127a850fe5059e3d0bd", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-f92a9cb3e50d4deb8305c8effab9a411", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-42ff12ff86a54cac830f0e3ea47007e2", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-db18e835526a4d8e904ab340db6be056", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-0205ec7ed3a4475f98dcc74ed3a778fa", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-56f43fe6724545b48635833fe619c0bc", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-32653c422ee64a5b94f273090756624b", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-54223955e9e5406ba0a76857f15a77d3", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-9948e0eb51c34f2c89aaf66a36f77903", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-0590e2ae4b5640ccbc58eae2d9e5bfcc", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-dc6c4e4930ed4529adc4db3e9abd6d0a", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-2543c4121aa84e4491f1d6733a85f3d2", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-3fdc89d9a1a7496bbf4331d7f41591df", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-717036e51a8c4da181d8c2a8f18ee978", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-2077f77862414208bb54acb4fff58ce0", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-7c0367f62e1e4488ae6d5bf6dee38ba9", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-b8e17117767a4224a6445ddceea8ac0c", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-8bdc065d2106432aaae208ef85c1045e", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-86bef72415dc425abf4e8946acfb98f0", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-33049c9b1f584aedabe82cf9a5467e4c", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-41f1cdc0294847ccb78eb5aa1949b970", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-d8ba51c0613f461eb9f575bcef3bdbc2", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-0dc35a2c77b04a5398e1282576b9cbf1", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-e782639106fe4580a0a3e55ddedb9f4a", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-1b078ee11a1d4e37a0f8ed21fbf9988c", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-c11854a19b5b44628a3ab7fe51dfae40", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-a9c7c2f1c05b4266a125429e0c9624a8", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-631f40efd7814f33a83b69730bc71894", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-17ed41393c414ab8913c4b1f2fd71484", "references": ["informal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1533-fe109f4fde76425498ee54adc401e722", "references": ["formal"], "task_id": "task1533_daily_dialog_formal_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1156-2c77fd5c3e9c4e3fb767ec6d8fa02b98", "references": ["scissors", "sword", "glass", "knife"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-6b004a872c0b44c283abdd57ab584f9e", "references": ["shovel"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-59fe660ae46a484fb5b396748f30b699", "references": ["broom"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-a500f78e5ccb4db0a3e31d770c2fdc69", "references": ["spoon"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-ef7fcda554434b049fc5f97d4667b1a4", "references": ["soap", "washcloth", "detergent", "rag"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-6da00e91c04048ba846cec151870151d", "references": ["vacuum"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-f3fc27a171174bec9230f4f83b5052c3", "references": ["knife"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-92190685d0934c1fabefa8d8bdc886e8", "references": ["hammer"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-581cb64acc274cfc8eb0e1e263d662f7", "references": ["mop"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-84daa6fb5db342d7af040cfaed407cdb", "references": ["iron"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-420e2c5438674617a5dbc4d0bb383830", "references": ["pan", "stove"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-c43fca1b44ff477eb9683cb1bccb136c", "references": ["oven"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-0e3d75b940bc4781acc7adb87bd793d3", "references": ["toaster"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-64b1c0fdf43e446d8acfe5d0b5f05b56", "references": ["nutcracker"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-eea3b838f5d741058f5c5d37140f50a9", "references": ["drill"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-447b9bfb8ed74e3cb56d765962ea8229", "references": ["pliers"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-24e4b6e2cc0445c5984b7f82830457db", "references": ["clamp"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-33827c84487345768a0266f62845d2bb", "references": ["keyboard"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-9af464b58521450cb6f2b649239651b4", "references": ["fork", "spoon"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-50344acbd8504310bab7398dc84e5f94", "references": ["brush"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-6a8dfeb2d9d540a691d2dbb0b076340f", "references": ["cloth", "blowdryer"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-eee061c19ebe498aa25558b0e3e58b33", "references": ["key"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-832de3166c694bf6958d6053d51a738c", "references": ["scissors", "sword", "glass", "knife"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-075d5f0e91b8405499af5faa1b019346", "references": ["shovel"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-23b88d86b5274834a76ceb456086259c", "references": ["broom"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-2c0ea9f62aac4259ad7cb8da03d7775f", "references": ["spoon"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-8fb89b2c0df842a8b014848c627c6985", "references": ["soap", "washcloth", "detergent", "rag"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-945c161f93b84278bf3706600a71631b", "references": ["vacuum"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-a3b03418e8244b6b900c371a4a1cd65a", "references": ["knife"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-33fabbc503b84556819789ba70710d82", "references": ["hammer"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-4f510a22b89f41cdad070a9040cd3447", "references": ["mop"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-a0848239b5884b1d85d9290ab91071b1", "references": ["iron"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-641fe6e9e9b5405485fdb739b4ea07ba", "references": ["pan", "stove"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-4f82748de7504a51838dd45d80fafdf2", "references": ["oven"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-63655a810bef4d68b45a5c7aaf1e1942", "references": ["toaster"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-2833370537864486976c9962ffebe61d", "references": ["nutcracker"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-19cc3401ca504c70adf31c8b4ef21602", "references": ["drill"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-9f6da274950a42d7a34522fe3d90a4b5", "references": ["pliers"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-f7f57d51559c494ba4cb1ab093d55aed", "references": ["clamp"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-6980fe42388849ccaa1c7ce28f85a73d", "references": ["keyboard"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-f9ff58f0704d405c818ac5dcce009dfd", "references": ["fork", "spoon"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-868c76b87b9448508356a04a56bfc4c5", "references": ["brush"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-b1fbb46002414815a06d620f2437d81c", "references": ["cloth", "blowdryer"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-f5df092617ff4b83856ac3c22032cd8a", "references": ["key"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-53611eb299744553b89360943147e1bb", "references": ["scissors", "sword", "glass", "knife"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-4f079c4023204f139b17ff3c1491b786", "references": ["shovel"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-6726cadc060a47fc9ab93eeabc96613b", "references": ["broom"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-3e7c2dcb53d6429e84752e1e0bf70245", "references": ["spoon"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-bceab350790143c4840b265087be7d2e", "references": ["soap", "washcloth", "detergent", "rag"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-40ec5be7f820494990b56e91ec9d4889", "references": ["vacuum"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-07d449fa168b4240bdc7cb2ca88c2ab2", "references": ["knife"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-e245efd49a5042029d269592cfe2956f", "references": ["hammer"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-a4f542b06efa47fa80b775c8a8f18b38", "references": ["mop"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-44fd67e303f549aebb3f4a1e54957641", "references": ["iron"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-8b2eb47842b641e1bff3be17894649fa", "references": ["pan", "stove"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-aea948387d82434381839b55d0a36cc5", "references": ["oven"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-79d51f4425d74712bded62937e79daad", "references": ["toaster"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-b3c4464cab214459bfa90b025e400e70", "references": ["nutcracker"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-edecfc0c342f4ecd8dfea57726fe0d34", "references": ["drill"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-76364058cecb48439ca881f494a3bbd9", "references": ["pliers"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-196b949b27a14f9d8b2789037d4c1202", "references": ["clamp"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-979cc410078d44109458b160e3d33fce", "references": ["keyboard"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-98ef40145a0f443884ccecccc8759d57", "references": ["fork", "spoon"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-d7d912d5260b4844acdec8aa82db8ae5", "references": ["brush"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-3fb37377987c44bf8afa807b94afdfb1", "references": ["cloth", "blowdryer"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-80dfe41e8cf3481a9dca1134d856a52a", "references": ["key"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-104a1d58bbab43e8af621fad3dd7572d", "references": ["scissors", "sword", "glass", "knife"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-6ebe0b3309b345dab658690d875aeca4", "references": ["shovel"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-84e7f30731364d3ba3d7b8ad2e06466e", "references": ["broom"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-9aaede78eba742ce8768ef042bbf63d0", "references": ["spoon"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-1963810cc996490cb943ad8de2799a01", "references": ["soap", "washcloth", "detergent", "rag"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-bb7dd42fba024f86be7e71a9ca04b4f1", "references": ["vacuum"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-9f6bb615ccdd421ebbc06400c2f42144", "references": ["knife"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-d9b1a4aa8a114b829d9a5978e7211c33", "references": ["hammer"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-0e5b68f4fc5840ca951b249a7d71a697", "references": ["mop"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-aafa40fa628e4513a66ca0c1cfbb4e89", "references": ["iron"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-b566b229c6ce47a0bf57eee7749f383e", "references": ["pan", "stove"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-7d85c1b744454bd7a0474dfcb8a61333", "references": ["oven"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-af6080957f57433f8beac1866fc6db39", "references": ["toaster"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-61947e27a2b64d30ba75b5d5e27d45e3", "references": ["nutcracker"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-21825e2e042f4d688135011d3c43dbbf", "references": ["drill"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-364aafacbe314e04b02b84532151591c", "references": ["pliers"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-7c937f6dac71428eb23f28840ceb3072", "references": ["clamp"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-8f4f5a44c8754d0a8b1a25bea1fff206", "references": ["keyboard"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-fed8418b677649f7b3b0f065ce9097c3", "references": ["fork", "spoon"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-e24b5a15ad384ddcb8171b2c10906a60", "references": ["brush"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-a3d8127adf764e7b995a637a2284487d", "references": ["cloth", "blowdryer"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-066646de0b22439aac52f7b7c2c98d9b", "references": ["key"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-15f966dfcf0546fbb74176516fc79a55", "references": ["scissors", "sword", "glass", "knife"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-7d81493a5bd149fc851ef634f326303c", "references": ["shovel"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-a1f0debb66c74c00a257863b83055fe1", "references": ["broom"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-8250789c5a28460d9776d442f061e050", "references": ["spoon"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-9d60b657c54b4e6397dde3adbd0d4afc", "references": ["soap", "washcloth", "detergent", "rag"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-cf42a6bf90654aeb87782b1cd836bec3", "references": ["vacuum"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-269a485327ee4b79bc91052a7356e544", "references": ["knife"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-a9f1a1ec191847ba82c81fd73f85180b", "references": ["hammer"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-dcc05bc9591c4b259f016c6fd260607d", "references": ["mop"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-222ede18aa9c459d801e0bad2c710420", "references": ["iron"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-ff2731d58bfc43ca9e1aebdf8d61a376", "references": ["pan", "stove"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1156-2b3bf3cee34b45949082db5f7a3d75db", "references": ["oven"], "task_id": "task1156_bard_analogical_reasoning_tools", "task_category": "Word Analogy", "track": "default"}
{"id": "task1659-e7833140f1b142fcaf28930a33260fd6", "references": ["To amend title 5, United States Code, to alleviate the pay-compression problem affecting members of the Senior Executive Service and other senior-level Federal employees, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-b24b646ab37c4bac9c10b78eda67bbfc", "references": ["To amend the Internal Revenue Code of 1986 to impose increased rates of tax with respect to taxpayers with more than $1,000,000 taxable income, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-25eed9a957794f2db92f7cd00add2fb5", "references": ["Every Prescription Conveyed Securely Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-d7079f64c563488d9ee7290bd274810b", "references": ["Crop Insurance Subsidy Reduction Act of 2013"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-cf5d8e526f2a42fcab006afed42a4962", "references": ["Read the Bills Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-c3f7b4c7e2814d0aaddc89f3f80370cd", "references": ["Measures to Encourage Results in Teaching Act of 1998"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-db16b3a1d0e1413fb25af1ac3f852532", "references": ["To develop programs that enhance school safety for our children."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-c7a763e6859f4a96be310647f7022c51", "references": ["To amend the Adult Education and Family Literacy Act to establish integrated English literacy and civics education programs, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-2194150c7e1c43af9e6d88b6c3e0752f", "references": ["To amend the Small Business Act to reauthorize the Paul D. Coverdell Drug-Free Workplace Program, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-edd5707df35f41cb8bbe96a7bc2f37e9", "references": ["To amend the Internal Revenue Code of 1986 to allow an increased credit for development and to extend and simplify the credit for increasing research."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-71cfa81708784b1e94cc6e52c27d0eb3", "references": ["To authorize the Secretary of Homeland Security to establish a program to award grants to institutions of higher education for the establishment or expansion of cybersecurity professional development programs, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-93fc6e674bc0449c8552719b8499661d", "references": ["Student Disciplinary Fairness Act of 2013"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-9be1d81db87241ec90dd92939a0ec8a5", "references": ["To amend the Internal Revenue Code of 1986 to modify and extend the credit for nonbusiness energy property."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-c79dd3174e3a474dbb17e3e8fdb696c8", "references": ["To authorize funds to prevent housing discrimination through the use of nationwide testing, to increase funds for the Fair Housing Initiatives Program, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-84b14474e3e54a4caca37d2f98f33bfd", "references": ["No Social Security Numbers and Benefits for Illegal Aliens Act of 2014"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-5302894baefe45d294380ad50e1a48dc", "references": ["Senior Citizens' Freedom to Work Act of 1998"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-da1b0734c4cb4aaaa4dddba7ee241d3a", "references": ["To amend the Homeland Security Act of 2002 to enhance homeland security information sharing, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-e81c7109716c42bc9cf6993df1d5125e", "references": ["Lower Brule Sioux Tribe Infrastructure Development Trust Fund Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-6f0da95f89e04509a004dbc018fb2b67", "references": ["Railroad Grade Crossing Safety Act of 1994"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-ab0fd50d350149f4a4e28810bf157562", "references": ["Patent Application Publication Act of 1995"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-bd0cf0a840ac479c8192a05955bedea0", "references": ["A bill to amend chapter 23 of title 5, United States Code, to clarify the disclosures of information protected from prohibited personnel practices, require a statement in non-disclosure policies, forms, and agreements that such policies, forms and agreements conform with certain disclosure protections, provide certain authority for the Special Counsel, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-bf9bc020b7014be2aa02a3590dfd42f8", "references": ["To direct the Secretary of Veterans Affairs to assign a temporary disability rating to certain members of the Armed Forces upon separation, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-21c738c7abb949eda5436868d500f2e6", "references": ["A bill to amend titles XVIII and XIV of the Social Security Act to improve the availability of accurate nursing facility staffing information, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-d9d6e8de1da14864b4bb4d55fa0f08a6", "references": ["Family Service Center Act of 1994"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-0fe99e6dcbba4a84ab0a53d65aa34638", "references": ["A bill to amend the Atomic Energy Act of 1954 to provide for thorium fuel cycle nuclear power generation."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-91df88b979ec4c16a2ea69c8b5cf1104", "references": ["A bill to provide collective bargaining rights for public safety officers employed by States or their political subdivisions."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-ab5536f0dd274b57870314b90cd175ef", "references": ["To support the establishment or expansion and operation of programs using a network of public and private community entities to provide mentoring for children in foster care."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-054918daf9474648ba964dfd8d34ef14", "references": ["To amend the Animal Welfare Act to strengthen the ability of the Secretary of Agriculture to regulate the pet industry."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-6bd13593faef44a387454deca18535fc", "references": ["Emergency Rural and Small Railroad Preservation Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-16e99fcd5ca248ee9d6afa8a604dd012", "references": ["Louisiana Purchase and Lewis and Clark Expedition Bicentennial Commission Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-16217e0436a946dca808c96f624373fd", "references": ["To amend the Internal Revenue Code of 1986 to allow a business tax credit for contributions to education scholarship organizations."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-5e562680abb54d6ba4f1e60ac1efac74", "references": ["Community Development Financial Institutions Fund Amendments Act of of 1999"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-5584948ab2474b88a286082b84cc6b1c", "references": ["To expedite the construction of new refining capacity in the United States."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-bfb00c18284947c4af126fd06752596b", "references": ["A bill to authorize the Secretary of the Interior to make grants to State and tribal governments to assist State and tribal efforts to manage and control the spread of chronic wasting disease in deer and elk herds, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-0c1b42e8abff45369eb0408154b10b66", "references": ["A bill to provide assistance to Best Buddies to support the expansion and development of mentoring programs, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-3fea5a999f524be4b9646d2f635ceb9b", "references": ["To improve the security of the Nation's ports by providing Federal grants to support Area Maritime Transportation Security Plans and to address vulnerabilities in port areas identified in approved vulnerability assessments or by the Secretary of Homeland Security."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-b076a62077db4ce2a41bf8da8b8a815f", "references": ["Encryption Standards and Procedures Act of 1994"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-5ee17fac1f7d4cbfa2505be65fee607a", "references": ["To establish a Federal incentive grant program for States that implement effective measures to prevent and reduce underage consumption of beverage alcohol, to evaluate the effectiveness and efficiency of anti-underage drinking programs funded with Federal dollars, and to provide appropriate reporting of Federal underage drinking data."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-2821563d2ea34e47ab5c3edafb375dd8", "references": ["Home Health Care Planning Improvement Act of 2017"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-1e313e2307854d5a9ef35afedf218558", "references": ["Small Business Regulatory Assistance Act of 1998"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-e37ba588b13b4319904226662a8d40ef", "references": ["To amend title XXI of the Social Security Act to extend through fiscal year 2012 funding under the State Children's Health Insurance Program (SCHIP)."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-9a43f8a1a03b4b82bf14ad0347128250", "references": ["A bill to amend the Internal Revenue Code of 1986 to make the capital gains and dividends rate permanent and to provide estate tax relief and reform, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-ae3cde70bc23404cb7d2e1217a07f50c", "references": ["To amend the National Voter Registration Act of 1993 to prohibit States from removing individuals from the official list of eligible voters for Federal elections in the State by reason of criminal conviction unless the removal is carried out in accordance with standards providing notice and an opportunity for an appeal, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-762acbeb39344a7d969e22819cdb8b13", "references": ["Secure Embassy Construction and Counterterrorism Act of 1999"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-985eb6c802de447c93e1f5ab290ba300", "references": ["SAFETY Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-b5bae5ae2f61483bbcf31de860245500", "references": ["Comprehensive Holocaust Accountability in Insurance Measure"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-03ec834a4fd54831af884f8c0b2e99f7", "references": ["To establish a Federal District Court of American Samoa."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-93ec717ba3f443acb68075478ebfc36e", "references": ["Tomb of the Unknown Soldier Centennial Commemorative Coin Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-527f7e31fa0f4530bbac259bfae3f23b", "references": ["International Insurance Capital Standards Accountability Act of 2015"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-89024f805b8443cd8b24a6f875f7d447", "references": ["Powering American Jobs Act of 2014"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-c659e3aeb8874d7cace49cbb16a3d269", "references": ["To amend title 10, United States Code, to provide for the retention on active duty after demobilization of members of the reserve components of the Armed Forces following extended deployments in contingency operations or homeland defense missions, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-947ab4e1cfc940c189f801a11905a45d", "references": ["To halt Saudi support for institutions that fund, train, incite, encourage, or in any other way aid and abet terrorism, to secure full Saudi cooperation in the investigation of terrorist incidents, to halt the issuance of visas to citizens of Saudi Arabia until the President certifies that the Kingdom of Saudi Arabia does not discriminate in the issuance of visas on the basis of religious affiliation or heritage, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-68b3a754958040daae094bc7b2432904", "references": ["A bill to provide for the liquidation or reliquidation of certain entries of educational toys entered in July 17 through October 30, 2004."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-b67e8b665456485783280fab187bbc7f", "references": ["Disabled Access Credit Expansion Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-43012709c8644c3e954a52c7ef152d38", "references": ["To amend the Securities Investor Protection Act of 1970 to determine a customer's net equity based on the customer's last statement, to prohibit certain recoveries, to change how trustees are appointed, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-aaf6147fa9d14e5c87a872698c4c42df", "references": ["To authorize the Secretary of Homeland Security to award grants on a competitive basis to regional biocontainment laboratories for maintaining surge capacity that can be used to respond to acts of bioterrorism or outbreaks of infectious diseases, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-41054e0138954c5782e6671e6d384cbf", "references": ["To combat illegal gun trafficking, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-0a38ce9d85324861bafe25577c1c76ea", "references": ["To amend the Internal Revenue Code of 1986 to provide for the exclusion from gross income of certain wages of a certified master teacher, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-6cc35f94ad8e41b1ae79364d176f17a9", "references": ["Standard Merger and Acquisition Reviews Through Equal Rules Act of 2014"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-f1545707cb004f019cf109a5f218fac1", "references": ["Tax Equity Act of 2017"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-f4af1047a8c64ddeadee96f7b76107bc", "references": ["To amend the Internal Revenue Code of 1986 to allow temporarily a reduced rate of tax with respect to repatriated foreign earnings."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-579f6a18343c41e7bdb2dd6ffd7e13de", "references": ["To eliminate certain inequities in the Civil Service Retirement System and the Federal Employees' Retirement System with respect to the computation of benefits for law enforcement officers, firefighters, air traffic controllers, nuclear materials couriers, members of the Supreme Court and Capitol police, and their survivors, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-428c6a57d8b64510ba3da542ff2a414e", "references": ["To amend the Elementary and Secondary Education Act of 1965 to establish the model school dropout prevention grant program and the national school dropout prevention grant program, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-f1a0ebd9c509414c97ca29f3591da1b4", "references": ["Women\u2019s History and Nineteenth Amendment Centennial Quarter Dollar Coin Program Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-c3fd2394a8384343b55fcb7032d58b1e", "references": ["To promote transportation-oriented development and encourage dedicated revenue sources for urban and regional rail corridor development."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-c3c147e2c036472e9adcfba4f2366e58", "references": ["Medicare Substitute Adult Day Care Services Act of 1998"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-523a893567f34ee686169e617aaa9bd8", "references": ["Clatsop-Nehalem Restoration Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-2bc3f1a578924bb8b5acc47b6623a830", "references": ["Blunt Reservoir and Pierre Canal Land Conveyance Act of 1999"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-87f7afa751534c7c9831e761295705cf", "references": ["Smarter Sentencing Act of 2015"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-383cdd24156b43c198b5877c68232df9", "references": ["A bill to amend the Internal Revenue Code of 1986 to extend certain provisions of the Creating Small Business Jobs Act of 2010, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-330867cbf58e46b8847edf8e259044b7", "references": ["James Guelff Body Armor Act of 1999"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-d4624768dc9144bf86c6c8bb5eaba539", "references": ["Serving our Rural Veterans Act of 2016"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-1bbc3b43a98344739c0b226e700a54df", "references": ["To amend title 38, United States Code, to expand and improve health care services available to veterans from the Department of Veterans Affairs for substance use disorders, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-799c19476c8c43b88b7132b9e4c6aa7c", "references": ["Results Through Innovation Act of 2017"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-27a224fbe69a43289f45631536cec42d", "references": ["Tiahrt Restrictions Repeal Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-e75482d4d8694dc1afa21f501dfce732", "references": ["Privatization of Humanities Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-d0ec6ef976a64a80bfc40a8a5e29e2f8", "references": ["To establish a program to transfer surplus computers of Federal agencies to schools and nonprofit community-based educational organizations, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-987b35e8deef4e75acbc02495a53fc66", "references": ["A bill to provide economic security for America's workers."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-65350613be704102837501bb174a0625", "references": ["A bill to establish the Nicodemus National Historic Site in Kansas, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-f22840cc70654f66b02b161aa1103031", "references": ["Voluntary Environmental Audit Protection Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-ffa76e0fc2cc4502a94530d45df7267a", "references": ["To increase the security of sensitive data maintained by the Federal Government."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-5943475d365b4df28e1224348a1ed35e", "references": ["A bill to amend the Public Health Service Act with respect to pain care."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-f919f7002276446895fd69160b5ff110", "references": ["Emergency Cervidae Tuberculosis Protection Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-03a9caa55d284bf8903a880114147c66", "references": ["A bill to increase corporate responsibility, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-3d5bb1ba9ce04e79ae98d0e22d2ea27d", "references": ["INSPIRES Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-cfc17411f0b84f0d85e5f4ee22a02342", "references": ["To amend the Internal Revenue Code of 1986 to provide a tax credit for new qualified plug-in hybrid motor vehicles."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-206422da96154fe2bf0f764fdffceaa0", "references": ["To amend the Internal Revenue Code of 1986 to provide for a credit which is dependent on enactment of State qualified scholarship tax credits and which is allowed against the Federal income tax for charitable contributions to education investment organizations that provide assistance for elementary and secondary education."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-61d97a9ab1f7437ba60079bb52cee16d", "references": ["A bill to establish a grant program for school renovation, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-7909c44018804997a2473eeb1cda4412", "references": ["A bill to establish a comprehensive interagency response to reduce lung cancer mortality in a timely manner."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-118f6cadd22c464393115dc3b8734667", "references": ["To amend chapter 1 of title 3, United States Code, relating to Presidential succession."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-90c4a123e0e54ba29d93e96ca0f0d1eb", "references": ["To amend the Internal Revenue Code of 1986 to allow small business employers a credit against income tax for certain expenses for long-term training of employees in highly skilled small business trades."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-fd0e6ccb74324de7a1f203bbe9c1b666", "references": ["To prohibit taxpayer funded abortions and to provide for conscience protections, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-5f83567480124464a3d43a417e7b6680", "references": ["Medicare Link Act of 2013"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-2d01a6cc665d4497a4a045f56b6f4a72", "references": ["To amend the Higher Education Act of 1965 to establish a scholarship program to recognize scholar athletes, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-33bd8a2535bc4fdaa73c13f031dfe6ea", "references": ["To designate the Cedar Creek and Belle Grove National Historical Park as a unit of the National Park System."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-ccc93d0c8ab7455f8ce0e9f2ba865881", "references": ["Alabama-Coushatta Tribe of Texas Equal and Fair Opportunity Settlement Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-e505d0720f1a436b9c932e324061b18c", "references": ["A bill to establish an Office of Entrepreneurial Support within the Small Business Administration, and for other purposes."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-6566868075e2495fbf5a20456e618cf1", "references": ["To amend the Internal Revenue Code of 1986 to repeal the 1993 income tax increase on Social Security benefits to increase the age at which distributions must commence from certain retirement plans from 70 1/2 to 80."], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-cc64c4fa0ce54a0181da5aa97101395d", "references": ["Flushing Remonstrance Study Act"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1659-285786a49d8a4a03b19340c0930254ff", "references": ["A bill to amend the Internal Revenue Code of 1986 to allow an above-the-line deduction for certain professional development expenses and classroom supplies of elementary and secondary school teachers"], "task_id": "task1659_title_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task1624-ef6dff90c9ae4c5fb513d3e0c4f16f19", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-67fceea06c3740e783fc951d3c8b16ee", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-7acdfa9deb6a433b99f0e51b2916412e", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-379928874c2740f9b9b16a48735d03d1", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-7352a4ba3397410a9391087652043eef", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-5fb0094e109f413790c9ab209f28c141", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-4ee69d901a2a447b91b311572a049e86", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-45ffbc33cf47400c81d429f19b83bb7f", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-143d4b4860d44cbd8575b110446e6aa8", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-370e9d73194b4a4491033e8edba94e4d", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-6afbcfcf345044d69960aff5162ca50c", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-b41a2533aa5c43fea95571b7d6842f77", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-7f46363b50a643b2bae77f015f0a3bcd", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-0a1b032381d24a2e9b38062781ec5799", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-37ba786977384ee98e9910494ed19269", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-c6363dbdf69f497ea42a5784467a8011", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-3c7cde165b5b417b96778f64b1bb4b35", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-d475b5d4b11642519101166da03e7b21", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-18a3e18b7f2045b6ae7fe0681e25fa79", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-7b0998f51d7542daa2cfdc7e72ce56ff", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-82d87c5550a4465aa06eaa9cde02e843", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-6bcd4d1e02fb4ca99406dc4ceb3a32ca", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-ec79bbb29a254af1a4f03a16833da4b1", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-bb952b289133422786f3cde7228926a9", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-566730effccf49759d28761b1aa562f4", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-61c76d66d4004da785c9cf26b333cda6", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-22684fd6c1ec4b3fb21bf8a364f32a07", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-9e5fc5e1dcd5419ca9012c3ea21023d6", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-6ff3891271594a36b02c73a2d98d161c", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-a37dcb252518423c810dec5d8b9edf46", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-a55fdc82af8f4078b170375d7f622bfb", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-278a5639476846449edeb32d314d2573", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-010cb9c4bb7145af897be23ae067eec6", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-d55727db02dc496296f467ce2c12217b", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-90b7a0c1c90f481ba0a2f27c8130192a", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-2a54a60519b244d3b11435d45248cb28", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-9d2a2e41a5f14fbbbcae3addeed17d48", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-6436145018ef47fa854bf664d76f4ac4", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-a317234026d44a95bace5645bca72127", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-52450d6257dc4dc787a1234d17896216", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-135d493a9fa6437ea809ce11b62f737e", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-f079366f97cd42deb06bde44e395fbcb", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-4557773f3fcc4d0eae0c4c5b6bb4226f", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-656944ac7a4c46898653c829e7c3fd29", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-ddf2d4e622094b6c8fe89ddd1daf5fa4", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-6a134e55c1d54fa896287a9f2eadd708", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-4649f90f1c734dfba012a3a91308175a", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-2afcc953c7e04c1e9f53197461e33ec5", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-3c86bec230ca43fb928d40cc5263ecfb", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-6e7e57d6f5b44d2b9a3ab22bfce41838", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-81e4262b5d3c4bb38e0226ace8e9e3bc", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-8e6bec956bd649b4aee5365bee575418", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-15e541f05f4041fe98dc57d8c7e57406", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-c319e80ce7f5496384757f991c385411", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-cdc91d33cb2147318b4db2fe695e04a4", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-5d17693f37174fbb98e993d38c97c833", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-eeaaded8ac634038a179e5ad46488eb6", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-c216ca0b56e24d4e8c59db33d50d4207", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-2fb4822148c142efbd645b9b4c84a6f7", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-2e37260cc3704ddeabbcf7dd179ce9e4", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-5c5c6180e5c44c448fc2db8c95cf3e4e", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-65e49972037c4fe3a67554f984f6e98f", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-f8955423bfdd4c0a80dc9e30fb4ca21d", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-4a33be48f95842bd97fe732de4168e4a", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-3a328b0a2fcd43f1b800d54670d2c7b7", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-c0f1af4b7a524a1fbaeab6876086f8a0", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-a0e33868a7c149d19593bdbc95ae32d6", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-e7b8930de0fc4ec8af561335f6fb0d3f", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-a14780ef4639484b9fca0c87564f1271", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-fdf118affef54096b1288ac76a9a1755", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-5994adab7f23450386a0500572086729", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-107297f1f96342a1b446049f1b275431", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-1b4344d7d2f24510bc02aab9dd607fc4", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-d58c724d82774d63a2ba38961c0ffc27", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-dfcb57cb98b642e3960059a527f14dc4", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-85cb21cc38574565b07a1f5617d11953", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-8b98058b164b4f90a74796791bc7e9fe", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-e48f00f832cf43819a2015a7cc870fca", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-a18a787511124a8b93c72c972e0c5f1f", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-eab263f3848b48d6be6af1aff4bc935a", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-c3f8384086da472a992fed4efb0a88f2", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-cd1a495483b44439bede0af601e55b3d", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-b79f78f2b1f440408f3f8d18f99cb2bd", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-b75d35007dee4d25ba56ff76e3621667", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-c1cca8003fd24831a775c0794f657269", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-36c3840afdca40f5b4ef96acbe1ac78f", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-372e626576a846f79774d028c40af40c", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-65825de455704e40adf42363edd259c8", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-409182bc671d4a83902f8fbcb54c839f", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-bfc4f66e9ebc4568825d3688e36108c2", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-43a6e1abb26547fa837b4ecb56759a02", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-b8f1c5394ea14e5780c396aed50d6efe", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-23055ecffdf24fb584af86efca596cec", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-dd566a2d93ca41a387a06d037f8e22a0", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-e3b6b65d463f4462997a08b99107bc3c", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-f89af55c4c6f45bfa1ee829ecd353f21", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-effa772c0eb448b8a35359384c0dcf8c", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-af12353bb1cc4d8ab29e555ee10d5943", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-d285d6284eaa4fae89ab39b4a5297262", "references": ["No"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1624-c31ca67848d94bb28301cb965aaef43f", "references": ["Yes"], "task_id": "task1624_disfl_qa_question_yesno_classification", "task_category": "Answerability Classification", "track": "default"}
{"id": "task1158-4c9c4bc0659248dabee363826e4afd6d", "references": ["pour", "squeeze"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-679e96f596054b25986fb423fd9e40a7", "references": ["peel"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-bc05fea93c2b4d4dbb493fbf9ba4cb30", "references": ["shell"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-aa94670bd1c14da095dd591a70e3ee6b", "references": ["turn"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-edafa643e4364dc2a0ac3f34a5e47c3e", "references": ["open"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-a0d29af917ad4506abf15a4639916ab6", "references": ["unlock"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-6e65ef1edd1445fb83c909fdb2ffa453", "references": ["pump"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-616695fec3b44407af9f4df65a7bb4f9", "references": ["unzip"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-97e991bf607a456a9466b6cd5861d0fe", "references": ["untie"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-62fe6f9a11694c2c82487b75b33a67d0", "references": ["unwrap"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-eb232eedbfee47d8b1dcec4d963e6227", "references": ["pour"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-e502023c9d83403fba611984fb0bc94a", "references": ["peel"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-1b0a2b787eb2440295a0bdaeef8539af", "references": ["shell"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-7f4a8f0f71954cb583d4ef0e84d4e9ed", "references": ["turn"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-372c9615cafe4099a1c4bf6ca49c5a3d", "references": ["open"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-79310eef0f1d4de39acd0881e8c4b17c", "references": ["unlock"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-0536e891cc994c09b329ab7686306734", "references": ["pump"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-714ae4ecef874f6d836f5f7f2cd14d30", "references": ["unzip"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-6801d7e9eb0e41fb943a652471723053", "references": ["untie"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-c8bf0f6039714c58994d44e22eb3613a", "references": ["unwrap"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-1a064059962744aaa50069289e0cb0cf", "references": ["pour"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-3fc1c1daae8c42f4a30eceef03affe94", "references": ["peel"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-89f2ccb6ca614067b3c60816b1f56e7f", "references": ["shell"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-c9f513e61fc443409aaa7ac8cdec5bce", "references": ["turn"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-aa161c9c1d124920a3c2c554cdb31e7f", "references": ["open"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-28d287a7a5304c2b97b3ad29b8e77029", "references": ["unlock"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-946d2212f9d54ec5af678715caf36519", "references": ["pump"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-c85ecb8405c14dc8854908e40bc9e401", "references": ["unzip"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-e44df2f8c77248e09fa078efa215af51", "references": ["untie"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-83545e77da294310823ce5fb3838f802", "references": ["unwrap"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-604c5888e5f94ce9bcb18c0dc032208b", "references": ["pour"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-01372125a3184bf8a5efc2d12b645fea", "references": ["peel"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-be793c89303b407cb427fea458ed0128", "references": ["shell"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-aec7f6e6eb60449e83fe0e96f36e87fb", "references": ["turn"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-72a2e3be323d484db0511f21b3a22b62", "references": ["open"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-aec05a5b6d0b494d907c4fca92e2f39d", "references": ["unlock"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-3b803c92b0bc43b7b2b496e350ce7cac", "references": ["pump"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-e320258b949645bab8c17810d5ab5dea", "references": ["unzip"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-a4ca795770de490fa7406b132e6a0726", "references": ["untie"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-e3b361072db94b70a29bfc4a797ac3aa", "references": ["unwrap"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-3a0dffc29b2f4352879448a79b996627", "references": ["pour", "squeeze"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-d72fa3f72d3a4dc889f21a8da708f5c5", "references": ["peel"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-5e2241816cf94f628f8dc06f27f07f19", "references": ["shell"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-2f43bbed041642eab500c98928ae2cbb", "references": ["turn"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-a4df1aaeeaa14825b8d17bb991a9a647", "references": ["open"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-4761b369d47c4913ba8e7d80c56f767d", "references": ["unlock"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-de7e5d34229547cfbea210998ade4a8a", "references": ["pump"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-d3bb868dd39e4d4fa355f892f15bcd44", "references": ["unzip"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-f6e2fa6d103e437fad1372fe2fd49a8d", "references": ["untie"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-7dd7a58677a346e689a73ea77f7ba595", "references": ["unwrap"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-33a2fe0e03f04e63a471a75ecd72ee40", "references": ["pour", "squeeze"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-a609632fc8d146cd938a1b53cacd2ae3", "references": ["peel"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-1dd5727671874c869e177cae1b458e13", "references": ["shell"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-bcafb6e6f35e449c859a593613f34374", "references": ["turn"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-1d8d370ba8dc4ff3a2b7d487b337627d", "references": ["open"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-2adb53b591c64e4da0fb9b0909a7043c", "references": ["unlock"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-2f4fbea4ac1c43e58015e3b38d8aef9a", "references": ["pump"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-55554ebcd5db4154a4c6db4721bd397d", "references": ["unzip"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-c75dcc4c4c584f23991aea56ce49c4f5", "references": ["untie"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-4be95fcd3393481f9541f9eb20db27b0", "references": ["unwrap"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-0c8bafcca2104450938f7df7f0650960", "references": ["pour", "squeeze"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-8af05413ad2c42268f8bbffe9dd6b71a", "references": ["peel"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-c4282db2776d4191a0beea7ce0a282aa", "references": ["shell"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-24fdbcdb74ab46e59cd8352ee8fa61d7", "references": ["turn"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-1e0b1c11eda646628c1ad8ef5efba4b1", "references": ["open"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-81c39a7432c84796b95488d63c8f0ca0", "references": ["unlock"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-33fbc1f8e145403d8c989088cb5a55b7", "references": ["pump"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-ba4164c663a94a82b9f4e52a33b7cf39", "references": ["unzip"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-312f6298be8b41e789f9f1f37ed381bc", "references": ["untie"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-79a7e91db6bf417d88645d1fc51adbd5", "references": ["unwrap"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-53c6709e24c3411d8c5a4af1e0dad4c6", "references": ["pour"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-89b7c471ba0b43089733528eb6011a1e", "references": ["peel"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-11063f190af346b5855709a57881c6f8", "references": ["shell"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-1eefdb6ab04644d58ad7b016eea5ea71", "references": ["turn"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-f3bf7b1580244f5eb028a4a16479d048", "references": ["open"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-3fc91a10baf04e40b9ca230eab1dd0e4", "references": ["unlock"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-e810e7eec2ec444a9a889387a0c1f313", "references": ["pump"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-52cccf2a9f27425faac282f8f7191345", "references": ["unzip"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-8691194c503b446d93230f5367546328", "references": ["untie"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-42136ac75538478794311d4f519dbdd7", "references": ["unwrap"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-53648b50f7af4947b3d09c9e494c3bdd", "references": ["pour"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-0680bca794114f8090ea1e66b7b2f362", "references": ["peel"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-218775d57e604f089c654bd7051af93a", "references": ["shell"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-ccc0d2affa794f42a81baed95272fd20", "references": ["turn"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-982bea63b9164f8ab10d139c50d28810", "references": ["open"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-50cb958e495e4e2eb29803d60cb536ad", "references": ["unlock"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-407e85bfe4d04875abd32fd6780411ba", "references": ["pump"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-1060ebca3e9a4743aedee4cac9789164", "references": ["unzip"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-32b3f3a28d9143ae8901ba95910f3cf0", "references": ["untie"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-f1eabcee86a6464c9837aa5a24c71a7e", "references": ["unwrap"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-4db6f7b79e81436ebdb467bde4e0457d", "references": ["pour"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-2ba7d44bb7a34f8c82bbd117452e4e87", "references": ["peel"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-8b1a11fb974d4b50b545d17dedbb9984", "references": ["shell"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-b1d4a24263ee44659b5165d5bdafb9bc", "references": ["turn"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-4d7c31377cde45b1ac65926985b74a28", "references": ["open"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-dae0bc005a564098b600e4e4c75b213c", "references": ["unlock"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-c679287b67ba4d22804e30ee66bca1e7", "references": ["pump"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-d6dc55f35aa24cc7b2ad720b56d920aa", "references": ["unzip"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-83e69d27a39741c9a3f882b97ca289a1", "references": ["untie"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task1158-43608022df6c48ffbb3855701d8877c1", "references": ["unwrap"], "task_id": "task1158_bard_analogical_reasoning_manipulating_items", "task_category": "Word Analogy", "track": "default"}
{"id": "task827-c1d63d24c8e54a119d58a4ef86094552", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-f5657b690ee84831a54a0ea587316bf0", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-20447f95655b4f82bea5d332b5caaeec", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-7d47d83a35b946af95af712fa8af4aec", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-9c22f4e5edde4c1e973745b38c8e2203", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-fb81b4f22c334f6586b0f5c5bb0fc560", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-b748aad564154cf792c9dd886f25bf41", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-b4d25e174ce547eaada77fe5035f4113", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-64a373356e7647109ef1573898e21f91", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-9a19282613bc403cb3e3086ad7e0e036", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-ee219f4dc1d24e5f9d1be24bed8cc07a", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-7380d7a9a8cb43db857b98d761e9dbcf", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-dfe0b3e003a34ab7a1b364cab17127a5", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-fc9cd9d3870c4e899173776b87e2cd0f", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-c437a8cb6208464ea9f24bd6a322da0d", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-83713c128ff24e399d70b3c0bdbe8fde", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-c6796782acd240c3962e159f631ba920", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-484842dec33f453eb0fb33a03b8a8cfd", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-e571cb1ea11c4388b731d2d23708406b", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-46d35eca1570445e88bc5ac2c2d632f1", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-1d469827229d414dbf79d52bfb926648", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-076d2731abb64f15a3dee995d7f2c65d", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-0f2fad86d28e436ba9cbcf24cf72f29d", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-7c44702f4b22446b938f10f3a3b93d88", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-81259dd135f74b7497cc4dddf8881468", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-2ee681d6e70147119ed59a4e16cc70e5", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-d3e158bb0a8e4e71a173b6efeae20eda", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-6d15b1f9b5fc4a3fa0bc59af8fb5fa55", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-8b13aa75681348d7acc43e2743ff5176", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-093ec4830ba44c858f8da875245a69ee", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-998ee389d5f24ab2aae14327d12a19e1", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-a9c76a4379e14caba5332704230e908a", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-5728d428ee3848c49a469af2c3f94903", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-9d87ce24ae5347a7825aeba7e95608fa", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-31ac03dda68447cf80fde6f822bfe294", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-88da85e67b6d4fdcbb806e6e72838e2f", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-ee359a48a5b94e2babddea6f8fb46b87", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-befcf258ccf2400cb220743a83e360da", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-b75c39eb517a41f4a7cbef9c5877cd44", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-241a25c7c99a44c0a1338f593135a2e7", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-b5136e30d7194833a747aaa0a4a849a1", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-b6ed8772d9c14db8b862cd6cc1617728", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-2eaddc7566884db6bab0f360a65efe0e", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-23345dfb9569411db8a3446e0dcbc65d", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-ec57c4fa2b48484babfa28802fc059cb", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-a26d99e249fb425dbb8a170bb920ca0d", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-05803994664e4054a70f6b916885a2cb", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-e15958901fb3453699a1c4903e42b4db", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-2809762e8b7a4ad080173857aaf3ccee", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-cb22bb004fa149ed99aa600b4031cc39", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-c8951b482b914e709017d147a10ab463", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-c692a6949dee48a8a8f5af44b244ebf5", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-e2ff11d0bc434ad4bc40004ff0d969af", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-6be0753d61834d798f037cc6f5b12299", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-b32a302f4fc14015a916882f19598d09", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-cd8659352f164960a1f62efcf523dffd", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-5077aa3a6e624fc99950d1652325cbed", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-455aad45dc084700a397789c20130bbf", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-141a0c42cdba4327bef683317ed395ea", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-2d2d7b1d308948c9bcff75ff8957f2b9", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-6910511d2bcb4aa9b9e2ce08069d4a75", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-fd0f14c15a7948e99db4c01d97744bde", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-ea1adeb978974ef0b29d371c7dfffa9c", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-ec9843bc890a4da6b358a2f7617d5cb1", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-7963b99250ee44e1a6864e3ef4aa48ec", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-66671b2e31ad44a880a72e9f68b698a8", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-05aa9c96e8404d8fa63f1630acbb7efa", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-9ec74e335f334f64874fa6be74ff3b04", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-39d398d74ca7415d81920da2e92abbd3", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-8eda10b7978841758a697599d17173a4", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-fcace16495d4447d8d5cddfdeaa722ab", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-69d9cda033384171b8677a97e7ab2016", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-328267af2ec7455b991ba44ea38dcf2a", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-36fd5b59f0ab4d528b88ed3e4b206b90", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-5d4b88d987d946eeb010e768a4f8e895", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-c269c41fecbf42edaf77362bedfd0ea0", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-3304dba1e1894701ab948083cb033a51", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-a9e7f8edc52d4a23a63aaa9b66fbf724", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-f6f125894a0a478e808d00c9a7f7c52f", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-0f3242ac9518412bb3cf295fcace4916", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-e619511494a44a75ba2afce91699f351", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-44be66cf701a4323a4ec496386ee2262", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-ff49b572002d4585af527aa8ef28c73c", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-a06b3d874a1442f9b929e51767e23266", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-1f308320024940b19aacb49dbb3246ac", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-34f6e6c5b97442619202a21ac071ce84", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-189a94dd63be49bf88d291404a7c4450", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-901a34ddc9bd46da811da132a32d4d48", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-b18fbe1ad3aa4be8b9faa00021cab75c", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-c6b1bea1ccda4aefa3abd1c8283cb8ce", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-f24fc36f19cd457698bd5f144260ce71", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-9a506726e46543b093b0942e7275573c", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-0ee72818766b47d1a62bc41ef9729ae8", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-1a29375d207540e2987cc6380a0ef879", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-c6129450f40e49a59cc2d4b43d142e88", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-cf5f5507ebcb4675857a62b9afbc8c24", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-a2b3952587f544d28196d7268383976b", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-a05cac3a6ff64e4cb3237c5eeb1c262d", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-f2fcaf67379a4f7195736dd7501fe4cf", "references": ["1"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task827-5ad6eb9875a14320935095d0583f0886", "references": ["2"], "task_id": "task827_copa_commonsense_reasoning", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1153-323576273c714ee39944d91e4404c008", "references": ["water"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-edddd05becab48f381721af557b1861c", "references": ["book"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-b1ec3792bdf34214889c969d57464035", "references": ["food"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-c7ac3c22a33e4e988b55b5b8348f0be0", "references": ["coat"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-5e976465380e4380b7fbf63603ed3690", "references": ["horse"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-5d29531a4ebc4734871a6d58a69d3706", "references": ["gift"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-7819803bd2724324a536957136e9e816", "references": ["enemy"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-f4d46a634a3542379f1531a755af2bb6", "references": ["word"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-f15e323803dd428e82308337277703ac", "references": ["door"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-f6eed21d283145f5bc13c5d9d35a39f7", "references": ["tree", "ladder"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-2decbc93043f40a7a241b0195fb3dbe6", "references": ["wound"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-508b06e1e51f4c7088357fc193a3ab8e", "references": ["disease"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-b072b2a575b9461fa4a50184d78d4700", "references": ["picture"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-ecc5fedbcade474183971ed0952b3f64", "references": ["machine"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-d7654b7b3d024007997482b470570f82", "references": ["game", "movie"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-a74adf6134e24a1a8f9d4663cb7b0ffe", "references": ["quarry"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-e63cb4c989cd4189973c7f45e9f2c0ae", "references": ["thief"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-34d7da4f5c5345db8c435302418743c7", "references": ["prey"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-045ef158bbd94116b22ec205c6fb6456", "references": ["boar", "goose"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-fdd8631aaa22431b8bd0949506829608", "references": ["gun", "arrow"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-dbcde03a726a4883909677a9dfa4182f", "references": ["boat"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-287b5b79f3da4a4e9f4c7c19243d4ff3", "references": ["hair"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-77601a075818442790aa4db1477653a4", "references": ["letter", "book"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-6a91c4f470ee474d83b4de2ee164e7a7", "references": ["sword"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-ca67eb6f1c4a47fba065b376c8ff5342", "references": ["burden"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-fc1e758ebb444040b2224683b71ab0f0", "references": ["dishes"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-13294bf9ba7d400285bdec6559d4070e", "references": ["pill"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-8769676aa4ea4aa69b5e0efeee7b46a8", "references": ["soup"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-7a56288e4c0641729b0eec9659b7e1ce", "references": ["oath"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-fd65c5c28cf34ed4b7125f8b89eca445", "references": ["promise"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-0b55713e27c641048a553ca449900a68", "references": ["president", "congressman"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-77aee10741a5459f925bdfc41b9148e7", "references": ["employee"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-a0792b012d024f5e8629e505dede7955", "references": ["symphony"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-0ba3b279321e40af8e55e7cc29a7d1b1", "references": ["cat"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-3ecc6d646613437485b9680255fc576e", "references": ["fly"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-fb7b75ce4af64334b05d27c25236d33a", "references": ["message", "text"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-f6ad8627fec5495db02cb9b7c3de30b6", "references": ["song"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-6d811a83f4ce45f08c66e730f01bb555", "references": ["car"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-23a1eda3cf2546f4a422e484d01591a3", "references": ["water"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-21bc62fe693e4dd2ac803dee94868017", "references": ["book"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-6cee58e0f6c84bd1a5fa254658011be7", "references": ["food"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-d7633a04bba1477d9168c2bd9783e54e", "references": ["coat"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-a3f2a0f8c67d4b65be90a65b890e53fa", "references": ["horse"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-9891b804933e4da6a495170a0d6e48d5", "references": ["gift"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-80d128ce94dd4989ac091c380c730135", "references": ["enemy"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-30d1c5dbe2234db39a861deaa99eef7e", "references": ["word"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-4cce4b34d27346798f4d2a664a4d24a7", "references": ["door"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-6fe03785c59b494fa8466091b97ab5bc", "references": ["tree", "ladder"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-166b2376b9854e3f9c93b66874c416fb", "references": ["wound"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-a19d197ae6e245a89af31112e54fe4c8", "references": ["disease"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-d84f047be0ae46e8a9f42089f081df1a", "references": ["picture"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-f591868e68844cb2823f3b4193858ddf", "references": ["machine"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-50a7d728890948c18c784722a70a4a85", "references": ["game", "movie"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-c8a0fbb031114910b50de26305df08d7", "references": ["quarry"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-729b3fa750f24b2ea86af645136a4e53", "references": ["thief"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-72a24253250e4b0c9cf29412f036d3f4", "references": ["prey"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-2b91be2de2cd4f24b46ca79f2b698bc8", "references": ["boar", "goose"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-10389a16468242708115433de0c720ca", "references": ["gun"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-8cd37bed845342528e8786fd1a6f63a1", "references": ["boat"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-232b0ed077b14585ae49040987a9c2cb", "references": ["hair"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-38b046312fbf4d09bd1c37707395d95c", "references": ["letter", "book"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-fc1d40b8d0b44ceba70b8c30765896dd", "references": ["sword"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-6cc9b09831fc41a39f1d4c9fc5ed2686", "references": ["burden"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-298978e7e81e43c9b29c62469a8e3b24", "references": ["dishes"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-af85fb62a63f423e8f5658cb957da57c", "references": ["pill"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-bb6dffc31f854f8aa6d9d59e716e6e0d", "references": ["soup"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-71593d3243434784a24495657771865f", "references": ["oath"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-efd8a37764bb47009680c7db85584434", "references": ["promise"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-76238a1bda5144f68be0dfbd561d6d07", "references": ["president", "congressman"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-fed58fabc63241ee9c3b3f3e2065b55f", "references": ["employee"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-6399ee38dc7b47e08062d0713036ded2", "references": ["symphony"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-8587f5700374417eadfe7e9bdaf6d7ed", "references": ["cat"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-ea43721b7b284bf6a46cc31e3e114417", "references": ["fly"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-3643f8a803ad4378a8606bc36cd2cf98", "references": ["message", "text"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-ec30e8a207454c7b99214ae3f0e2dfb9", "references": ["song"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-7f444758e652494681e26f705240fa58", "references": ["car"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-770ceac97d4f493483d5658bdca6fa86", "references": ["water"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-494827e24ae94c79bd09376f021d0c3c", "references": ["book"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-94da178f9a8e4d5383b1a1742a9dc355", "references": ["food"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-2be595c3dcdb4596a0c3e02f10f081bd", "references": ["coat"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-b687415bcfff45b5bf401b853d30666d", "references": ["horse"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-24611e5a43654899b6e1937c8de24cd1", "references": ["gift"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-2274d8a569204e4aa46d8d28ddb0d8df", "references": ["enemy"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-ddd0488c9f4d478d8ad39ee3a0cbc006", "references": ["word"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-df2ca83ce0b7421c8318342f69556c7d", "references": ["door"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-014728fb58f745c5ae8d212d8a69ecaf", "references": ["tree", "ladder"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-794e4e0da53f49db9d72d1003800015f", "references": ["wound"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-40ff3347353249339b7a1e52c7ec7215", "references": ["disease"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-17cf9c00605b42d689222f74d28b3ea3", "references": ["picture"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-f0cb6856d06d4249b7e477959fe290f0", "references": ["machine"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-249e57a8d51245298f2c7280ee6b9964", "references": ["game", "movie"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-408fd081727b454fbdf044a172d7aec1", "references": ["quarry"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-9ec306d96ad8430fa13e9bc594272cc2", "references": ["thief"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-e4132eafdcba4ff2bd845cf3bf3e1b99", "references": ["prey"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-60980043111246c18b5b2c953aa84518", "references": ["boar", "goose"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-c4ebd22383da4150a67bdc0ab742d575", "references": ["gun", "arrow"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-7db181919f284cc2971f03b815901b98", "references": ["boat"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-a8cc5ebec9364dbba17356aa74e18374", "references": ["hair"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-904479b76f254128b13898c48338a14c", "references": ["letter", "book"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task1153-5e0637070db34ea884d3281fcbb9128c", "references": ["sword"], "task_id": "task1153_bard_analogical_reasoning_affordance", "task_category": "Word Analogy", "track": "default"}
{"id": "task393-a0728867ea124129a69ce05dc4b44a60", "references": ["they had to be checked", "the surgery had to be cancelled", "resulting in the death of the patient", "he had to go back to the hospital", "resulting in the patient's death", "they came back to the hospital", "he was sent to the hospital", "the doctor sent him to the hospital", "the patient filed a malpractice lawsuit against the physician", "there was no need for surgery", "she went to the hospital"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-634c747785084befad5b6f9edccf6c1d", "references": ["it's not like he's going to be able to get away with it", "I thought I'd mention it", "the transaction became official", "it must have been true", "he can't do anything about it", "I had to edit it", "I don't know what he's talking about", "he's got that going for him", "it shouldn't be too much of a problem for him", "it's not like it's a secret", "I don't know what the hell he is talking about", "he should know what to do", "he knows what he's talking about"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-44494a3f28b943869e4b069773da19f5", "references": ["the gun went off"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-40569651e2da4669bc604e26f42dba81", "references": ["I decided to ask the teacher", "I asked her to repeat it", "I did my own research", "I searched online to learn more", "I decided to check it out", "i decided to check it out", "I looked it up on the internet", "I looked it up online", "I asked him to explain it to me", "I took a deep breath", "i asked the professor questions", "I asked him about it", "I clicked on the link to see what was going on", "I had to read it", "i paid attention to the professor", "I thought I would ask", "it took me a while to figure out what was going on", "I asked him what was going on", "I asked a few questions", "I thought I'd ask here", "I didn't know what to expect"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-f5f9c616c2784a38862248fc1a9a33f0", "references": ["I didn't bother reading it", "they had to cancel the book", "I couldn't read the book", "I didn't get a chance to read it", "they decided to leave it out", "this was the first time I read it", "I didn't get to read it", "they had to come up with something else", "I wasn't allowed to share it", "I decided to review it", "it is not recommended for children under the age of 18", "I had to sign off", "I was thrilled to find it here", "there wasn't much they could do about it", "he decided to create his own", "it is not recommended for children under the age of five", "it was removed from the library", "schools banned it from its libraries", "we didn't talk about it"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-3157d5b5fc4f4247a7ff95ad824e854e", "references": ["they thought it would be a good idea to teach her how to make a", "they decided to do something about it", "they gave her a gift", "they came up with the idea", "they made a gift for them", "they knew they had to share", "they decided to write about it", "they decided to join in", "they started their own production", "they decided to take her up on the offer", "they jumped at the opportunity", "they decided to open their own store", "they thought it would be a good idea to teach her how to create a", "they encouraged her to become an artist"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-8956d8cfe3554bc7b7bebbd76f0c6c15", "references": ["in order to get a glimpse of what was inside", "he had to take it off", "in order to get a better look at the contents of the jar", "the lid came off"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-fc0d664c02a84bd58dd572c2a437a815", "references": ["I had to take her to the doctor", "I had to check it out", "I had to go home", "he gave it to me", "we decided to go back", "we decided to take a break", "we took her with us", "we called it a day", "her mother put her down for a nap", "we went back to her"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-e8489d4ae7164447800000854ce3a703", "references": ["they were able to kill him", "they couldn't do anything about it", "in order to kill as many people as possible", "it was bound to happen", "they decided to attack him", "there's no one to blame but themselves", "he could get away with it", "there was nothing they could do", "there was nothing we could do", "resulting in the deaths of hundreds of thousands of people", "they know what they're doing", "resulting in the deaths of thousands of people", "they knew what they were doing", "they beat him to death", "the bomb exploded"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-f9443e232f2d44de9fa0dee9ba0fdbeb", "references": ["there was no need for law enforcement", "they went to court to investigate", "they did the right thing", "in order to protect the identity of the victim's family", "they did the next best thing", "in order to protect the life of the victim", "they withheld the victim's name from the public", "it was not a problem", "they couldn't afford to be arrested", "they didn't pay for it", "they don't have to worry about it", "in order to protect the identity of the victim", "as not to reveal their identity"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-25a14763cf7544ef921d31b8ed7acaf5", "references": ["the case was quickly investigated", "the jury was left to investigate", "the case was taken to court", "the police proceeded to investigate the evidence", "the police investigated the case", "the suspect was convicted", "resulting in a two-year prison sentence", "they knew what was going on", "resulting in a four-year prison sentence", "the case was resolved", "everyone was aware of the situation", "the case was opened", "they decided to investigate further"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-8c61a72f2b384d16818c9bfe28e3d322", "references": ["I decided to give the product a try", "we decided to give it a try", "I decided to check it out", "I decided to try it", "I did a quick search", "it wasn't a total shock", "everyone had to buy it", "it wasn't a hard sell", "I thought I'd give it a try", "I decided to check it out first", "I was a little skeptical of the product", "of course I had to check it out", "it was a no brainer", "I decided to give it a try", "consumers recognized the product", "it had to be good", "I figured it would work", "I thought I would give it a try"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-758b4cad36ff4f8b9e25b5e783c5a542", "references": ["in order to get the car out of the way", "it was easy to do", "it's all good now", "it was a good thing", "the car accelerated", "I knew something was going on", "I should be good to go", "I don't think that's the problem", "in order to get the car out of the way as fast as possible", "it was an easy drive", "I was able to drive", "I knew it was working"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-bb701a8ccaec4b48ba2de6c0c9738e93", "references": ["as not to damage the engine", "the motorcycle shot forward", "it seemed to be working"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-77c4677d03ed4a42891f75ba291096a2", "references": ["I opted for the red wine", "results in a high concentration of alcohol in the wine", "I don't think that's the problem", "I decided not to drink it", "it was a good substitute", "results in a very high concentration of alcohol in the wine", "I didn't want to drink too much", "it was a little too sweet", "I had to add more sugar", "I did not use it", "the juice turned to wine"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-411c0f68cc874bc19c0bb03eccbfbd14", "references": ["he got a new one", "he grew a beard", "he had to wear a hat", "he had to get it cut", "he wore a hat", "he had to cut it off", "he decided to get a new one", "it had to be washed", "he went back to his old hairdresser", "he decided to do something about it", "he couldn't do anything about it"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-d11795119a7c4d21b49131cba46220cf", "references": ["I could get a good night's sleep", "I gave up and went to sleep", "we had to act fast", "i took extra time to get ready", "I was able to get a good look at the clock", "I went back to sleep", "I don't know what the problem was", "I thought it was time", "I could get my sleep", "I don't know what was going on", "I had to check it out", "I decided it was time", "I could get a good look at what was going on", "I called it a day", "the clock showed the wrong time", "I decided to try again"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-775d3b4c22614fd7b6022125a71c77af", "references": ["in order to get rid of some of the dirt", "the soap foamed", "I could wash my hands", "I could clean it up", "in order to get rid of all the dirt", "the scent got better"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-bd42f83eaf024498bde697726b4551ee", "references": ["the student spit out the gum", "he went to check it out", "he had to pick it up"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-8968aa6364c64a54b5028b8697978a1f", "references": ["the fugitive dropped his gun", "the fugitive fell to the ground", "resulting in the death of the fugitive", "resulting in a violent reaction"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-7ff41b34eec142a5951dd8453975634f", "references": ["he went to get food", "he went home to eat", "he went out to eat", "I asked him what was going on", "he went to get some food", "he didn't have to cook", "he couldn't get any food", "his dog ran over to eat the food", "he was hungry and hungry", "he didn't get any food"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-efca5ba474e447d3802989cdb634d274", "references": ["it worked out pretty well", "she was able to catch it quickly", "the lasso grabbed onto the horse", "the horse trotted into the barn", "I hopped on the horse"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-d689b547d531479e9c376bbb494a54cb", "references": ["we were able to eat", "in order to give them a taste of what he had to offer", "we didn't have to worry about that", "it was a quick lunch", "there was plenty of room for everyone", "we were able to get a table", "his guests were gracious", "we were able to start the meal early", "in order to give them a taste of what was in store for them", "I didn't have to worry about that", "all was well in the kitchen", "the guests were all satisfied", "he didn't have to worry about the food", "we were able to enjoy dinner", "we were able to eat lunch together", "there was plenty to eat", "we decided to try it", "we didn't have to cook", "there was plenty of room for everyone to eat", "he was able to cook dinner", "we were able to finish our meal"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-b4f683272d934ca2acb47a14f5c9b2c0", "references": ["we had to supplement with water", "I gave her a bottle of water", "they took him to the emergency room", "the nurse gave him an iv", "she had a bottle of water", "resulting in an increase in blood pressure", "they had to drink more fluids", "she went to the bathroom to get some water", "she was sent to the hospital", "he was taken to the emergency room", "the doctor sent him to the hospital", "she took him to the hospital", "they brought him to the hospital", "leading to a condition known as hyponatremia", "they had to drink more"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-13eb5d19874049a38534d3b327c38686", "references": ["she tried to ignore him", "she didn't talk to him much", "she decided to give up", "she decided to take matters into her own hands", "she decided to do something about it", "she tried not to think about it", "she went to talk to her", "she went to a friend's house", "the girl spread a rumor about her friend", "she didn't want to talk to him", "she didn't say anything to him", "she decided to go home"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-c6691fd5ee014937832ea374289431d3", "references": ["I knew I was in the clear", "the woman was relieved", "she was able to get away with it", "she could get away with it", "I guess it worked out", "she got a second chance"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-f166e8b38409438182ecd14e286fd576", "references": ["i apologized to him", "I could get to know her better", "I could get a better look"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-2023e0caaad7474c8f7e7bc920654d94", "references": ["she picked it up and gave it to me", "she went along with it", "she didn't have to worry about it", "she gave it to me", "we had to return it", "she knew it was the right one", "I had to check it out", "she gave it to her", "it worked out pretty well", "I guess it all worked out", "she went back to it", "she completed the puzzle", "I gave it a shot", "she gave it a second chance", "she got a second chance", "she knew what was going on", "I thought I'd give it a try", "this is what we came up with", "she decided to take a look"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-96ba7790ac2e409e9f1a19f079327616", "references": ["the police didn't know what was going on", "they didn't know what was going on", "he knew what to do", "the fugitive remained at large", "they couldn't do anything about it", "he was able to get away", "he didn't get a chance to investigate", "in order to avoid being arrested for driving under the influence of alcohol", "he knew he was safe", "he could get away with it", "he could get his free life", "the guards could not help him", "he didn't get the benefit of the doubt", "in order to avoid a confrontation with the police", "in order to avoid being arrested by the police"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-735d6d6fcb3b497e9c71af9501398596", "references": ["she decided to check it out", "she went to check it out", "she decided to look it up", "she looked the term up in the dictionary", "she talked to her teacher", "I explained it to her", "she came up with an idea", "she asked the teacher to explain it to her", "she went to the library to find out what it meant"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-7557c231a65746e18e1bbc4b70b647c8", "references": ["we'll see how that goes", "I know what I'm doing", "it's a good time to plant seeds", "in order to give them a chance to grow", "the seeds sprouted", "i watered the soil", "I know it can be done", "we'll see how it goes", "I would have something to look forward to", "that's not an issue for me", "I knew it was coming", "I would have something to look forward to in the future", "they would grow"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-f9086ed7c3cc40969e11319b988976bd", "references": ["I don't know where they came from", "it was hard to find what you were looking for", "there was nothing I could do", "we had to start all over again", "i put them into alphabetical order"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-8587d6be05b644b5bcb700c69132e609", "references": ["it was easy to use", "it didn't bother me at all", "I knew it was coming", "I know it was there", "the liquid in the bottle poured out", "I don't think it's water", "I can't comment on the taste"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-79822d8c71c346c8bcb63d2abe444b57", "references": ["he could not work", "he couldn't work", "his family offered him financial support", "the family moved in with him"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-f9cfd50d1ba24fd781ffdbc92844473a", "references": ["she tried to hide it", "she decided to do something about it", "she hid the scar with makeup", "she didn't say anything at all", "she pretended to be inside", "she took matters into her own hands", "she used the same product", "she decided to go to the doctor to see if there was anything wrong with", "she decided to take matters into her own hands", "she explained the scar to strangers"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-e50a098c2c6c47468f138084b75ea423", "references": ["she was a happy girl", "I knew I was on the right track", "she's got that going for her", "I'm looking forward to seeing what she's up to", "she didn't have to deal with it", "I guess it was a success", "we gave it a shot", "I guess that's a good sign", "I had to check it out", "let's give her a chance", "she bought a yacht", "it was a good thing", "of course she had to do it", "she must have done something right", "she must be doing something right", "we decided to check it out", "she could win the prize", "she could move here", "I thought I'd give it a try", "she was able to get away with it", "we gave it a try", "it was a win-win situation", "she could buy clothes for her", "I don't have to worry about that", "there shouldn't be any problems"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-234203ca94dc477b99c69e2306a7b5cc", "references": ["it should be up soon", "I thought I'd share it with you all", "please stop by and check it out", "I thought I'd share it with you", "I thought I'd put it up here", "we'll have to wait and see", "be sure to check it out", "you can read it for yourself", "I can share it with all of you", "I'll get back to it later", "be sure to check back", "I hope you like it", "I thought I'd share it here", "let me know what you think", "we'll see how it turns out", "be on the lookout for that", "I thought I would share it with you", "i turned to the next page", "I thought I'd share it with all of you"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-a13b64d341124f4184a0ec0fce505d72", "references": ["I couldn't see what was going on", "I had no idea what was going on", "that's out of the question", "I had to buy a new battery", "i replaced the batteries", "I couldn't tell what was going on", "it was a bit dark", "I had to open it", "I didn't take any pictures", "I turned on the light", "we didn't stay too long", "I decided to skip it", "I had to open the windows", "it was time to call it a night", "it was impossible to see what was going on", "there was no way to see what was going on", "i took it apart", "it was hard to see what was going on"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-07801d24016a4597947281a02e08c29d", "references": ["I don't think that's the problem", "in order to determine the extent of the injury", "he put the patient's arm in a cast", "he knew what to do", "there was no need to worry", "in order to get a better idea of the extent of the injury", "he discovered that the patient's arm was broken", "in order to get a better idea of what was going on", "he could go to work tomorrow", "he was able to get the surgery done quickly", "he didn't have to worry", "he could see her elbow"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-f8cca69d247a4f0c9a34749de8145d11", "references": ["it was easy to sew", "it had to be removed", "it would not be lost", "it was easy to insert", "the thread wrapped around the needle", "the thread went through the fabric", "there was no need for glue", "the threaded needle was in the middle of the fabric", "the threaded needle was in the center of the fabric", "I didn't have to sew it", "I didn't have to worry about that"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-3d6d8f182a7343648c8b87676d4c74b7", "references": ["I went to the other side of the street to see what was going on", "I went downstairs to check", "I came over to help", "I gave her a call", "I pulled over to check it out", "I ran over to see what was going on", "I went to see what was going on", "I tried to help her out", "I walked over to see what was going on", "i called 911", "I had to check it out", "I walked over to take a look", "I went to check it out", "I decided to check it out", "I went over to see what was going on"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-b3ba8110756b437ca2c308f94c393ecb", "references": ["I had to search for it", "the dog emitted a foul smell", "he had to leave it", "we had to be careful"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-230a1d81aa1c459c84f22fb648debe82", "references": ["I decided to test it out", "in order to be able to run Windows 7", "she went ahead and installed it", "it could run better", "in order to be able to run the latest version of Windows 7", "she didn't have to worry about it", "it was easy to install", "I gave it a shot", "she knew what she was doing", "she knew what was going on", "we had to use it", "she didn't have to worry about that", "she was able to access the web", "she was able to test it out", "it was easy to use", "she didn't have to deal with it", "she can use the internet", "she didn't need a new one", "I'm not sure what the problem is", "she deleted old files on the computer", "she could play with it", "she went online to check it out", "she installed new software on the computer", "in order to be able to run Windows XP on her computer"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-e686a173451d47ddaa56fc0797bfd05b", "references": ["there was nothing he could do", "he had to leave the door open", "we had to take him home", "he went to his friend's house", "he took matters into his own hands", "he had to get a new one", "he went to his neighbor's house", "he couldn't call for help", "he couldn't get back to him", "he did the next best thing", "he couldn't go back to his home", "he left the door open", "his landlord repaired the door", "his landlord unlocked the door"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-91c98e0ad8114149b2dcd4d08c751c35", "references": ["I decided to take a break", "I know how hard it is", "I knew I was on the right track", "I had to cut it short", "I could not walk", "I went to the doctor", "I couldn't go to the gym", "i shook my foot", "I had to go to the doctor", "I didn't go to the gym", "I had to come back to the doctor", "I wasn't sure how long it would take"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-49edfac08b024a40969bd7984c75efc4", "references": ["I had to wash it", "my itch went away", "I knew it was coming", "I'm looking for a good moisturizer"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-0d2ad12c957a486b8d705336ad91fe09", "references": ["we decided to check it out", "I didn't get to see much of her", "it's not like there's anything wrong with her", "of course she was involved", "I jumped at the chance", "her parents came to watch the recital", "I got to watch it", "we got to see her", "I decided to check it out", "I'm sure she knows what she's talking about", "she knew what she was doing"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-2214e3dcbf17432b85abb6ceefaa8aea", "references": ["I got to hear it", "I had to check it out", "i sang along to it", "I gave it a shot", "I decided to listen to it", "I was happy about that", "I thought I'd sing it", "I thought I'd give it a try", "I knew it was a good song", "this is right up my alley", "I was excited to hear it again", "I couldn't wait to hear it", "I was inspired by that", "I had to sing it", "I was looking forward to it", "I jumped at the chance", "I thought I'd share it with you all", "it was nice to hear it", "I had to listen to it", "I was very excited to hear this song", "I was able to listen to it", "thanks for sharing it with us", "I thought I'd share it with you"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-f583a4399e0a411b99201305841768f1", "references": ["it wasn't a total loss", "the photographer quickly snapped the child's photo", "it was a good photo", "it worked out pretty well", "I had to check it out", "it was great to see him again", "she didn't have to worry", "made his job easier", "it wasn't too hard to get a picture", "I jumped at the chance", "it was a win-win situation for both of us", "I guess that's all that matters", "it wasn't a total waste of time", "I had to take a picture", "it was a win-win situation for both of them"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-1a0b7e2fe7e14e1fa370ea61c3358e81", "references": ["in order to gain access to the other side of the house", "it was easy to get in", "we didn't stay too long", "he went back to his truck", "we had to go downstairs", "I had to check it out", "the security alarm went off", "we had to get a new one", "I got a new one", "in order to gain access to the property", "in order to gain access to the home"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-4759913505b2406e84bc998d70099787", "references": ["I took him to the doctor", "the mother picked up the baby", "I went to check on him", "I got up to check on him", "we had to get up early", "I decided to take him to the doctor"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-0b92885bbd5b40e0bf4a0e8f3aef7caa", "references": ["it was great to see her again", "she was taken to the hospital", "the woman's daughter came over to clean her house", "she was sent to the hospital", "it was time to move on", "she had to go home", "the woman's daughter moved in to take care of her"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-461162a52d7b44ffa95c94a16541ca03", "references": ["she offered to take her place", "she decided to help her", "in order to make a difference in the lives of others", "she found a way to help", "she decided to join the group", "she took matters into her own hands", "she gave it to her", "she donated blood", "she called the police and asked for help", "she jumped at the opportunity to help", "she came up with a solution", "she asked her husband to help her", "she volunteered to take her place", "she decided to give it a try", "she came in to help", "she went to the doctor's office", "she jumped at the opportunity", "she worked to help him"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-d4c22d4e1c074f819320e98e6de74d4d", "references": ["the woman joked around with him", "in order to move on with his life", "he let it go", "it was bound to happen", "he wouldn't have to deal with it", "he could get a job", "he could get away with it", "he could go on with his life", "he didn't have to worry about it", "the woman took pity on him", "he decided to try again", "he could finish his life", "he could move on with his life", "he could live his life", "he wouldn't have to feel guilty about it"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-f3d62705cf4143239c4bbc8ac974f937", "references": ["I got back in the car", "i played in the puddle", "I didn't stay too long", "I hopped in the car", "i leapt over the puddle", "I knew I was on the right track", "I snapped a couple of shots of the water", "I went over to look", "I had to check it out"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-49ac549280724f529ddd804c5ee561b1", "references": ["the police took him to the emergency room", "he went to the emergency room", "he went blind", "he took him to the hospital", "he was taken to the emergency room", "the doctor took him in", "he went to the hospital", "the officer took him to the hospital", "the officer took him to the emergency room", "the doctor took him to the emergency room"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-5af7231185834e79a302bcf4e7940d58", "references": ["he knew what he was doing", "it had to be removed", "his stubble disappeared"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-35c9920818f84aa4b4ab41aa04777805", "references": ["she could close the door", "I could see her face", "I could see her again", "I could get closer to her", "she could see my face", "I could look at her"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-197e5d3a28e943ac8ca857d477695187", "references": ["she wrapped herself in a towel", "she had to wear a towel"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-b9bccf26221449a0b07ec56e55fc97d1", "references": ["she opened the door and opened it", "she decided to give it a try", "she assumed she was safe", "she called her mother to check", "there was no need to worry", "she assumed it was the same", "she opened the door to see what was going on", "she stopped by the window", "it didn't bother her at all", "she was reminded of her childhood", "she didn't mind too much", "she opened it up again", "she assumed it was food", "she didn't worry about it"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-9b36ce67e210468fb558f4775227c877", "references": ["I could not see what was going on in the bathroom", "she couldn't see what was going on", "I couldn't see what was going on", "she turned to the other side of the room", "it was hard to tell what was going on", "I turned around to see what was going on", "I could not see what was going on", "I couldn't see what I was doing", "I could not see anything"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-35cf43ed2b834d9391133080c206b186", "references": ["they needed to be removed", "I got rid of them", "I had to throw them away", "i threw away unnecessary contents", "they could not be viewed", "I had to get rid of them", "I didn't want to use them", "we had to start all over again", "we had to go through them"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-206c6e9409fb4e8b9a4ac946e9d5a2fa", "references": ["there shouldn't be any problems", "there was no need for an emergency", "it could be injected directly into the patient's body", "they could begin treatment immediately", "they were able to complete the procedure", "there was no problem with the procedure", "it could be injected into the patient's body", "the patient tensed up", "she knew what to expect", "there was no harm done", "as not to delay the procedure"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-4c3ede64c3b147ac912c7cfe6597aed0", "references": ["I didn't have to worry about it", "I didn't get to talk to him", "she could get a better understanding of what was going on in the classroom", "we didn't have to wait too long", "it was easy to follow", "she could see the child", "she could attend the meeting", "we were able to get in", "I didn't get to talk to her", "she identified the students that were absent"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-f07189c65da84a24b06dcd03bbc5890f", "references": ["it was easy to adjust", "the car turned", "it was ready to go", "it was easy to fix", "it didn't pose a problem", "it was not a problem", "there shouldn't be any problems", "there was no problem with the drive"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-b842abd533aa422ca78e2486d3abb666", "references": ["she would not know what he was talking about", "his mother told him to speak up"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-518d00142f8340bf8f0d74397c193ee7", "references": ["they came up with a plan", "they were forced to leave", "they took it to court", "they built a fence around their property", "they had to come up with a way to get rid of their nosy neighbors", "they had to come up with a way to get rid of the nosy neighbors", "they decided to do something about it", "they had to move out", "they took care of themselves", "they had to sell their property", "they set out to find a new home"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-14c7e906fd80421f834782220c3ad308", "references": ["it swung back and forth", "it's ready to go right now"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-84cd874f228f49b2bacc58b4f77662f1", "references": ["to prevent it from falling into enemy hands", "they knew they were going to die", "the ship's debris sunk in the sea", "the ship crashed into the pier"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-fe7ed4c42e624d21854bbe11af84b6fc", "references": ["in order to increase the size of the body", "in order to increase the size of his body", "her muscles became fatigued", "it felt more powerful", "in order to get the most out of the workout", "as not to lose weight", "as not to lose muscle", "as to increase the weight of the body", "as to minimize the damage", "that's what I'm going to do"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-34ec2e2fc7cb4df584f41eac58940876", "references": ["we had to start all over again", "the blocks scattered all over the rug"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-0522cd8a047b45ee9fbc6ea7c1b534ea", "references": ["she decided to take a deep breath", "she decided not to go", "she turned around and went back to the room", "she called it a night", "she decided not to do it", "she took a deep breath", "she didn't say anything at all", "she got up and went to the hospital", "she pulled over to see what was going on", "she wasn't able to talk", "I decided to take her in", "she decided to sit down", "she lost her balance"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-a2dbfffec118447fb200ac15741a8516", "references": ["it did not stick to the bottom of the bowl", "it didn't take too long", "in order to keep the mixture from sticking to the bottom of the bowl", "the water would mix", "I didn't have to worry about that", "it shouldn't be a problem", "it didn't stick to the bottom of the pan", "it was ready to cook", "the ingredients blended together"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-ecad2bd1487d4b16a238d2a826bacfd3", "references": ["he couldn't see what was going on", "he didn't take a look", "he couldn't see his face", "he realized he was early", "he didn't get to see him"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-3bddef1c1f2c46dd9112a166e3c14c46", "references": ["we decided to go there", "we were on the left", "we decided to check it out", "we walked down to the bar", "all we had to do was sit on the balcony in the middle of the", "we sat there for a while", "we went to check it out", "we sat down", "we didn't have to wait long", "we didn't have to wait too long", "we were able to get a good seat", "it was a good experience", "we were able to get in quickly", "it worked out pretty well", "the film began", "we were able to enjoy the show", "it was a great night", "all we had to do was sit on the balcony"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-24278deac4cb4ce3879fdb37cad7326e", "references": ["people skated on the pond", "I knew it was cold", "I'm not sure if I'll be able to get to the bottom of the pond", "we had to walk back to the forest", "the rain was a welcome sight", "there was a lot of ice", "we didn't get to see much of it"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-fb8925dbc4264ae89067b3c680c2a40f", "references": ["she decided to see if she could find someone to hang out with", "she came over to say hi", "she went to talk to him", "she agreed to help him", "I decided to join her", "she went to check it out", "she came over to talk to him", "she decided to see if she could find someone to talk to", "she tried not to think about it", "she decided to do something about it", "she took a deep breath", "she gave up on it", "she renovated her kitchen", "she agreed to go with him", "she decided to go for a walk", "she decided to give it a try", "she decided to give him a call", "she came over to help", "she adopted a cat", "she decided to leave home"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-b4d1b85d9a5d4431a05a81086377059b", "references": ["I thought I'd give it a whirl", "it was easy to do", "I thought I'd share it with all of you", "I was happy about that", "I thought I'd share it here", "it worked out just fine", "I'm off to a good start", "I am very happy with it", "i thought i would give it a try", "I thought I'd give it a try", "the box will be my favorite", "i know what you mean", "it may be out of stock", "it worked out pretty well", "I was able to test it out", "I thought I'd give it a go", "I thought I would give it a try", "it was a pretty good deal"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-319e42b1a39f4a05807b3123f2809bb7", "references": ["I thought I would share", "I knew I was in the right place", "I'll have to check it out", "she excavated ancient artifacts", "we got to see it", "we had to go there", "I guess it all worked out", "in order to get a better understanding of the history of the site", "she read about the site's history", "in order to find out what was going on", "I thought I'd share it with you", "I went to check it out", "I could see what was going on", "we could find this out", "in order to get a better understanding of the history of the area"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-47264cf28ddb4f9899de5aaa2a2a7fdc", "references": ["there was nothing to lose", "he was sent back to jail", "he was sent to jail", "there was nothing we could do", "he was forced to go back in", "resulting in a prison sentence of up to three years", "resulting in a prison sentence of up to two years", "he was able to kill him", "the case was opened", "resulting in a prison sentence of up to five years", "she was sent back to jail"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-f20bb7952d3447a39d6673df89f9a8d9", "references": ["the wave carried her to the shore", "all was well in the end"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-8fc80d593c8d43a1b551dc38e01f030d", "references": ["we'll see how it goes", "I was able to move forward", "it was a good thing", "I could play with it", "it was flying down", "I don't think that's the problem", "the ball could move forward", "in order to get it out of the way", "I could see it coming", "in order to get the ball out of the way", "the ball hit the ceiling", "it was up to me to hit the ball", "I could get a good look at it", "it fell down on the floor", "it was easier to play"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-ff3f98042c5d4a638aa13fee0c64b451", "references": ["I could wake her up", "I could wash them", "my friend awoke", "I had to leave early"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-fff520b48721447abec197c26b0051dc", "references": ["the band signed autographs", "leading to one of the most memorable performances of the year", "I decided to join them", "it was a pleasant surprise", "resulting in one of the most memorable performances of the year", "we knew it was going to be good", "the band reappeared on the stage"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-29d7b574d6f44e3fa0702e7425151c38", "references": ["we'll have to see how that plays out", "I asked her about it", "she knew it was coming", "she did the only logical thing", "we'll have to wait and see what happens", "to prevent him from speaking to her", "she didn't get a lot of response", "the speaker won the debate", "she did the right thing"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-80a1870384794d489c4f96530a34ba29", "references": ["I just stopped eating it", "i wiped my mouth", "I had to stop eating it", "i brushed my teeth", "it was easy to tell"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-2d6db97ece3b40ea8e97688c62a4853c", "references": ["we had to get up early", "I didn't get a good look at it", "the moon became visible in the sky", "it was nice and dark", "we hopped in the car", "we knew we were in for the night", "we had to go downstairs", "I didn't have to go outside", "made it even more difficult to see what was going on", "we got up and went inside", "we didn't stay too long", "made it all the worse", "made it even more difficult for me to see what was going on in the", "we didn't get a good look at it", "made it harder for me", "it couldn't have been too bad", "we called it a night", "we decided to head back to the hotel", "we headed back to the cabin", "we did it in the dark"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-281a491477304a2d931d38e2ad6fd110", "references": ["he didn't have to worry about it", "he was fined for littering", "he had to find a place to hide", "he didn't have to walk", "he just let it go"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-5334fd563b3d4717962026324f579854", "references": ["I couldn't get to her", "I didn't get to talk to her", "I guess it all worked out", "I didn't get to see her", "she didn't have to walk too far", "all was well in the end", "I had to check it out", "I was able to see her", "she got out of the way", "her bike swerved", "it shouldn't be a problem", "her bike sped up", "she called it a day"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-4bb7fe86a8b8476391b717cd586609b1", "references": ["he didn't know what was going on", "there was no one to talk to", "he drew a map", "he was left to his own devices", "he asked for directions", "I asked him what happened", "it was good to see him again", "he didn't know what to do", "he had no choice but to return"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-0409b20a11644d8daeb99229130feead", "references": ["she went to the bathroom to check it out", "she went to the bathroom", "the ring went down the drain"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-41a0d0f5c11a44a3865c05e8d283da41", "references": ["in order to get a better look at what was going on", "we didn't have to wait long", "they were able to get out quickly", "we had to take precautions", "in order to get the boat out of the water", "their boat was rescued", "it didn't take too long", "we had to be careful"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-d52ae7c697ec4a0db2709ee60ab5d7ca", "references": ["I talked to him a little", "I went to check it out", "I didn't get a good look at him", "i smiled at him", "I got to talk to him", "I ran to the front of the line", "I couldn't see what was going on", "i confronted him", "I couldn't get a good look at him", "that's exactly what I did"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-6de8f7587303481aa9b98f89a3d1249d", "references": ["of course she had to do it again", "the mother grimaced"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-91fa6cb8d3c44560b8bfa5e5a46844d1", "references": ["she got what she wanted", "he opened the door for her", "he asked her if she liked sushi", "she got a new dress"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-c140b8da6bc9441ab8cba6cbe0b8afda", "references": ["the dog jumped up", "the dog scratched its fur"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-ceab7ee80ec444b0b7ae7bb732bac827", "references": ["in order to have a better understanding of the world around him", "he decided to give it a try", "he decided to give it a whirl", "he had to be careful", "he could see the beauty of it", "a piece could be created"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-2f912ae3b2be493081f5a6268ed8e0bc", "references": ["my balance wavered"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-fd603900c6ee4412b643d1ce6fe48b55", "references": ["it worked out pretty well", "she began going to church", "she went along with it", "leading to a spiritual awakening", "she wasn't sure what to expect", "in order to be able to see the world through the eyes of God", "she had to change her mind", "resulting in a spiritual awakening", "she wasn't interested in him"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task393-dddcb1afe9b2416c9fc21b59f2b5f015", "references": ["it was easy to use", "I knew it was clean", "the wood became smooth", "in order to give it a smoother finish", "I don't think that's the problem", "it shouldn't be a problem", "in order to give it a smoother surface"], "task_id": "task393_plausible_result_generation", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task879-65588d02e54245ab9e5dfc6135618c9d", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-f7a75743baef4118bb6a971873a72dea", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-f707ca581f9a4044bb7db909c18458ea", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-5c21d2e691e24ad2a9aef0ea135d8bf8", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-1837c90446f24cf2860ca032a6e4f3ed", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-9196dddc122541629df7c438bcbf3f84", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-4c6a17f3ce614139875768ea22171348", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-08983dc10ef040d3bf48657c0debebbf", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-996c148f52834fdb87fcdcc823928e73", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-362655142f3141a388114ca90881f363", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-fb04dfc122c442059c3e10a41c0a332b", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-ca2319db590048f3804a16d1abd25ec2", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-d05d97c6ba1442eb87267dc4026fc52e", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-dc0e8ec609cd4c2da1ff766d2f8d58ce", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-9260651d17b34224989329a9df271cbd", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-25b2387efcf64b00bdd01899e47a1ab7", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-c057b9ef878d4e4785936bbe36588a4f", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-e670682b2820423ba6bb50166f103c86", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-0afcd597c0fb4e4a917e8a627dcc82c7", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-d9d646fb808542c694caf44f40e18b10", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-85607e16920a4d9d9f0d9aff9e4716b6", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-cc88a1c6d5da4195b95af5d1ba30ea28", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-ce3b30f6c472412ea0e0adf023f5775d", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-f0ed44b49ca341599d04efe2b612185a", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-66234462e239481bb449a77157ffc80c", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-4ff3133a2363466eac6a30f1f486405d", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-10aabbaa24ec45419ad8cdc4a01c0e1a", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-65228bb49a394e81ad1b754b4f540942", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-7e306d7b59554277b1866b996d6a7d4d", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-f11201d78b234aefb32c149b0282738a", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-70aefd3532bc4d01804d16fc9d356c08", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-cd52b17f2ce641da83b9011b05068f69", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-f4f91f5d6baa42a0b5d6e0d9f6199c06", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-d332bb031db04f44bf6313f3788c571e", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-d72b7de0378b4b078c0721a9ff8a4c3c", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-b4fb85462edf422b874eb8deeac3668d", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-23fd91f81255472e87f7e190ecf8037e", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-dc565d07e4314185b7f890f11db4da04", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-0c97232e34bb44909b25d1ca2a9f7561", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-ad0367ef65934cb29a66a081ba0ada8a", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-91a0ce38a4cb446f9f884b23216b386b", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-b8df60c9b2834c8da1020b82c8d6802a", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-4f9a431d826348868d3fd286a2738e13", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-d225764d8c4e456a87ff93733a7006e6", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-8d9c8ceeaa3b40d69f95307491900227", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-cc1768ba046243e29621a262e45fb9d4", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-a7b098c5028e448aa680b77c85f4fa4b", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-7dba9f0cad8543489bf3e4275ebff4c2", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-beb51772f5d643d0948df18f024f634b", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-3b8127fe290c4595bea330ac346f94d3", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-e1f8539c5a2a4b92b410d45922a55900", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-26ea7d72296449f0b1dcb05e5ecfe95e", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-feeb892be70840f3871cc50cdcfd41e2", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-24add4995d00471599638e0d1e148e4e", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-1833a564a4a247949fc56d0ecffd685c", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-eb098446e2de49b4b7987064b6206aa0", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-7ae96970436741b98a3e1c5dc5795f59", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-27988766346645d393162549e54ba6d1", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-f8f395a8069141f88a88e9b78e434664", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-6d4e0785607a467286fbbdba602120d8", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-342891675ff7499386d2236fc7641a61", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-d146ddc661be42f184bb148e2711243a", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-0b8f7e8f0136435584207e17149ec985", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-f742778042ff4f2ca17002e1401e2bf4", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-083b8d13840842fc812678e2097846d9", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-9701f796a48048a1a09ba71c920e046c", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-077019a3cfd1494ea9fba3c70c3ddb4a", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-356945efcc684ebc9b5da180ef39baeb", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-8b526bb9cc2b47d6a13d35794ab0bc81", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-1ad5a95ee3d84b94b5b0dbecec637b21", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-524412b599594153b90875b2c8793bb2", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-be8ab289ccf2428b95ab4de9698db724", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-1429af51a8cb4a5496bea5b7296e9113", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-55c708cb60fc46e781616b41a0b09ec2", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-4e67e348ea5c4477ace473a717ef83a8", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-f3c8891aa50e471fb9ec143992f977fe", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-afb87ed586574bd180a46701163be028", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-87ead099704e4b7689f0ec27041805e9", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-ea15e2d960174952a70904d75c84937c", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-352b041f5a664ea598715103b74a10cf", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-ade7ec89f02c47968288c8615fb9ed6a", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-a0ac6ca8f67b45dfa32cccf990549f54", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-13f2d3facb6443bdb52dc0cc4262a070", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-a43d1e1f892043c9b209630185226250", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-01ac84462fc24bb288a2f5d48b9619e2", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-ad1066c0ff434f1ab2952bc3e774e02c", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-f6d80598975e4229a6db993210e2d63b", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-5cc0e881c3844ba7a3b24f57488b8d90", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-a13fc1f8b05948589670526655e9cb74", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-13425669a045428ca5879f270602226a", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-f8530f0012224367960260bf55cd9466", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-77f8baa6eb8946c990373feb76b0b66f", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-fe122a60562341f88284e29917380e5b", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-b212311eee1f44a8b23ea8e097a47369", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-fe5bbab2b55342bcb04beb8de00e5ab5", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-b197864e66c34430954e2e876a9de204", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-3ecb87037a164e9d97662381dde5cfce", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-7244baaac6324677addeb84bd8c540ec", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-ba91fc1cb7f34072991c99acf8fa28b4", "references": ["Yes"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task879-770a3a57f0d94edebac1777266f88237", "references": ["No"], "task_id": "task879_schema_guided_dstc8_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task613-87da911859a9457897c924ddff743061", "references": ["guns"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-6b527f5fa0b14f988ad4c0ab88422849", "references": ["transportation"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-d86a4d9c272d41a5ac70143426c41c47", "references": ["income", "taxes", "wealth", "workers"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-b84d6536e64740a8b9d99915a2c06934", "references": ["foreign-policy", "public-health"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-acb919ed9f3f4e28aba85a8bcd3c49f4", "references": ["abortion"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-a36b11b5e2b64481972affc6ea90f5a2", "references": ["candidates-biography", "message-machine-2012"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-2c41a17bdeb940dea29cf4e2ccd6623b", "references": ["deficit", "federal-budget", "history"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-245b0fe06e8f4b09a43455cde2001154", "references": ["jobs", "state-finances"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-3087bae121af4d3fa39d38542cc4cabf", "references": ["education"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-239ab419985148f1aa6f7485314f38d0", "references": ["government-regulation", "health-care", "polls"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-86a81f35c73b4f6e9e9c587865be7e2b", "references": ["corporations", "diversity"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-40ec4948f8804764a41b0bd98abff20c", "references": ["elections", "state-budget"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-ce785ceb98284474bf2033b4533500c2", "references": ["economy", "jobs", "message-machine-2012"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-1c13f0357b4142a5b34e8ba454a44061", "references": ["civil-rights", "education"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-ca45ca21723f4e2886b9c6e4075873a4", "references": ["job-accomplishments"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-c1b938a099254503bc4eb08270a19f09", "references": ["congress", "diversity", "history", "population", "women"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-8526e765ec894a1da9e61503ee679abc", "references": ["medicaid"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-2f3c664f21de47ae8465cd41e8322ef6", "references": ["crime", "immigration"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-6f7c84a311e2464893451b5d22e18cf1", "references": ["families", "women"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-37edfdbfda9d48d68801c8b890156b19", "references": ["islam", "terrorism"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-82baa4302acb48959f8ac1755a0f67cb", "references": ["gays-and-lesbians", "government-regulation", "sexuality"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-0e463643293041158dc200d4b7685f7a", "references": ["state-budget"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-a6c4fcdca02f46859088c4c23100b274", "references": ["social-security"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-f90f5ec493d64a4f8a8ee309d8d0d431", "references": ["immigration"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-31902da2c6a045d0a6d405ba91791974", "references": ["disability", "federal-budget", "social-security"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-9dcf5b34a5e54cf4a4da3d0433fda7db", "references": ["debt", "economy", "message-machine-2012"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-7434bba49de04378ac2ce7ab8302f6bb", "references": ["campaign-finance", "climate-change", "environment", "message-machine-2014", "campaign-advertising"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-5ba63a2869b84e2cb55b61e551aad2be", "references": ["nuclear"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-4e8ec01554614833ab9f14c24f828f3a", "references": ["ethics"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-d1105f40a8e64f1eb83e7d729e000425", "references": ["health-care"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-a4ba6944f3854c6c82c90eb63449402d", "references": ["debates", "debt", "deficit", "economy", "job-accomplishments", "campaign-advertising", "pensions", "unions", "workers"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-47216ed7399d4b83ac45ae12d5127fe0", "references": ["medicare", "social-security"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-47a94b924cf24a179b67f5c790d182f2", "references": ["environment"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-3a842973aab54bf1bb4273535854d84b", "references": ["climate-change", "energy", "gas-prices", "message-machine-2012"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-13be14defd794f27bab1b3fc9a423831", "references": ["stimulus", "transportation"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-34db15bc20c0430798219fbf0a7123b3", "references": ["city-government", "civil-rights", "county-government", "gays-and-lesbians", "sexuality"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-6bf0b44b9e63483d8a83c248fcc279de", "references": ["sports"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-4c1eb23db8a645a082f8a033d801745b", "references": ["agriculture", "economy", "history"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-78ca6c944b79464788af4a42784b78cf", "references": ["bankruptcy", "deficit", "federal-budget"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-d61d362747c345dabcfd8bbb3418e099", "references": ["ebola", "public-health"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-fbdb9f481dc24ea08fcbf997a946c7b5", "references": ["afghanistan", "foreign-policy", "military", "terrorism"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-840b7f2cf5704dc2a533031d9c84da47", "references": ["energy", "environment", "oil-spill"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-c1e79539c6a54d3c9545e3d7b3f2a17a", "references": ["marriage", "message-machine", "women"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-f267b7443dca412fabd7c49c7357ea87", "references": ["pensions"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-599db91751864dd1afb6ae0e0ec43a94", "references": ["children", "disability", "health-care", "history", "medicare", "social-security"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-9b5c868666e945a48d8453ce426763e7", "references": ["corrections-and-updates", "education"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-e94dd107c70e45c88dddbc59e1c05176", "references": ["financial-regulation"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-b9fb370cd1624c5cacd2d996a87336c5", "references": ["religion", "science"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-06572beff96b48e099ca7b1851969d85", "references": ["criminal-justice", "women"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-d6ead8bda94a49f094b5da43b504d369", "references": ["pundits"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-3e9538291d264825aa0db5c8017ee40e", "references": ["cap-and-trade", "climate-change", "economy", "health-care", "message-machine"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-02f263ac117c47b49a8c712fdda54de7", "references": ["taxes"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-9b1a017e858d45ff89c42e693b210693", "references": ["terrorism"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-78c8a5def475454eb35ec3a29dd5a818", "references": ["bipartisanship", "health-care", "legal-issues"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-0f9ec39a28fa4bc59e9f1ea916bbc239", "references": ["infrastructure", "sports", "taxes"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-4bfe85950a3f453bb71cc14f7521a385", "references": ["legal-issues"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-834a8378918640d3ad2b1932820c5677", "references": ["labor", "workers"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-e88c0c7441254ff4a0083d65b7ecb6b5", "references": ["iraq"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-07d54c88d161484f8089442e17e48590", "references": ["kagan-nomination", "legal-issues"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-de67b146ae0f43558b2b003137638d30", "references": ["federal-budget", "health-care"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-91329fbb87d24ce9a57445bc1ee9870b", "references": ["housing"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-6563ee74e11149c894aaadd5eab119ac", "references": ["veterans"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-834d1f7cdb0a42bd96bf65d8c6936132", "references": ["homeland-security", "immigration", "message-machine"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-9e76e89cf2da486691e67130f51543b5", "references": ["israel", "terrorism"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-8d95680da8004eacbec1c81206b6f21b", "references": ["animals", "climate-change", "economy", "energy", "environment", "history", "population", "public-health", "science", "weather"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-177bf81434694161bbb71fac5d4e44f1", "references": ["message-machine", "voting-record"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-ed7e878b2b574bb387edec1a39a0010a", "references": ["history"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-a479b71f792a4e7dad170a5ad3221744", "references": ["autism", "public-health", "science"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-ee62bd3d15f246c2b7fe411634f7079d", "references": ["florida", "market-regulation"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-c50c58da760548408e6747beaedebff3", "references": ["consumer-safety", "financial-regulation"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-9e9b879fa3cd44b39b309eb9bed8c918", "references": ["city-budget", "city-government", "transportation"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-7980b69a9d2a4e7d8b41cefeaf8476f0", "references": ["county-budget", "county-government", "public-safety", "state-budget", "taxes"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-28fbb8435cb9434dadd29014d7503066", "references": ["welfare"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-30671d4f4e194046b4edfde71704d354", "references": ["small-business", "workers"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-0cc5541dd7ad409eb41f876561697fef", "references": ["state-finances"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-be84cf2132af489bbd7f913ba5237723", "references": ["florida-amendments"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-867a435f2bec4568a2c6939d59ccec02", "references": ["government-efficiency", "housing"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-0528444141464e218c2d45032064f417", "references": ["trade"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-841fbea5b58e413796b78d3dd6504efa", "references": ["occupy-wall-street", "public-safety"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-dcfdb6e844824fd280782aba433b4944", "references": ["hunger", "immigration", "poverty"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-6b2e427f2d64496181584e95964d9ee4", "references": ["voting-record"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-7450daef9f974940bc8339ed149ecb38", "references": ["public-health"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-846f6c2081704a9d9944d515ec88fe2c", "references": ["unions"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-95c5b2d815be46beb596df4b446a975f", "references": ["bush-administration", "iraq", "military"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-cf1ad4c6c91c4fefb9c7cf5172962c2b", "references": ["county-government", "elections"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-c306dc73c48e4ce088ee9412be4a4520", "references": ["drugs", "health-care"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-63f8b5a856fc437f93fd9dc29eb49e9f", "references": ["population"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-a9fff2f648324afcadb8ce804833a33b", "references": ["public-safety", "transportation"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-fb20d6e9cb4946399385fe77131fec30", "references": ["diversity", "history"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-7e35806959e64a6f8d2054a5506d6b75", "references": ["message-machine-2012", "state-budget"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-fcdd0ae24c544a42a60479d79a2e182d", "references": ["science"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-c147b7fe4cdb473b97c470b089eaed13", "references": ["Alcohol", "education", "guns"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-93ca984e38134b6888b45de2ba3aae22", "references": ["campaign-advertising"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-3e28f9ac206e4f22a01b22adddb017df", "references": ["poverty"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-03e4d9daf3d04e0b94bac65b7e8a6ab4", "references": ["oil-spill"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-1096c6867e0a442fb1b9cc408ed1d616", "references": ["military", "terrorism"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-2db8026c4a234aa8b3244839770480ad", "references": ["redistricting"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-1958c85c7a3e453091e20cb8c15bcdc3", "references": ["message-machine-2014", "voting-record"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-a7c374cde9ee4ae0a0983e3f93efc74e", "references": ["transparency"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task613-312fc9b9653345e4858d2b9d75808f23", "references": ["marijuana"], "task_id": "task613_politifact_text_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task219-312806edf7c24d18862c32fb75c2cd37", "references": ["Chipped Tooth"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-c56d849a9fb04aa29d3099dcb6cb9637", "references": ["The knock"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-313eab2d237a493da222693eaac0cdce", "references": ["The Lake"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-c822f4f3120f4248bdc7a347fbe8b909", "references": ["Stranger in the bed"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-95e5f80da747446882aa8a846a45950c", "references": ["The Amateur Golfer"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-dccf81b9f1694c31b8d68b3e6aab0809", "references": ["Time To Grow Up"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-677d5843782e4ff0a569873fc43194ff", "references": ["Ghost Stories"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-0598473a9a1c4f13a088e46d87bcbf91", "references": ["Bird Watching"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-bcbc9e1dd6f648d5b6c3ac84b2479483", "references": ["Missed Flight"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-d9027cd868ce4b5093c9fd585ad343bb", "references": ["Lost dog"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-b987b56bc2a74161b87b3e671bae22a4", "references": ["Game Show Tickets"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-8e3cf52c849140219d3bb7eb5f9b6c1c", "references": ["No More Cheese"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-6eadac64cc4345b0a085cdd638ec0802", "references": ["Annoying Guest"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-57a6f6e150f847279e80a41409a36830", "references": ["Dancing waiter"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-6e79d11237bf4ad18238aeb4b517220d", "references": ["Joe's Training"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-9ff3180d107a401696804d74935bb552", "references": ["Milk"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-641314edac624e00ab0b92cfa09d1555", "references": ["The Fish"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-788c790285624f4fbb407f767b8cebc7", "references": ["Snowman"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-315a693bb7f0474e8e3d451889476c88", "references": ["Beating His Idol"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-f2e22588ee9e498bb7e73117ca71295b", "references": ["Drews big date"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-61f8b43cf22f409baa395505f76e49ae", "references": ["Wine"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-fa90cf0657994c75a1464694f71a938f", "references": ["A fish story"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-d6ebd347fe014fc487f133a8ae33b77d", "references": ["Old Bed"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-d2c82d315aa44f71b0f819d1f5fbdd2c", "references": ["The Relationship"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-4c60f9579289413887991bc0b4831e3f", "references": ["Elephant"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-1194a576375d43fc91ea5878ee59f74a", "references": ["First Day"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-c18b4e7a4e9146cb8d6797e29b95eb03", "references": ["Her Mom"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-095a2c140659477da9e92db29fd7ce51", "references": ["Wedding Mistake"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-7deda594725d4dac809a9481a44e6c79", "references": ["No cell reception"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-08c50c2cbdd44a1099988a09906c9e86", "references": ["Bad lipstick color"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-8ef67ca1c84a432282af37a1b32db56e", "references": ["Swimming Lessons"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-3b87f5f9b4c44c35b3283f2c4ab4daff", "references": ["Cat Bread"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-d429359580a546bba847e7289799a2ee", "references": ["Bored Fans"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-768e730483884d7b899f894cc1fb6fce", "references": ["Lost Match"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-a12f9e634dbe46ccaa0a290c982f6cd7", "references": ["Faulty Laptop Battery"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-3f3993babfe54bea838f7fdf4fa147bf", "references": ["Quilt Blocks"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-e011c66892814395b6b4966322e00f05", "references": ["Movies"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-ee3a612427ce4b3e8ad53846b7f92f60", "references": ["Santa"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-1a6c98adb60d4f75a33ebcb93dc699b2", "references": ["Star"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-f37731f9f93d41828059d40b06a2bf7a", "references": ["Cactus"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-780c1025baa84779b89f8398973836a0", "references": ["Lesbian"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-357fe6adb9df4e7cb5b37cdc0c6b35b6", "references": ["High Dive"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-33af98e0621744619dfeccab099b9fe6", "references": ["The Poll"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-7b20cf02872a4198a27e0d6d38af1c74", "references": ["Calm Words"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-7e5057f687fa414392572cb0e2e98cca", "references": ["Gaming"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-3fa30e233cec41e3add43af2990f0747", "references": ["Surfing Derick"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-ec40f33280ce49f5b06a340c95cc859e", "references": ["A rainy day at the fair"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-32ecccc297b741468bc6424fa2f8a1e0", "references": ["Gloria's Secret"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-c3dfcb361c7b40ceb1770c291b08e4c0", "references": ["New Chef"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-08afe6f6a6b74d03bbb480ebd037e20d", "references": ["Drunken Rage"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-dbac7bf821aa4c74bab0d208f19d35f3", "references": ["Allergies"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-effa60b3c06c4b94a254284cf54c5988", "references": ["New life"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-b7d6bb8df30f42c2bd0ceb7717d3e1c8", "references": ["The Swings"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-f4df9794437340e3a7fd830deff68c58", "references": ["Lost Cat"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-ffc09b10a3234c31bc8c50e25eb3a130", "references": ["The Dealership"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-8b1e0c70472645fa8f63053bc4dff57e", "references": ["Dishes"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-741b116a54844a5ba43c2f88515816ad", "references": ["The bear"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-62b0878922c04042a8ca97a91eb9a207", "references": ["Happy To Fly"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-7c092bafb1004f81acbb68e7b29b56cd", "references": ["Fantasy Football"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-42d5144471a24b90931c69193b60f065", "references": ["Go Anyway"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-57a777e9a2f246a38bd41928f4897fa6", "references": ["Election Choices"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-05d39c58aeee4547881005aacbc4a0dc", "references": ["Passing"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-ed7f269c3ec148df926b53bd62889b9c", "references": ["new video game system"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-edea468f0adb4775bfc5059085cac60f", "references": ["Burned"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-7ac2ea8a55bf4a3b8616b0c38f6508c5", "references": ["Ryan's poker game"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-137cbcb411ec4c5ea3ee9cdd706d31f2", "references": ["Hot stove"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-6b1f0e4763ab41e78fd7b59e74f51af7", "references": ["Old School Books"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-b574e4da578b4a119ec838247e730206", "references": ["Bite My Tongue"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-58857142d6f34cc59af154d3aff5ccd4", "references": ["Going to dee Friday the 13th at drive in"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-ac752aa32d4e4da18f9e829248cd687d", "references": ["Apple Sauce"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-f452f3642a9c41519c31a3846fd4c461", "references": ["pomspitz"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-19171819e7a34b17be1142088dc3a62b", "references": ["Doing a Backflip"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-913aee4ba78c46b28cf5c7c477e95dfc", "references": ["Bad Food"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-fe85e0ba5dca49c39ef1d9c9f16c3010", "references": ["New teeth!"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-d324034b2f94485593c721ff085f0ed5", "references": ["Vacation"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-0afa90093fe0488f8a529012280d4762", "references": ["Red Meat"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-4ec27e254e4d4147a8622da8645188e5", "references": ["A Day of Fun"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-2553312b813d4d5790940cabf879ffe5", "references": ["Small Gloves"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-56805aae22ba401d9defd2eabb003a00", "references": ["community service"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-7978bc5d6dd04b25a4b969a91bdf86bd", "references": ["Hunting"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-c1caed8dcd004d3cad4a0697f5f61cfd", "references": ["The Blue Light"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-21512d1f55ad4083beab058254ddb676", "references": ["Rocket Launch"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-8bb30c6c824649068fa48012bfe7c6ed", "references": ["Slow Internet Speed"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-cd6aee971d2b42a5b6f6bb3ea6d46929", "references": ["Thwork!"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-a43ea80e417d4c429836141fc4949679", "references": ["Starry sky"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-a939476aa6a346b48fef038b8a6641bd", "references": ["Boom Box"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-6194665e54f747a5bcf4b5f1d00d3020", "references": ["The Falling Closet"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-da844581fc544b6b9dfea8c40ce1d31b", "references": ["Deciding"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-3c3f3eaef107451486caf8df52b37f72", "references": ["Science Project"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-bdc1cb850651401a8ae3e43d02284c00", "references": ["The Shoes"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-fc04a8b7fa3b408d91729dc8954220de", "references": ["Change It"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-360afa51bb774ef4a7277e8d9c85df2e", "references": ["Soup"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-7bc928b3cd4a481f857e7fdb81632735", "references": ["Decorations"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-a5d5f2ed639541ae9e849f513da7e0ed", "references": ["My flashlight went dim."], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-c4e307821bc748cf82500cb42f3b0252", "references": ["Private jet ride"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-2345e72010d6446c99c6ed3a85e0315e", "references": ["Unused"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-4d799a4149ba4164a117a8a1321f82e0", "references": ["Stitches"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-e71b920980064a9f83285ecb2e545298", "references": ["Hot Dogs"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-ac7c0ca162c54c388d26ade2709161de", "references": ["House"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task219-97649ff5907c427da3405151f4dfad92", "references": ["Chuck's Trip"], "task_id": "task219_rocstories_title_answer_generation", "task_category": "Title Generation", "track": "default"}
{"id": "task190-a5ad3cd192e840f28b2de287229b23f9", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-05c3037a68bb4a1992ba711b50e474d8", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-96e1d8fd3942478a93f9a2c68c853ad8", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-370fe3cae74c4716827373cb1ff098f8", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-6eaafb90f9f740b982c70f32afe1a3e1", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-0a4087da20634e38a518b6419e8f9bf2", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-473180905d9b4b0a87e677df1843c38c", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-93dbb46c677b49a9985bdf8a63e9fc5c", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-953805c2d9024ec68a515a6ff5c88422", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-e68adff323a5472783b2195aefc22506", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-a50af88137fb4f2c80e47b464fde94be", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-37984da3b854409ebadaf34e0bfb5775", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-67a32bdbd2a243e0af3e7795efa0097b", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-4a4b59b051314b0fa76ddb45df014ff1", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-a78a6cc721f14436be5031426bb27f91", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-cb6e1ab695294b168fd1f31f847faa55", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-d8c5cce3f8924a2f946ca44bcc875d96", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-0135b24169db41599cf0114e52b47603", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-6809ac9300484fe1b619d711562e2799", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-4d4eb9169bf448549b8683f7e07559bd", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-29677cfc649142dfa195cd4afcf4067c", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-ffef5c37fd37460e8e4a60fd53758299", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-651d557b4f594c6c86ed3a8ae366fac8", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-94cc30ae10bf4a928a761baa5ac7cc60", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-2722ce9ffe214cb0b359d3306cf8a792", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-08b9f88495b042be9e73df8fdfb64600", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-9e998371d7884963a2a6c83ec271d18e", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-362552b8d33e48bf97d19a8aebf4be33", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-9c0335ea98554dacac2dd43ce799a52e", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-19d5aff139304581b53b5c065bb184fd", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-401f319d20134c70b591fb545e1d4504", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-17e50cf9e8794efba9479eb6bd1fbd0e", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-c2c9fc1c84a84941a41dbcd7b068852b", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-84bb9fba9cb849fb9a0209b137adfb9b", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-67fa69a4efd74c02b60775db4ded28f9", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-a37e9c0b87994447a7960560abcab26f", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-5296361fcb314f19933b1d6653174958", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-4242c6f1992b43d7845f4858a5e32db8", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-e63f1d62dcdb4e2783dba19777c008aa", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-5b1d1dd6efd2482dae124318af4a981f", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-3884c3f3eefc433298bbad1e7aeabc5b", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-b8595502185547fd9525ab7abc60b6fc", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-5ea4f8d200884735b7fd1b1495a00300", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-3197d39a439d4753bb3e1a7427b3c79f", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-7d059d03c2be4baa8a20fe73480984b7", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-748a66c58ad24032944422da8eb22161", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-6a487d0dd3ca44eaaca550587c252c31", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-0a29fa1189584e609d35b42c075cae05", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-27757adf7151493781c0a53c9e2e5a59", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-b8ba93d4960e430f89df6488fc18b40d", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-213105a8a3a04fd4bae412d23849d411", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-11945bf2f4774ad5a620419f6317431e", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-ecc58f60dae34e87bb768feecc823fa2", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-f9c7361532594ffea2feaa0787ea6ed8", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-f73da7b228a34395a34c7fed7fae98a4", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-419305c0cb03481fb80691c3c8b33296", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-15db72a10c484dad97f6bf06ab4d3280", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-5390a9e21f3947349114f38406b90252", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-4093dc132575426cb3986b297c867558", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-b13bfc76b27440e2a0e94b758d26c299", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-af92249823164197b3e6a38124a7d7a9", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-43658f912ab2463fa3185d33f1218296", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-edf34ec283554cedb728655219edd699", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-b3a01e3abf2f4f68892173d3b0d1a622", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-fa9a276bf4fb4c6abf40a723c5edac80", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-d65d59b50d28483588966408eea33c06", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-3ea03620dda943fbb5a33674c3bb8d5d", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-63257a21399a4cbdab1bf6d081ab2062", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-abd67d8af15a4aea81b943e3e055e2c1", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-60939abdb55e430a9612559330cb606b", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-43a91ab657d34d3c9d6924fabe0d5dfd", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-8f2b6586b7c0482d8759edc41b2c356e", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-fa21cb7587884873a8ed383053d526c1", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-a1a58f16fae54d93adebdfd6b7992278", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-581759f5559a4b9bbdc86dd881fb56e6", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-33d0e668e497470a98c68a8fe08bbac4", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-045b0ea239654e99b7c225b659d415d4", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-e8b8c85ff9a24c3a838230b9f9b0063c", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-8adf5cd4bb0a4593882efa856d323850", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-b30793af78d44561a1ffed54813ad581", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-5fe87ea82f0f4caf92faa53f1a515f8b", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-9ed78900ad5d43cf9ff3ae1fb89aa4f0", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-031b1fa2268246b5bbcf395b76ca7d84", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-ebc37bdd89774fc89d4b6eb837a6806b", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-766a3208847b40da95ad336735de9a57", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-64f3a0c2b9674623a227a756fd77701a", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-89faefabad934dd39a6026e9ed6faf98", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-a498024db206408385f8dbd10ee61d41", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-3dce48e41d44461d9539c8fd251d6921", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-43978298111a4f25afe810ce15fe929e", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-3d4e0e4af0b84ffaa72b6046f6a06d6c", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-cf9ed2fb8eb54afb98e5831c7dc628dd", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-73887e6d19754bf28c813d9472a3e58d", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-5f4d9aa1191443319ceec09bccaff22f", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-b3240038110c4b8fa39545d01cc25c9f", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-f9f0db3342be431b8e39ffaaaa341a38", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-6885875f10e74123a40bb72489eb3df3", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-1bd079ecf7274e21b70441baf2290fec", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-3b261a7890804718834fc0bd60ebecb9", "references": ["C"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task190-d2675363a17f4e1392a43eb904996ee8", "references": ["N"], "task_id": "task190_snli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-56ef0bb630d247e6815080598a04d02f", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-8a53afabd52c4c679b90b07198906a27", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-f942712c700b4e01ad6ad40f6f557f6e", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-a2a9fef5d479424fa1957ca4474a701a", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-8ddbfdba94384b35ad731ca81a39ef06", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-16f3cb62b82e4f15abf1e5ca455c99dc", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-c650772252dd43f28a6d47cc401f9e6a", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-8d602557336749baacbe64d3692ac299", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-e0c43064cf48400b8754aed3e8a48be6", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-5c25a64586c14adb8bdaaaabc9490a73", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-399cfda306bf4e8895e7942868d2728b", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-d0901d77012c4ba199eca650e30a6796", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-d8cd21ebc5aa40088388ae63f2a98352", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-3d9f7f2864844c41955236a13ab74b68", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-988cd702152b441a996574dc747dc58f", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-7728de8a1ca64ef7b0ef0a3262f7bb67", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-1f564ea3a8124b09a28ec1198ed2e483", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-e63dbbf764394240a22ff06a6741cf64", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-44323f2cda734a08833450b34a43100d", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-4a33049a535147acaed5f5deab979004", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-8f51c0160a1e432ebe33f9989783647d", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-acfa05af5c97427dbf7967cbd9e95952", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-85cb081a3e91401d8f7af92153fe438a", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-2bc1d9931aaa40a793554ca1d4007b65", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-80ab740ced074a238e78af0e4a6d57b3", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-bb7a479d1a6b43629b8160c6dfbdb687", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-804d05a90a4e418688b3ae169963acd3", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-6668e0db009d4960866b25278ab80e7c", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-f9b49f1496ce4980b6dea078c9c0338c", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-8bc1272cce1c4066975752aade8ac261", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-969818784a4f4545bf2f07ce85447e0c", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-907687e8c19a4cd9b3e09c88d1c41b77", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-4b75839b85b84b3bbb63afed756b7382", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-3b84cb2f8f734b88a3a07ec88388e913", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-5655e2e279b74ff1b9c6d6c5d01f65fd", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-cc7cebb5e2e44c19a3899d8615204dff", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-bf19d6fd6a8348499059611a1f1bd38b", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-7bfa4ac5c1584d8594d68601d7ad4ab6", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-71f7b37264594379b42852864e07b49e", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-455473e032404376b4312efe3396a811", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-82cdb6197cf74391af01353a28d8c3bc", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-9c71bdd4e60f42159d6a14f389e3301a", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-75fd59ee45ee4cc58261ecfb066c22cc", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-c3ddc482101443c7be8d01622ac9e7c0", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-0f61378dcac34a2289bbd5c3725b4f82", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-cead7c9c0bba48acb8d028bfe86a335c", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-9b44222f3d324366bc619877dbb35c06", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-23279481274847e0b75778b8f8c9c347", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-48b19d1213a049d9af1143264baf2a0f", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-19eec9b72c4342aaab9acd67a42660c0", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-e8fc9c64be3d410d8bed16e131ac3dd9", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-263ae37eefc14d28938bf1b8577e0cb5", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-7eb2675f1d69427f84e9030f854cd18b", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-1c1d629c39894070b241d6f9e4a4c080", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-b2974ddcfe344904a91e65a71f6e2185", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-707dac3b8160436ba787ef5e5ca1ae0d", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-3f20d65ec9904357944fd11d0e95c4f9", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-ec3474b0ca7b48759d7ecb0b359f21b1", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-1458c4861b2e45c9b68594805359437f", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-c6f877e446ea46b882044b2273d294ac", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-38ed987f6dcd4f66bc6b9f20fc291cbc", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-8a34e612714f4f9aa977849544b1d2c9", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-9c217e99a6e84ce59986099298d5653e", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-5369433242eb425eb8fc7821bd8f7ea0", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-c3f4cefe763b45608a16b5153002e5f6", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-d9c995fe17504d379f0921b8ad3c00a4", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-3517cd90cf904f65961c4b7999995c78", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-14dc603bc0dc4e949ac4b627fb0a8a85", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-4ae2fb798bd44f26a81465412b853ad0", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-fd7aaee0d9cf4d38b8b6f92661802f6a", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-d217542046cd49e4979fe5b72c2a6cc7", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-82765f7c63a34be39c11e4902400a7d9", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-93708dd5a8f74a878714cbaad4847ea9", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-8312abe3eb674b8091b8029e3c278d4b", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-34bd6e3211c94da88f7ca900c2a4dc33", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-d002f00a5297451caf7cfd2e8baf60ce", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-cebca0e13bf94da7b4d942f992ed267d", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-5bd6dc34d80d40e69c98203b46e8709a", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-887703c95b0b4907bd96c856072088a3", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-ad0dcee546724451a5f02a46558fc723", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-c7201c24883c42389b396a8885805a3e", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-ee80087984be4c748690656657a3f413", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-33aee7c96e004212abcfc2335591974f", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-74a1466bda0c40f6b3b36ac50208f98f", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-5043444411d04ec893970bb83fb8837b", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-ba9600806a4a49e59fc3e691a1836a58", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-91638fca55104577be643e92c0c1c3d8", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-f4e33111749740089dc5d21f348cd450", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-ae1728d34e2947e18d95d67a4a785340", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-070e8761c4c54d798341a38ae00c5c8e", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-d2a80383919c4effb6fbefb610cbc300", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-b6494c655a00439ea4bd4a5316bb711b", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-28de8f175ecc43789a3f3b50ca16b3ab", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-3c5ada14f8544d42a1e7cbb006e9a1aa", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-75f3f41030fe485396351668057bfaef", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-55a3b18a423a415fbf7afdec73f4f143", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-2945a607deba464fb7987fde4ac9b5b2", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-f8646d283f77417dbad3aa0763a039d8", "references": ["2"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-66599b030e864145a3b3a589d850fdd8", "references": ["3"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task200-08d1e7dc61bb4a079df4a57325902a06", "references": ["1"], "task_id": "task200_mnli_entailment_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1534-bf9b26cfb1624a09a1a4c70d6b2f9cb5", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-70bedbe40c1747ad8df129e4b9d1e354", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-8bbaecd73bd44f42b3af6426dc31bd83", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-8e44b6a0a4214b99bda2d48b5155da8c", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-11147ddf42cc42aea26511d0d2cc2cff", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-9bcbdb04e4e54eb8a9bd147e27e95392", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-0b3baafab7bb429b97c3084261f65117", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-f7a9228fa8f54041b5d65674b5ecc5e0", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-dc30dd075f734d0f96e3a03722f74668", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-f43469d0f506403b9526c12fcd367b47", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-b6a3eb859a0d4358bea5a78b83d09942", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-614c4c639a624e738ae48905ec95e866", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-0a6bda38a9cf4cce9dee5039f9036fc9", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-82b2fad1729042369ed6819bb9e22fa0", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-c55869afcb1b4a479ee479537edf6977", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-79f8f410593344f49f2d90ad2e29e859", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-de63cca1efad41ffa32a0ed7e55f7cfe", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-d2c8430bb3014dc0a6a2bb60ccca4f6d", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-450e0932e5bb463e917bed02aa16b5d0", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-2f8b1b110c384fde84bd1d0ec8dfd037", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-32b5e8747329472cba242e98eb61b2e7", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-8db6b97dafaa44de88dc3a1ea901d3ab", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-ef45ff91622245a9a692f9c8540029ab", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-22d45254bfc946c7b486c872533fee9f", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-1319d076f11d431c88a517af22f30ebe", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-e3afd4f4380944a1854d8cd6666d5763", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-ca68d4a952ff483f89922fa5b0dcc198", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-bb02e5c1a11248aea0b5613d3b4bdba2", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-97c32895298c4d319537e4eea62e3f81", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-000dafa346ab4c79b02192dbf66c622d", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-56129de4793c4693b7cf38049375233d", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-569515a3643c4535b4403f79a33adeb7", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-7095284ed02a40eda1ce3432b6cbfc0e", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-2fd51c0b7f574108a720c6900501de64", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-e7524d5238dd42f88a1b8e9e45be7243", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-a21ee6fc63f64506b6d6bcab23d5f945", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-3e9f9cb9387a4ddfbb6b701fc8967381", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-fd2bf3a767774d49ad3df1210557d473", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-29f34ca4ae6442e8956766397f77a219", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-cefd352b2fd7452489695fe2bdad79f1", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-6688772d3ad24f19b86f73d393cf4846", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-43968e76e4294149a79978d759949392", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-98c18e2a5aa84cbd80092054a8d85302", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-862fcc20654748a398aca06143c68d0a", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-959c9ae2cf9942378efe0e1e1a6e1615", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-dc534fa26979414f8d5fca871713b441", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-7dd8ae22f31d4f2d8362c2dee6d93a3e", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-3286740771fb455ca39de565b45cd939", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-2b5e0c957c404e07a3ad8fc005b08cb3", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-45311567aa514484a85d8316ea545796", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-178dd0f3c885466cb5afbd675ee1c3e3", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-bb665c7ba431449bb6bea480e89b9b4d", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-447520a97b43445b8d027a50db0b23ab", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-0ab942d076c04e3abe8fa1db70f9218d", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-e8e9b7a39f124ff3b50f0f937fcb90e9", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-379e51fa78e94e98807573ac3ebcdfeb", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-157d0bbbfacd43a5a3f3e330bc615b0f", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-55aaa49203a54e56a93b4d0f5c043543", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-68ffeff68d344c8b83fd876712720d80", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-90b607ae3b434041ae7df0ee813b336f", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-b9b2c6d8ea9247bb9d53494fa91ca442", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-4f4d478129dc4928ba46aabeedb664c2", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-856951ee5a58428ea2548ca60f09e13f", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-5e6a9391516a44c59eb6091be20a44e8", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-af8c2560333c4a08a6b5f74a18d3d3e7", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-abe5acd955484084992a55860d206fb7", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-cbf70bdc5e404a9395eaa10c178dd401", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-be90ab5e98f348bc9b3687aacac0515b", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-0083b92845094f34a772d77dfe41d1f4", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-f2981826e6c84c9896fa1cfc1c377055", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-5d0fb44904db4bfbb90f2157aa07242d", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-a4d08376c4a9476ab2e7be39d0632800", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-ec4da58f434f45fa898f3d5686cf278d", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-275b102703a34be88ce9ee4f150ecfcc", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-044b04b9935b48df88e26f1bd7eb1ed5", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-a281941b3ee94a90ad1bbb6f40020fb8", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-ec9eec2c9418489792f71840e7139a85", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-c5cd770b76784fcaa9e2c1c410ddf89e", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-1db5965a453c4ddaab09dbf841ea957e", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-747d53fd48fa4e79a10a0b1145bff4ad", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-595ead3844334588b2dc69fe5df726ee", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-23c09aeb62254454abc8dde226a26dbb", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-2f7e3c30a7554cf69ecb19a9f1f655fb", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-95cd388a202a482b88bba27a286888bd", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-534b6125367a4eb591739a98f1fa7175", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-5832dcbbed7e4eae961875c6fa86df4a", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-d1e2076b29854838a8d0c293198de7b7", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-7be2042a54d144b093deeb84858c274a", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-ae6705e69c7241b0a03e8dca2a1eb43d", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-8d0c3e05e8994af9b43ed141c576a8bc", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-ba6db5dfa6f045e9a6bc675f7034bbdb", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-60cf23612d05407d9f5495b6e6112cae", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-d7af13831f474093b8fe19115d7f13d9", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-adc8a28d21d240b9875be88211806368", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-e7a4694f17ac419fbc8535d0ed294dad", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-f2b6801bef5740cca20475268a3b22ed", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-8732bfa17fbf4a2eb35e607a4d0b3165", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-ad26e525f1b243c6a54acab642ae0a42", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-925f96b63f324883a0a335d2915fa63c", "references": ["0"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1534-0a08c62ba9c94d35b9a2b99e1c06d145", "references": ["1"], "task_id": "task1534_daily_dialog_question_classification", "task_category": "Dialogue Act Recognition", "track": "default"}
{"id": "task1540-191d8e8b688c45e4897c4993e2a6ef3f", "references": ["Decoy Bandits Dueling on a Poset"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-01f34d9ee6cd4c6b8baa29bd8fc1cd7e", "references": ["Multi-period Trading Prediction Markets with Connections to Machine Learning"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-5dfaaae27d804500abaae8a5321ccea8", "references": ["Deep Speaker Feature Learning for Text-independent Speaker Verification"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-3f2dda70e3e7434abc5de1102c3736f1", "references": ["Multi-Task Cross-Lingual Sequence Tagging from Scratch"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-be75d7112fd44d87958fe3d8aa34f558", "references": ["A Mathematical Trust Algebra for International Nation Relations Computation and Evaluation"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-d072b2dc576941288943867497088077", "references": ["On the analysis of set-based fuzzy quantified reasoning using classical syllogistics"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-949339e9a46f4bff8f41252d8997dcdc", "references": ["Accelerated Gradient Temporal Difference Learning"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-b0c5e478669d43e0b816d44f9a0ddf6c", "references": ["FWDA: a Fast Wishart Discriminant Analysis with its Application to Electronic Health Records Data Classification"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-d3456466c16647029187eb4068c5ef70", "references": ["Reinforcement Learning with Deep Energy-Based Policies"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-40f122b87c7e402098700f4c7d73f4a8", "references": ["Unsupervised Diverse Colorization via Generative Adversarial Networks"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-19b78eb552004c75879c5a2981b53d58", "references": ["LANDMARK-BASED CONSONANT VOICING DETECTION ON MULTILINGUAL CORPORA"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-cb71fdd1c45e445a89d3804dcea78b83", "references": ["Natural Language Comprehension with the EpiReader"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-63ec1b56dbb04fbc97d9ef4d4176d192", "references": ["Rethinking Skip-thought: A Neighborhood based Approach"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-e2ca987482dd43e7938c8e9bb86fa857", "references": ["Supervisor Synthesis of POMDP based on Automata Learning"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-d812fa8fc5dc4f1893997e3e8c63f81f", "references": ["ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-d60d067c1c4f46ba830fdcccc285672c", "references": ["Between Pure and Approximate Differential Privacy"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-8c934a14c1034bcc90fbfefdee44ac47", "references": ["MAPPING: AN EXPLORATORY STUDY"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-079c8b63d69046cba0e93bc77ab2625d", "references": ["Parallel Large-Scale Attribute Reduction on Cloud Systems"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-68e8a229a66e4b5892786e6ec8abdb7d", "references": ["Anytime Planning for Decentralized POMDPs using Expectation Maximization"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-bdc75fc77cfe40a8ab6a3baf79e412af", "references": ["Functional Distributional Semantics"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-d37c0a84a36146c48ce8fdb879700633", "references": ["Probabilistic Belief Change: Expansion, Conditioning and Constraining"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-add3e88a4a504e17956f0e8511c56a2d", "references": ["Video Description using Bidirectional Recurrent Neural Networks"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-cec10bf29ceb42a09ad00a1d7206690c", "references": ["Zero-Shot Event Detection by Multimodal Distributional Semantic Embedding of Videos"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-ee84939b383649edb0ea91b5f6ef42a0", "references": ["Oblivious Bounds on the Probability of Boolean Functions"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-dda9ddc2550c4f299b48d9042c8a2cea", "references": ["Robust Order Scheduling in the Fashion Industry: A Multi-Objective Optimization Approach"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-c4033d90e4c740b7a0e669147550f111", "references": ["Levels of Integration between Low-Level Reasoning and Task Planning"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-e02ffe11163d4ee0b6e87e65f0e3bc5a", "references": ["Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-f67492689bf54838824bb58eab8c6b4e", "references": ["Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-9569434b45f04920af90a346a072e114", "references": ["An ensemble-based online learning algorithm for streaming data"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-973a9c421b184a0b81a8bf0c9065a331", "references": ["Rehabilitation of Count-based Models for Word Vector Representations"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-d510f8f7d6ec49f19ef67f769dbc568f", "references": ["Implicitly Incorporating Morphological Information into Word Embedding"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-2479aee0207845029a47031141ee29fb", "references": ["On Stochastic Belief Revision and Update and their Combination"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-10c9071f442f48dfb69993492584097c", "references": ["Outlier Detection from Network Data with Subnetwork Interpretation"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-921d9509f4904984b1e10daaab677fde", "references": ["Lost Relatives of the Gumbel Trick"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-88f380c99a7242fb833b0b966b9c69e7", "references": ["One Representation per Word \u2014 Does it make Sense for Composition?"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-abcebdf6e6bb4559ab3ed74f45d85dae", "references": ["Answering Complex Questions Using Open Information Extraction"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-ba35066d206c457bb526b7383e8fd8d3", "references": ["Ancestral Causal Inference"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-844c9ebbeffc48059e5b53999223dbef", "references": ["Embedded Binarized Neural Networks"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-d856f0c5548f4323b0e1eeb0dcb94b53", "references": ["Matroids Hitting Sets and Unsupervised Dependency Grammar Induction"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-2b03e8d43fbf40a2821152c44deb1859", "references": ["Feature Selection via Sparse Approximation for Face Recognition"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-7c99258a91cc49f0b8487ecf9d013ab2", "references": ["Cause Identification from Aviation Safety Incident Reports via Weakly Supervised Semantic Lexicon Construction"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-62ffdc104a164f2cb0966d7a2d99efc0", "references": ["Dropout Training of Matrix Factorization and Autoencoder for Link Prediction in Sparse Graphs"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-34abd95b88b445fc9e23ea2f9778988f", "references": ["A CONTEXT-AWARE ATTENTION NETWORK FOR INTERACTIVE QUESTION ANSWERING"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-bd5de377b5ef404faf3704e08cb98acc", "references": ["Solving DCOPs with Distributed Large Neighborhood Search"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-464cb6c27f7040e5ada67176b17fd7f4", "references": ["Deep Speaker: an End-to-End Neural Speaker Embedding System"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-0c737af2fe464dfb9f43baf2d971b906", "references": ["Does Learning Imply a Decrease in the Entropy of Behavior?"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-18633a974e0743ed909d0ae9d54d92ca", "references": ["A Generalized Kernel Approach to Structured Output Learning"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-c00e6a074c234485892fb3dd2c62fb7b", "references": ["Ontologies for the Integration of Air Quality Models and 3D City Models"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-818d6dca97b64b32b33e3d0d6e002985", "references": ["Semi-supervised Learning with Regularized Laplacian"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-d6b9052a731b40c7ae878109d93b63dc", "references": ["Improving Slot Filling Performance with Attentive Neural Networks on Dependency Structures"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-8b56d0a230964ae9a29a4913fcfc0723", "references": ["Dependency Parsing as Head Selection"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-6c51b494ad164c9c872cba92a43fc445", "references": ["Video Pixel Networks"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-7ea5a452459d4758a18d0622db9b2d0f", "references": ["Attention-Sensitive Alerting"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-239ad8397412402e8a9fbafb2e2c7720", "references": ["A Modality-dependent Cross-media Retrieval"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-af2951272f23498ca304c2cb6ddf12ea", "references": ["Transition-Based Dependency Parsing with Stack Long Short-Term Memory"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-a8f1e045fa7e47d48dd2275182833a99", "references": ["Discrete Elastic Inner Vector Spaces with Application to Time Series and Sequence Mining"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-dce2bcd7beed4379af01a8d2ea92aeb3", "references": ["Learning Lexical Entries for Robotic Commands using Crowdsourcing"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-337548099326456f8f19045e9c2e5e35", "references": ["Evaluation of YTEX and MetaMap for clinical concept recognition"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-0e5785abbbce450086c1a19dac7627e9", "references": ["The Power of Asymmetry in Binary Hashing"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-9a99451c17d54622a5f058d794e590ee", "references": ["Batch Reinforcement Learning on the Industrial Benchmark: First Experiences"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-001094e72c21467c9854e5d83b7cb8a9", "references": ["Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-1fdb059df3b74d2e91d84910fc9fac98", "references": ["Poseidon: A System Architecture for Efficient GPU-based Deep Learning on Multiple Machines"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-b67935c1df5b483da68cf226bac9d656", "references": ["Generating Sequences With Recurrent Neural Networks"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-002a1bf58151426eb9a8e6e630f69d75", "references": ["The Loss Surface of Multilayer Networks"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-619696e14ce54c26a6aba2b8ad329ac1", "references": ["HIGHWAY LONG SHORT-TERM MEMORY RNNS FOR DISTANT SPEECH RECOGNITION"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-86fe51e8f61240519b6c55867aa6fce2", "references": ["A Note on Improved Loss Bounds for Multiple Kernel Learning"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-52ad377eccfd4615935ac2fef102e8ff", "references": ["Skip Connections as Effective Symmetry-Breaking"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-397bdd63ae954ac681acd5fc0c59442e", "references": ["Incremental Sampling-based Motion Planners Using Policy Iteration Methods"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-65085d065ea84a489bc76a7659a78f5d", "references": ["Topic Browsing for Research Papers with Hierarchical Latent Tree Analysis"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-93fb814c935040cb99340cf9e2da52f5", "references": ["WIKIREADING: A Novel Large-scale Language Understanding Task over Wikipedia"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-ab9c809b82ad4b009ac9de83d16585f2", "references": ["Scribbler: Controlling Deep Image Synthesis with Sketch and Color"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-8506f887f27f435b816703c7697f1745", "references": ["CAS-CNN: A Deep Convolutional Neural Network for Image Compression Artifact Suppression"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-707d120900bf4b528dfac0e241d70023", "references": ["A Functional View of Strong Negation in Answer Set Programming"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-06f7b0b89e454da1b32f4f5afbf91451", "references": ["Joint Inference of Multiple Label Types in Large Networks"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-775dd81d3f3249d1a91caf1ec106bbb0", "references": ["Un re\u0301sumeur a\u0300 base de graphes, inde\u0301pe\u0301ndant de la langue"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-503a2cbf26c442d1ac89d5805c206630", "references": ["Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-1c096accd05842719c5a6820fe250a4f", "references": ["Risk and Regret of Hierarchical Bayesian Learners"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-5a3867e1930149e1a3073e73d104936d", "references": ["MULTIMODAL NEURAL MACHINE TRANSLATION"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-01ad07e55c694cf8b07544e099b8f556", "references": ["Enhancing Support for Knowledge Works: A relatively unexplored vista of computing research"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-9e192a9d66d74d27a818c8e535a4abe0", "references": ["Maximum-Likelihood Augmented Discrete Generative Adversarial Networks"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-5dabf77d143949c89740249eda0ba663", "references": ["Streaming Word Embeddings with the Space-Saving Algorithm"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-118cdaaa22884c5f9d778516d4396248", "references": ["A Bayesian Model for Generative Transition-based Dependency Parsing"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-70e96eff7f67406292efb506f8d2a870", "references": ["On the Detection of Mixture Distributions with applications to the Most Biased Coin Problem"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-4da0f039d55f4c418565628c678405d2", "references": ["Ethical Artificial Intelligence - An Open Question"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-61cebf6c0b754d72806d3ca29e994fb0", "references": ["A CONVNET FOR NON-MAXIMUM SUPPRESSION"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-0c385306784948f5ae57d6abc81feec4", "references": ["ESmodels: An Epistemic Specification Solver"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-6130dad97f7a429181f65055befa7c8c", "references": ["DEFEXT: A Semi Supervised Definition Extraction Tool"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-4ded55397a204932b8bbae216bfd8cf4", "references": ["Pixels to Graphs by Associative Embedding"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-01c881ac4a7b429d90c2b67f034c117b", "references": ["Employing traditional machine learning algorithms for big data streams analysis: the case of object trajectory prediction"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-fb2056a239514997a462550215f4e0c7", "references": ["Shallow Parsing Pipeline for Hindi-English Code-Mixed Social Media Text"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-774a099de40d4e57a845a84ed14a5859", "references": ["Clustering-based Source-aware Assessment of True Robustness for Learning Models"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-786c909d74f148abab2b9f613ded821d", "references": ["A Deep Compositional Framework for Human-like Language Acquisition in Virtual Environment"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-2f75d54056ee401fa17d15ef873163da", "references": ["Constraint Programming for Planning Test Campaigns of Communications Satellites"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-38b52bd3318242159db68b841413a07d", "references": ["Building a Stochastic Dynamic Model of Application Use"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-9cada47c194e4129bf154c28885b8e2d", "references": ["Empirical Analysis of Predictive Algorithms for Collaborative Filtering"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-bd4fae7320eb447582d379c33f404a86", "references": ["Online Collaborative Filtering on Graphs"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-6b24b25669a54ee5a550bcc3044c09ed", "references": ["Identifying Consistent Statements about Numerical Data with Dispersion-Corrected Subgroup Discovery"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-4e3609fe661e4228a4d59f95f90af888", "references": ["Polynomial Time Efficient Construction Heuristics for Vertex Separation Minimization Problem"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-72d0a52dac9544bc992c110889df2ba6", "references": ["DIET NETWORKS: THIN PARAMETERS FOR FAT GENOMICS"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task1540-18597cbdd7ba4d61bde5f470279ead81", "references": ["Learning Disjunctions of Predicates"], "task_id": "task1540_parsed_pdfs_summarization", "task_category": "Title Generation", "track": "default"}
{"id": "task442-f66c5d8f3c79422dac9a145dd1289347", "references": ["what years did cale yarborough win winston cup champs?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-fcf42d95931b4a96918093c951db3f95", "references": ["who is the voice of carl in phineas and ferb?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-822588369df1429aa53a2c78fb4445ae", "references": ["who is james potter the harry potter father?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-ec22781fd9124fb8b1cbb09d5ff1e606", "references": ["what was rick riordan's first book written?", "what was the first book rick riordan wrote?", "what is rick riordans first kids book?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-892f1fedf8f44645a526e13585039eab", "references": ["when was the movie remember the titans filmed?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-f97856e27381481eb18cc96c47f95d04", "references": ["what largest city in africa is on the banks of the nile river?", "largest city on the nile river?", "largest city by the nile river?", "largest city in africa built on the nile river?", "what is the largest city in africa that is on the banks of the nile river?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-42d93f93cbd248e583e23edb35dc6589", "references": ["what religion is of andy hurley?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-818845af72c64a6fbc8cc4e1f2a617a1", "references": ["on what continent is tobago and trinidad located?", "where is trinidad and tobago located on?", "what continent is trinidad and tobago located at?", "where is trinidad and tobago?", "trinidad and tobago is located in which continent?", "where in the caribbean is trinidad and tobago located?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-e11891470e60481db488087f802fcdd5", "references": ["when did 17th amendment?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-c6fdc2df1e304e6188f56b5d24a4eeb6", "references": ["what is the city with the biggest population in europe?", "what was the largest europe city by population?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-c7a15f43043a4d2a8935343dc313980d", "references": ["who won the world series in 1994?", "who was the winner of the world series in 1994?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-65445feae6a24677806dbbbefe1cc83b", "references": ["who is king charles i kind of england married to?", "who was king charles 1 married to and what was her name?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-fd38f173c3af4547bccdbdc05b6ec862", "references": ["what are some languages spoken in spain besides spanish?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-1aa5685af7244164a3bed9edc9d4dfd7", "references": ["what is the layer with the coolest temperature?", "which earth layer is the coolest in temperature?", "which layer of the earth has the lowest temperature?", "which layer of the earth is the coolest in temperature?", "which layer has the coolest temperatures?", "which layer of the earth has the coolest temperature?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-a9bdaaf659b844b1bb955f391ff60d73", "references": ["when does portugal celebrate its independence day?", "what day is portugals independence day?", "when did portugal acquire its independence?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-496bff5ba8e2414a8eefcefa93a26d5f", "references": ["what movie starred wesley snipes as an nyc transit cop?", "which movie starred woody harrelson and wesley snipes as nyc transit cops?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-49ca971a2b964785a121866db6b113e0", "references": ["what was bob dylans first album?", "what was bob dylan's first album called?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-b3ae1895664448e7819d7eec1637dbae", "references": ["what range separates china from india?", "which one mountain range separates china from india?", "what mountain range lie between china and india?", "what is the range of mountains that lies between china and india?", "what mountain range between india and china?", "what mountain system separates china from india?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-1e4f2fa682914086a22104386dfac2f7", "references": ["what was nicki minaj 1st album?", "what is nicki minaj's first album?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-aa85b4b936a645b3af8f6eb3b4f789e6", "references": ["which is the world largest city area wise?", "area wise largest city of the world?", "largest city area wise in the world?", "which is the largest city in the world in area?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-5e8921a443ac48cb9b0d1478e7b4d18c", "references": ["when did harry s truman get married to bess wallace?", "when did harry truman and bess truman get married?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-4bd839b4a05e400ebdb885561ae7d1d2", "references": ["what is the highest point in puerto rico?", "highest point in puerto rico?", "what is the tallest point in puerto rico?", "what is puerto rico's highest point?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-f91e826bf2bf4589867cb2507c20db72", "references": ["when was larry bird hired as coach of indiana pacers?", "when did larry bird coach the indiana pacers?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-b7f223df417b4385bba0a25029d3d8ea", "references": ["what is marco polos death date?", "what day was marco polo's death?", "marco polo death?", "when was marco polo date of death?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-b2d4d201cb1345f08292c9e013caee75", "references": ["what is the population of erie county ny?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-dae9065f1ee04ad2b1795b28bf57220d", "references": ["when was the very first super bowl?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-2aba32792d854e308bd050aecee21237", "references": ["what is the population of the us today?", "what is the population of the usa today?", "what is the population of the us?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-508aedb0d4394d85a5870ebd91e7156f", "references": ["what is tyler perry mom name?", "who is tyler perry's mother?", "who is the mother of tyler perry?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-12b9bff345a642dea8ff943ec4bd87ec", "references": ["where did george washington live before his presidency?", "where did george washington live before he was a president?", "where did washington live before he was elected president?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-56d9b39745b945a8a67a659fdb63c1bb", "references": ["when was christopher wallace born?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-a708273f0fe44fbea202600528f92719", "references": ["who is first president of zimbabwe?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-c4c577262ec9492bb75715f9ed405016", "references": ["who were the two influences in einstein's life?", "influences of einstein?", "albert einstein early influences?", "what is albert einstein's two influences?", "albert einstein influences?", "who were the influences of albert einstein's?", "what was albert einstein influenced by?", "two people who influenced albert einstein?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-ad36df23e43845b4858a64f9db2799d0", "references": ["which college did cher lloyd go to?", "what school does cher lloyd attend?", "where does cher lloyd go to school?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-4ef5f7b986ab4d44bbe2eaf0bbd12f4c", "references": ["what was judy garlands first husband?", "who was the first husband of judy garland?", "who was judy garlands first married to?", "who was judy garland's first man?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-ed30f60e0f3a40cdb8999f9de0c77683", "references": ["what is mahummad ali birth name?", "what was the first name given to mohammad ali jinnah by his parents?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-18c6a31a27bd470787ee8f352839c41d", "references": ["when was king richard 3rd was the king?", "when did richard 3rd become king?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-67f90df4cf6e49fe91076e1aadf13a6b", "references": ["what is the name of lindsay lohan's little sister?", "who is lindsay lohan's sister?", "what is lohan sister name?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-be056447685140a1abc7ded5ea9775b0", "references": ["what are the rivers in trinidad?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-ccc90799179241929005c6a7bb4224fe", "references": ["which countries are touched by both atlantic and pacific oceans?", "what countries have coasts on both the atlantic and pacific oceans?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-c7f4f53352e0419098931b0c527f7b10", "references": ["what is kate winslet husband name?", "who is the husband  of kate winslets?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-252d3c29a0ea40f78d7172b88174f71d", "references": ["what are washington's largest cities?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-76d1e09e3ef448b291398360ef7e2ba1", "references": ["what film did patrick swayze star in with keanu reeves?", "what movie did patrick swayze act in with keanu reeves?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-40e9fba1ffe746589dc0ac8a62316d53", "references": ["who was president truman's successor?", "who was the successor of president harry truman?", "who was president harry truemns successor?", "who was harry truman's successor?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-89dea81db93a45d19a6311f6661be75c", "references": ["what where the names president children?", "who was ricahard m nixon children names?", "who are nixons children?", "names of nixons children?", "what was the names of president nixons children?", "what is president nixons childrens names?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-075f85d4e414423499cb25b274546456", "references": ["what is one of the highest volcanoes in central america?", "one of the highest volcanos in central america?", "what is one of the highest volcanoes in central america?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-6fbe7a6509db4a888fc753f590fd9392", "references": ["what year did the movie billy jack premiere?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-830386b17611498e810c4320d47dae7c", "references": ["who the one first to be thomas edison wife?", "who was thomas edisons first wife?", "who was the first wife of thomas edison?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-f1eefaf79bd240dbbcb228e4f72e3840", "references": ["when did abraham lincoln get inaugurated as president of the us?", "when was lincoln president of the us?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-33f5115180b04c38aeeca0393abf2e39", "references": ["when did sherily temples husband die?", "which date did shirley temple husband die?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-fed5e3e5e10048298e18288377c9827b", "references": ["world war 2 battles in england?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-6478eae566314cadb1eb134cce3661de", "references": ["what month did hernando de soto die?", "when did hernando de soto die?", "what is hernando de soto date of death?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-49d8e65e78074ae0b6475ad06ca0291c", "references": ["what is magic johnsons dads name?", "who is the father of magic johnson?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-a7366369661a44d4bcfb32760b8a4164", "references": ["what is the largest dessert and mountain range in africa?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-e60adbd7d1a441f9b19ad035c1dfb398", "references": ["which president held office for the shortest period of time?", "which person in the us held office for the shortest amount of time?", "which us president ruled for the shortest time?", "what president of the united states held office for the shortest period?", "what us president stayed in office for the shortest amount of time?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-4d28f9297d2845bc80105a1a08ac5328", "references": ["which was the first american president who belongs to the republican party?", "which president of the us was first to belong to the republican party?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-0412fc3012be4680987fad12dffbb816", "references": ["when was death valley founded?", "when was it found national death valley park?", "when death valley was found?", "when was death valley founded in?", "when was death valley national park found?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-9f924f6d988846789f967c4723c81646", "references": ["which brother of michael jackson died?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-9724ec27c0764c31a2e6f1383eff5740", "references": ["in which us city was the first skyscraper built in 1885?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-a777bf1cd5874cc18603ee3f1559bf21", "references": ["what was the age of john steinbeck when he died?", "what age was john steinbeck when he died?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-e35891cde5e549aa8ab15e8e04e32072", "references": ["which is the first film for vijay?", "what is the 1st movie of vijay?", "what is vijay 1st movie?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-75804b8de01c48a2acba8cf60b10f0cf", "references": ["what sea borders southern sweden and denmark?", "which sea lies beside sweeden and denmark?", "what sea separates denmark and sweden?", "what sea is between denmark and sweden?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-ae07ccdba361449eb1385c6d8fe1c731", "references": ["who was the president in poland may 2009?", "president of poland in may 2009?", "who is the president of poland in 2009?", "name the polish president in 2009?", "who was the polish president in 2009?", "who is the 2009 president in poland?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-936cbe93c0a3404380a466e754f91372", "references": ["first female olympic athlete on wheaties box?", "who was the first female athlete to be on the wheaties box?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-ffb126e46c6c4672a2371a97d347382e", "references": ["what did george bush do for work before he was president?", "what did george w bush do before he was president?", "what was george w. bush before he was president?", "what was george bush before he was president?", "what was george w bush's profession before he was president?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-04a593adaf06495ea80e69ba83bb6512", "references": ["when was sidney nolan die?", "what year did sidney nolan die?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-1d0cdc2ba1d340208f3d0371e976a2c4", "references": ["who are the mowry twins parents?", "who is tia and tamara mowry parents?", "what are tia mowrys parents names?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-c5a15fc0312c4cbabb118dba9816eb60", "references": ["when was nicki minaj's frist album?", "when did nicki minaj make her first album?", "when did nicki minaj bring her first album out?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-e8609ec6cfbc45039dec036888af20bb", "references": ["which movies have won the best actor and best actress?", "which movie won best actor and best actress awards?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-9598cfd1200147a6aa772c51abc2f5e5", "references": ["what ferdinand magellan famous job?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-31adb40c0d634c76b6f98953bb537323", "references": ["country that borders honduras on the west?", "country that borders honduras to the west?", "what is the country that borders honduras on the west?", "country that touches honduras on the west?", "country that borders honduras on the west side?", "what is the country that borders the honduras on the west?", "country borders honduras on the west?", "what is the country that touches honduras to the west?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-e44437c4a8bc44ce92a1e3137026d0c5", "references": ["what are 4 countries germany occupied during world war 2?", "which countries which were occupied by germany during war world ii?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-bbfc99b0cc324f65a8ac7bd22fd8bc5f", "references": ["how many times was john steinbeck married?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-99c712e56c0a4396b09d6a8bec420018", "references": ["what novel launched charles dickens career?", "what was the first novel that charles dickens write?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-1330bf147a8c4c9ebe74ea0dfb51fd35", "references": ["bands steve winwood was in?", "name all the rock bands steve winwood was in?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-12d72475a0d1469cb31243b7a18487bc", "references": ["when was franklin roosevelt president of the us?", "when did franklin d roosevelt rule?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-3eccb033bffe41c09f4515caffd8f9a8", "references": ["what kind of music does billy joel write?", "what type of music does billy joel sing?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-ce9588a30adf473f8c860ec23333c77d", "references": ["what place was jackie robinson born?", "where was jackie robinson birth place?", "jackie robinson birth place?", "where was jackie robinson's place of birth?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-0567e0e235444cd2a154db7548017b0b", "references": ["what film did penelope cruz win an oscar for?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-050bd4d646a44db492df51a080b1f6e1", "references": ["which film did george clooney and brad pitt star in?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-a2a868316c16461d87d0e10ae0e0510e", "references": ["what are the contributions of marie curie and pierre curie?", "what did marie curie and pierre curie give?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-2c63e32bb3484371accc454fd7f2a10e", "references": ["who were hathors kid?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-9da7fdc561aa45168d442a6d0549e8bd", "references": ["who was lincons opponent in the 1864 election?", "who was lincolns democrat opponent in the election of 1864?", "lincolns election opponent in the election of 1864?", "who was lincoln's major opponent in the 1864?", "who was lincons opponent in the 1864 elections?", "who was the opponent of lincoln in the election of 1864?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-e347f086ddaa4581b9d959a9f05e4b07", "references": ["mason musso have a brother?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-8e56695def484ee1af4d40aa17c518ee", "references": ["what is the capital of the state that shares a border with california nevada and the coast line with the pacific?", "what is the capital of the state that borders california and nevada and the coast line on the pacific?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-0cdcbd072239472195ad738d57a43954", "references": ["what was michael jackson 2nd album called?", "what is the second album of michael jackson?", "what was michael jackson's 2nd album?", "what was michael jackson's second album?", "what was michael jackson second solo album as an adult?", "the second album of michael jackson was?", "michael jacksons second album?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-6b353d235345490b8aeea9d00244eedf", "references": ["when did thomas edisons wife mina miller die?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-99b7fc58bb7a481b822e99cf0c62f708", "references": ["what ocean borders keyna to the southeast?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-92448ee3744642af95ba407f24244f06", "references": ["who plays the role of marty on life with derek?", "who acted as marty from life with derek?", "which actor played the role of marty in life with derek?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-01607e5e49524d3b9342f2f615b0e2e7", "references": ["which is the highest mountain in alps?", "what is the highest mountain in the alps?", "this is the highest mountain in the alps?", "highest mountain in the alps?", "what is the tallest mountain in the alps?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-647fe379d09447ac941c9e3ef995a48c", "references": ["when did rihanna start sing?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-ded6d0cd12a44e79bb53fcb41ec82c8b", "references": ["who plays albus dumbledore in harry potter and the half blood prince?", "who plays dumbledore in harry potter and the half - blood prince?", "who is the actor who plays albus dumbledore?", "who plays albus dumbledore in harry potter and the half blood prince?", "who plays dumbledore in harry potter the half blood prince?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-3b689bed9dc647999860527fd404471c", "references": ["what was the first piece mozart wrote?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-e4ea1a77384840a1b617048888862dd5", "references": ["when was president lincoln voted president?", "when was lincoln president?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-f549ecb9fb044e81aae86917845544e1", "references": ["what three countries in the south america are in both the northern and southern hemisphere?", "what three countries in south america are both in northern and southern hemispheres?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-3480f7391b7c4b00bdaa896560971c88", "references": ["what is the highest point of the australia mainland?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-0c46f0575f804b8c894268cf8246498c", "references": ["which civilization was the first tp build an irrigation system of ditches?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-b4d7c98a20f240c4afd47eeb60a30f92", "references": ["what are the 3 biggest cities in tennessee?", "what are the names of the three largest cities of tennessee?", "what are the three largest cities in tennessee called?", "the three biggest cities in tennessee?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-bcabce46e11049f8925ae20493ff4dd0", "references": ["what was martin luther king jr fathers' job?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-b3cd48ae197b48a39212e888d5c31815", "references": ["what time zone is hawaii called?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task442-9b6adcb59b2743fbaf28d28b7dc160c3", "references": ["where was ludwig van beethoven born?", "where in germany was beethoven born?", "in which german city was the composer ludwig beethoven born?", "where was beethoven born?", "which german city is the place of birth of the composer ludwig van beethoven?"], "task_id": "task442_com_qa_paraphrase_question_generation", "task_category": "Question Rewriting", "track": "default"}
{"id": "task392-7ff46271b58a49be8fdd08560a0173fc", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-0011cbd0b9cb4036a94ba6c588e8ea17", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-ac7bcdf2b6f14bf1b02beea771deeac0", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-0b96d047e96e46189f8b3d8fcac5c1b9", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-8f7bbc88555a4042b72e8871f98761bc", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-2543e7552ad24a7ba8388e4ed8475ec6", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-9cad92ac5b374960bbb616a7544d6775", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-e28bfd36ece34837ada277bfca9f87d2", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-ecf28e8a98644728af1d74471040518b", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-2d36cd3e89314b2eb0b116cdd8d90d9c", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-6b5986ab3b794ffea9df29534e2055d3", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-0e120a0bad144ac0a709d8a326600756", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-500df7ad39fe46469aa6b99a71ffec47", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-498f2341767e49e69eed78d681054591", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-86caafb6fd7946c8ba46c3f7b13dd108", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-7b2db01200034e119c9a02791751c800", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-48e0c21d0cd54365b7467d14f1926d43", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-9b5d84ddde2b4072a22d9ff34025e2a7", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-865bf66c38e445e2a42fb481ac564669", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-6c2f866ff99849908db92e44cc1a8d44", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-1fc34ac5dc0e4805bda21f28945d4141", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-bc9eda364c92499e8802a9814fd159e5", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-d0b37ee196524acf9f6e557777f432e7", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-b21115818ff54aed958b3cdd4c6186b8", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-d9daf402f4dd45049c1ce16dbf8f4eee", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-ed026dce63524e9fbdc1be4c866aad65", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-b69d58b3c49a42f2accf76b2b7156a2d", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-b9af44a91a7849e3bee02acecd6d0e5d", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-b1f167358c6244a8b01896d14aba6538", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-6b1750072cf2403cba5711a44f204be0", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-72702be6ca8043e3bda0e54c8e61e208", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-2f2ea5643eb1419bb32c8c1394cd85e0", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-cfb5b2c7f4cc4b27a6bb535d506e63e4", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-dea4b9e4f62242ba8539ddcf9eddf13b", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-3e90874e92fc4b51b0e05b65cd7bc9a4", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-08b4f78aca3f47db9027d56e772b5d01", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-0616e0b04e9f4fce88e0680270f62c6b", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-2e78079e27a84ea6805e17c7c20927e3", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-a8298f66e72f471ea32a9c6c5c75766e", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-fe43ef1c835e4ae0b59fa5f56ab17b26", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-2d18217b63814ec18ae7ca90aa64a297", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-b37bdd89f2f84293a6479416a7df4368", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-bfc33a675fd847c0abb1b539dcf6e9a0", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-42c55f2f8b9f4ac8acc61f1e96d9b1d1", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-53f1b241bae34bd781f19825a0aae525", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-8e5db237b7164206ab2a4b25c48ab682", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-4152885863ea466db44d4b08dddd949a", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-cdaa226219f5425e9b3ac0be929bc4a5", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-6833af42691749dabcbda2a9ee687a3e", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-be611cd7385d4b42a6c26c367e256b1c", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-31ac77be6fce47099eb09d5cc0ff60d6", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-19ebb8985d5545d9875d08e2a5376bb4", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-9fac26c731cf4a5fb374b5ebf416b18b", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-22f77f450c8e4d1997c66480e4efee0e", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-102c7dffa08f492aa50328f123071525", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-b664cbbb61bf4f998e9294eb0073bf8f", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-7320146001074e30b3c3e20a711aeb0c", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-7c12f9c277f94e5883c5debc19e721b8", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-b8104d0b04c5405b99789aea0408924a", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-45e2a92329e944198f6089bb27f67329", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-f5877013ceed4594839d987b0bb011d2", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-f2aee9ef2af546f9ba340d489c4d3f80", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-9e2481a37fa546a4a209b42fd517da64", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-35593edbdbf64787bf50d96981ec84ee", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-29477fd45b7740c99ae33adecf68efbe", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-b28e7923c33d4796a81348e41ec8985d", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-b6f818204e6c4cfd9504d490cae66859", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-0d6c89ab36ef4acabe943f36c9f6dc2e", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-d6b4a8ba99c844cf990642688b557e70", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-e5c5970295ff43e4aae67c1d1471ff4c", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-4908e9410b8b4ea39a76a0772cebe96a", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-c0fb81794d1f45d39d65d28b96b3511d", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-9ca3b467f36d4372b42ee247e2e03725", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-075132588fcc4ed997f24108243324d5", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-4ef5e8e932a3417c9d00c428e702edfc", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-baf0a5d590e742959e9ce22b7016a182", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-e7b0dc4bbb3f49aaa05d5ff8bfab798f", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-08145e935b7b4cff94d90ee4279a44a9", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-e97305ae17d247dc95dc6d6fd3bf161d", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-3321f6bd3baa41be8167f40ba7c3e8c5", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-44ce1c0dc3314c12bfb02f2f171dc0f4", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-3ee8fe1cc5f9491c9a0f20e08ce0cd2b", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-0d0450ba72a2431d924d7381d3c34e8c", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-6db6a9d2ee4a44b791dc570cbdeb5797", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-abb19a24441d4f898614b588ef77a8d9", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-aedab3b42211443a8ffd9429ccc3460d", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-94e3a0edee0c454991e1ca03a6f04a1a", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-ef0e8464c3294eef840504e5e4300c48", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-364430494b354f7cb14cc8e2903cb694", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-77796f8cc5394aeab00954838967a53e", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-d63c60a8b82b44298a30f84fe315ee84", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-f680473a13214748b6d7004f93618de7", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-65b4d229e53a4b8f89606f9969b4b548", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-5b7467cebd35487189b722827d82248c", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-97a74607112d4a56a25f034c3ea1ae40", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-d2206ad45f6e41f1b2d682838a182daa", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-03c0ea2d5cf842379f7ea9c858302484", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-40470fce597d4539b8c5ef31247350cf", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-a540ed6e953740359148e9877acc29ad", "references": ["plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task392-38df67dc23854e78aa7c70e0e605d06b", "references": ["not plausible"], "task_id": "task392_inverse_causal_relationship", "task_category": "Cause Effect Classification", "track": "default"}
{"id": "task1562-f240397c06214edf9fcb021ad191af20", "references": ["Which animals that have four legs can be seen in this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-e690a12da5274bcf80f2ffab7dcb4bce", "references": ["Are there bears found at this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-d563b14879c04a99a43cb00250d3c7b1", "references": ["What is the lower end of the amount of puppies this dog breed typically births on average?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-b7544330dda54e5b92107dfc31676ea9", "references": ["What is the maximum average lifespan of this dog breed?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-f06ded43ad194e77937d929f0d4d88b3", "references": ["Can anyone go to this national park anytime during the year?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-7a96fbb72ab3422589a138f843c448f5", "references": ["Will this national park close during any season?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-01ad02d28e2a40fa9ab1def414c20604", "references": ["Can we make campfires in this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-670e1b2890ae4442aac7029c64d8891a", "references": ["Are mountain bikes allowed at this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-23535379dfa04198861743fc0d5ce117", "references": ["Are people allowed to kayak in a lake in this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-b2524ead8c874162a712e705760ba812", "references": ["Are there ever black or brown spots on the coat of this dog breed?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-b64850592b5e450c9c9f0712c512e725", "references": ["What color of fur is often found on this dog breed?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-ecdedb6da77d4c20be56a093d593f60d", "references": ["Is there more than an inch of fur on this dog breed typically?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-aceb8bbcea1c4c38b8d394781f2a0ec0", "references": ["What length tails should this dog breed have after having them docked?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-cda0ec474de44ed5b4c541705aef7e7c", "references": ["Does this national park have two or more caves?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-7347cc884a8d40b098d1b1b96824cada", "references": ["Are there always spots on this dog breed?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-30734117b07a4e329a78d2f9633200a4", "references": ["Are there any fun activities this president enjoys doing in their spare time?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-9d2b7a064ae34e45829779236ff12a28", "references": ["Does this national park have natural lakes?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-b60ab6d603ab4e02b255ff293f9052cd", "references": ["Are there at least three restaurants at this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-8c623f14b3aa4f308896bd4debe62a2e", "references": ["Are there boats for rent at this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-eee58af0bbf64120b0ae91bea41b6477", "references": ["Do brown bears live in this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-0f35cf696cc74778a3ee1cbbab32e73f", "references": ["Is there a waterfall to hike to at this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-7bcf62c5e1a7428f8918c0f04e26b6b9", "references": ["Are there endangered species at this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-89fb516d1bb2492cbdabae5e86d345c1", "references": ["Are there grilling facilities in this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-43134bdc6b1b4be08555b7fa2b36a330", "references": ["Are there more than three waterfalls within this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-afa6f054fbc84d3387e3a4b486c825e8", "references": ["What are spots to eat at this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-db4ca88f6ad24ad4a264eebba80f1ac9", "references": ["Are there often multiple colors of fur on this dog breed?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-a5d3940853e94c95a28b6f33f02dcf26", "references": ["In this national park, what boating activities are available?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-2e52f22b24194b89b2806818f2d9c81a", "references": ["Is there a place to buy homemade souvenirs in this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-a761e187ae2e4afba85f8baad0578ec2", "references": ["In which areas are you able to drive in this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-6a3ace5805ac4436a167a81d6fd4c1b4", "references": ["Is camping allowed during all times in the year at this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-ddfcb688aeb94ec1acca43e20f2e8382", "references": ["Do people take tours at this national park in the summer?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-0b02109f87814c7596fcb9069e306b6e", "references": ["In this national park, what types of trees are there?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-4f9b159672ff49b79d1daee61488e1d4", "references": ["Are white coats an approved color for the standards of this dog breed?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-a4bbed96507845f6b860e97a17cfff25", "references": ["Does this dog breed always have a solid color on their coat?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-7a9c2f2a5852430881eee7bc134e3656", "references": ["Is the fur of this dog breed long and straight?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-98f92e691fb0469bae22114658349c51", "references": ["Is entropion a frequent problem for this dog breed?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-a0100a6b7d9d482584d4d83f6b3a8ba0", "references": ["Can this dog breed cause an allergic reaction?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-32cd5e61f0434cb1838082124d4f03ff", "references": ["Can this dog breed have a spotted tongue and still be healthy?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-0110487a32114321a9b072e959be8a7c", "references": ["Did this dog breed originate in the united states?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-9afeb10da4214ece8e413286d4f8b1e8", "references": ["Is spelunking at this national park allowed?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-628ff14afa5d4772b47ced4f9a638247", "references": ["What number of bird species does this national park contain?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-e10a20f0be5348de8ad88e730a524d27", "references": ["Could you mention the camp zones in this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-13173d07a39b4570b05cbf48dbbbd665", "references": ["Were there more republican members in congress than members of another party when this president was president?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-c89ece351451415586acfac87d6fbac2", "references": ["Did anyone try to shoot this president?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-a866fb16fc584eeeb55fa414d748b36e", "references": ["Which year was this president sworn in during?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-e3c913bc3b6a4efb9f761b8841155cf2", "references": ["How well did the stock market do during this president's term?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-08dff85300264ecf98de12e9123c9bae", "references": ["Was the budget shortfall lower when this president left office than what he inherited?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-99fecf56020c4736bd459c342bb393d1", "references": ["Did the name of this dog breed come from a place?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-a0c6006666f64407ac981ecd46ca3eb1", "references": ["Did this president deal with any illnesses as a child?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-399a3e0106b74f26a3f2017ece8bbe45", "references": ["Did this president die after having lived 80 years?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-9e378fb1c6874f769baf4600db8a341f", "references": ["Does this president usually get ranked in the top half of all presidents?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-15d1a9f49b794860841aec3145863fa1", "references": ["How high were the approval ratings of this president?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-78af7d5d12e2477b9397bf0b90c47fa7", "references": ["What political elections did this president attempt to win but lose?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-cbe6fae460a049ada36e8ede842a2e73", "references": ["Which person was the vice president of this president?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-40b9999cddae44c5a5dbbee0a12e1d37", "references": ["Was russia a place that this president visited?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-8750b4c97af741eeb7ec9d95cbe8de73", "references": ["Did this president's marriage happen before he ended his presidency?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-c646bf0abbec41ed9f0cb59eba638676", "references": ["Is this president from an eastern u.s. state that touches the ocean?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-44c3fce0a7b5433f93748f40fb096452", "references": ["What academic credentials does this president hold?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-52a3146601024548a91b2043d6cc96ee", "references": ["In which state was this president raised?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-7d9c496738d84e24aa1fb0893c50943d", "references": ["Was the wife of this president native to the united states?\u201d"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-63f92fb0aade44d8a30f7f2f649ba4a5", "references": ["Was there a pet in the white house while this president was there?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-f49561557fba4f86a87908114d1505bf", "references": ["Did this president have children of each gender?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-51e78d92ffbd427999c1b1b27744c545", "references": ["What was the area of focus of this president during their years of higher education?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-e34a0b079d4047a1b74a7897c8b5bafa", "references": ["Did this president serve as an east coast state governor?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-099dd23c38ac4911827871f8b886e3e1", "references": ["Was any of the legislation signed by this president during his presidency related to health care?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-6d7425b9b65146deb4d6bfbaafb3b181", "references": ["Do farmers use this dog breed to watch their animals?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-caa98091140b489b99c4063b6a796b75", "references": ["What do medical professionals call the common illnesses that this dog breed experiences?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-dbd7e3a3c6cf4770a3a68bb5d67da46c", "references": ["Was an extinct breed the origin of this dog breed?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-a12d703b0d4f4ad2aa32129345d63c07", "references": ["Is the weight of this dog breed more than 25 pounds, on average?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-b9ab65bcb6dc46b4bc5737f99fc5fc1d", "references": ["Does canada border this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-6200a54f6463459caef6a3660151304f", "references": ["Is this dog breed typically hairless?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-ad30bf56f0ce43c5841f0ee9a7801a6a", "references": ["Is this dog breed friendly with other dogs?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-cb88212b1b734ad69211086d994d6a83", "references": ["Is barking frequently or uniquely something this dog breed is known for?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-84f6b3a486cd4e93b165f88662cd1535", "references": ["Is this dog breed better suited for cold environments?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-a4186104450c488ca6e80348e1182c60", "references": ["Is this president pro-choice?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-ed808acb981b43c794892b060ddeb974", "references": ["Has this president been accused of violating the law?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-71ad1c0b2f794a908198bd8875911e9d", "references": ["What wars employed this dog breed for tracking?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-4d5595e2bbfc475a885fcc5841c406db", "references": ["What was the most current year that this president served in office?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-d8760fe8c752446e86ac5f8e285a3195", "references": ["What behaviors does this dog breed show around strangers?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-f1fe9a75a27b466c954ca1d043187fbd", "references": ["How big are the glaciers in this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-a996ab9f4a2a43a9a5632cf486dbe8aa", "references": ["How long is the cave in this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-262969094a504fc7ad42ae12e4eb077a", "references": ["How long is the un-docked tail of this dog breed?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-88c47bdcb14243dfbcee6b92cf8a17ca", "references": ["What number of electoral votes did the person running against this president get?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-a38bb74302e94f768eed5b18f4f335ea", "references": ["How many waterfalls are located in this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-537459e6edd543e5817054d22e673e28", "references": ["What do you need to do to groom this dog breed?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-79d193210bd946e79e5cb193c0167e70", "references": ["In which state is this national park in?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-d0b3bb91fd234210a510edb9c8988472", "references": ["What coat colors are not allowed in competitions with this dog breed?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-c24ad3a82a524b029fc31dadb99daa61", "references": ["Which significant rivers run through this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-5ae6f8b488e54d898a7f15d97c4fc8da", "references": ["Is this dog breed commonly taller than two feet?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-b107b5a933b542f29968c0c96d4a4fe6", "references": ["Is there a hotel on the ground of this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-10de24661ff24f87bcb129445cfdb3cc", "references": ["What countries have bans or restrictions on owning this dog breed?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-614cc2b2e7d546d58afd4059bc3d4aef", "references": ["What day of the month did this president wed?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-f46c734b13c04d94ac6326cb5c67e928", "references": ["The rocks in this national park formed in which era?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-382a755f0f654fc1ab5b69c67b9f8dce", "references": ["Was this president taller than 6 feet tall?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-85348b66abed45f899461087ac5329ed", "references": ["What was this president's party affiliation?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-9978c12f5d514dc784485020be49e413", "references": ["Where does the father of this president originate from?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-856400f489ff41f796708b7e21cf5d07", "references": ["What films have included this dog breed in them and have the word 'dog' in the title?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-d123ea79e1814bb39ae8ad1e87ae5d72", "references": ["What are the most visited tourist attractions in this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-74eb665ed9ee49a795659fa185338574", "references": ["What are some good places to watch birds in this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1562-56e5f858975f4a41bfa74fd3ef838b82", "references": ["What usually attracts people in this national park?"], "task_id": "task1562_zest_text_modification", "task_category": "Question Rewriting", "track": "default"}
{"id": "task640-569a669bfbde4e8799f1c30961b47442", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-07f98115a18d42c0b583ac805e177ecf", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-0d56fbad415c42aaa8b01834301d4a40", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-1c7ed2696103459f95fbec0166a67935", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-39258d40909b4b06a967361fe7f3df26", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-52df45cd7213485b96ac4afed2fcfb02", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-1c9327c19e8244019e408ce8c7ec800e", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-578665366ee2422492d556f6b4c65846", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-c4594e7eb4804e6a98877f2376fc6c58", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-25731f96b846400ab3bf4d74ca3245e6", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-444d7e3515c94284aca76960c260cf19", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-db0503822e384e1f8c1ebdfe6ffaebe2", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-854cb9b96d07488eb92b125be99f8358", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-5baff45edbda49ef9ae38ed62edc78ff", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-6ff31c5590964b83af68c9e542901319", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-4bb956072c72435fb3f7a34d202f41bd", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-c987b38a07a8406b8d8537b410f696de", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-76059b1840f440cb83317498c73db026", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-069f4512a32a4a9694aa6e965c68d06f", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-28881b23b539486da1fe3382d2f09f08", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-0b83b46cd74741e5a969fb5890f747a4", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-42c40110c188424cabf5c5d4d323995e", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-0797bb59ee24441cb4907daa0d185c49", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-498d9d6e563641c8990560f311ef4516", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-08032afc2d0b44ad8b8e2eeab22f842d", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-f3c15563924441b8b94a32f16e0ee668", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-37b00b1858ad4cc5b831771c8d63d4da", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-10c8353f03a94977a1acce28c56382d5", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-2d92f8fe23184b3d9285de31ea26ff5a", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-37e72ee9c82e4d6da57e00c62dba433d", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-4af9ec5b3fc44ad1bd57ddde3bb1690c", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-644f2a71e43d4a6dbd20b6ea71b32d07", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-9fba45e6812245ffb665be47fe96ed7f", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-7ef5524b2ffb45ea94af6c8753edcbc8", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-b42733b36bdc42a18675574a4d528df6", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-a35d8c0259cc40c99c6badffdcc9ca01", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-020b0e31ebf14d50a3c4d26ac2896b5e", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-648cfef5a0824d258b7cab4b035fd5e8", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-03ff9577c16b4f80a62d587bd0714ea8", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-eafe5fc9e10b4d8a94ec1951b301ea94", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-54138892f933416797217ad09cfac22f", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-156bcaf9dbe549579ea877f7990128f8", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-962d6893ac8c4184b3b06d05986ce08a", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-05d2f029284f4bd7815e05dee9709230", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-ae940f0189224ff6809bbb3943ffcd0d", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-8975bc38702e440ca6b5a6f37e7b1922", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-c923dde3eb084517994371983a1c879c", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-89de9d62bd8a4d8996ea3f394cbbc054", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-a7e3efbecd40439ba25698fa6d8f7d2a", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-618965a881724652ab77dc3ef9bf0691", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-b62b70caa746472b8bbfb8126641856a", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-15c0602462a0479ca90d498e1ce98a92", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-5aac6ad189b942c6a2fced0707da5746", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-17bd9d44604e4c14a6d89678f5aec89f", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-f784f9bf18c84fb2b710ace2db417a02", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-af189f4302a846b2bf6172c9b4620b70", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-e24e93374b7d453993b16ec78a55aa0f", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-3b5b1f6dda0d4d55a2e83190107e8b21", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-932185afb49543b3ba84a9482dc3a8e6", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-b3fa17d801f14f71b27b1946afeaa068", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-55a15a77e7d340f1b80829400beaa121", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-ff60d59141324b04a81053b1297018b7", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-45937fb468db4fb58d42df6e6aaed1e9", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-765a3777e6fb4884820a3cf87d0624ae", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-2b1057745fe546ebb6baa13f012d92ea", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-cc8ce0de229141c28b88f0e08ddffd17", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-1cc2f3e778e644f68aa1ce855ee39c30", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-08d9eb2169f946a7b7fae1735f6c571e", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-481002d07f094e54ab7d23296b0bb977", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-b03c74254bfa4aea96383c1902db407f", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-cbb156fff7ec470baa84b38faf10ddb9", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-2102a855f5b9474db1de950effb65116", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-e0082838943f45b3b7ad26b96291ce5f", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-91d010a5349947aabce029493242c53a", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-149ddceb339f4c70b3ef514b1c6bf0a0", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-0094066e49e64971a69080be4d61c4d3", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-15f1b603b7ed48649f0f6be432782ae6", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-ece96c15750d4443b03cc07eaed84617", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-a66ad92ee871480f956ae2199cc35fee", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-3e846752a1a24b8fb0ea36d25008fd38", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-a662ae1343d94f6dba91bfd610071736", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-d49dd2a3b0ee466bb389d3f31cfefb25", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-0dedff4bc1b94fb0be098be51d8a220f", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-312a2792823e479fb32562a52951b145", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-294439d88c214b82818cdaa7de208c24", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-1f4e139a0a4e401c9ce803a190897d69", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-25e1c4ff574c4afbb1c90a46cb8814e0", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-90183ba36be74ddfbd242db1ad9c25e6", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-c7037f0713b340d5aef13daa1a79bbea", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-c1014cfbab8643539f9c901f0790ae42", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-2878108ef91d4d3fa2a6f422390cd281", "references": ["neutral"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-781c8b74a0cf4e53a728b4928e8db744", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-fb167fbfa79f450f858daaa8e1771a45", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-df0fec5bb6c046a9b71a4e787c88abf2", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-e59cc9028e9c4eae9879d5f544442c3e", "references": ["contradiction"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-34b9fd84afba47858ef9ccf33537e01a", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-24946d3f192d474d92d557f22017f411", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-765f6be6330d4274b5fbe245c05c8fde", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-e8deef24fe8047de82d9aa33ef6a0d52", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task640-f523accc305046489555c1d32ddf4ec9", "references": ["entailment"], "task_id": "task640_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task1622-9962656742aa46ecb551ea3898727879", "references": ["What describes the proportionality of acceleration to force and mass?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-7657fc62850a44b7af770454cf5ed0cd", "references": ["Which organization faithfully summaarized the WGI report?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-484d2761ac4f4c69b36e088db57c0f3f", "references": ["Cilia can g ow up too what length?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-24ea67ec633d4273930ceb29f8168ad7", "references": ["How do socialists think the means of production shouldn't be owned?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-938b6fb306704a27bcc63cfa06499084", "references": ["When did the Holocene end?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-421c4abb502c4385a919eed6b9b6be43", "references": ["What does the internal cavity contain?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-7d9f9e9edd534d6aa773d28fbeeb618f", "references": ["What does focusing on morality help a judge to achieve?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-5d1a4e26de1b4542b4860e7afa805384", "references": ["What function is related to Basel numbers?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-3a593536ead344e6ba9d19bcac83c155", "references": ["How often are elections for the counsel held?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-8435a8c1f95543cea201d1f5b4d97987", "references": ["How did William Shirley feel about French advancement?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-b7f66b51251346fba84f1da0b4cc45c4", "references": ["What did William Smith cross about 50 times?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-ee6ad8affdff42269a4ab053282d0c58", "references": ["The student government is led by the president and chaired by who?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-9c285884ee58441fb3fad52bf4d3438b", "references": ["Who first fully explained the origins of magnetic and electric fields?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-3172d1bb4e6b46d5963031bcbe03c982", "references": ["What player first won the Heisman Trophy for the university?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-346e67b49fe649b395600b8792379f46", "references": ["What does increasing inequality harm?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-958d5bd8071345f8ad5c739fce5b190f", "references": ["MHC antigens on normal body cells are recognized by what receptor on NK cells?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-6a23cc8e02de4a46bfccec7d13a8cf5e", "references": ["Where were French South Americans settled?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-992a4308948247c385a4027d08df9ee5", "references": ["How many auricles do plankton have?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-f33324dbd81c4969b7cda665e961d389", "references": ["What is latency between nodes?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-e929b47363c44f15a655b3ae35c621de", "references": ["What cell is unable to act as a scavenger?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-5989fed4059f4f508faca4f603c6eae3", "references": ["X.25 uses what type network type"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-b2434f72637f440baa255af865c5900c", "references": ["Which theory states that slow geological processes are still occurring today, and have occurred throughout Earth's history?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-35cff815ab1e471083208639d46e8942", "references": ["What career does Joseph Stiglitz have?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-6d716d2367714c70a9faae8905f14e36", "references": ["Deceleration can be described through what?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-b5bf78532ee947e3bca58d53403eaec9", "references": ["What plans of the British didn't this attach on Oneida Carry set back?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-1aec280b640f4970a4bac596fa1d3855", "references": ["Low doses of anti-inflammatories are sometimes used with what classes of drugs?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-496231ae433a48c89b7ffa736e1dab85", "references": ["What nationality is Hoesung Lee?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-ada015316a9841a1a165c5600d4c0384", "references": ["How is packet switching charecterized"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-7f716f118fd146e181369256dbaeca14", "references": ["The second buildings of the U of C are known as what?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-e2402dbebce64c8bb23a1b043cb89ac0", "references": ["What triggers a slow killing response?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-9d4f1de14000429f880f7df7cfcf7341", "references": ["What was college considered a vehicle of for the wealthy in 1945?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-4ab8fca05e074a07affaeab54758ca1f", "references": ["What are the Siouan-speaking tribes?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-7dbfd0c08a874d408138a99e832e9a70", "references": ["What development has the University of Biological Physics played a lead role in?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-4ee4c00f703b489a8af67e450fe08c2d", "references": ["When is the oldest armed seal of Warsaw from?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-144edf590e7d4f81ab4bc8615f0a9460", "references": ["Who do some tax agents want to make and impression on during an arrest?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-5f6396f53c214b44b6dba5440420add1", "references": ["Building construction is usually further divided into what categories?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-8a0e77267f2f404aae788f139c181105", "references": ["What did Donald Davies Develop"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-6c5ca58a464a4ec58a0e9e91e47ec84a", "references": ["What has the tendency to increase wages in a field or job position?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-a0b42c3a767a43f683ec56dff5fa3e98", "references": ["How many bits are often in the primes used for RSA public key cryptography algorithms?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-e0b0716a6bc6433786b8cbbe79138046", "references": ["What prohibition agent was also an alumni at the university?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-92f11f14c4254306bb43269187f27720", "references": ["Which timeline is further expanded in the fourth scale?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-b956682bf42646dcb3284f2579df7718", "references": ["What is it estimated that about half of all variation in homicide rates can be accounted for by?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-1ac1116d761d4d4da3a4b9c560b3af9f", "references": ["How many US presidents are alumni of the school?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-8ae91718a18541219702b65e41dba8b8", "references": ["Ideal strings transmit what delayed forces?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-0d6b2744b54d42f0892a36e106e016c6", "references": ["To what extent did Fermat confirm the validity of Euler numbers?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-fc37ceb645c4499d86d8f7f4d8c9d067", "references": ["How often do plankton need to feed?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-16c7ff8935e147728750ac3a655632c7", "references": ["What non-dynasty came after the Yuan?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-31a2277c0bb24c7791d90e87944422cf", "references": ["Cristian Bay's encyclopedia concludes that civil disobedience does not only include what behavior?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-a073acd9cd46455eb770fa59ef18fdc5", "references": ["What two things can pharmacy informatics not bring together?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-1a7933219f51469eadf71db22eb11221", "references": ["Who was the leader of Russia in the 1960's?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-248ea6a27e074264a916fe0a9985324f", "references": ["Who refused to act until Loudoun disapproved plans?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-9123dd8f307d455b9b31f7fa06787fc4", "references": ["What is a term for schools that receive government assistance?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-e046c89002dc4282a379176eb718c7ad", "references": ["When using a probalistic primality, how is the probability that the number is composite expressed mathematically?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-42f9baca24884e7c8aaed409676b4fef", "references": ["The University enrolled 3,468 students in the College  and how many in it's graduate divisions?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-cc91ec383f074ced89500742548ec39a", "references": ["What countries use a blue stylized A to signify pharmacy?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-9f3d90fdbbc1492792fef5c9260e3aba", "references": ["Who was the vice president in 1962?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-c23710a4d0214af6bdd8873ae34936de", "references": ["At what village did a Triton stop to rest on a sandy beach"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-d22937ecd7e345b9a453b637e025b79a", "references": ["For how many years had temperatures been studied in the 2001 report?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-3ca6075932f5488395166903ae147619", "references": ["Who thought that the Yuan's social class system should be called social classes?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-97bc8e98ab2a44feb12a5dbc50740eef", "references": ["What must local regulations conform to?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-a3379971a6b3452fb686719108f40b4a", "references": ["The movements of the lobates combs are controlled by what?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-f7ed4bbfb24a451ab55cca35a5cf43e6", "references": ["What category do all Catholic schools fall into?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-059ec724edc84475b3bb8c638c1374df", "references": ["Where does HAMAS not want to establish an Islamic state?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-a23af03ac45144dcb7418dfb21e44c46", "references": ["Which species moves by a darting motion?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-696c9743f56e4d32bb363ad77016c8c2", "references": ["Up until 1990, Saudi Arabia played an unimportant role in restraining what groups?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-9e648cc8ddb14ca0ba3c099423041403", "references": ["What are horizontal layers of sand called?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-14b3f33033bb4c218a709509c8364c72", "references": ["When large groups of people all boycott a system or don't pay taxes it can be considered?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-66fc26e08e5e4e71abd4c15f717cfa06", "references": ["How do rotifers act as when looking for a meal?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-b67843d7bfea4c73a5ba62f75f107846", "references": ["In what month does the Harvard Crimson men's ice hockey team play?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-b41dd91d665c46338369f571689175e1", "references": ["In what year did Henry Ware die?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-5e530f36ae08488dbd65c236e21424c3", "references": ["What Chinese system did Kublai's government not compromise with?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-5a9162fd347c4104aa8e2718e608e609", "references": ["How long after a banquet with Tugh Temur did Kusala die?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-1da67befe2d749509380c417571543b8", "references": ["When aren't inequalities in wealth justified, according to John Rawls?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-db65927fea1641da9c27572374aace2f", "references": ["Who has the ability to alter income tax in Scotland by up to 33 pence?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-ce7a2b9cdfe94003b2a89fb924ad9849", "references": ["What had the number of people living in Warsaw declined to by 1945?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-75ee82d6c84f4f4cbb5fabb2e551a63d", "references": ["How wide is the Rhine?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-f482111ecf994c4bb7d06f4344dbea17", "references": ["When did the North American French and Indian War end?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-0f9b248c0d19450b90b575536686dec9", "references": ["Secular Arab nationalism was blamed for both the success of Arab troops as well as what type of stagnation?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-6ba7ec1d7e894991931f9b1550952713", "references": ["When was Europe fully forested and recovered from the last Ice Age?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-ac59f1f7b1b9403dbde3ed28f6d521bd", "references": ["In what country is Seville located?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-c03302502b8d4677ba9a457edc26f7da", "references": ["Imperialism is responsible for the slow spread of what?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-3a08b00a93d94cd9b76723b943104f84", "references": ["What uprising began in 1351?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-81e30d83bd224c638b8d4365c7d6cc22", "references": ["Are the sizes of packets variable?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-a3674cd1495f48e0a796f0f7a98a096e", "references": ["Which date was affirmed by the IPCC?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-7b3e15d19c804901b72770e85d5a4407", "references": ["The Rhine forms an inland delta into which lake?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-174dbbd2df134166a2ade465295a5a10", "references": ["When rock formations are found on top of a fault that have not been cut, then they must be older or younger than the fault?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-346f86eff43b47b7b9c4901a359c2a80", "references": ["What committee corresponds with one or more departments in the Spanish Government?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-282774935d514506a99adce01bf0a788", "references": ["The presence or absence of what can be used to determine the relative age of the formations in which they are found?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-1c9376d202934dee85ce08a43c14d662", "references": ["Who are FDA laws against importing medications aimed at?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-593e682b8c0449cfb8bc230c3e7b5da1", "references": ["How many vice presidents are on the Student Board?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-65e4323e933444abb880289de218db14", "references": ["What artificial method of spreading immunity causes disease?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-1c9d9b6355594ab3bbc32649212308a3", "references": ["What is one type of private key cryptography algorithm?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-2cb50bfb83a64cde85a2440662c67387", "references": ["Who was the Karluk Kara-Khanid ruler ranked below?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-748e6180c3c947f3bdde6e054ca7bfaf", "references": ["What is the largest city directly linked to an interstate?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-f70cc070382b4daab62f4333cc5f9334", "references": ["How many troops were victorious for British in BAttle of Carillon?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-928641a2e1ca434bbf29548fa6cce021", "references": ["What was Srodmiescie ranked the 7th greatest of?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-f75bf94afebe4f1cadc6514d84808877", "references": ["Where did Fresno residents move to during the flooding?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-f1ebf53e07624993a6508253f34bf49b", "references": ["When was James Hutton born?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-8e2ce6eb816745f9bb5dc836d794f290", "references": ["What did Article 12 of the Allied Armistice terms require Germany to withdraw from?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task1622-6be890f50bc44d9a90a865bdea532b2d", "references": ["What is the annual budget for the Scavenger Hunt?"], "task_id": "task1622_disfl_qa_text_modication", "task_category": "Question Rewriting", "track": "default"}
{"id": "task623-9e502461de6643df994eb920ae39939e", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-8add84cdde4344b6b9c656cfd0f3ffa7", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-dd751dfb913e4cd799820b45bb786d18", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-49e9a7f06837435491caf2b908eddb92", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-2932e433ec2d4f479d1d4c18461bf01b", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-4578e54b8b2546edbb7662555a7529ef", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-2e0586ace2f6478dab2b2181664963b0", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-7de5cf5d28394fb7acb05025cfe7d86b", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-9b82a57e1c0347f0ab0dfe283043fccb", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-f7369cf287b1487c9536df093be29105", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-c3e7efd61ffe47b397a57949efe4fa13", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-dbf972cd74b343a589deb301f3b86c8e", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-ac0fb58acbe646069ff534f00ede644d", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-f10f69aa9b5a4279b3b2b5f82a6081a6", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-31954b58e6234e20898979e5f6a26ff4", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-7360e4b48f7b43fd93eb9b991c8e161b", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-3e5d5e84bceb4f659edf857c6b0bef4e", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-2ae1f14c975340e8a24cf8873f032aa2", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-ca5abcf9c5134cd3a07a82613ac99a1c", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-421d012326514a8fbe8348299d2d6d5b", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-c01722004b024c868f19020ea9898874", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-c8aa26b2aa844d5faf1b3f2b3c1365d5", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-ad32596622db4a018fee183603e519de", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-ed405bb62a644a6bb1e84c55847febd6", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-5ce0321834d44015bca5ab413730b126", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-8d982843738641cda4b403565b57c91c", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-b0fff9c466b74251b6810431640ebb91", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-e4aa834789c74c1e9641156ec9467810", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-50938b74abbc4fe88f58cb384c2c50e4", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-ccb93f3fb7b3485a88f861b7d83bd6e6", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-b8598948d06f47afb573fed6ee001681", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-83f21b6988eb4261bef6993d4a0e884a", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-c4943441136f46eaa8d98ea005f372de", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-5bce2c532eb1485995f9895c4c61b5db", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-dd4c4a7e4fd14c91a0fe7d29ef5af391", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-1e60d1a0e2ac46d5bb53ce680bfc2774", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-a4e772ca097145be950bc89437bd72fb", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-acb609e4c24e4ee587462cc59af6f8be", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-c46975efd6444a56a2c67660d4bffbfd", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-693661343d364f1bb4f1466998091e3f", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-2dd34374c02a45529ddc37a977a8f481", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-1383d4fd43c34648a7db63365ca0c895", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-b42c259467d5442993998190e7fabc64", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-d6aadcf0dbcd43a586f1bf530101b65c", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-9ec1b35e87d64947aec3de3afed823ae", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-be2002b5eb114f7e9f3dc9dc4ffb8263", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-bd538880b29349e588744021a95dc6be", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-728b4acd1aae4ceaa19ed153f5d07e91", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-d61db363c5ca44dc99be46ef32d454bb", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-d7f656c47a4144888730f733bd4433de", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-7c3536eb121442eb8c2bcc80b542c113", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-86f381c5611045adad7bf7d20a855cee", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-122598ec10af4d0fb3bed83530579805", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-6a275070c95c44a581e0226bc3af9922", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-d95130884e5448b09dee6237365c945b", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-a6ad7103cda24ee4a18520e323512a22", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-f5bca8296c9f48a996c80013a8af1bcc", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-9f25550a090a4d6e8784aa047fb2daa7", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-cb0f8e3a7beb4f8da4f7d9ca93d9aaaf", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-3c0edcf59f8f4b81932ca1467b74d3d2", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-e34509e987024d71880846da4914551b", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-1c07829335564cb1a3fc8cc79027fe82", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-4b4051099c554f84a2ad0189dd7bc4d4", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-fa5b4323ee304cb2bd8f7311d127fa6d", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-89afb667423d41e08c55ef9237600b44", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-f764dbd4fb3c427e83bb191339ec76e8", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-55bbec29cd064ab88cdd6a7709e023f8", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-f09d1a0f9a7a4c1ab1d9c57642ba0fbb", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-7d96dccb93a54e7b9cf5e97033a26b3a", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-cf3a9d239c784f01aa9e0f5008d2e82d", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-46b4a4b9f8dd4de7860c4f6a89c287f8", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-f51d738aae244307b47cd95d99f5068b", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-905b077b69e64c579f928b9b00ebfec4", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-8438ea7996fa4642b5acaf0157a3c440", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-e01e9f95f2994f78a9a9bc0ef5105553", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-fe5572089c79481faee28f80bd049f3b", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-c97fb357c0184320aaeb8641c1da394a", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-47ff1c7ea771414fa98450c260217e48", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-73e137b3d91e4c18a4ec1e3834bc8341", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-1ec0e990bd9c45489b2e1559102f172d", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-10cbf8de5a1e4caa9113e831571bf203", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-4a81ceaf5f044ea58191eb9aaf14890c", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-1917a335c12348adb8ca74436925c02c", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-ddfb60c65535483d94c98dae07c074eb", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-0bedfb7bce4346ac86184d644dca232a", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-5c18644ff30240fbb4690d9ca8696c10", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-f52d15cd36774bf6ac5f655f59b295a8", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-4ac2577f94d44f7582b2a7b92d948e5f", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-fd7ee59d755f4a9298d3865107c69a6e", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-f203af3d09444d9997e838ced3494124", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-d22bf05b149a45cca5ce21f005630589", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-7de4b91348f44090a2f29f9fb9cca463", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-2f98532263254dfd9c72dd1c0633428e", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-8ac36ba194824116a70ec1c7b9329361", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-a6b0f4639ee549a89278a1a240ff136e", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-e793d55e81ce42be97e68004b046849b", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-4c6a9707662644aca3222b365007a7a6", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-0261093e88a24412af97d09c7bd83c30", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-ec682d2fa72844d3973b943b8885823d", "references": ["Yes"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task623-f3249018a29b41bda9ed76ab22cbeb57", "references": ["No"], "task_id": "task623_ohsumed_yes_no_answer_generation", "task_category": "Keyword Tagging", "track": "default"}
{"id": "task020-3b643de54d474381854c2499cd185a74", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-fb71bc528319459a81172fee86bc3531", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-6f9ee4a0e3074d2ab12e8d6dccf28545", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-7906fd5a5c6f48c69130e5f84c6e3a62", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-2b3428203384452ba13c8ab50a53caf6", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-0001124623ba4633826d00d9857d4c20", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-1aa8520cb09f4f0d8611277fceb6a1ec", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-9ecdf69cad1649ce82929d47234e7e2a", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-87f0329ebba748a59ec7ce3720379e55", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-c7330edb07634f20b9500f10c1bfa1b4", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-3748507c524d458398fc79fe264cf017", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-95d0182abd974820b3a8d28bf9347119", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-b04b52ed4cdb475ea71f886b291d8da2", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-17f7a102f79b458e9a4200d1100396fc", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-95fcbfd5169e4257bd9f6013765b4755", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-8ecee5b16d5243ea86688083741897b3", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-45805df0fe054d5d888a5a14751391f0", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-1bb0045907a24e71a342c8786cd07411", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-23644f62db6541eaae4dd16ccb7642b5", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-629372845be1479e90b22c61ee8b5cd1", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-00ceaebf277a460ab50aa0bff4ea8c5d", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-11a0b598025c4afe84fcfb6db8522d71", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-e66139944fb84cfc8770a570a886c3f1", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-689bde7f68244755818bb75beb577dfc", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-ed1c9bccb9c9481f86ce0fbb1941826f", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-b6855997f9a845d4b278e831afc880c7", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-5f0e02e52bfc41bc8e3861a01a54823d", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-21f9fa3556da4a7ca48827bd478bb4c6", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-e86ab13ca26a49a4b1a61727f758ef06", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-6ff3172233cd44eda9d0063f979c5c2e", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-5823093badd444fc875cae2baca4b750", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-041638eec1db42489206aac7c3c280d4", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-e02e5e96ea2b461fb5d060bd61f77836", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-5be61b0108114a079579c3a107be8781", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-0fde97fec58441ce98b5b41bfe00a6c1", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-4698dfd00b514233a3131fead591af42", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-62de63096aa34c1ebcc70d84dc1df284", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-37c9fe8880d34957b4dfac12fe4519d2", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-a24b7134b60347a99ea02218f0efeaa5", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-f926c498b5f543078af31796daa7e468", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-475724f471d04cdcb7e2bc150e5eaa07", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-6edf2768b483478391772ab3d8fa72ab", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-e1b51051e25c4a3c8f83afa85cdb7aca", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-b97b4e60e1d64a88886251f4f048f373", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-13d0114f772349ffa29d91103bfd4831", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-a33534c5b1ce4044b79537bc60fe32e5", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-bbdf086e4344441db12a0e3ce6c25399", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-848a7854a8ab465099a19c84646997d8", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-d1a09b5156744c078bfa514519e626ac", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-2b93f9ee2b424519a16c9a36a3b876a0", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-96341c5425204c009e621055ec722ff1", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-ab7698566bdd443db3fb1aea02352cad", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-fe84bd5cc4b8489691ccc98bf7a4edb3", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-78466745930f4e2a9775ae05df7dafd2", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-e202759d657a4150b356082ab77664b9", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-e07f09bcd030480597fbfd94dea140e9", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-f92c067c81d148df94071bf080c9bad7", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-d4e5eec1b5b9496db4a56eed57113a2a", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-d9017029c6754c7687d32687ee83a155", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-157cd9dca51344b4993fc2e70103d8bc", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-0ddf2915172f4904b60566a79e1e55ac", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-41eff53b9e414805b93a1ee8f7d69397", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-8916b24a83b04aaa8594a93446091ec5", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-b6305dc8f0b246a58850939294afe686", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-5ab0e617496947db8bf4c7cfaa10a221", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-13c6e4287f5e49cabe75f42dc95d85ee", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-db5fcf96acd44422b7b5f2543d2a4869", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-fa2317066a8241b4bd5e46569ffabd19", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-a929fbe6370a4b1aa437fd335c3390eb", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-32c1ffe507064cf0aa563045cc2642b2", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-fe2703a8fb934e12b9949726e3e56822", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-4cf6f2a132d348e6964053f53b8120f0", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-f65edfbd3db9441390143bb95995e489", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-443f5e5c4808452ea9fa4e51adfbe239", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-299ef05c36994cffa9378a05f80a28cb", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-d901eb653a614e88a2c00f644d7a8c6d", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-975ef955122e449f80b1b499068e30c9", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-9bc43cb7e3ba49509222615e17de72e2", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-642291e463c14e2d860e9455dc63c772", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-98455c48f1284d8a8d19fb303cf810a2", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-01f40e58d7914299872c3672863f1675", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-cbcc1ed044204b44bdb199612b74f9f5", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-fd31c11cc4144989bbcb3647eaa8dcc4", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-6499ff45aef64927a2a1e27b89bf674b", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-12d4fccb09d54a57a1534be710a47552", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-bd706877cb444eca853f80b76cedf3e2", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-69d87cb7b14f49cfa58731f7aec6d490", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-9949c1dbef99492caf66c7b6ea8279fc", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-c003776d5149440496b873f6a58266fe", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-34fa16139324413eacf58c5aa2125f7b", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-e1233872a83a4ab79f05fe0a65027f12", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-efdc9f6f73ba4a398d01771c3fe82665", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-4c4020b14f364bbaa040b35857abb9da", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-a863123d9633430bbd0ca074b049f2a3", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-a43e35cf69de43d4bbe78eb50ab5a501", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-e66d39b2a2a7493b8e3770665da78e92", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-23074e5055c440f9ad306fd76b61e336", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-4d34e28eb6e245659d951966dbc4df4c", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-986e328e679b46f9b1065dd6df9d5506", "references": ["No."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task020-30da048e5ed34de3b40d11d94b6d36a5", "references": ["Yes."], "task_id": "task020_mctaco_span_based_question", "task_category": "Answerability Classification", "track": "default"}
{"id": "task642-63c7a3258adf4356b6aa44dfe8097460", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-5143abe586704147988cd01f5c7aeae3", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-91594780a5ab47c8a2c141929d3a4f1b", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-12bcbeceb7a74d69881737d418101fc9", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-d460590a47cb4c8ab51d88130c5a645e", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-7050854f0a7d4013b4e29136938a5bc1", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-0e607bd7c8d34fd88902b852977d54b0", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-a20d42a5ab9a4711920c93569d199255", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-dc80d6b683a9463aa385709d65ce2c3c", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-f0b00dd85bf14caaa2f4283571ecca6f", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-524b9a1de3df4bafb1c043a6211c99e3", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-ddac9936dcc54342bbabee26fd3641e2", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-152cce547db84eed949a8dde2a21ebe8", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-8d102ad7416c4b4bb5244d0e6b7a1d44", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-3b1afe6fb26a42d489d5968052d1539c", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-773209e57f95480f9530f1f1f9fce16c", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-f95c440d81784a8ca581278fef25139b", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-6e80c7ce4f0a4bed95c3e55bb2b77541", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-0718238e5bde46abaa67cb46612f1f40", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-59331febf52e4f758e83483dcae81e5c", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-9662d0cf289042bea3b85f66b0488b64", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-f1b932e6b989465287341a1d1772cc99", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-a1335a2ba2ff4c3eb14e5190efad2c70", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-46f4efd6ad9f488c90285950d7e9da4e", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-6a9880c602344bc18830d6b3e5acc87c", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-5142df0cd8ab4d209c8f5f43e37add4e", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-9e78528fdc6f49398166ae14259feed6", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-fa8c02f0b21f4c92b5a83ff8c402ce12", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-f31bda13dfea437aa5bc9f6f0d3cbd9c", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-f56e2bb3a7f944a78165620939393afe", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-075b22c74f7646cc837510372cbc30fb", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-45cf0077e80b432a8cf8e2712276da13", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-0d859467a13840fc851662a829e4d206", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-347d3fad411c43b585ea6353b7c6fcf4", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-85252853dd5d449e9a1c40637b9ed688", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-38a566e27fba4cea8ab7bc13e8719245", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-3d4b54a75a2e4468b2305cf96078f43e", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-f1d0cff8f6174cfcb9acbc8228ed07ac", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-2ef6680aef7a4c2781467190ac40738b", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-b6486b1c7287469e878d4b266f90cab5", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-4402e10b741045e3a967eb0bb5ee9911", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-0038c936156a40449c1beb6cddbb3adc", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-fecaeba985074b0091bd9ebd28897b63", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-11613bf6084d45da9859f099b3b719ac", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-c20ddb96522c41a095edc73f5e0c37e4", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-965749ccf0974cf9b679f3f19099ad3b", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-fa8900f54df2461abeee448e2bc6af3c", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-54ba79b68a88435ab2d7eb935435e2d3", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-63f4777c50674bf8aaf4e3f31ccd1e22", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-0a2f57013fe148c482027d243745bd08", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-b21062e724e94915abde9d8856088194", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-dbb4677750a6427c9f1c377b71856fa0", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-10d25abd139442f5ab9a07b1cb89da00", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-200387128af54940ab2dfb56f7dfe1fd", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-3158216123574e05801df39c06e1de42", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-36ddd06e79614bf8ac99534d9fcfcfbc", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-4060787af85a4222bfd83fdaaa10ffc4", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-820e855cdd1246968881ce069c82a514", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-695b165c6dcc46169df59b175a1e5841", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-81ca9ad6c8c3466fbed06cec30c7a12c", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-17adb551766a4b60a639ea76ae7a55ce", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-5abd7cc766f846eeb2a881da9155733a", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-665039acac8948609d9cee80269bcb23", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-7c89f6b6613a48b4b52d5b0b57d89d01", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-2ca215bc3fb14b63b164fd4dcef4f290", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-7b7d6e47b661412dbd36f6f5eee5762d", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-8453bfa3a4e94dbb9f52d09cbf44cb80", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-348e899181c849428de5ab286a205396", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-8f7993e0083f4c3481480a8d61abd4d7", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-186bca2896e340839bacd04f04110c76", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-ff13b95356644d30ad0421ebaf444510", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-dbd5e0a840ff4e38b947c5e0c1661a8d", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-979a86246c374c99bf186c1826246f49", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-6e6d3a622f1a45edbec0674db1ad1c35", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-dfd1e5689d9f48ab9b5df41fb4b98c14", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-83241864fc7e478281c5c4c571391944", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-50d722ee698245f189938b7879cb7de0", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-8541459fd16d4710abfba0cf94a9dca8", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-ba3bb4003ac44835b92d278ff24a2d81", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-cac8a361ffbc43ba82c64a8f49417eac", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-9cf489129fce47f6b2c9a72e2f21bde0", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-b026f1729b2d4c0f87c45e0fb29ecafe", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-aabe79b7f8214db09cd9c338263f78b2", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-a56e31b812dc4a12b6768057bb8499a7", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-1fc4b53119ff4708b9e80919a642f2a7", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-4efb5a4ffb7f4c20bb72185fbee9ad2b", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-101c738c29704bc6b363d563e1da764a", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-79300b5fec8a4c5b9e3f9e665281a8f3", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-c95a6c0db9704476958527d8640aabc7", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-cb89358212fa495f9a4f388a88033ccb", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-1bd1d69f51284778b49d820658187331", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-a3b92d9b4cda4c5083c7f7959929ed0b", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-7041d0fdd2b24dbbb5b0e31867796796", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-5e0c3701330d4a2aa6d1ecaa22529e4d", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-400ada6bf5c94ba892e36b807eab4b0f", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-f16bf47d51d248c3854f3dbf1f2d84b0", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-01dc00fe76a9435396d703552f653645", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-1285678f667d48628ed291030657cb0e", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-003cbeac3b964802a54388f58626a283", "references": ["yes"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task642-8904cd5bc01e41eab8dc4b5662f0702a", "references": ["no"], "task_id": "task642_esnli_classification", "task_category": "Textual Entailment", "track": "default"}
{"id": "task102-02e80f9106a74cd69188da363de8a491", "references": ["The dog is wagging his tail.", "A dog wags his tail at the boy.", "a dog wags its tail with its heart"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-7a8ca68b33e645159843f6a338d8eef6", "references": ["woman paddling canoe on a lake", "paddle an open canoe along lake .", "a man paddles his canoe on the lake."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-5da80f4ec3564b0a80091855285685ab", "references": ["a train pulls into station", "train pulling in to station .", "the train pulling into station"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-86e007b9aed3447b8820779fd701043b", "references": ["A horse is eating hay.", "The horses are eating hay.", "A horse eats hay in the barn"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-38ffba293cbc426b8657ce08668f1ce8", "references": ["watch a match with fans", "the fans watch the match", "a fan watches during the match"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-3d024ebf2c16494faf3141f3c4658258", "references": ["a lake surrounded by mountains .", "lake from the surrounding mountains", "one of the mountain ranges that surrounds lake ."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-b8a4c19331e04cc7b6f17bea12bf5be9", "references": ["A dog laying on a rug.", "The dogs laid down on the rug", "Brown dog chews on bone while laying on the rug."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-f60f207b24924bc0a1b7a476a0415f76", "references": ["hanging a painting on a wall at home", "paintings of horses hang on the walls .", "There is only one painting hanging on the wall."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-9bf9dd837015411693f426693e74160c", "references": ["boy carries a tray of food .", "people carrying food on trays", "The woman is carrying two trays of food."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-fbe0e30d04d946669798edbc85cf8939", "references": ["soccer fans watches a league match in a stadium", "A stadium full of people watching a tennis match.", "supporters watch the match from a hill outside the stadium"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-ddd079b7cf0642a3a6bfd5899971ea6c", "references": ["A cat licks his paws.", "A cat is licking its paw", "the cat licks the pad of his front paw"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-017d31a9e5ae4864b0a921625cc7805a", "references": ["a bath room with a toilet and tiled walls", "Three men tile a wall in a large empty room", "A wall mounted urinal in a checker tiled rest room."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-e6dd4f27b1b44aee9690f47d5a404076", "references": ["canoe on a shore of lake .", "canoe on shore with rainbow across the lake", "Several canoes parked in the grass on the shore of a lake "], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-e2c488223f564cc398e40f30e2f604e9", "references": ["A skier on his way to the mountain.", "skiers make their way down the mountain", "A skier making her way down a snowy mountain."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-5f68da8e4adb400c8ba3a52aec96330b", "references": ["driving boat on a lake", "a boat is being driven through a lake", "A fisherman drives his boat on the lake"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-bb7b7cb1fb1f4cb1b545d738677e2aac", "references": ["A horse is eating grass.", "The horses are eating grass.", "The old horse ate grass all day."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-793cd08ac45043748060b051f3da2abe", "references": ["train coming down the track", "A train is coming along on a track.", "a long train in coming down some tracks"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-b7b2ddc433ab48beae768336432603a9", "references": ["train moving on the tracks", "A red train is moving down a track", "A train moves slowly on some empty tracks"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-0a990fdf92624b37aa50a5f5626ff27f", "references": ["a train leaves the station", "a train leaving station bound", "a fast train about to leave station"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-2dce14d05d154810b3098eabf81fc32c", "references": ["train and passengers at the station", "passengers leaving a train on a station", "a train at station with no passengers joining"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-c3a27c76dae945fe8e2c48f2fde3d0fd", "references": ["a train arrives at station", "train arriving at the station", "subway train arrives in the station"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-1b5f269cf9a94e62986c9dc8bd367bbc", "references": ["a train sits at the station", "A train that is sitting in a station.", "A red train sitting at an empty station."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-ed5d123f6d7b4a06af1eb6b97d1bb9b1", "references": ["a tea of horses pull a wagon", "horse pulling man on wagon .", "A wagon is being pulled by horses."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-d7a506121f974ff59d4ca10b3a2b7fe8", "references": ["train is stopped at a station", "trains stopping at the station", "The empty train is stopped in the station."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-55b10d10645c43369978cbb59f3417ee", "references": ["A plane sits on the runway", "An old plane is sitting on a runway.", "Two planes are sitting out on the runway."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-78025e355f7841e4b7e2d194ca9572e9", "references": ["plane flying into the clouds", "flying plane against a cloud .", "A plane flies over head in the clouds."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-b924530cc9e247db9c1295d40dba3e1f", "references": ["A dog herds a sheep.", "A dog is herding sheep.", "The dogs are herding sheep."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-27726e59c754474682799147b702dfc4", "references": ["boats sitting on the beach", "a boat is sitting up on a beach", "Pelicans sit on a blue boat at the beach."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-71e469e1ac364d4fb6e4085243b939ff", "references": ["a train coming into station", "tube train comes to station .", "train coming in to the station"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-0c6408d686cc40c682c5c9c6f7fbdc9f", "references": ["clouds floating in the sky", "clouds float through a blue sky", "shot of clouds that float across the sky"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-fef6672144554151826a562092f90a1c", "references": ["elephants pulling grass to eat .", "An elephant is eating grass in Kenya.", "a bunch of elephants are eating grass"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-b655198837874466be67f49cd0af38e5", "references": ["family spend time in the park", "spending time with the family", "family spend time at a holidays"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-71059e10a5914ab6a36b00b3a3df1e19", "references": ["black walls and tiles in the bathroom", "A bathroom has green tile on the walls.", "A bathroom with blue walls and pink tile."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-7e31eed95f3744a6be135a25b407aac4", "references": ["A red light shines in his window.", "Light from a window shining onto a bed. ", "A beam of light shines through the window."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-d24514b2e10b4378b2694550e444ea89", "references": ["light coming through the windows", "Some light comes in from the windows.", "Very faint light comes in at a side window."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-447fc98730324c1299f063769fe4b3f0", "references": ["a subway train enters a station", "The train is entering the station.", "people enter a tube train at station ."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-fb495ef61c7f4eb7a613e6e1878d7f6e", "references": ["building and clouds moving in the blue sky", "white clouds in a blue sky reflecting off buildings", "building and clouds against blue sky seen from a city"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-c18a25bd13f046e88b6ed82846c1c67c", "references": ["feeding giraffe at the zoo", "A boy feeding a giraffe at a zoo", "A woman feeds two giraffes at the zoo."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-86c85eff204448b79813a343a120531d", "references": ["cat lay on the grass", "cat lying in the grass", "cat lying on grass in a public park"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-9c8552f1ca3443eb8edf55d2401b6d47", "references": ["tourists spend time on a beach", "people spend time at the beach", "spending some time on the beach"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-87fdccda3ce94464ba5b247a32fb6d74", "references": ["cat eating corn on the cob", "child eating a cob of corn .", "A boy is trying to eat corn on the cob."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-48bbe2f039ef4fc6b135e03bbad36888", "references": ["sheeps eat hay on the farm .", "several sheep eating hay in a barn", "sheep in a pen with some hay to eat"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-bee286ae953d40f88fba90b8ca894049", "references": ["a student works on a computer", "student working at a computer", "students working on the computer ."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-ea0ac1126ac448aca10e4ea421791b98", "references": ["People wait for their luggage at an airport.", "Man waiting next to his luggage at an airport.", "Several men waiting on their luggage at the airport."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-adb1087d95b04bf59d4cec95e83d56a8", "references": ["the ship passes under the bridge", "with cargo ship passing under bridge", "A bridge rises to let two ships pass."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-a26bbc3780c947438b416b2f4c374f91", "references": ["a bride with a bouquet of flowers", "the bride chose glorious flowers for the bouquets", "a bouquet of yellow and orange flowers for the bride ."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-11f5f9292df94cffabe8c0d1450061a5", "references": ["the couple stands by the cake", "The couple is standing next to a cake.", "A couple stands behind their wedding cake."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-6aafe6ad2ae04b92b38d1624aef4ea5d", "references": ["cat staring at the window", "A cat staring out a window.", "The cat stares out of a four paned window."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-41bfa0cf63ba4958a2e8619ae39862fe", "references": ["nurse with a patient in the hospital", "nurse visiting her patient at the hospital", "nurses take care of a patient at a hospital ."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-40d402e314af41099679b8a9e98cf32d", "references": ["pug dog lying under a blanket", "A man lies on a blanket with his dog.", "The dog and her puppies lay on a blanket."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-f9b3232975f9477aa443c8270e8bad45", "references": ["A farmer plows his field.", "The farmer is plowing his field.", "A couple of farmers plowing a field"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-6e75a31cb48a4483bcf7f5479d93fdb9", "references": ["cat lying on a blanket", "Two cats lay together on a blanket. ", "cat lying in a blanket in the sunlight"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-b1f16d21e65e4a3099fe5ae55a614be3", "references": ["living room with painting on the wall", "A room that has a lot of paintings on the wall.", "there were numerous paintings on the walls in this room ."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-96273466e8d34ef897e848f9d2bd2c71", "references": ["The skier is headed down a slope.", "A group of skiers heading down the slopes", "Skiers are headed down a small slope by their cabin."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-a27754ef59c84dd5a2301257167524e2", "references": ["mother and daughter smiling in the winter", "mother and daughter embrace and smile on a beach", "A mother and daughter are smiling close together."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-bb3822c7a929497a9fa92cd52ed47edb", "references": ["gather your friends and family .", "friend and family gathered round the font", "friends and family gather often for a buffet ."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-c1a2dfe33dcb4a40abc29f4beaf34ad1", "references": ["A horse is pulling a plow.", "The horses are pulling the plow.", "women guide a plow pulled by draft horses"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-2901be24d22347499248435d4112b2f9", "references": ["people sharing a meal at a table", "a large dining table for guests to share meals", "The woman and children share a meal at the table."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-890fe5887faa4b488fc963223ee328b0", "references": ["cranes and buildings under construction", "large crane over a building under construction", "construction the buildings with the help of cranes"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-324b3b15b5a94931b2e8d71da8f355f4", "references": ["gold ribbon tied in a bow", "tie a basic bow with ribbon", "white gift box with red ribbon tied in a bow"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-36b4e3a3ae66461fae8e2231a5304a37", "references": ["family on the picnic in park", "A family has a picnic at the park.", "young family having picnic in a park"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-621f225eb0374534ace3f52bb7683229", "references": ["sunlight reflected in the water", "sunlight reflecting off the water", "rays of sunlight reflect off chilly waters"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-e75beadf738b45bba9aadebee945c590", "references": ["passengers sitting in a train", "passengers sit aboard a train at the station .", "A cable train with passengers sit on it tracks."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-368d6e3e98434b08ac7f0175b4c48669", "references": ["A small clock on a wall in a room.", "A clock rests along the wall of a room.", "There are two clocks on the wall in the room."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-a6addf14b78c464db25b4c3fb5002e47", "references": ["a dog chews a bone", "The dog is chewing a bone.", "Dogs lay down and chew bones"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-5725d6ea62dc44629aa966225a578bfd", "references": ["planes waiting at an airport", "a plane waits to depart from airport", "plane waiting on the apron of airport"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-3cc15ce0009d46569c7dd41465a96356", "references": ["A skier navigating his way down a slope.", "Skiers are making their way down a slope.", "a skier makes his way down the slopes on tuesday ."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-e5372913e9cd4fcd85f9a7cee73a30ab", "references": ["The skier descended down the slope.", "A skier slowly descending the slope.", "A skier bends his knees as he descends a slope."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-1a1643dca2ca47e0af8593e043b07178", "references": ["fresh salad with vegetables in a bowl", "A bowl full of salad and different vegetables", "A bowl of soup, bagel with hummus and fresh vegetable salad"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-7d083525a84b47069d634ff98be0a2ca", "references": ["kitten sleeping on a pillow in bed", "man in bed trying to sleep with a pillow", "a small baby sleeping on the bed with pillows"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-dd9b4a64ad18477fb782ff60ee6f87a8", "references": ["herd of sheep on a pasture .", "herd of sheep in the pasture", "A herd of sheep grazing in an open pasture."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-6f5c4dd65bea44a4865de1e3e014c0da", "references": ["A man is collecting shells on the beach.", "some sea shells collected along beaches .", "another great idea for the shells we collect at the beach"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-5f865189daf248a48c7fb5fb17caf14f", "references": ["travelers board a plane at the airport", "Plane being boarded by people in an airport", "Plane boarding passengers while at a fancy airport"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-0a6fcb3fbd5643daa50de45c24b66ec6", "references": ["passengers wait for their flights", "passengers wait to get on a flight", "passengers waiting for flight at the terminal ."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-6383cc759e464aa3bda19f3e6aa1bbb6", "references": ["leaves floating on the surface of still water", "He dives into the water and floats to the surface.", "a man floats at the surface with his face in the water ."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-308d70219ee14fc9b42e29f977a81ab2", "references": ["bench in the shade of an oak tree", "two benches near a tree under the shade", "A bench positioned among the tall trees for shade"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-8b56dceac27c43b0b99df2b1b21cfad9", "references": ["a grey horse tied to a post", "Two brown horses tied up at a post.", "The horse is tied up to the post outside."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-c98d9542ab1f41058d9d4b8b83d848a7", "references": ["A boy feeding pigeons at a park.", "A woman feeds pigeons in the park.", "flock of pigeons feeding in the grass of public park"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-4a6310a2eb9c499a9c51cebc18ecb89e", "references": ["cars moving along the tracks .", "A boy is moving toy cars on a toy car track.", "A trolley car moves down the tracks in a city."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-1fd2bf8ccabe41cfb780989b78406b60", "references": ["A skateboarder skating at a skate park.", "A group of skateboarders are skating at the park.", "A skateboarder skates next to a ledge in a skate park."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-79f6dbb71ec14ea6854b00888df5d493", "references": ["Two dogs lay on the carpet.", "boy and his dog lying on a carpet", "old sad grey and black dog lies on a carpet"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-bcd871a2ee094f88913e8aeff67a817f", "references": ["A live room with beige furniture and carpets", "A large living room has a lot of furniture and carpet.", "A man is vaccuming the carpet in a room with no furniture."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-a33181f5b531486585f1d8dc3f98b153", "references": ["a rabbit is eating a carrot", "a group of rabbits eat carrots", "bunny rabbit eating carrot in the garden"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-6a49572ca0b94803b4e4f78c9e01ea65", "references": ["boxers fighting in a ring", "boxers fight on the boxing ring", "A pair of kick boxers fighting inside of the ring."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-fbce212c0928485c9391f22f64724e36", "references": ["A skateboarder skating down a street.", "The skateboarder skated down the street.", "The skateboarders skate through the city streets."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-8668e01a369948e781c725229fdff77b", "references": ["Seagulls fly over a boat.", "a fishing boat with seagulls flying around", "There are lots of seagulls flying near a boat."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-26484f2859b04a01b377c2a162bb6ecd", "references": ["waiting for the rain to stop", "school girls wait for the rain to stop before continuing with class", "Five men wait in a tunnel for the rain to stop during their bike race"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-d8e4e10d102a448cb593b8aa42a90905", "references": ["bread baked in an old oven", "A woman is baking bread in the oven.", "Two loaves of bread baking in an oven."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-e46d3322989e45e595738a0fd9c70ae5", "references": ["a cheese and pepperoni pizza in an oven", "Cheese and herb pizza fresh from the oven.", "An oven filled with three pizzas covered in cheese."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-7e4cce34685245c39a9c12174823400d", "references": ["A room with stone walls and colorful carpet.", "A man is installing carpet along a wall in a room.", "A wildly decorated living room with a carpet on the wall!"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-2202bbe9f96d4fbab8ef982ad1ce0ca2", "references": ["skier on a slope against the cloudy sky", "A skier is headed down a slope on her skies.", "A jet in the sky and some skiers on snowy slopes"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-97559f2d070b41dc8d3f40d9a6b18c5b", "references": ["A dim light shines on the dark sky.", "flight through shining lights in the sky", "A bright white light shines in front of a cloudy sky."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-3cd7a86fa6864818ae89773d362024c0", "references": ["Three sheep walk on the hill.", "a herd of sheep walk on a grassy hill", "A dark sheep is walking over the hill."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-d9c884c30505436ba3ec405f08adde1d", "references": ["A plane flies over a boat.", "a large boat with a plane flying above", "a blue and red plane is flying by a boat"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-5573e442a68c46e889cdd826a62afa3a", "references": ["A bench on a hill overlooking a beach.", "A tree and some benches overlook the beach", "This is an image of a bench overlooking a beach."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-02d33f6c3fcb4c198b13a586aceae43c", "references": ["book standing on a shelf", "A cat stands on a shelf full of books.", "the book standing up in a white shelf ."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-753129bf99024b6489c7dde2a3504a9d", "references": ["The diver dives out into the water.", "We see the divers dive in the water.", "group of divers before diving into the water"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-58fc32f715a1489a8cb6020fd6d33370", "references": ["putting cheese on the pizza", "A woman is putting cheese on a pizza.", "A female child putting cheese on poorly made pizzas."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-07ff7def65a146c69d3459eb98fbb2c6", "references": ["A hand reaches and grabs him.", "He reaches up to grab someone's hand.", "hands of zombie reaching out to grab a gift box"], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
{"id": "task102-1c1168f3344c45c18a178f92327f6ecd", "references": ["A man sits in a canoe on a lake.", "canoes sit on the shore of a lake .", "Two men in two canoes are sitting in a lake."], "task_id": "task102_commongen_sentence_generation", "task_category": "Data to Text", "track": "default"}
